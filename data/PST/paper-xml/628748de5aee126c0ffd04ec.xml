<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph Enhanced BERT Model for Event Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
							<email>xding@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhangyue@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Xiong</surname></persName>
							<email>kxiong@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph Enhanced BERT Model for Event Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance. To address this issue, we consider automatically building of event graph using a BERT model. To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process. Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable. Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the semantics of events and their underlying connections is a long-standing task in natural language processing <ref type="bibr" target="#b17">(Minsky, 1974;</ref><ref type="bibr" target="#b21">Schank, 1975)</ref>. Much research has been done on extracting script knowledge from narrative texts, and making use of such knowledge for predicting a likely subsequent event given a set of context events.</p><p>A key issue to fulfilling such tasks is the modeling of event relation information. To this end, early work exploited event pair relations <ref type="bibr" target="#b1">(Chambers, 2008;</ref><ref type="bibr" target="#b9">Jans et al., 2012;</ref><ref type="bibr" target="#b7">Granroth and Clark, 2016)</ref> and temporal information <ref type="bibr" target="#b19">(Pichotta, 2016;</ref><ref type="bibr" target="#b20">Pichotta and Mooney, 2016)</ref>. The former has been used for event prediction by using embedding methods, where the similarity between subsequent events and context events are measured and used for candidate ranking. The latter has been used *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corresponding author</head><p>Figure <ref type="figure">1</ref>: (a) An example for event prediction. (b) Given an event sequence, retrieval-based methods lookup structural information of events from event graph. However, in the test process, part of events may be not covered by the event graph, hence their connection information is unavailable. Different from retrieval-based methods, GraphBERT is able to predict the connection strength between events.</p><p>for neural network methods, where models such as LSTMs have been used to model a chain of context events. There has also been work integrating the two methods <ref type="bibr" target="#b25">(Wang et al., 2017)</ref>.</p><p>Despite achieving certain effectiveness, the above methods do not fully model the underlying connection between context events. As shown in Figure <ref type="figure">1</ref> (a), given the facts that Jason had been overstretched at work, He decided to change job and Jason finds a new job, the subsequent event Jason is satisfied with his new job is more likely than Jason feels much stressed at his new job, which can be inferred by understanding the fact that the reason for his new job search is stress in his job. <ref type="bibr" target="#b13">Li et al. (2018b)</ref> and <ref type="bibr" target="#b11">Koncel et al. (2019)</ref> consider such context structure by building event evolutionary graphs, and using network embedding models to extract relational features. For these methods, event graphs serve as a source of external structured knowledge, which are extracted from narrative texts and provide prior features for event correlation.</p><p>One limitation of their methods is that the effectiveness of their methods heavily relies on the coverage of the event graph. As shown in Figure <ref type="figure">1</ref> (b), <ref type="bibr" target="#b13">Li et al. (2018b)</ref> and <ref type="bibr" target="#b11">Koncel et al. (2019)</ref>'s methods work by looking up the event tuples in the event graph to retrieve the connection information between events for predicting the output. This is done by the standard knowledge graph lookup operation. However, if the context events are not in the event graph, the method cannot find relevant information. Figure <ref type="figure">1</ref> (b) shows an extreme case. In event sequence β, although the context events be starving and go for a meal are highly similar to the event graph content feel hungry and go for lunch, the retrieval-based methods can fail to match context events in the event graph and utilize the event graph knowledge. However, in practice, it is infeasible to construct an event graph that covers most of the possible events. As an event is the composition of multiple arguments, so the same event can correspond to various semantically equivalent expressions, such as "feel hungry" vs "be starving", or "hunger", etc. This would limit the performance of the retrieval-based systems.</p><p>To address this issue, we consider automatically predicting the event links using a graph-enhanced BERT model (GraphBERT). As shown in <ref type="bibr">Figure 1 (b)</ref>, we collect event structure information into a BERT model with graph structure extension. Given a set of event contexts, we use the Graph-BERT model to construct an event graph structure by predicting connection strengths between context events, instead of retrieving them from a prebuilt event graph. Specifically, we extend the BERT model by introducing a structured variable, which captures the connection strengths between events. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, during training, both context events and external event graph information are used to train the structured variable. During testing, the structured variable which describes connection strengths between events is obtained using the context event only, which is used for finding the next event. Subsequently, we encode the predicted link strength for making a prediction.</p><p>Experimental results on standard datasets show that our model outperforms baseline methods. Further analysis demonstrates that GraphBERT can predict the connection strengths for unseen events and improve the prediction accuracy. The codes are publicly available at https://github.com/ sjcfr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As shown in Figure <ref type="figure">1</ref> (a), the task of event prediction <ref type="bibr" target="#b18">(Mostafazadeh et al., 2016;</ref><ref type="bibr" target="#b13">Li et al., 2018b)</ref> can be defined as choosing the most reasonable subsequent event for an existing event context. Formally, given a candidate event sequence X = {Xe 1 , . . . , Xe t , Xe c j }, where {Xe 1 , . . . , Xe t }are t context events and Xe c j is the c j th candidate subsequent event, the prediction model is required to predict a relatedness score Y ∈ [0, 1] for the candidate subsequent event given the event context.</p><p>Event graphs <ref type="bibr" target="#b13">(Li et al., 2018b)</ref> have been used to represent relationships between multiple events. Formally, an event graph could be denoted as G = {V, R}, where V is the node set, R is the edge set. Each node V i ∈ V corresponds to an event X i , while each edge R ij ∈ R denotes a directed edge V i → V j along with a weight W ij , which is calculated by:</p><formula xml:id="formula_0">Wij = count(Vi, Vj) k count(Vi, V k )<label>(1)</label></formula><p>where count(V i , V j ) denotes the frequency of a bigram (V i , V j ). Hence, the weight W ij is the probability that X j is the subsequent event of X i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline System</head><p>Before formally introducing the GraphBERT framework, we first introduce a retrieval-based baseline system. As Figure <ref type="figure" target="#fig_0">2</ref> (a) shows, given an event sequence X = {Xe 1 , . . . , Xe t , Xe c j }, the baseline system retrieves the corresponding structural information for each event within X from a prebuilt event graph G, and then integrates the retrieved structural information into the BERT frame for predicting the relatedness score Y . For an arbitrary event tuple (X e i , X e j ), if it is covered by the event graph G (i.e., both X e i and X e j are nodes of G), then we can retrieve the corresponding node embeddings e i and e j , together with the edge weight A ij by matching the event tuple in the event graph. The representation vector of the events within X further form into an embedding matrix E, and the edge weights form into an adjacency matrix A. To make use of the retrieved structural information for enhancing the prediction process, we first employ a graph neural network to combine the event representation matrix and the adjacency matrix: Given an event sequence, the baseline system retrieves event node features and connection strength from a prebulit event graph. (b) In addition to the baseline system, GraphBERT introduces an additional aggregator to obtain event representation from the hidden states of BERT, and learns to predict the connection strength between events in the training process using the inferer. So that in the test process, the connection information can be predicted for arbitrary event.</p><formula xml:id="formula_1">E (U ) = σ(AEW U )<label>(2)</label></formula><p>where WU ∈ R d×d is a weight matrix; σ is a sigmoid function; E (U ) is the event representation matrix updated by A.</p><p>Then the combined event graph knowledge can be merged into the frame of BERT for enhancing the prediction process. To this end, we employ an attention operation to softly select relevant information from the updated event representations E (U ) , and then update the hidden states of BERT. Specifically, we take the hidden states of the s 1 th Transformer layer of BERT (denoted as H s<ref type="foot" target="#foot_0">1</ref> ) as the query, and take the updated event representation E (U ) as the key:</p><formula xml:id="formula_2">E (U ) * = MultiAttn(H s 1 , E (U ) )<label>(3)</label></formula><p>where E (U ) * carries information selected from E (U ) and relevant to H s 1 . Then we merge E (U ) * with H s 1 through an addition operation, and employ layer normalization to keep gradient stability:</p><formula xml:id="formula_3">H s 1 * = LayerNorm(E (U ) * + H s 1 )<label>(4)</label></formula><p>H s 1 * contains both the node feature information and the connection information between events. By taking H s 1 * as the input of the subsequent (s 1 + 1)th Transformer layers of BERT, the event prediction process is enhanced with the predicted event graph knowledge.</p><p>This retrieval-based baseline system can be regarded as the adaption of <ref type="bibr" target="#b13">Li et al. (2018b)</ref> and <ref type="bibr" target="#b11">Koncel et al. (2019)</ref>'s retrieval-based methods on a pretrained model BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GraphBERT</head><p>A critical weakness of the retrieval-based baseline system is that it heavily relies on the coverage of the event graph. In other words, if an event is not covered by the event graph, then the structural information (i.e., node features and the adjacency matrix) would be absent from the constructed event graph, which further limits the model performance.</p><p>In this paper, we propose a predictive-based framework GraphBERT. GraphBERT uses the transformer layers of BERT as an encoder to obtain the representation for arbitrary events, and then learns to predict the link strength between events in the training process, so that the sparsity issues in the retrieval process can be avoided.</p><p>To this end, as Figure <ref type="figure" target="#fig_0">2</ref> (b) shows, in contrast to the retrieval-based baseline system, we introduce two more modules: (1) An aggregator to obtain event representations from the BERT framework;</p><p>(2) an inferer to predict the link strength between events based on the event representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Event Encoding</head><p>Given an event sequence X, to calculate the event representations and predict the link strength for events within X, GraphBERT first encodes X into a set of token-level distributed representations by taking the 1st-s 0 th Transformer layers of BERT as an encoder. Then an aggregator is employed to aggregate the token level representations into event representations. Token Level Representations For an event sequence X = {X1, • • • , Xt+1}, where Xi = {x1, . . . , x l i }is an event within X and with l i tokens, the s 0 th Transformer layer of BERT encodes these tokens into contextualized distributed representations H s 0 = {(h 1 where h i j ∈ R 1×d is the distributed representation of the jth token of event X i . Then we conduct the graph information prediction as well as the predic-tion task based on the token representations. Event Level Representations An aggregator module aggregates tokens representation of events derived from the hidden states of BERT (i.e., H s 0 ) to obtain the event level representations. For an arbitrary event X i ∈ X, we employ a multi-head attention operation <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> to aggregate information from the corresponding token representations H s 0 i = (h i 1 , . . . , h i l i ) and obtain the vector representation of X i . Specifically, we define the query matrix of attention operation as</p><formula xml:id="formula_4">q i = 1 l i h i</formula><p>l , and take H s 0 i as the key matrix as well as the value matrix. Then the representation of X i is calculated as:</p><formula xml:id="formula_5">êi = MultiAttn(q i , H s 0 i , H s 0 i )<label>(5)</label></formula><p>where êi ∈ R 1×d .</p><p>In this way, we can obtain the representation of all events within X, which we denote as Ê = {ê1, • • • , êt+1}, where Ê ∈ R (t+1)×d is a matrix. Note that through the embedding layer of BERT, position information has been injected into the token representations. Thus Ê carries event order information.</p><p>Then the event representation matrix Ê is used for predicting the link strength between events. Hence, the performance of link strength prediction can be strongly influenced by the quality of Ê. By deriving Ê from the hidden states of BERT, the abundant language knowledge within BERT can be utilized to obtain the event representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Strength Prediction</head><p>Given the event representation matrix Ê as node features, we employ an inferer module to predict the connection strength between arbitrary events within X, regardless of whether these events are seen in the training process. The output is a matrix t+1) , where Âij models the probability that event j is the subsequent event of event i.</p><formula xml:id="formula_6">Â ∈ R (t+1)×(</formula><p>We stack n graph attention (GAT) layers <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref> for consolidating event features. For an event X i , the GAT layer works on the neighborhood of X i to aggregate information. Since the connection between events are unknown a priori, we set the neighborhood set of event X i as N i = {X j }, where X j ∈ X, j = i.</p><p>Therefore, at the kth graph attention layer, given the representation of the ith event êk i , we calculate the attention coefficients between other events and derive deep event representation as:</p><formula xml:id="formula_7">αij = softmaxj,j∈N i (Relu(u[Wαê k i ||Wαê k j ])) êk+1 i = σ( j∈N i αijWαê k j ) (6)</formula><p>where u ∈ R 1×2d , Wα ∈ R d×d are trainable parameters, •||•is a concatenation operation. At the first GAT layer, ê1 i is initialized by êiderived from the aggregator.</p><p>After n graph attention operations, we employ a bilinear map to calculate a relation strength score between two events within X based on their deep representations:</p><formula xml:id="formula_8">Γ ij = ên i W R T(ê n j )<label>(7)</label></formula><p>where W R ∈ R d×d are learnable parameters, T (•) is the transpose operation. For all t + 1 events within X, the relation strength score between arbitrary two events forms a matrix Γ ∈ R (t+1)×(t+1) , with each element Γ ij measuring the relation strength between X i and X j .</p><p>Then we normalize the relation strength scores using the softmax function:</p><formula xml:id="formula_9">Âij = softmax j (Γ ij )<label>(8)</label></formula><p>After the layer normalization, j Âij = 1.</p><p>Hence, with the aggregator and the inferer, GraphBERT can obtain representation and connection strengths for arbitrary events, regardless of whether or not the event is covered by the event graph. Then the predicted adjacency matrix Â and event representations Ê can be used for prediction, and the process is same as the retrieval-based baseline, as described in Eq.(2)-Eq.(4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training of Inferer</head><p>In the training process, we employ a tutor module to supervise the prediction of Â using the structural information from a prebuilt event graph. Given an event sequence X, the tutor obtains an adjacency matrix A based on the edge weights of the event graph. Formally, the weights of A are initialized as:</p><formula xml:id="formula_10">Aij = Wij, if V i → V j ∈ R, 0, others.<label>(9)</label></formula><p>where V i , V j are nodes in the event graph corresponding to the ith and the jth event of the candidate event sequence. The same as the predicted event adjacency matrix Â, A is also a</p><formula xml:id="formula_11">R (t+1)×(t+1) matrix.</formula><p>We scale A to make each row sum equals 1. Therefore, each element of A models the probability that the jth event is the subsequent event of the ith event in X. In the training process, through minimizing the distance between Â and A, the inferer module is supervised by the tutor to learn to predict the event connection strength based on the event representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization</head><p>The overall loss function is defined as:</p><formula xml:id="formula_12">L = LEvent Prediction + λLGraph Reconstruction (10)</formula><p>where L Event Prediction is a cross-entropy loss measuring the difference between predicted relatedness score Y and golden label, L Graph Reconstruction assess the difference between A and Â, λ is an additional hyperparameter for balancing the prediction loss with graph reconstruction loss.</p><p>For calculating L Graph Reconstruction , we cast both A and Â as a set of random variables, and employ the KL divergence to measure their difference:</p><formula xml:id="formula_13">LGraph Reconstruction = i KL(MultiNomial( Âi)||MultiNomial(Ai)) (11)</formula><p>where i denotes the ith row, and MultiNomial(•) denotes the multinomial distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our approach on two event prediction tasks: Multiple Choice Narrative Cloze Task (MCNC) <ref type="bibr" target="#b7">(Granroth and Clark, 2016)</ref> and Story Cloze Test (SCT) <ref type="bibr" target="#b18">(Mostafazadeh et al., 2016)</ref> by constructing an event graph based on the training set of MCNC to train the GraphBERT model and then adapts the GraphBERT model trained on the MCNC dataset to the SCT dataset to evaluate whether GraphBERT can predict the link strength between unseen events to enhance the prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>Multiple Choice Narrative Cloze Task The MCNC task requires the prediction model to choose the most reasonable subsequent event from five candidate events given an event context <ref type="bibr" target="#b7">(Granroth and Clark, 2016)</ref>. In this task, each event is abstracted to Predicate-GR form <ref type="bibr" target="#b7">(Granroth and Clark, 2016)</ref>, which represents an event in a structure of {subject, predicate, object, prepositional object}. Following <ref type="bibr" target="#b7">Granroth and Clark (2016)</ref>, we extract event chains from the New York Times portion of the Gigaword corpus. The detailed statistics of the dataset are shown in Table <ref type="table" target="#tab_0">1</ref>. Story Cloze Test Task The SCT task requires models to select the correct ending from two candidates given a story context. Compared with MCNC which focuses on abstract events, the stories in SCT are concrete events and with much more details. This dataset contains a five-sentence story training set with 98,162 instances, and 1,871 foursentence story contexts along with a right ending and a wrong ending in the dev. and test dataset, respectively. Because of the absence of wrong ending in the training set, we only use the development and the test dataset, and split the development set into 1,771 instances for finetuning models and 100 instances for the development purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Construction of Event Graph</head><p>The event graph is constructed based on the training set of the MCNC dataset. Each event within the training set of MCNC is taken as a node of the event graph, and the edge weights are obtained by calculating the event bigram frequency. Note that, as shown in Table <ref type="table" target="#tab_0">1</ref>, although the events have been processed into a highly abstracted form to alleviate the sparsity, there are still nearly half of the events in the development and test set of MCNC remains uncovered by the event graph. In the test process, for retrieval-based methods, given a candidate event sequence with length t + 1, the edge weights for events not covered by the event graph are all set as 1/(t + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>We implement the GraphBERT model using pretrained BERT-base model, which contains 12 Transformer layers. We aggregate the token representations from the 7th Transformer layer of BERT, and merge the updated event representations to the 10th Transformer layer of BERT. The aggregator has a dimension of 768, and contains 12 attention heads. The inferer contains 1 GAT layer. The balance coefficient λ equals 0.01. During the training and testing process, we concatenate the elements of the Predicate-GRs to turn the Predicate-GRs into strings, so that the event sequences can conform to the input format of the GraphBERT model. More details are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines for MCNC</head><p>Event Pair and Event Chain Based Methods (i) Event-Comp <ref type="bibr" target="#b7">(Granroth and Clark, 2016)</ref> calculates the pair-wise event relatedness score using a Siamese network. (ii) PairLSTM <ref type="bibr" target="#b25">(Wang et al., 2017)</ref> integrates event order information and pairwise event relations to predict the ending event.</p><p>(ii) RoBERTa-RF <ref type="bibr" target="#b16">(Lv et al., 2020)</ref> enhances pretrained language model RoBERTa with chain-wise event relation knowledge for making prediction. Event Graph Based Methods (i) SGNN <ref type="bibr" target="#b13">(Li et al., 2018b</ref>) constructs a narrative event evolutionary graph (NEEG) to describe event connections, and propose a scaled graph neural network to predict the ending event based on structural information retrieved from the NEEG. (ii) Het-erEvent <ref type="bibr" target="#b28">(Zheng et al., 2020)</ref> encodes events using BERT, and implicitly models the word-event relationship by an heterogeneous graph attention mechanism. (iii) GraphTransformer <ref type="bibr" target="#b11">(Koncel et al., 2019)</ref> retrieves structural information from event graph and introduces an additional graph encoder upon BERT to leverage the structural information. Pretrained Language Model Based Methods (i) BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> refers to the BERT-base model finetuned on the MCNC dataset.</p><p>(ii) GraphBERT λ=0 refers the GraphBERT model optimized with the balance coefficient λ set as 0. Hence, the structural information cannot be incorporated through the graph reconstruction term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Settings for SCT</head><p>To test the generality of GraphBERT, we examine whether GraphBERT can utilize the structural knowledge learned from MCNC-based event graph to guide the SCT task. To make fair comparisons, we also trained the BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, GraphTransformer <ref type="bibr" target="#b11">(Koncel et al., 2019)</ref> on the MCNC dataset, then finetuned them on the SCT dataset. In the following sections, we use the subscript "MCNC" to denote the model which has been trained on the MCNC dataset.</p><p>However, in the finetuning and test process, GraphTransformer still relies on an event graph to provide structural information. To address this issue, we abstract each event in the finetuning set and test set of SCT into the Predicate-GR form, which is the same form with the nodes in the MCNCbased event graph. As a result, structural information for an event in SCT can be retrieved from the MCNC-based event graph using its corresponding Predicate-GR form, once the event is covered by the event graph.</p><p>In addition to the above-mentioned methods, on the SCT dataset, we also compare GraphBERT with the following event-chain-based baselines:</p><p>(i) HCM <ref type="bibr" target="#b2">(Chaturvedi et al., 2017)</ref> trains a logistic regression model based on contextual semantic features. (ii) ISCK <ref type="bibr" target="#b3">(Chen, 2019)</ref> integrates narrative sequence and sentimental evolution information to predict the story ending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Overall Results</head><p>We list the results on MCNC and SCT in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>, respectively. From the results on MCNC (Table <ref type="table" target="#tab_1">2</ref>), we can observe that:</p><p>(1) Compared to event-pair-based EventComp and event-chain-based PairLSTM, event-graphbased methods (i.e. SGNN, HeterEvent, Graph-Transformer, and GraphBERT) show better performance. In addition, GraphBERT outperforms event-chain based RoBERTa-RF, though RoBERTa-RF is built upon a much more powerful language model. This confirms that involving event structural information could be effective for this task.</p><p>(2) Compared to BERT and GraphBERT λ=0 , graph enhanced models GraphTransformer and GraphBERT further improve the accuracy of script event prediction (T-test; P-Value &lt; 0.01). This shows that linguistic and structural knowledge can have a complementary effect.</p><p>(3) Compared to the retrieval-based method GraphTransformer, GraphBERT shows efficiency of learning structural information from the event graph (T-test; P-Value &lt; 0.01). This indicates that GraphBERT is able to learn the structural information from the event graph in the training process, and predict the correct structural information for unseen events in the test process.</p><p>Results on the SCT dataset (Table <ref type="table" target="#tab_2">3</ref>) show that:</p><p>(1) Comparing GraphBERT with BERT MCNC , GraphBERT λ=0,MCNC shows that the graph information can also be helpful for the SCT task.</p><p>(2) Though incorporated graph information, the performance of GraphTransformer is close or inferior to BERT on SCT. This could be because of the limited size of the SCT development set, which contains 1,771 samples and might be insufficient to adapt GraphTransformer to the SCT problem. However, GraphBERT shows a 1.3% absolute improvement over BERT, which indicates the efficiency of GraphBERT in predicting the link strength between unseen events for predicting the ending event.</p><p>Methods Accuracy(%) Random 20.00** EventComp <ref type="bibr" target="#b7">(Granroth and Clark, 2016)</ref> 49.57** PairLSTM <ref type="bibr" target="#b25">(Wang et al., 2017)</ref> 50.83** SGNN <ref type="bibr" target="#b13">(Li et al., 2018b)</ref> 52.45** BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> 57.35** GraphTransformer <ref type="bibr" target="#b11">(Koncel et al., 2019)</ref> 58.53** HeterEvent <ref type="bibr" target="#b28">(Zheng et al., 2020)</ref> 58.10** GraphBERT λ=0 57.23** RoBERTa-RF <ref type="bibr" target="#b16">(Lv et al., 2020)</ref> 58.66** GraphBERT 60.72 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy(%) HCM <ref type="bibr" target="#b2">(Chaturvedi et al., 2017)</ref> 77.6** ISCK <ref type="bibr" target="#b3">(Chen, 2019)</ref> 87.6** BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> 88.1* BERTMCNC 88.5* GraphTransformer MCNC <ref type="bibr" target="#b11">(Koncel et al., 2019)</ref> 88.9 HeterEventMCNC <ref type="bibr" target="#b28">(Zheng et al., 2020)</ref> 88.4* GraphBERT λ=0,MCNC 88.3* GraphBERT MCNC 89.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Influence of the Accuracy of the Predicted Link Strength</head><p>We investigate the relationship between the accuracy of the predicted link strengths with the model performance. However, for events in the test set, the golden event graph is unavailable. To address this issue, we split the original training set of MCNC into a new training and evaluating set, containing 120,331 and 20,000 instances, respectively. For each sample, we calculate the Pearson correlation coefficient between the predicted connection strengths and connection strengths derived from the event graph, as well as the relationship between such correlation coefficient and model performance. The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. We observe that, in general, GraphBERT can predict the connection between arbitrary events with reasonable accuracy. Also, the model performance improves as the connection prediction accuracy increases. This confirms that correctly predicting the event connections for unseen events can be helpful for the event prediction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Influence of the Coverage of the Event Graph</head><p>We conduct experiments to investigate the specific influence of the sparsity of the event graph on model performance. Based on the original test set of MCNC, we build new test sets with different proportions of uncovered events, and compare the  performances of the GraphBERT framework with retrieval-based method GraphTransformer (Koncel et al., 2019) on these test sets. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, as the proportion of uncovered events increase from 0 to 1, the performance of GraphTransformer shows a negative trend in general. This is because, for retrieval-based methods, with the increase of sparsity, the availability of structural information decreases. Compared to GraphTransformer, the performance of GraphBERT is more stable. These results indicate that predicting the structural information can be useful for enhancing the performance of event prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>Table <ref type="table">4</ref> provides an example of prediction results from different models on the test set of SCT. The event context describes a story that a bear appeared in the campus and policemen came to tranquilize the bear. Given the event context, GraphBERT is able to choose correct ending E 1 The bear fell asleep, while GraphTransformer chooses the incorrect ending E 2 The bear became very violent.</p><p>To correctly predict the story ending, a model should understand the relationship between gave a tranquilizer and fell asleep. However, event gave a tranquilizer is not covered by the event graph. Hence, the retrieval-based method Graph-Transformer is unable to obtain structural information from the event graph. On the other hand, in the event graph, there is a directed edge from a node obj. sedated to node subj. slept. This indicates that, Event Context Candidate Subsequent Event Model A: I heard that my school's campus had been closed. B: The message said there was a bear on the grounds ! C: The police had to come and help get the bear away. D: They gave the bear a tranquilizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E1:</head><p>The bear fell asleep. ( √ ) GraphBERT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E2:</head><p>The bear became very violent. (×) GraphTransformer Table <ref type="table">4</ref>: An example of event predictions made by GraphTransformer and GraphBERT on the SCT dataset.</p><p>GraphBERT can learn the structural knowledge from the MCNC-based event graph, and predict the connection between gave a tranquilizer and fell asleep for instances in the SCT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The GraphBERT model employs a structure variable Â to capture the "is next event" relationship between events. By introducing more parallel structural variables { Â1 , . . . , Âk }, it can be extended to simultaneously learn multiple kinds of event relationships, such as temporal or causal relationship. Furthermore, previous researches demonstrate that the graph-structured relationship extensively exist between other semantic units, such as sentences <ref type="bibr" target="#b26">(Yasunaga et al., 2017)</ref>, or even paragraphs (Sonawane and Kulkarni, 2014). However, similar to the situation in event graph, it would be impractical to construct knowledge graphs that cover all possible connection relationships between all the sentences or paragraphs. This restricts the applicable of retrieval-based methods in these situations. On the contrary, our generative approach suggests a potential solution by learning the connection relationship from graph-structured knowledge base with limited size, then generalizing to the unseen cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The investigation of scripts dates back to 1970's <ref type="bibr" target="#b17">(Minsky, 1974;</ref><ref type="bibr" target="#b21">Schank, 1975)</ref>. The script event prediction task models the relationships between abstract events. Previous studies propose to model the pair-wise relationship <ref type="bibr" target="#b1">(Chambers, 2008;</ref><ref type="bibr" target="#b9">Jans et al., 2012;</ref><ref type="bibr" target="#b7">Granroth and Clark, 2016)</ref> or event order information <ref type="bibr" target="#b20">(Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b19">Pichotta, 2016;</ref><ref type="bibr" target="#b25">Wang et al., 2017)</ref> for predicting the subsequent event. <ref type="bibr" target="#b13">Li et al. (2018b)</ref> and <ref type="bibr" target="#b15">Lv et al. (2019)</ref> propose to leverage the rich connection between events using graph neural network and attention mechanism, respectively. Different from script event prediction, the story cloze task <ref type="bibr" target="#b18">(Mostafazadeh et al., 2016)</ref> focuses on concrete events. Therefore, it requires prediction models to learn commonsense knowledge for un-derstanding the story plot and predicting the ending. To this end, <ref type="bibr" target="#b12">Li et al. (2018a)</ref> and <ref type="bibr" target="#b8">Guan (2019)</ref> propose to combine context clues with external knowledge such as KGs. <ref type="bibr" target="#b14">Li et al. (2019)</ref> finetune pretrained language models to solve the task. Compared to their works, our approach can use both the language knowledge enriched in BERT to promote the comprehension of event context, and the structural information from event graph to enhance the modeling of event connections.</p><p>A recent line of work has been engaged in combining the strength of Transformer based models with graph structured data. To integrate KG with language representation model BERT, <ref type="bibr" target="#b27">Zhang et al. (2019)</ref> encode KG with a graph embedding algorithm TransE <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref>, and takes the representation of entities in KG as input of their model. However, this line of work only linearizes KGs to adapt the input of BERT. Graph structure is not substantially integrated with BERT. <ref type="bibr" target="#b8">Guan (2019)</ref> and <ref type="bibr" target="#b11">Koncel et al. (2019)</ref> propose retrievalbased methods to leverage the structural information of KG. However, in the event prediction task, the diversity of event expression challenges the coverage of the event graph, and prevents us from simply retrieving events in the test instances from the event graph. We propose to integrate the graph structural information with BERT through a predictive method. Compared to retrieval-based methods, our approach is able to learn the structural information of the event graph and generate the structural information of events to avoid the unavailable of structural information in test instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We devised a graph knowledge enhanced BERT model for the event prediction task. In addition to the BERT structure, GraphBERT introduces a structured variable to learn structural information from the event graph, and model the relationship between the event context and the candidate subsequent event. Compared to retrieval-based methods, GraphBERT is able to predict the link strength between all events, thus avoiding the (inevitable) sparsity of event graph. Experimental results on MCNC and SCT task show that GraphBERT can improve the event prediction performances compared to state-of-the-art baseline methods. In addition, GraphBERT could also be adapted to other graph-structured data, such as knowledge graphs.</p><p>10 Experimental Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Training Details</head><p>To conform to the input format of BERT, for an event described in the Predicate-GR form {subject, predicate, object, prepositional object}, we first concatenate each element within the predicate-GR into a string "subject predicate object prepositional object", so that an event described in a structured form is turned into a string. Then for satisfying the requirement of BERT, the candidate event sequence is further preprocessed into the form of:</p><formula xml:id="formula_14">[CLS] e1 [SEP] . . . et [SEP] candidate [SEP]<label>(12)</label></formula><p>On the MCNC dataset, the GraphBERT model is trained for 3 epochs, with a batch size of 64, and a learning rate of 2e-5. While during the finetuning process on SCT, GraphBERT is optimized with a batch size of 16, and a learning rate of 1e-5, with 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Searching for the Balance Coefficient</head><p>In this paper, the objective function is composed of two components. Through minimizing the graph reconstruction loss, model learns to modeling the bigram event adjacency patterns. While through minimizing the prediction loss, model is trained to choose the correct ending given an event context. These two components are balanced with a coefficient λ.</p><p>To investigate the effect of the balance coefficient, we compare the prediction accuracy of the GraphBERT model trained with different λ and show the results in Figure <ref type="figure" target="#fig_3">5</ref>. From which we could observe that, the prediction accuracy increases as the balance coefficient increase from 0 to 0.1. This is because the additional event graph structure information is helpful for the event prediction task. However, as the λ exceeds 0.5, the model performances start to decrease. This is because the overemphasis of graph reconstruction loss would in turn decrease the model performance.</p><p>(4, 10) (5, 10) (6, 10) (7, 10) <ref type="bibr">(8,</ref><ref type="bibr">10)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Searching of Start and Merge Layer in BERT</head><p>Different transformer layers of BERT tend to concentrate on different semantic and syntactic information <ref type="bibr" target="#b4">(Clark et al., 2019;</ref><ref type="bibr" target="#b5">Coenen et al., 2019)</ref>. Therefore, which layer is selected in the BERT to start integrating event graph knowledge, and which layer is selected to merge graph enhanced event representations can affect the performance of the model. We study such effect in two ways: first, we fix the start layer and change the merge layer. Second, we fix the gap between start and merge layer, and change the start layer. Results are shown in Table <ref type="table" target="#tab_3">5</ref>. The tuple (n 1 , n 2 ) denotes the (start, merge) layer. From which we could observe that, under the same gap between merge and start layer, employing the 7th transformer layer of BERT as the start layer can achieve the best result. While setting the merge-start gap as 2 is more efficient than other choices. Interestingly, <ref type="bibr" target="#b10">Jawahar et al. (2019)</ref> find that the syntactic features can be well captured in the middle layers of BERT, especially in the 7-9 layer. This indicates that the middle layers of BERT focus more on sentence level information, and implicitly support the reasonableness that choosing the 7th and 10th transformer layer of BERT as the start end merge layer.</p><p>11 Enhancing Different Kinds of Pretrained Transformer-based Pretrained Language Models with Event Graph Knowledge</p><p>In this paper, we propose the GraphBERT framework, which enhances the transformer-based pertrained language model BERT with event graph knowledge through an additional structural variable Â. We argue that, using the structural variable, we can also equip other transformer-based pretrained language models, such as RoBERTa, with the event graph knowledge, and then enhance the event prediction process. This could be achieved by adapt the aggregator, inferer and merger module upon the other transformer-based frameworks.</p><p>Using the above-mentioned manner, we implemented a GraphRoBERTa model and examined its performance on the MCNC dataset. The results are shown in Table <ref type="table" target="#tab_4">6</ref>. We observe that, compared with BERT, RoBERTa and GraphRoBERTa show better performance. This is because, during the pretraining process, RoBERTa can acquire more abundant linguistic knowledge for understanding the events through the dynamic masked token prediction mechanism. Moreover, the comparison between GraphBERT with BERT, and between GraphRoBERTa with RoBERTa show the effectiveness of our approach in incorporating event graph knowledge with multiple prevailing transformerbased pretrained language models, to consistently enhancing the performance of event prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model Structure. (a) Architecture of the baseline system.Given an event sequence, the baseline system retrieves event node features and connection strength from a prebulit event graph. (b) In addition to the baseline system, GraphBERT introduces an additional aggregator to obtain event representation from the hidden states of BERT, and learns to predict the connection strength between events in the training process using the inferer. So that in the test process, the connection information can be predicted for arbitrary event.</figDesc><graphic url="image-2.png" coords="3,93.55,70.87,408.19,121.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The distribution of Pearson correlation coefficients between the predicted connection strength and connection strength derived from the event graph. (b) Relationship between correlation coefficient and model performance.</figDesc><graphic url="image-3.png" coords="7,322.51,70.87,185.52,85.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance of GraphBERT and GraphTransformer under different proportion of uncovered events.</figDesc><graphic url="image-4.png" coords="7,338.88,207.00,152.78,92.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of model trained with different balance coefficient λ.</figDesc><graphic url="image-5.png" coords="10,327.97,70.87,174.62,96.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the MCNC dataset.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Dev.</cell><cell>Test</cell></row><row><cell>#Documents</cell><cell>830,643</cell><cell cols="2">103,583 103,805</cell></row><row><cell>#Event Chains</cell><cell>140,331</cell><cell>10,000</cell><cell>10,000</cell></row><row><cell>#Unique Events</cell><cell>430,516</cell><cell>44,581</cell><cell>47,252</cell></row><row><cell>#Uncovered Events</cell><cell>0</cell><cell>24,358</cell><cell>24,081</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of GraphBERT and baseline methods on the test set of MCNC. Accuracy marked with * means p-value &lt; 0.05 and ** indicates p-value &lt; 0.01 in T-test.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Model performance on the test set of SCT. Accuracy marked with * means p-value &lt; 0.05 and ** indicates p-value &lt; 0.01 in T-test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Influence of start layer and merge layer on model performance.</figDesc><table><row><cell>(9, 10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Performance of the event graph knowledge enhanced RoBERTa model (Graph-RoBERTa) on the MCNC dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">, . . . , h 1 l 1 ), • • • , (h t+1 1 , . . . , h t+1 l t+1 )},</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the New Generation Artificial Intelligence of China (2020AAA0106501), and the National Natural Science Foundation of China (62176079, 61976073).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Translating embeddings for modeling multirelational data. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics-08: HLT</title>
				<meeting>the Association for Computational Linguistics-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Story comprehension for predicting what happens next</title>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1603" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating structured commonsense knowledge in story completion</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI on Artificial Intelligence</title>
				<meeting>the AAAI on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6244" to="6251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What does bert look at? an analysis of berts attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing and measuring the geometry of bert</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
				<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8594" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Meeting of the North American Chapter</title>
				<meeting>the 2019 Annual Meeting of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What happens next? event prediction using a compositional neural network model</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Granroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Story ending generation with incremental encoding and commonsense knowledge</title>
		<author>
			<persName><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6473" to="6480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><forename type="middle">Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter</title>
				<meeting>the 13th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Unicomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Iñiguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Márton</forename><surname>Karsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Léo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Márton</forename><surname>Karsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Sarraute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Fleury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Meeting of the North American Chapter</title>
				<meeting>the 2019 Annual Meeting of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-attention based neural network with external knowledge for story ending predicting task</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Mao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="1754" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructing narrative event evolutionary graph for script event prediction</title>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018b. 2018</date>
			<biblScope unit="page" from="4201" to="4207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07504</idno>
		<title level="m">Story ending prediction by transferable bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sam-net: Integrating event-level and chain-level attentions to predict what happens next</title>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanhui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longtao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6802" to="6809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integrating external event knowledge for script learning</title>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
		<title level="m">A framework for representing knowledge</title>
				<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Annual Meeting of the North American Chapter of the</title>
				<meeting>the 2016 Annual Meeting of the North American Chapter of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using sentence-level lstm language models for script inference</title>
		<author>
			<persName><surname>Pichotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical script learning with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Battles in Language Processing: Scaling Early Achievements to Robust Methods</title>
				<meeting>the Workshop on Battles in Language Processing: Scaling Early Achievements to Robust Methods</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scripts, plans, and knowledge</title>
		<author>
			<persName><surname>Schank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international joint conference on Artificial intelligence-Volume</title>
				<meeting>the 4th international joint conference on Artificial intelligence-Volume</meeting>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph based representation and analysis of text document: A survey of techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sheetal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><forename type="middle">A</forename><surname>Sonawane</surname></persName>
		</author>
		<author>
			<persName><surname>Kulkarni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>International Journal of Computer Applications</publisher>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating order information and event relation for script event prediction</title>
		<author>
			<persName><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nbd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
				<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. CoNLL 2017</date>
			<biblScope unit="page" from="452" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Ernie: Enhanced language representation with informative entities</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks to predict what happen next</title>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
