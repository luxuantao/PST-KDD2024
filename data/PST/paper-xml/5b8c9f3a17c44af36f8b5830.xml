<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Selective Overview of Sparse Principal Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hui</forename><surname>Zou</surname></persName>
							<idno type="ORCID">0000-0003-4798-9904</idno>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lingzhou</forename><surname>Xue</surname></persName>
							<email>lzxue@psu.edu</email>
						</author>
						<author>
							<persName><forename type="first">I</forename><surname>P R I N C I Pa L C O M P O N E N T A N</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Ly</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<postCode>55455</postCode>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<postCode>16801</postCode>
									<settlement>State College</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Selective Overview of Sparse Principal Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AAE14FC20BFB769CA31C2811C08FC70A</idno>
					<idno type="DOI">10.1109/JPROC.2018.2846588</idno>
					<note type="submission">received January 30, 2018; revised May 28, 2018; accepted June 8, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Covariance matrices</term>
					<term>mathematical programming</term>
					<term>principal component analysis (PCA)</term>
					<term>statistical learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides a selective overview of methodological and theoretical developments of sparse PCA that produce principal components that are sparse, i.e., have only a few nonzero entries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the literature, which will be reviewed in Section II. Their formulations are different but related, because the regular PCA has several equivalent definitions from different viewing angles. These definitions are equivalent without sparsity constraints, and differ with sparsity constraints. To be self-contained, we briefly discuss the several views of PCA in the following.</p><p>From a dimension reduction perspective, PCA can be described as a set of orthogonal linear transformations of the original variables such that the transformed variables maintain the information contained in the original variables as much as possible. Specifically, let X be an n × p data matrix, where n and p are the number of observations and the number of variables, respectively. For ease of presentation, assume the column means of X are all 0. The first principal component is defined as Z1 = È p j=1 α1j Xj where α1 = (α11, . . . , α1p) T is chosen to maximize the variance of Z1, i.e., α1 = arg max α α T Σα subject to α1 = 1</p><p>(1)</p><p>with Σ = (X T X)/n. The rest principal components can be defined sequentially as follows:</p><formula xml:id="formula_0">α k+1 = arg max α α T Σα<label>(2)</label></formula><p>subject to α = 1 and α T α l = 0, ∀1 ≤ l ≤ k.</p><p>(</p><formula xml:id="formula_1">)<label>3</label></formula><p>This definition implies that the first K loading vectors are the first K eigenvectors of Σ.</p><p>The eigendecomposition formulation of PCA also relates PCA to the singular value decomposition (SVD) of X. Let the SVD of X be where D is a diagonal matrix with diagonal elements d1, . . . , dp in a descending order, and U and V are n × p and p × p orthonormal matrices, respectively. Because the columns of V are the eigenvectors of Σ, V is the loading matrix of the principal components. By XV = U D, we see that Z k = U k d k where U k is the kth column of U . Note that SVD can be interpreted as the best low-rank approximation to the data matrix.</p><p>PCA has another geometric interpretation, as the closest linear manifold approximation of the observed data. This definition actually matches the construction of PCA considered in <ref type="bibr" target="#b45">[45]</ref>. Let xi be the ith row of X. Consider the first k principal components jointly</p><formula xml:id="formula_2">V k = [V1| • • • |V k ].</formula><p>By definition, V k is a p × k orthonormal matrix. Project each observation to the linear space spanned by {V1, . . . , V k }. The projection operator is</p><formula xml:id="formula_3">P k = V k V T</formula><p>k and the projected data are P k Xi, 1 ≤ i ≤ n. One way to define the best projection is by minimizing the total 2 approximation error</p><formula xml:id="formula_4">min V k n i=1 xi -V k V T k xi 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>It is easy to show that the solution is exactly the first k principal components. In applications, variables can have different scales and units. Practitioners often standardize each variable such that its marginal sample variance is one. When this practice is applied to PCA, the resulting covariance matrix of standardized variables is the sample correlation matrix of the raw variables. Note that the eigenvalues and eigenvectors of the correlation matrix can be different from those of the covariance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. M E T H O D S F O R S PA R S E P R I N C I PA L C O M P O N E N T S</head><p>Each principal component is a linear combination of all p variables, which makes it difficult to interpret the derived principal components as new features. Rotation techniques are commonly used to help practitioners to interpret principal components <ref type="bibr" target="#b30">[30]</ref>. Vines <ref type="bibr" target="#b51">[51]</ref> considered simple principal components by restricting the loadings to take values from a small set of allowable integers such as 0, 1, and -1. This restriction may be useful for certain applications but not all. Simple thresholding is an ad hoc way to achieve sparse loadings by setting the loadings with absolute values smaller than a threshold to zero. Although the simple thresholding is frequently used in practice, it can be potentially misleading in various respects <ref type="bibr" target="#b9">[9]</ref>. Sparse variants of PCA aim to achieve a good balance between variance explained (dimension reduction) and sparse loadings (interpretability).</p><p>Sparse learning is ubiquitous in high-dimensional data analysis. Prior to the sparse principal component problem, an important question is how to select variables in highdimensional regression. For the regression problem, the lasso proposed in <ref type="bibr" target="#b50">[50]</ref> is a very promising technique for simultaneous variable selection and prediction. The lasso regression is an 1 penalized least squares method. The use of 1 penalization yields a sparse solution and also permits efficient computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SCoTLASS</head><p>Inspired by the lasso regression, Jolliffe et al. <ref type="bibr" target="#b31">[31]</ref> proposed a procedure called SCoTLASS to obtain sparse loadings by directly imposing an 1 constraint on the loading vector.</p><p>SCoTLASS extends the standard PCA by taking the variance maximization perspective of the PCA. It successively maximizes the variance</p><formula xml:id="formula_6">a T k Σa k (5)</formula><p>subject to</p><formula xml:id="formula_7">a T k a k = 1 and (for k ≥ 2) a T h a k = 0, h&lt;k<label>(6)</label></formula><p>and the extra 1 constraints</p><formula xml:id="formula_8">p j=1 |a kj | ≤ t (7)</formula><p>for some tuning parameter t. However, SCoTLASS is high computational cost which makes it an impractical solution for high-dimensional data analysis. It motivated researchers to consider more efficient proposals for sparse principal components. A related but more efficient approach is the generalized power method presented in Section II-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse Principal Component Analysis</head><p>After SCoTLASSO, the first computational efficient SPCA algorithm for high-dimensional data was introduced in <ref type="bibr" target="#b61">[61]</ref>. Their method is named sparse principal component analysis (SPCA). Before reviewing the technical details, let us consider the application of SPCA to the pitprops data <ref type="bibr" target="#b28">[28]</ref>, a classical example showing the difficulty of interpreting principal components. The pitprops data have 180 observations and 13 measured variables. In <ref type="bibr" target="#b61">[61]</ref>, the first six ordinary principal components and the first six sparse principal components are computed. Here we only cite the results of the first three principal components in Table <ref type="table" target="#tab_0">1</ref>. Compared with the standard PCA, SPCA generated very sparse loading structures without losing much variance.</p><p>In the original lasso paper, Tibshirani used a quadratic programming solver to compute the lasso regression estimator, which is not very efficient for high-dimensional data. Efron et al. <ref type="bibr" target="#b17">[17]</ref> derived the first efficient algorithm named LARS for computing the entire solution path of the lasso regression model with high-dimensional data. Motivated by LARS, Zou et al. <ref type="bibr" target="#b61">[61]</ref> proposed to tackle SPCA extends the linear manifold approximation view of the PCA to derive sparse loadings. Recall that the first principal component can be defined as</p><formula xml:id="formula_9">α1 = arg min α,β n i=1 xi -αα T xi 2 subject to α 2 = 1. (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>We reformulate <ref type="bibr" target="#b8">(8)</ref> as</p><formula xml:id="formula_11">arg min α,β n i=1 xi -αβ T xi 2 subject to α 2 = 1 and α = β. (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>The following theorem says that we can drop the equality constraint in <ref type="bibr" target="#b9">(9)</ref> and still recover the first loading vector exactly.</p><p>Theorem 1 <ref type="bibr" target="#b61">[61]</ref>: For any λ0 &gt; 0, let</p><formula xml:id="formula_13">(α, β) = arg min α,β n i=1 xi -αβ T xi 2 + λ0 β 2 subject to α 2 = 1.<label>(10)</label></formula><p>Then, β ∝ V1.</p><p>In Theorem 1, the extra 2 term λ0 β 2 is not needed when p &lt; n. When p &gt; n, any λ0 &gt; 0 should be used and it does not affect the normalized β1. By dropping the equality constraint α = β, we can use an alternating minimization algorithm to optimize the criterion in <ref type="bibr" target="#b10">(10)</ref> because α and β are separated variables. With a fixed α, the optimization problem over β is a regression problem.</p><p>Based on Theorem 1, we can impose a sparse penalty on β to gain zero loading because the normalizing step does not change the support of β. The SPCA for the first principal component is defined as</p><formula xml:id="formula_14">(α, β) = arg min α,β n i=1 xi -αβ T xi 2 + λ0 β 2 + λ1 β 1 subject to α 2 = 1<label>(11)</label></formula><p>and the output loading vector is V1 = β/ β . For n &gt; p, we can let λ0 = 0 and solving β with a fixed α is a lasso regression problem, which can be done efficiently. When n &lt; p, we need to use a positive λ0 (e.g., λ0 = 10 -3 ), solving β with a fixed α is an elastic net regression problem <ref type="bibr" target="#b60">[60]</ref>, which can be done efficiently as well.</p><p>Theorem 1 can be generalized to handle the first k principal components simultaneously, as stated in the next theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 [61]: Suppose we are considering the first k principal components. Let</head><formula xml:id="formula_15">A p×k = [α1, . . . , α k ] and B p×k = [β1, . . . , β k ]. For any λ0 &gt; 0, let ( Â, B) = arg min A,B n i=1 xi -AB T xi 2 + λ0 k j=1 βj 2 subject to A T A = I k×k . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>Then, βj ∝ Vj for j = 1, 2, . . . , k.</p><p>Then, the SPCA criterion for the first k sparse principal components is defined as</p><formula xml:id="formula_17">( Â, B) = arg min A,B n i=1 xi -AB T xi 2 + λ0 k j=1 βj 2 + k j=1 λ1,j βj 1 subject to A T A = I k×k (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where different λ1,js are allowed for penalizing the loadings of different principal components. Zou et al. <ref type="bibr" target="#b61">[61]</ref> proposed an alternating algorithm to minimize the SPCA criterion <ref type="bibr" target="#b13">(13)</ref>.</p><p>For each j, let</p><formula xml:id="formula_19">Y * j = Xαj . It is shown that B = [ β1, . . . , βk ] and each βj is obtained via βj = arg min β j Y * j -Xβj 2 + λ0 βj 2 +λ1,j βj 1. (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>One can use either the LARS-EN algorithm <ref type="bibr" target="#b60">[60]</ref> or the cyclic coordinate descent algorithm <ref type="bibr" target="#b21">[21]</ref> to solve <ref type="bibr" target="#b14">(14)</ref>. Both algorithms are efficient for high-dimensional data.</p><p>If B is fixed, the optimization problem of A is arg min</p><formula xml:id="formula_21">A n i=1 xi -AB T xi 2 = X -XBA T 2</formula><p>subject to A T A = I k×k . This is called a reduced rank Procrustes rotation problem in <ref type="bibr" target="#b61">[61]</ref> because when k = p, it is the Procrustes rotation problem <ref type="bibr" target="#b41">[41]</ref>. Zou et al. <ref type="bibr" target="#b61">[61]</ref> derived an explicit solution to the reduced rank Procrustes rotation problem. We compute the SVD</p><formula xml:id="formula_22">(X T X )B = U DV T (15)</formula><p>and set Â = U V T . The SPCA algorithm iterates between the elastic net regression step and the SVD step till convergence. The output is the normalized</p><formula xml:id="formula_23">B matrix Vj = βj / βj , 1 ≤ j ≤ k.</formula><p>Zou et al. <ref type="bibr" target="#b61">[61]</ref> derived another SPCA criterion to further speed up the computation efficiency. The derivation is based on the observation that Theorem 2 is valid for all λ0 &gt; 0. It turns out that a thrifty solution emerges if λ0 is taken to be a large constant.</p><p>Theorem 3 <ref type="bibr" target="#b61">[61]</ref>: Let Vj(λ0) = ( βj/ βj ) (j = 1, . . . , k) be the sparse loadings defined in <ref type="bibr" target="#b13">(13)</ref>. Let ( A, B) be the solution of the optimization problem</p><formula xml:id="formula_24">( Â, B) = arg min A,B -2Tr A T X T XB + k j=1 βj 2 + k j=1 λ1,j βj 1 subject to A T A = I k×k . (<label>16</label></formula><formula xml:id="formula_25">)</formula><formula xml:id="formula_26">When λ0 → ∞, Vj(λ0) → ( βj / βj ).</formula><p>Solving ( <ref type="formula" target="#formula_24">16</ref>) can also be done via an alternating minimization algorithm. Given A, we have that for each j βj = arg min</p><formula xml:id="formula_27">β j -2α T j (X T X)βj + βj 2 + λ1,j βj 1 (17)</formula><p>and the solution is given by</p><formula xml:id="formula_28">βj = S X T Xαj , λ1,j 2</formula><p>where S(Z, γ) is the soft-thresholding operator on a vector Z = (z1, . . . , zp) with thresholding parameter γ and</p><formula xml:id="formula_29">S(Z, γ)j = (|zj| -γ) + sgn(zj), 1 ≤ j ≤ p.</formula><p>Given B, the solution of A is again Â = U V T where U , V are from the SVD of (X T X)B: (X T X )B = U DV T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A Semidefinite Programming Approach</head><p>We introduce some necessary notation first. We use Card(M ) to denote the number of nonzero element of M , where M can be a vector of a matrix. The notation |M | means that we replace each element of M with its absolute value. Let 1p be the p-vector of 1.</p><p>Consider the first k-sparse principal component with at most k nonzero loadings. A natural definition of the optimal k-sparse loading vector is</p><formula xml:id="formula_30">arg max α α T Σα subject to α = 1, Card(α) ≤ k. (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>When k = p, then the above definition gives the loadings of the first principal component. However, ( <ref type="formula" target="#formula_30">18</ref>) is nonconvex and computationally difficult, especially when p is large. Convex relaxation is a standard technique used in operational research to handle difficult nonconvex problems. d'Aspremont et al. <ref type="bibr" target="#b15">[15]</ref> developed a convex relation of <ref type="bibr" target="#b18">(18)</ref>, which is expressed as a semidefinite programming problem.</p><p>Let P = αα T . We write α T Σα = Tr( ΣP ). The norm-1 constraint on α leads to a linear equality constraint on P : TrP = 1. Moreover, the cardinality constraint α 0 ≤ k implies Card(P ) ≤ k 2 . Hence, we consider the following optimization problem of P : </p><p>The above formulation in ( <ref type="formula" target="#formula_32">19</ref>) is still nonconvex and difficult to handle due to the cardinality constraint and the rank-1 constraint. By definition, P is symmetric and P 2 = P . Observe that</p><formula xml:id="formula_33">P 2 F = Tr(P T P ) = Tr(P ) = 1.</formula><p>By Cauchy-Schwartz</p><formula xml:id="formula_34">1 T p |P |1p ≤ Õ Card(P ) P 2 F ≤ k.</formula><p>Therefore, d'Aspremont et al. <ref type="bibr" target="#b15">[15]</ref> suggested to relax the cardinality constraint in <ref type="bibr" target="#b19">(19)</ref> to a linear inequality constraint 1 T p |P |1p ≤ k. Furthermore, they dropped the rank-1 constraint and ended up with the DSPCA formulation</p><formula xml:id="formula_35">arg max P Tr( ΣP ) subject to TrP = 1 1 T p |P |1p ≤ k P 0. (<label>20</label></formula><formula xml:id="formula_36">)</formula><p>The above is recognized as a semidefinite programming problem and can be solved by software such as SDPT3.</p><p>DSPCA only solves for P but not α. To compute the loading vector α, d'Aspremont et al. <ref type="bibr" target="#b15">[15]</ref> recommended truncating P and retaining only the dominant (sparse) eigenvector of P . For the second sparse principal component, it is suggested to replace Σ with Σ -(α T Σα)αα T in <ref type="bibr" target="#b20">(20)</ref>. The same procedure can be repeated to compute the rest sparse principal components.</p><p>For larger problems, d'Aspremont et al. <ref type="bibr" target="#b15">[15]</ref> discussed a Nesterov's smooth minimization technique to handle DSPCA. The computation complexity of the algorithm is O(p 4 Ô log(p)/ ), where is the numerical accuracy of the solution. d'Aspremont et al. <ref type="bibr" target="#b14">[14]</ref> discussed a greedy algorithm to speed up the computation. An alter-This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>Zou and Xue: A Selective Overview of Sparse Principal Component Analysis nating direction method of multipliers was proposed in <ref type="bibr" target="#b39">[39]</ref>.</p><p>DSPCA formulation generated many interests in the operational research and machine learning communities. Some follow-up works include <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b53">[53]</ref>, and <ref type="bibr" target="#b13">[13]</ref>, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Iterative Thresholding Methods</head><p>PCA can be done via the SVD of the data matrix. Thus, it is natural to consider a SPCA algorithm based on the SVD of X. This idea was explored in <ref type="bibr" target="#b48">[48]</ref> and <ref type="bibr" target="#b56">[56]</ref>.</p><p>Let the SVD of X be X = U DV T . Consider the first principal component. We know the loading vector is V1, the first column of V . It is a well-known result that SVD of X is related to the best lower rank approximation of X <ref type="bibr" target="#b16">[16]</ref>. Specifically, let Ũ be a norm-1 n-vector and Ṽ be a p-vector. Consider Ũ Ṽ T as a rank-1 approximation of X. The best rank-1 approximation is defined as</p><formula xml:id="formula_37">min Ũ, Ṽ X -Ũ Ṽ T 2 F subject to Ũ = 1 (<label>21</label></formula><formula xml:id="formula_38">)</formula><p>and the solution is Ũ = U1 and Ṽ = d1V1 where d1 is the first singular value.</p><p>Based on ( <ref type="formula" target="#formula_37">21</ref>), Shen and Huang <ref type="bibr" target="#b48">[48]</ref> proposed the following optimization problem:</p><formula xml:id="formula_39">( Û, V ) = arg min U,V X -UV T 2 F + λ V 1 subject to U = 1 (<label>22</label></formula><formula xml:id="formula_40">)</formula><p>and the sparse loading vector is normalized V , ( V / V ).</p><p>An alternating minimization algorithm is used to solve <ref type="bibr" target="#b22">(22)</ref>. Note that given V , the optimal U is U = XV / XV . Given U , the optimal V is arg min</p><formula xml:id="formula_41">V -2Tr(X T UV T ) + V 2 + λ V 1</formula><p>and the solution is given by the soft-thresholding operator</p><formula xml:id="formula_42">V = S X T U, λ 2 .</formula><p>Thus, Shen and Huang's method is an iterative thresholding algorithm. Note that the above procedure is similar in spirit to the SPCA algorithm in <ref type="bibr" target="#b16">(16)</ref>. The big difference is that SPCA solves k components simultaneously, but Shen and Huang's method only deals with one component at a time.</p><p>Shen and Huang <ref type="bibr" target="#b48">[48]</ref> proposed to sequentially compute the rest sparse principal components. Suppose that we have computed the first k (U, V ) pairs, then let</p><formula xml:id="formula_43">X (k+1) = X -È k l U l V T l</formula><p>and then the iterative thresholding algorithm is applied to X (k+1) to get (U (k+1) , V (k+1) ). The normalized V (k+1) is the loading vector of the (k + 1)th sparse principal component. The λ parameter is allowed to differ for different principal components.</p><p>In the same vein, Witten et al. <ref type="bibr" target="#b56">[56]</ref> proposed a penalized matrix decomposition (PMD) criterion as follows:</p><formula xml:id="formula_44">( Û, V , d) = arg min U,V,d X -dU V T 2 F subject to U = 1, U 1 ≤ c1 V = 1, V 1 ≤ c2. (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>By straightforward calculation, it can be shown that ( <ref type="formula" target="#formula_44">23</ref>) is equivalent to the following optimization problem:</p><formula xml:id="formula_46">( Û, V ) = arg max U,V U T XV subject to U = 1, U 1 ≤ c1 V = 1, V 1 ≤ c2 (<label>24</label></formula><formula xml:id="formula_47">)</formula><p>and d = Û T X V . They also used an alternating minimization algorithm to compute <ref type="bibr" target="#b24">(24)</ref>. Given V , we update U by solving</p><formula xml:id="formula_48">max U U T XV subject to U = 1, U 1 ≤ c1. (<label>25</label></formula><formula xml:id="formula_49">)</formula><p>Given U , we update V by solving</p><formula xml:id="formula_50">max V U T X V subject to V = 1, V 1 ≤ c2. (<label>26</label></formula><formula xml:id="formula_51">)</formula><p>The equality constraint U = 1, V = 1 in ( <ref type="formula" target="#formula_48">25</ref>) and ( <ref type="formula" target="#formula_50">26</ref>) can be replaced with inequality constraint U ≤ 1, V ≤ 1 and the solutions remain the same. So, ( <ref type="formula" target="#formula_48">25</ref>) and ( <ref type="formula" target="#formula_50">26</ref>) are examples of the following convex optimization problem:</p><formula xml:id="formula_52">Ẑ = arg max Z Z T R subject to Z ≤ 1, Z 1 ≤ c. (<label>27</label></formula><formula xml:id="formula_53">)</formula><p>It is easy to see that the solution to ( <ref type="formula" target="#formula_52">27</ref>) is</p><formula xml:id="formula_54">Ẑ = S(R, Δc) S(R, Δc)</formula><p>where S is the soft-thresholding operator and Δc is selected as follows:</p><formula xml:id="formula_55">Δc = 0 if (R/ R ) 1 ≤ c, otherwise Δc &gt; 0 is chosen to satisfy Ẑ 1 = c.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. A Generalized Power Method</head><p>Consider the first principal component. By the variance maximization definition, a direct formulation of 1 constrained sparse principal component is</p><formula xml:id="formula_56">arg max α =1 α T X T X α subject to α 1 ≤ t. (<label>28</label></formula><formula xml:id="formula_57">)</formula><p>Equivalently, we can solve</p><formula xml:id="formula_58">arg max α =1 Ô α T X T X α subject to α 1 ≤ t. (<label>29</label></formula><formula xml:id="formula_59">)</formula><p>Journée et al. <ref type="bibr" target="#b32">[32]</ref> considered the Lagrangian form of ( <ref type="formula" target="#formula_58">29</ref>)</p><formula xml:id="formula_60">arg max α =1 Ô α T X T Xα -λ α 1.<label>(30)</label></formula><p>They offered a generalized power method for solving <ref type="bibr" target="#b31">(31)</ref>.</p><p>Their idea takes advantage of this simple observation: let Ũ = argmax U =1 U T Z, then Ũ = Z/ Z and Ũ T Z = Z . Thus, an equivalent formulation of ( <ref type="formula" target="#formula_61">31</ref>) is</p><formula xml:id="formula_61">(U * , α * ) = arg max U,α U T X α -λ α 1 subject to U = 1, α = 1. (<label>31</label></formula><formula xml:id="formula_62">)</formula><p>Note that the formulation <ref type="bibr" target="#b31">(31)</ref> is the Lagrangian form of the PMD formulation <ref type="bibr" target="#b24">(24)</ref> without imposing the 1 constraint on U . For any U , the optimal α and X T U must share the same sign for each component. Let zj = |αj |, and Z = |α|. Then, the optimal Z * must satisfy</p><formula xml:id="formula_63">Z * = arg max Z p j=1 (|X T U |j -λ)zj subject to zj ≥ 0, p j=1 z 2 j = 1. (<label>32</label></formula><formula xml:id="formula_64">)</formula><p>When |X T U |j -λ ≤ 0, z * j = 0. By Cauchy-Schwartz, it is easy to see that the solution to (32) is</p><formula xml:id="formula_65">z * j = (|X T U |j -λ)+ Õ È p j=1 (|X T U |j -λ) 2 + (<label>33</label></formula><formula xml:id="formula_66">)</formula><p>which yields</p><formula xml:id="formula_67">α = S(X T U, λ) S(X T U, λ) (<label>34</label></formula><formula xml:id="formula_68">)</formula><p>where S is the soft-thresholding operator. Plugging <ref type="bibr" target="#b33">(33)</ref> back to the objective function in <ref type="bibr" target="#b31">(31)</ref>, we obtain a new optimization criterion of U U * = arg max</p><formula xml:id="formula_69">U : U =1 Ú Ù Ù Ø p j=1 (|X T U |j -λ) 2 +</formula><p>or equivalently</p><formula xml:id="formula_70">U * = arg max U : U ≤1 p j=1 (|X T U |j -λ) 2 + . (<label>35</label></formula><formula xml:id="formula_71">)</formula><p>Once U * is solved, we have</p><formula xml:id="formula_72">α * = S(X T U * , λ) S(X T U * , λ) .</formula><p>Solving U * is an n-dimensional optimization problem, although the original formulation ( <ref type="formula" target="#formula_61">31</ref>) is a p-dimensional optimization problem. When p n, the generalized power method achieves great computational savings. Moreover, the objective function in <ref type="bibr" target="#b35">(35)</ref> is differentiable and convex, and the constraint set is compact and convex.</p><p>Journée et al. <ref type="bibr" target="#b32">[32]</ref> used an efficient gradient method to compute U * and analyzed its convergence property. They also showed that the generalized power method can be extended to handle the first k principal components jointly.</p><p>There are other proposals for constructing spare principal components such as the truncated power method in <ref type="bibr" target="#b58">[58]</ref> and the exact and greedy algorithms in <ref type="bibr" target="#b42">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. T H E O R E T I C A L R E S U LT S</head><p>Theoretical analysis of SPCA received considerable attention in the past decade. In what follows, we first discuss the inconsistency of the classical PCA in the high-dimensional setting, and then present recent theoretical developments of SPCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inconsistency of PCA Under High Dimensions</head><p>Statistical analysis of PCA views Σ as the empirical covariance matrix, and there is the population PCA on the true covariance matrix Σ. In the conventional setting where the dimension is fixed and the sample size increases, the principal eigenvectors of the sample covariance matrix are the consistent estimates of the principal eigenvectors of the corresponding population covariance matrix <ref type="bibr" target="#b3">[3]</ref>.</p><p>However, the sample principal eigenvectors are inconsistent estimates of the corresponding population principal eigenvectors in the high-dimensional setting where the dimension is no longer fixed and may be much larger than the sample size. The inconsistency phenomenon was first observed in the unsupervised learning theory literature in physics (for example, <ref type="bibr" target="#b7">[7]</ref> and <ref type="bibr" target="#b55">[55]</ref>). About a decade ago, a series of papers in the statistics literature (for example, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, and <ref type="bibr" target="#b33">[33]</ref>) investigated the inconsistency results of the classical PCA when estimating the leading principal eigenvectors in the high-dimensional setting. Baik and Silverstein <ref type="bibr" target="#b5">[5]</ref>, Paul <ref type="bibr" target="#b44">[44]</ref>, and Nadler <ref type="bibr" target="#b43">[43]</ref> showed that when limn→∞ p/n = γ ∈ (0, 1), the largest eigenvalue λ1 is of unit multiplicity and λ1 ≤ √ γ, the leading sample principal eigenvector v1 is asymptotically orthogonal to the leading population principal eigenvector v1 almost surely, that is</p><formula xml:id="formula_73">P ( lim n→∞ |v T 1 v1| = 0) = 1.</formula><p>Johnstone and Lu <ref type="bibr" target="#b29">[29]</ref> considered the rank-1 case and gave the sufficient and necessary condition for the consistence estimation of the leading population principal eigenvector. Let R(v1, v1) = cos α(v1, v1)be the cosine of the angle between v1 and v1, and let ω = limn→∞ v1 2 /σ 2 be the limiting signal-to-noise ratio. Johnstone and Lu <ref type="bibr" target="#b29">[29]</ref> proved that</p><formula xml:id="formula_74">P lim n→∞ R 2 (v1, v1) = R 2 ∞ (ω, c) = 1</formula><p>where c = limn→∞ p/n and</p><formula xml:id="formula_75">R 2 ∞ (ω, c) = (ω 2 -c)+/(ω 2 + cω).</formula><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>Zou and Xue: A Selective Overview of Sparse Principal Component Analysis</p><p>Note that R 2 ∞ (ω, c) &lt; 1 if and only if c &gt; 0. Thus, v1 is a consistent estimate of v1 if and only c = 0, which implies the inconsistency of the classical PCA in the highdimensional setting. Jung and Marron <ref type="bibr" target="#b33">[33]</ref> further studied the strong inconsistency of the leading sample principal eigenvector in the high dimension and low sample size context where the sample size is fixed and the dimension increases.</p><p>These inconsistency results call for new formulation of principal components that are consistent estimators of the population principal components under high dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Consistency of SPCA</head><p>In recent years, there have been a series of papers to develop the theoretical properties of the SPCA in the statistics literature. The consistency results are established for various regularized estimators of the leading eigenvectors. Under the rank-1 scenario with n -1 log(n ∨ p) → 0 as n → ∞, Johnstone and Lu <ref type="bibr" target="#b29">[29]</ref> established a consistency result for the classical PCA performed on a selected subset of variables satisfying σ2 ≥ σ 2 (1 + αn), where αn = α(n -1 log(n ∨ p)) 1/2 . Specifically, Johnstone and Lu <ref type="bibr" target="#b29">[29]</ref> proved that the estimated principal eigenvector vI 1 obtained via the subset selection rule is consistent</p><formula xml:id="formula_76">P lim n→∞ α(v I 1 , v1) = 0 = 1</formula><p>when the magnitudes of ordered coefficients of v1 have rapid decay, i.e., the r-th largest magnitude of v1 is no greater than Cr -1/q , r = 1, 2, . . . , for some 0 &lt; q &lt; 2 and 0 &lt; C &lt; ∞. This marginal variance selection method fails when the variables have equal or almost equal variance. Nevertheless, Johnstone and Lu <ref type="bibr" target="#b29">[29]</ref> proved the first theoretical justification for SPCA. Shen et al. <ref type="bibr" target="#b47">[47]</ref> established the consistency of the SPCA in the high dimension and low sample size context. Amini and Wainwright <ref type="bibr" target="#b2">[2]</ref> studied the support recovery property of the semidefinite programming approach of <ref type="bibr" target="#b15">[15]</ref> under the k-sparse assumption for the leading eigenvector in the rank-1 spiked covariance model. Ma <ref type="bibr" target="#b40">[40]</ref> proved the consistency of the iterative thresholding approach under a spiked covariance model. Lei and Vu <ref type="bibr" target="#b37">[37]</ref> provided the general sufficient conditions for sparsistency for the Fantope projection and selection method. In a very recent paper, Jankova and van de Geer <ref type="bibr" target="#b27">[27]</ref> proposed a debiased SPCA estimator and studied the asymptotic inference of the sparse eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Minimax Rates of Convergence</head><p>The minimax rate of estimation is another important theoretical development for the SPCA. The seminal paper by Birnbaum et al. <ref type="bibr" target="#b8">[8]</ref> studied the minimax rates of convergence and adaptive estimation when the rank is a fixed number and the ordered coefficients of each principal eigenvector have rapid decay. Specifically, Birnbaum et al. <ref type="bibr" target="#b8">[8]</ref> established a lower bound on the minimax risk of estimators under various models of sparsity for the population eigenvectors. Ma <ref type="bibr" target="#b40">[40]</ref> showed that the iterative thresholding estimator attains the minimax rate of convergence over a certain Gaussian class of distributions when the rank is treated as a fixed constant. By allowing the rank increase with the sample size, Cai <ref type="bibr" target="#b10">[10]</ref> and Vu and Lei <ref type="bibr" target="#b52">[52]</ref> studied the minimax optimality and adaptive estimation of the principal subspace for the SPCA in the high-dimensional setting. Following <ref type="bibr" target="#b10">[10]</ref>, we assume that the n × p data matrix X is generated as follows:</p><formula xml:id="formula_77">X = U DV T + Z</formula><p>where U is the n × k random effects matrix with independent identically distributed (i.i.d.) N (0, 1) entries, D = diag(λ</p><formula xml:id="formula_78">1/2 1 , . . . , λ 1/2 k ) is a diagonal matrix with ordered eigenvalues λ1 ≥ • • • ≥ λ k &gt; 0, V</formula><p>is an orthonormal matrix, Z is a random matrix with i.i.d. N (0, σ 2 ) entries, and U and Z are independent. Denote by Σ the covariance matrix of X. Note that Σ = V ΛV T + σ 2 Ip and also that the estimation of span(V ) is equivalent to the estimation of V V T . Now, we consider the optimal estimation of the principal subspace span(V ) under the commonly used loss function</p><formula xml:id="formula_79">L(V , V ) = V V T -V V T 2</formula><p>F and the following parameter space for Σ :</p><formula xml:id="formula_80">Θ(s, p, k, λ) = {Σ = V ΛV T + σ 2 Ip : κλ ≥ λ1 ≥ • • • ≥ λ k ≥ λ &gt; 0, V T V = I k , V w ≤ s}</formula><p>where κ &gt; 1, Λ = diag(λ1, . . . , λ k ), and V w = maxj=1,••• ,p j V (j) * 0 is the weak 0 radius of V . Note that the union of the column supports of V is of size at most s. Cai et al. <ref type="bibr" target="#b10">[10]</ref> used the local metric entropy <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b57">[57]</ref> to construct the lower bound, and then obtain the minimax risk bound in the high-dimensional setting as follows:</p><formula xml:id="formula_81">inf V sup Σ∈Θ(s,p,k,λ) E[L(V , V )] λ/σ 2 + 1 n(λ/σ 2 ) 2 k(s -k) + s log ep s ∧ 1.</formula><p>Cai et al. <ref type="bibr" target="#b11">[11]</ref> studied the minimax rates under the spectral norm, which is directly related to estimating the rank of the factor model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Statistical and Computational Tradeoff</head><p>It is important to point out that there is a fundamental tradeoff between statistical and computational performance. In general, there are no known computationally efficient methods to obtain the minimax rate optimal estimators for the SPCA. Several seminal papers highlight the tradeoff between computational and statistical efficiency for the SPCA, including <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b54">[54]</ref>, and others. Amini and Wainwright <ref type="bibr" target="#b2">[2]</ref> proved that no algorithm can reliably recover the sparse eigenvector under the single-spike covariance model when k ≥ Cn/log p for some positive constant C and all sufficiently large n. <ref type="bibr">Krauthgamer et al. [35]</ref> further proved that the semidefinite programming approach <ref type="bibr" target="#b15">[15]</ref> does not close the gap between computational and statistical efficiency as long as k ≥ C √ n for some positive constant C and all the sufficiently large n. Berthet and Rigollet <ref type="bibr" target="#b6">[6]</ref> considered the optimal detection of sparse principal components in high dimension</p><formula xml:id="formula_82">H0 : x ∼ N (0, Ip) versus H1 : x ∼ N (0, Ip + θv1v 1 )</formula><p>where v1 has a fixed number of nonzero components. To this end, Berthet and Rigollet <ref type="bibr" target="#b6">[6]</ref> studied a minimax optimal test based on the k-sparse largest eigenvalue of the empirical covariance matrix. The computation of this sparse eigenvalue statistic depends on a wellknown decision problem associated to finding whether a graph contains a clique of size k, whose computational complexity is proved to be NP-complete in general <ref type="bibr" target="#b34">[34]</ref>. In the follow-up paper, under the hardness assumption of the planted clique problem <ref type="bibr" target="#b20">[20]</ref>, Wang et al. <ref type="bibr" target="#b54">[54]</ref> showed that there is an effective sample size regime in which no randomized polynomial time algorithm can achieve the minimax optimal rate for new and larger classes satisfying a restricted covariance concentration condition. Recently, Gao et al. <ref type="bibr" target="#b22">[22]</ref> obtained the first computational lower bounds for SPCA under the Gaussian single spiked covariance model and closed the gap in SPCA computational lower bounds left by <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b54">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A P P L I C AT I O N S</head><p>SPCA can be used in applications where PCA is normally used. For example, the use of SPCA in clustering can lead to sparse clustering algorithms <ref type="bibr" target="#b12">[12]</ref>. PCA is a part of the integrated omic-data analysis, where SPCA can be used to replace the regular PCA <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b59">[59]</ref>. We discuss a few recent applications of SPCA in medical imaging, ecology, and neuroscience, respectively.</p><p>Shape/image analysis: Sjöstrand <ref type="bibr" target="#b49">[49]</ref> applied SPCA to landmark-based shape analysis of the CC brain structure. The author extracted 5, 20, and 50 nonzero principal components out of the total 156 components corresponding to landmarks, and he also applied the standard PCA as a benchmark. In the subsequent analysis, he used the univariate regression to study the relationship between the resulting deformations based on extracted variables and four clinical outcome variables (gender, age, walking speed, and verbal fluency). His findings confirmed the male/female mean shape differences and identified the deformation of the CC corresponding to the measure of walking speed. The results for verbal fluency were also meaningful anatomically. Sjöstrand <ref type="bibr" target="#b49">[49]</ref> found that SPCA is useful to derive localized and interpretable patterns of variability while PCA did not provide much interpretational value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ecological study:</head><p>Motivated by generating meaningful combinations of the explanatory variables, Gravuer et al. <ref type="bibr" target="#b23">[23]</ref> applied SPCA to perform the dimension reduction before fitting the ABT model. The sparsity helps the interpretability of their model. Specifically, Gravuer et al. <ref type="bibr" target="#b23">[23]</ref> used SPCA to study a range of human, biogeographic, and biological influences on the invasion of Trifolium species into New Zealand. The sparse principal components were obtained from 29 categorical and continuous variables for three invasion stages (i.e., introduction, naturalization, and spread), and studied the relationship of sparse principal components to invasion success by using aggregated boosted trees. Specifically, the authors identified eight sparse principal components on 22 variables for intentional introduction and unintentional introduction-naturalization stages, seven sparse principal components on 25 variables for naturalization of intentionally introduced species, and seven sparse principal components on 28 variables for relative spread rate. Gravuer et al. <ref type="bibr" target="#b23">[23]</ref> found that SPCA simultaneously improves interpretability and maintains high explained variance.</p><p>Neuroscience study: SPCA was used in <ref type="bibr" target="#b4">[4]</ref> to study the light-driven Ca 2+ signals of the GCL cells given a set of standardized visual stimuli in a probabilistic clustering framework. Baden et al. <ref type="bibr" target="#b4">[4]</ref> first used SPCA to extract features that are localized in time and readily interpretable from the responses to the chirp, color, and moving bar stimulus, and then used a Gaussian mixture model on the extracted feature set for clustering. The authors extracted 20 features from the mean response to the chirp, six features from the mean response to the color stimulus, eight features from the response time course, and four features from its temporal derivative. Many classically used temporal response features were identified, including ON-and OFF-responses with different kinetics or selectivity to different temporal frequencies. They also tried the standard PCA and found the results lead to inferior cluster quality.</p><p>SPCA is implemented in the R package elasticnet available from CRAN: http://cran.r-project.org/.</p><p>The Matlab implementation of SPCA is available in the toolbox SpaSM from http://www2.imm.dtu.dk/ projects/spasm/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. C O N C L U D I N G R E M A R K S</head><p>In our discussion, we have only presented the use of 1-norm for sparsity, but there are other equally suitable penalty functions to be used in the SPCA methods, including SCAD <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref> or 0, among others.</p><p>We now have a good understanding of the role of sparsity in PCA and ways to effectively exploit the sparsity. There are still remaining issues. A very important issue to be investigated further is automated SPCA. By "automated" we mean that there is a principled but not overly complicated procedure to set these sparse parameters</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>subject to TrP = 1, Card(P ) ≤ k 2 P 0 and rank(P ) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Compare PCA and SPCA on the Pitprops Data. Empty Cells Mean Zero Loadings. The Variance of SPCA Is Expected to Be Smaller Than That of PCA, by the Definition of PCA. The Differences in Variance Are Small the sparse principal component problem from a regression formulation. The resulting algorithm is SPCA.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of H. Zou was supported in part by the National Science Foundation (NSF) under Grant DMS-1505111. The work of L. Xue was supported by the National Science Foundation (NSF) under Grant DMS-1505256.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>Zou and Xue: A Selective Overview of Sparse Principal Component Analysis in SPCA. This question is particularly challenging when we solve several sparse principal components jointly. We would also like to have more empirical results to help us understand the pros and cons of each proposed SPCA technique, which may also inspire new and better approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A c k n o w l e d g m e n t s</head><p>The authors would like to thank Prof. T. Bouwmans, Prof. Y. Chi, and Prof. N. Vaswani for inviting them to contribute this review paper to the special issue. They are grateful for the very helpful comments from three anonymous reviewers. His research interests include highdimensional statistical inference and largescale optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B O U T T H E A U T H O R S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hui</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>E F E R E N C E S</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Singular value decomposition for genome-wide expression data processing and modeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="10101" to="10106" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-dimensional analysis of semidefinite relaxations for sparse principal components</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5B</biblScope>
			<biblScope unit="page" from="2877" to="2921" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An Introduction to Multivariate Statistical Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The functional diversity of retinal ganglion cells in the mouse</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roseön</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Euler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7586</biblScope>
			<biblScope unit="page" from="345" to="350" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eigenvalues of large sample covariance matrices of spiked population models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Silverstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivariate Anal</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1382" to="1408" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal detection of sparse principal components in high dimension</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1780" to="1815" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical mechanics of unsupervised structure recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mietzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A, Math. Gen</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1885" to="1897" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimax bounds for sparse PCA with noisy high-dimensional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1055" to="1084" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Loading and correlations in the interpretation of principle compenents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cadima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="214" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse PCA: Optimal rates and adaptive estimation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3074" to="3110" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal estimation and rank detection for sparse spiked covariance matrices</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Theory Related Fields</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="781" to="815" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biclustering with heterogeneous variance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="12253" to="12258" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying small mean-reverting portfolios</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Finance</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="351" to="364" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal solutions for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1269" to="1294" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A direct formulation for sparse PCA using semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="448" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936-09">Sep. 1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variable selection via nonconcave penalized likelihood and its oracle properties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">456</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strong oracle optimality of folded concave penalized estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="819" to="849" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical algorithms and a lower bound for detecting planted cliques</title>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pathwise coordinate optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse CCA: Adaptive estimation and computational barriers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2074" to="2101" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strong human association with plant invasion success for Trifolium introductions to New Zealand</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gravuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6344" to="6349" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face processing: Human perception and principal components analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory Cogn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="40" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gene shaving&apos; as a method for identifying distinct sets of genes with similar expression patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning; Data Mining, Inference and Prediction</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">De-biased sparse PCA: Inference and testing for eigenstructure of large covariance matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jankova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.10567" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two case studies in the application of principal component</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeffers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On consistency and sparsity for principal components analysis in high dimensions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">486</biblScope>
			<biblScope unit="page" from="682" to="693" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation of principal components: Choice of normalization constraints</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A modified principal component technique based on the lasso</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized power method for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Journée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="517" to="553" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PCA consistency in high dimension, low sample size context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6B</biblScope>
			<biblScope unit="page" from="4104" to="4130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducibility among combinatorial problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Complexity Comput. Symp</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Watson Research</surname></persName>
		</editor>
		<editor>
			<persName><surname>Center</surname></persName>
		</editor>
		<meeting>Complexity Comput. Symp<address><addrLine>Yorktown Heights, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="85" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Do semidefinite relaxations solve sparse PCA up to the information limit?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vilenchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1300" to="1322" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convergence of estimates under dimensionality restrictions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lecam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="53" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sparsistency and agnostic inference in sparse PCA</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="322" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An augmented Lagrangian approach for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="149" to="193" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Alternating direction method of multipliers for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oper. Res. Soc. China</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis and iterative thresholding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="772" to="801" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bibby</surname></persName>
		</author>
		<title level="m">Multivariate Analysis</title>
		<meeting><address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectral bounds for sparse PCA: Exact and greedy algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="915" to="922" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finite sample approximation results for principal component analysis: A matrix perturbation approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2791" to="2817" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Asymptotics of sample eigenstructure for a large dimensional spiked covariance model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sinica</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1617" to="1642" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Liii. on lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Mag. J. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Methods of integrating data to uncover genotype-phenotype interactions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Pendergrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Genetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Consistency of sparse PCA in high dimension, low sample size contexts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivariate Anal</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="317" to="333" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis via regularized low rank matrix approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivariate Anal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1015" to="1034" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparse decomposition and modeling of anatomical shape variation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sjöstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1625" to="1635" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple principal components</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Vines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="451" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Minimax sparse principal subspace estimation in high dimensions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2905" to="2947" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fantope projection and selection: A near-optimal convex relaxation of sparse PCA</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. Neural Inf</title>
		<meeting>26th Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2670" to="2678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Statistical and computational trade-offs in estimation of sparse principal components</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1896" to="1930" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimal unsupervised learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L H</forename><surname>Watkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Nadal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A, Math. Gen</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1899" to="1915" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Information-theoretic determination of minimax rates of convergence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1564" to="1599" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Truncated power method for sparse eigenvalue problems</title>
		<author>
			<persName><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="899" to="925" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">High-dimensional genomic data bias correction and data integration using MANCIE</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11305</biblScope>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
