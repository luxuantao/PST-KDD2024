<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>94920</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moises</forename><surname>Goldszmidt</surname></persName>
							<email>moises@erg.sri.com</email>
							<affiliation key="aff1">
								<orgName type="department">SRI International</orgName>
								<address>
									<addrLine>333 Ravenswood Avenue Menlo Park</addrLine>
									<postCode>94025</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
							<email>tomlee@erg.sri.com</email>
							<affiliation key="aff2">
								<orgName type="department">SRI International</orgName>
								<address>
									<addrLine>333 Ravenswood Avenue Menlo Park</addrLine>
									<postCode>94025</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">86B54FAB0BB54C387BFAABBF994F124C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a recent paper, Friedman, Geiger, and Goldszmidt [8]   introduced a classifier based on Bayesian networks, called Tree Augmented Naive Bayes (TAN), that outperforms naive Bayes and performs competitively with C4.5 and other state-of-the-art methods. This classifier has several advantages including robustness and polynomial computational complexity. One limitation of the TAN classifier is that it applies only to discrete attributes, and thus, continuous attributes must be prediscretized. In this paper, we extend TAN to deal with continuous attributes directly via parametric (e.g., Gaussians) and semiparametric (e.g., mixture of Gaussians) conditional probabilities. The result is a classifier that can represent and combine both discrete and continuous attributes. In addition, we propose a new method that takes advantage of the modeling language of Bayesian networks in order to represent attributes both in discrete and continuous form simultaneously, and use both versions in the classification. This automates the process of deciding which form of the attribute is most relevant to the classification task. It also avoids the commitment to either a discretized or a (semi)parametric form, since different attributes may correlate better with one version or the other. Our empirical results show that this latter method usually achieves classification performance that is as good as or better than either the purely discrete or the purely continuous TAN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The effective handling of continuous attributes is a central problem in machine learning and pattern recognition. Almost every real-world domain, including medicine, industrial control, and finance, involves continuous attributes. Moreover, these attributes usually have rich interdependencies with other discrete attributes. Many approaches in machine learning deal with continuous attributes by discretizing them. In statistics and pattern recognition, on the other hand, the typical approach is to use a parametric family of distributions (e.g. Gaussians) to model the data.</p><p>Each of these strategies has its advantages and disadvantages. By using a specific parametric family, we are making strong assumptions about the nature of the data. If these assumptions are warranted, then the induced model can be a Current address: Institute of Computer Science, The Hebrew University, Givat Ram, Jerusalem 91904, Israel, nir@cs.huji.ac.il. good approximation of the data. In contrast, discretization procedures are not bound by a specific parametric distribution; yet they suffer from the obvious loss of information. Of course, one might argue that for specific tasks, such as classification, it suffices to estimate the probability that the data falls in a certain range, in which case discretization is an appropriate strategy.</p><p>In this paper, we introduce an innovative approach for dealing with continuous attributes that avoids a commitment to either one of the strategies outlined above. This approach uses a dual representation for each continuous attribute: one discretized, and the other based on fitting a parametric distribution. We use Bayesian networks to model the interaction between the discrete and continuous versions of the attribute. Then, we let the learning procedure decide which type of representation best models the training data and what interdependencies between attributes are appropriate. Thus, if attribute B can be modeled as a linear Gaussian depending on A, then the network would have a direct edge from A to B. On the other hand, if the parametric family cannot fit the dependency of B on A, then the network might use the discretized representation of A and B to model this relation. Note that the resulting models can (and usually do) involve both parametric and discretized models of interactions among attributes.</p><p>In this paper we focus our attention on classification tasks. We extend a Bayesian network classifier, introduced by Friedman, Geiger, and Goldszmidt (FGG) <ref type="bibr" target="#b7">[8]</ref> called "Tree Augmented Naive Bayes" (TAN). FGG show that TAN outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. They tested TAN on problems from the UCI repository <ref type="bibr" target="#b15">[16]</ref>, and compared it to C4.5, naive Bayes, and wrapper methods for feature selection with good results. The original version of TAN is restricted to multinomial distributions and discrete attributes. We start by extending the set of distributions that can be represented in TAN to include Gaussians, mixtures of Gaussians, and linear models. This extension results in classifiers that can deal with a combination of discrete and continuous attributes and model interactions between them. We compare these classifiers to the original TAN on several UCI data sets. The results show that neither approach dominates the other in terms of classification accuracy.</p><p>We then augment TAN with the capability of representing each continuous attribute in both parametric and discretized forms. We examine the consequences of the dual representation of such attributes, and characterize conditions under which the resulting classifier is well defined. Our main hypothesis is that the resulting classifier will usually achieve classification performance that is as good or better than both the purely discrete and purely continuous TAN models. This hypothesis is supported by our experiments. We note that this dual representation capability also has ramifications in tasks such as density estimation, clustering, and compression, which we are currently investigating and some of which we discuss below. The extension of the dual representation to arbitrary Bayesian networks, and the extension of the discretization approach introduced by Friedman and Goldszmidt <ref type="bibr" target="#b8">[9]</ref> to take the dual representation into account, are the subjects of current research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REVIEW OF TAN</head><p>In this discussion we use capital letters such as X; Y; Z for variable names, and lower-case letters such as x; y; z to denote specific values taken by those variables. Sets of variables are denoted by boldface capital letters such as X; Y; Z, and assignments of values to the variables in these sets are denoted by boldface lowercase letters x; y; z.</p><p>A Bayesian network over a set of variables X = fX 1 ; : : : ; X n g is an annotated directed acyclic graph that encodes a joint probability distribution over X. Formally, a Bayesian network is a pair B = hG; Li. The first component, G, is a directed acyclic graph whose vertices correspond to the random variables X 1 ; : : : ; X n , and whose edges represent direct dependencies between the variables.</p><p>The second component of the pair, namely L, represents a set of local conditional probability distributions (CPDs)</p><p>L 1 ; : : : ; L n , where the CPD for X i maps possible values x i of X i and pa(X i ) of Pa(X i ), the set of parents of X i in G, to the conditional probability (density) of x i given pa(X i ).</p><p>A Bayesian network B defines a unique joint probability distribution (density) over X given by the product P B (X 1 ; : : : ;</p><formula xml:id="formula_0">X n ) = Q n i=1 L i (X i jPa(X i )) :<label>(1)</label></formula><p>When the variables in X take values from finite discrete sets, we typically represent CPDs as tables that contain parameters xijpa(Xi) for all possible values of X i and Pa(X i ).</p><p>When the variables are continuous, we can use various parametric and semiparametric representations for these CPDs.</p><p>As an example, let X = fA 1 ; : : : ; A n ; Cg, where the variables A 1 ; : : : ; A n are the attributes and C is the class variable. Consider a graph structure where the class variable is the root, that is, Pa(C) = ;, and each attribute has the class variable as its unique parent, namely, Pa(A i ) = fCg for all 1 i n. For this type of graph structure, Equation 1 yields Pr(A 1 ; : : :</p><formula xml:id="formula_1">; A n ; C) = Pr(C) Q n i=1 Pr(A i jC).</formula><p>From the definition of conditional probability, we get Pr(CjA 1 ; : : : ; A n ) = Pr(C) Q n i=1 Pr(A i jC); where is a normalization constant. This is the definition of the naive Bayesian classifier commonly found in the literature <ref type="bibr" target="#b4">[5]</ref>.</p><p>The naive Bayesian classifier has been used extensively for classification. It has the attractive properties of being robust and easy to learn-we only need to estimate the CPDs Pr(C) and Pr(A i j C) for all attributes. Nonetheless, the naive Bayesian classifier embodies the strong independence assumption that, given the value of the class, attributes are independent of each other. FGG <ref type="bibr" target="#b7">[8]</ref> suggest the removal of these independence assumptions by considering a richer class of networks. They define the TAN Bayesian classifier that learns a network in which each attribute has the class and at most one other attribute as parents. Thus, the dependence among attributes in a TAN network will be represented via a tree structure. Figure <ref type="figure" target="#fig_3">1</ref> shows an example of a TAN network.</p><p>In a TAN network, an edge from A i to A j implies that the influence of A i on the assessment of the class also depends on the value of A j . For example, in Figure <ref type="figure" target="#fig_3">1</ref>, the influence of the attribute "Iron" on the class C depends on the value of "Aluminum," while in the naive Bayesian classifier the influence of each attribute on the class is independent of other attributes. These edges affect the classification process in that a value of "Iron" that is typically surprising (i.e., P(ijc) is low) may be unsurprising if the value of its correlated attribute, "Aluminum," is also unlikely (i.e., P(ijc; a) is high). In this situation, the naive Bayesian classifier will overpenalize the probability of the class by considering two unlikely observations, while the TAN network of Figure <ref type="figure" target="#fig_3">1</ref> will not do so, and thus will achieve better accuracy.</p><p>TAN networks have the attractive property of being learnable in polynomial time. FGG pose the learning problem as a search for the TAN network that has the highest likelihood LL(B : D) = P B (D), given the data D. Roughly speaking, networks with higher likelihood match the data better. FGG describe a procedure Construct-TAN for learning TAN models and show the following theorem. Theorem 2.1: <ref type="bibr" target="#b7">[8]</ref> Let D be a collection of N instances of C; A 1 ; : : : ; A n . The procedure Construct-TAN builds a TAN network B that maximizes LL(B : D) and has time complexity O(n 2 N).</p><p>The TAN classifier is related to the classifier introduced by Chow and Liu <ref type="bibr" target="#b1">[2]</ref>. That method learns a different tree for each class value. FGG's results show that the TAN and Chow and Liu's classifier perform roughly the same. In domains where there is substantial differences in the interactions between attributes for different class values, Chow and Liu's method performs better. In others, it is possible to learn a better tree by pooling the examples from different classes as done by TAN. Although we focus on extending the TAN classifier here, all of our ideas easily apply to classifiers that learn a different tree for each class value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GAUSSIAN TAN</head><p>The TAN classifier, as described by FGG, applies only to discrete attributes. In experiments run on data sets with continuous attributes, FGG use the prediscretizion described by Fayyad and Irani <ref type="bibr" target="#b6">[7]</ref> before learning a classifier. In this paper, we attempt to model the continuous attributes directly within the TAN network. To do so, we need to learn CPDs for continuous attributes. In this section, we discuss Gaussian distributions for such CPDs. The theory of training such representations is standard (see, for example, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>). We only review the indispensable concepts.</p><p>A more interesting issue pertains to the structure of the network. As we shall see, when we mix discrete and continuous attributes, the algorithms must induce directed trees. This is in contrast to the procedure of FGG, which learns undirected trees and then arbitrarily chooses a root to define edge directions. We describe the procedure for inducing directed trees next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE BASIC PROCEDURE</head><p>We now extend the TAN algorithm for directed trees. This extension is fairly straight forward and similar ideas have been suggested for learning tree-like Bayesian networks <ref type="bibr" target="#b11">[12]</ref>. For completeness, and to facilitate later extensions, we rederive the procedure from basic principles. Assume that we are given a data set D that consists of N identically and independently distributed (i.i.d.) instances that assign values to A 1 ; : : : ; A n and C. Also assume that we have specified the class of CPDs that we are willing to consider. The objective is, as before, to build a network that maximizes the likelihood function LL(B : D) = log P B (D).</p><p>Using Eq. ( <ref type="formula" target="#formula_0">1</ref>) and the independence of training instances, it is easy to show that LL(B : D) = P i P N j=1 log L i (x j i j Pa(X i ) j ) = P i S(X i j Pa(X i ) : L i );</p><p>(2) where x j i and Pa(X i ) j are the values of X i and Pa(X i ) in the jth instance in D. We denote by S(X i j Pa(X i )) the value attained by S(X i j Pa(X i ); L i ) when L i is the optimal CPD for this family, given the data, and the set of CPDs we are willing to consider (e.g., all tables, or all Gaussian distributions). "Optimal" should be understood in terms of maximizing the likelihood function in Eq. ( <ref type="formula">2</ref>). We now recast this decomposition in the special class of TAN networks. Recall that in order to induce a TAN network, we need to choose for each attribute A i at most one parent other than the class C. We represent this selection by a function (i), s.t., if (i) = 0, then C is the only parent of A i , otherwise both A (i) and C are the parents of A i . We define LL( : D) to be the likelihood of the TAN model specified by , where we select an optimal CPD for each parent set specified by . Rewriting Eq. ( <ref type="formula">2</ref>), we get</p><formula xml:id="formula_2">LL( : D) = P i; (i)&gt;0 S(A i j C; A (i) ) + P i; (i)=0 S(A i j C) + S(C j ;) = P i; (i)&gt;0 (S(A i j C; A (i) ) ? S(A i j C)) + P i S(A i j C) + S(C j ;) = P i; (i)&gt;0 (S(A i j C; A (i) ) ? S(A i j C)) + c;</formula><p>where c is some constant that does not depend on . Thus, we need to maximize only the first term. This maximization can be reduced to a graph-theoretic maximization by the following procedure, which we call Directed-TAN:</p><p>1. Initialize an empty graph G with n vertices labeled 1; : : : ; n. 2. For each attribute A i , find the best scoring CPD for P(A i j C) and compute S(A i j C). For each A j with j 6 = i, if an arc from A j to A i is legal, then find the best CPD for P(A i j C; A i ), compute S(A i j C; A j ), and add to G an arc j ! i with weight S(A i j C; A j ) ? S(A i j C). 3. Find a set of arcs A that is a maximal weighted branching in G. A branching is a set of edges that have at most one member pointing into each vertex and does not contain cycles. Finding a maximally weighed branching is a standard graph-theoretic problem that can be solved in low-order polynomial time <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>4. Construct the TAN model that contains arc from C to each A i , and arc from A j to A i if j ! i is in A. For each</p><p>A i , assign it the best CPD found in step 2 that matches the choice of arcs in the branching. From the arguments we discussed above it is easy to see that this procedure constructs the TAN model with the highest score. We note that since we are considering directed edges, the resulting TAN model might be a forest of directed trees instead of a spanning tree. Theorem 3.1: The procedure Directed-TAN constructs a TAN network B that maximizes LL(B : D) given the constraints on the CPDs in polynomial time.</p><p>In the next sections we describe how to compute the optimal S for different choices of CPDs that apply to different types of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DISCRETE ATTRIBUTES</head><p>Recall that if A i is discrete, then we model P(A i j Pa(A i ))</p><p>by using tables that contain a parameter aijpa(Ai) for each choice of values for A i and its parents. Thus,</p><formula xml:id="formula_3">S(A i j Pa(A i )) = X j log P(a j i j pa(A i ) j ) = N X ai;pa(Ai) P (a i ; pa(A i )) log aijpa(Ai) ;</formula><p>where P ( ) is the empirical frequency of events in the training data. Standard arguments show that the maximum likelihood choice of parameters is P(x j y) = P (x j y).</p><p>Making the appropriate substitution above, we get a nice information-theoretic interpretation of the weight of the arc from A i to A j , S(A i j C; A j ) ?S(A i j C) = N I(A i ; A j j C). The I() term is the conditional mutual information between A i and A j given C <ref type="bibr" target="#b2">[3]</ref>. Roughly speaking, it measures how much information A j provides about A i if already know the value of C. In this case, our procedure reduces to Construct-TAN of FGG, except that they use I(A i ; A j j C) directly as the weight on the arcs, while we multiply these weights by N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CONTINUOUS ATTRIBUTES</head><p>We now consider the case where X is continuous. There are many possible parametric models for continuous variables. Perhaps the easiest one to use is the Gaussian distribution. A continuous variable is a Gaussian with mean and variance 2 if the pdf of X has the form '(x : ;</p><formula xml:id="formula_4">2 ) = 1 p 2 2 e ? (x? ) 2 2 2</formula><p>: If all the parents of a continuous A i are discrete, then we learn a conditional Gaussian CPD <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> by assigning to A i different mean aijpa(Ai) and variance 2 aijpa(Ai) for each joint value of its parents. Standard arguments (e.g., see <ref type="bibr" target="#b0">[1]</ref>) show that we can rewrite S(A i j Pa(A i )) as a function of E A i j pa(A i )] and E A 2 i j pa(A i )]-the expectations of A i and A 2 i in these instances of the data where Pa(A i ) take a particular value. Standard arguments also show that we maximize the likelihood score of the CPD by choosing</p><formula xml:id="formula_5">Aijpa(Ai) = E A i j pa(A i )] 2 Aijpa(Ai) = E A 2 i j pa(A i )] ? E 2 A i j pa(A i )] :</formula><p>When we learn TAN models in domains with many continuous attributes, we also want to have families where one continuous attribute is a parent of another continuous attribute. In the Gaussian model, we can represent such CPDs by using a linear Gaussian relation. In this case, the mean of A i depends, in a linear fashion, on the value of A j . This relationship is parameterized by three parameters: AijAj;c ; AijAj;c and 2 AijAj;c for each value c of the class variable. The conditional probability for this CPD is a Gaussian with mean AijAj;C + A j AijAj;C and variance 2 AijAj;C . Again, by using standard arguments, it is easy to show that S(A i j A j ; C) is a function of low-order statistics in the data, and that the maximal likelihood parameters are</p><formula xml:id="formula_6">AijAj;c = E AiAjjc]?E Aijc]E Ajjc] E A 2 j jc]?E 2 Ajjc] AijAj;c = E A i j c] ? AijAj;c E A j j c] 2 AijAj;c = E A 2 i j c] ? E 2 A i j c] ? (E AiAjjc]?E Aijc]E Ajjc]) 2 E A 2 j jc]?E 2 Ajjc]</formula><p>In summary, to estimate parameters and to evaluate the likelihood, we need only to collect the statistics of each pair of attributes with the class, that is, terms of the form E A i j a j ; c] and E A i A j j c]. Thus, learning in the case of continuous Gaussian attributes can be done efficiently in a single pass over the data.</p><p>When we learn TAN models that contain discrete and Gaussian attributes, we restrict ourselves to arcs between discrete attributes, arcs between continuous attributes, and arcs from discrete attributes to continuous ones. If we want also to model arcs from continuous to discrete, then we need to introduce additional types of parametric models, such as logistic regression <ref type="bibr" target="#b0">[1]</ref>. As we will show, an alternative solution is provided by the dual representation approach introduced in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SMOOTHING</head><p>One of the main risks in parameter estimation is overfitting. This can happen when the parameter in question is learned from a very small sample (e.g., predicting A i from values of A j and of C that are rare in the data). A standard approach to this problem is to smooth the estimated parameters. Smoothing ensures that the estimated parameters will not be overly sensitive to minor changes in the training data. FGG show that in the case of discrete attributes, smoothing can lead to dramatic improvement in the performance of the TAN classifier. They use the following smoothing rule for the discrete case aijpa(Ai) = N P (pa(Ai)) P (aijpa(Ai))+s P (ai) N P (pa(Ai))+s</p><p>where s is a parameter that controls the magnitude of the smoothing (FGG use s = 5 in all of their experiments.)</p><p>This estimate uses a linear combination of the maximum likelihood parameters and the unconditional frequency of the attribute. It is easy to see that this prediction biases the learned parameters in a manner that depends on the weight of the smoothing parameter and the number of "relevant" instances in the data. This smoothing operation is similar to (and motivated by) well-known methods in statistics such as hierarchical Bayesian and shrinkage methods <ref type="bibr" target="#b9">[10]</ref>.</p><p>We can think of this smoothing operation as pretending that there are s additional instances in which A j is distributed according to its marginal distribution. This immediately suggests how to smooth in the Gaussian case: we pretend that for these additional s samples A i , A 2 i have the same average as what we encounter in the totality of the training data. Thus, the statistics from the augmented data are</p><formula xml:id="formula_7">Ê A i j pa(A i )] = N P (pa(Ai))E Aijpa(Ai)]+s E Ai] N P (pa(Ai))+s Ê A 2 i j pa(A i )] = N P (pa(Ai))E A 2 i jpa(Ai)]+s E A 2 i ] N P (pa(Ai))+s</formula><p>We then use these adjusted statistics for estimating the mean and variance of A i given its parents. The same basic smoothing method applies for estimating linear interactions between continuous attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEMIPARAMETRIC ESTIMATION</head><p>Parametric estimation methods assume that the data is (approximately) distributed according to a member of the given parametric family. If the data behaves differently enough, then the resulting classifier will degrade in performance.</p><p>For example, suppose that for a certain class c, the attribute A i has bimodal distribution, where the two modes x 1 and x 2 are fairly far apart. If we use a Gaussian to estimate the distribution of A i given C, then the mean of the Gaussian would be in the vicinity of = x1+x2</p><p>2 . Thus, instances</p><p>where A i has a value near would receive a high probability, given the class c. On the other hand, instances where A i has a value in the vicinity of either x 1 or x 2 would receive a much lower probability given c. Consequently, the support c gets from A i behaves exactly the opposite of the way it should. It is not surprising that in our experimental results, Gaussian TAN occasionally performed much worse than the discretized version (see Table <ref type="table" target="#tab_1">1</ref>).</p><p>A standard way of dealing with such situations is to allow the classifier more flexibility in the type of distributions it learns. One approach, called semiparametric estimation, learns a collection of parametric models. In this approach, we model P(A i j Pa(A i )) using a mixture of Gaussian distributions: P(A i j pa(A i )) = P j '(A i : Aijpa(Ai);j ; 2 Aijpa(Ai);j )w Aijpa(Ai);j ; where the parame- ters specify the mean and variance of each Gaussian in the mixture and w Aijpa(Ai);j are the weights of the mixture components. We require that the w Aijpa(Ai);j sum up to 1, for each value of Pa(A i ).</p><p>To estimate P(A i j pa(A i )), we need to decide on the number of mixture components (the parameter j in the equation above) and on the best choice of parameters for that mixture. This is usually done in two steps. First, we attempt to fit the best parameters for different number of components (e.g., j = 1; 2; : : :), and then select an instantiation for j based on a performance criterion.</p><p>Because there is no closed form for learning the parameters we need to run a search procedure such as the Expectation-Maximization (EM) algorithm. Moreover, since EM usually finds local maxima, we have to run it several times, from different initial points, to ensure that we find a good approximation to the best parameters. This operation is more expensive than parametric fitting, since the training data cannot be summarized for training the mixture parameters. Thus, we need to perform many passes over the training data to learn the parameters. Because of space restrictions we do not review the EM procedure here, and refer the reader to <ref type="bibr">[1, pp. 65-73]</ref>.</p><p>With regard to selecting the number of components in the mixture, it is easy to see that a mixture with k+1 components can easily attain the same or better likelihood as any mixture with k components. Thus, the likelihood (of the data) alone is not a good performance criterion for selecting mixture components, since it always favors models with a higher number of components, which results in overfitting. Hence, we need to apply some form of model selection. The two main approaches to model selection are based on crossvalidation to get an estimate of true performance for each choice of k, or on penalizing the performance on the training data to account for the complexity of the learned model. For simplicity, we use the latter approach with the BIC/MDL penalization. This rule penalizes the score of each mixture with log N 2 3k, where k is the number of mixture components, and N is the number of training examples for this mixture (i.e., the number of instances in the data with this specific value of the discrete parents).</p><p>Once more, smoothing is crucial for avoiding overfitting. Because of space considerations we will not go into the details. Roughly speaking, we apply the Gaussian smoothing operation described above in each iteration of the EM procedure. Thus, we assume that each component in the mixture has a preassigned set of s samples it has to fit.</p><p>As our experimental results show, the additional flexibility of the mixture results in drastically improved performance in the cases where the Gaussian TAN did poorly (see, for example, the accuracy of the data sets "anneal-U" and "balance-scale" in Table <ref type="table" target="#tab_1">1</ref>). In this paper, we learned mixtures only when modeling a continuous feature with discrete parents. We note, however, that learning a mixture of linear models is a relatively straightforward extension that we are currently implementing and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DUAL REPRESENTATION</head><p>The classifiers we have presented thus far require us to make a choice. We can either prediscretize the attributes and use the discretized TAN, or we can learn a (semi)parametric density model for the continuous attributes. Each of these methods has its advantages and problems: Discretization works well with nonstandard densities, but clearly loses much information about the features. Semiparametric estimation can work well for "well-behaved" multimodal densities. On the other hand, although we can approximate any distribution with a mixture of Gaussians, if the density is complex, then we need a large number of training instances to learn a mixture with large number of components, with sufficient confidence.</p><p>The choice we are facing is not a simple binary one, that is, to discretize or not to discretize all the attributes. We can easily imagine situations in which some of several attributes are better modeled by a semiparametric model, and others are better modeled by a discretization. Thus, we can choose to discretize only a subset of the attributes. Of course, the decision about one attribute is not independent of how we represent other attributes. This discussion suggests that we need to select a subset of variables to discretize, that is, to choose from an exponential space of options.</p><p>In this section, we present a new method, called hybrid TAN, that avoids this problem by representing both the continuous attributes and their discretized counterparts within the same TAN model. The structure of the TAN model determines whether the interaction between two attributes is best represented via their discretized representation, their continuous representation, or a hybrid of the discrete representation of one and the continuous representation of the other. Our hypothesis is that hybrid TAN allows us to achieve performance that is as good as either alternative. Moreover, the cost of learning hybrid TAN is about the same as that of learning either alternative.</p><p>Let us assume, that the first k attributes, A 1 ; : : : ; A k , are the continuous attributes in our domain. We denote by A 1 ; : : : ; A k the corresponding discretized attributes (i.e., A 1 is the discretized version of A 1 ), based on a predetermined discretization policy (e.g., using a standard method, such as Fayyad and Irani's <ref type="bibr" target="#b6">[7]</ref>). Given this semantics for the discretized variables, we know that that each A i is a deterministic function of A i . That is, A i state corresponds to the interval x 1 ; x 2 ] if and only if A i 2 x 1 ; x 2 ]. Thus, even though the discretized variables are not observed in the training data, we can easily augment the training data with the discretized version of each continuous attribute.</p><p>At this stage one may consider the application of one of the methods we described above to the augmented training set. This, however, runs the risk of "double counting" the evidence for classification provided by the duplicated attributes. The likelihood of the learned model will contain a penalty for both the continuous and the discrete versions of the attribute. Consequently, during classification, a "surprising" value of an attribute would have twice the (negative) effect on the probability of the class variable. One could avoid this problem by evaluating only the likelihood assigned to the continuous version of the attributes. Unfortunately, in this case the basic decomposition of Eq. ( <ref type="formula">2</ref>) no longer holds, and we cannot use the TAN procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MODELING THE DUAL REPRESENTATION</head><p>Our approach takes advantage of Bayesian networks to model the interaction between an attribute and its discretized version. We constrain the networks we learn to match our model of the discretization, that is, a discretized attribute is a function of the continuous one. More specifically, for each continuous attribute A i , we require that P B (A i j A i ) = 1 iff A i is in the range specified by A i . It is easy to show (using the chain rule) that this constraint implies that P B (A 1 ; : : : ; A n ; A 1 ; : : : ; A k ) = P B (A 1 ; : : : ; A n ) avoiding the problem outlined in the previous paragraph.</p><p>Note that by imposing this constraint we are not requiring in any way that A i be a parent of A i . However, we do need to ensure that P(A i j A i ) is deterministic in the learned model. We do so by requiring that A i and A i are adjacent in the graph (i.e., one is the parent of the other) and by putting restrictions on the models we learn for P(A i j A i ) and P(A i j A i ). There are two possibilities: 1. if A i ! A i is in the graph, then the conditional distribution P(A i j A i ; C) is determined as outlined above; it is 1 if A i is in the range defined by the value of A i and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">if A i ! A i is in the graph, then we require that P(A i j</head><p>A i ; C) = 0 whenever A i is not in the range specified by A i . By Bayes rule P(A i j A i ) / P C P(A i j A i ; C)P(A i ; C); Thus, if A i is not in the range of A i , then P(A i j A i ) / P C 0 P(A i ; C) = 0. Since the conditional probability of A i given A i must sum to 1, we conclude that P(A i j</p><formula xml:id="formula_8">A i ) = 1 iff A i is in the range of A i .</formula><p>There is still the question of the form of P(A i j A i ; C).</p><p>Our proposal is to learn a model for A i given A i and C, using the standard methods above (i.e., a Gaussian or a mixture of Gaussians). We then truncate the resulting density on the boundaries of the region specified by the discretization, and we ensure that the truncated density has total mass 1 by applying a normalizing constant. In other words, we learn an unrestricted model, and then condition on the fact that A i can only take values in the specified interval.</p><p>Our goal is then to learn a TAN model that includes both the continuous and discretized versions of each continuous attribute, and that satisfies the restrictions we just described. Since these restrictions are not enforced by the procedure of Section 3.1, we need to augment it. We start by observing that our restrictions imply that if we include B ! A in the model, we must also include A ! A . To see this, note that since A already has one parent (B) it cannot have additional parents. Thus, the only way of making A and A adjacent is by adding the edge A ! A . Similarly, if we include the edge B ! A , we must also include A ! A.</p><p>This observation suggests that we consider edges between groups of variables, where each group contains both versions of an attribute. In building a TAN structure that includes both representations, we must take into account that adding an edge to an attribute in a group, immediately constraints the addition of other edges within the group. Thus, the TAN procedure should make choices at the level groups. Such a procedure, which we call hybrid-TAN is described next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">HYBRID-TAN</head><p>We now expand on the details of the procedure. As with the basic procedure, we compute scores on edges. Now, however, edges are between groups of attributes. Each group consisting of the different representations of an attribute.</p><p>Let A be a continuous attribute. By our restriction, either A 2 Pa(A ), or A 2 Pa(A). And since each attribute has at most one parent (in addition to the class C),we have that at most one other attribute is in Pa(A) Pa(A )?fA; A ; Cg.</p><p>We define a new function T(A j B) that denotes the best combination of parents for A and A such that either B or B is a parent of one of these attributes. Similarly, T(A j ;)</p><p>denotes the best configuration such that no other attribute is a parent of A or A .</p><p>First, consider the term T(A j ;). If we decide that neither A nor A have other parents, then we can freely choose between A ! A and A ! A. Thus T(A j ;) = max( S(A j C; A ) + S(A j C); S(A j C) + S(A j C; A)); where S(A j C; A ) and S(A j C; A) are the scores of the CPDs subject to the constraints discussed in Subsection 5.1 (the first is a truncated model, and the second is a deterministic model).</p><p>Next, consider the case that a continuous attribute B is a parent of A. There are three possible ways of placing an edge from the group fB; B ginto the group fA; A g. These cases are shown in Figure <ref type="figure" target="#fig_0">2</ref>. (The fourth case is disallowed, since we cannot have an edge from the continuous attribute, B to the discrete attribute, A .) It is easy to verify that in any existing TAN network, we can switch between the edge configurations of Figure <ref type="figure" target="#fig_0">2</ref> without introducing new cycles. Thus, given the decision that the group B points to the group A, we would choose the configuration with maximal score: T(A j B) = max( S(A j C; B ) + S(A j C; A); S(A j C; A ) + S(A j C; B ); S(A j C; B) + S(A j C; A))</p><p>Finally, when B is discrete, then T(A j B) is the maximum between two options (B as a parent of A or as a parent of B ), and when A is discrete, then T(A j B) is equal to one term (either S(A j C; B) or S(A j C; B ), depending on B's type). We now can define the procedure, which we call Hybrid-TAN:</p><p>1. Initialize an empty graph G with n vertices labeled<ref type="foot" target="#foot_0">1</ref>; : : : ; n. 2. For each attribute A i , compute the scores of the form S(A i j C), S(A i j C), S(A i j C; A i ), etc. For each A j with j 6 = i, add to G an arc j ! i with weight T(A i j A j ) ? T(A i j ;). 3. Find a maximal weighted branching A in G.</p><p>4. Construct the TAN model that contains edges from C to each A i and A i . If j ! i is in A, add the best configuration of edges (and the corresponding CPDs) from the group A j into A i . If i does not have an incoming arc in A, then add the edge between A i and A i that maximizes T(A i : ;).</p><p>It is straight forward to verify that this procedure performs the required optimization: Theorem 5.1: The procedure Hybrid-TAN constructs in polynomial time a dual TAN network B that maximizes LL(B : D), given the constraints on the CPDs and the constraint that A i and A i are adjacent in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">AN EXAMPLE</head><p>Figure <ref type="figure">3</ref> shows an example of a hybrid TAN model learned from one of the folds of the "glass2" data set. 1 It is instructive to compare it to the network in Figure <ref type="figure" target="#fig_3">1</ref>, which was learned by a TAN classifier based on mixtures of Gaussians from the same data set. As we can see, there are some similarities between the networks, such as the connections between "Silicon" and "Sodium," and between "Calcium" and "Magnesium" (which was reversed in the hybrid version). However, most of the network's structure is quite different. Indeed, the relation between "Magnesium" and "Calcium" is now modulated by the discretized version of these variables. This fact, and the increased accuracy of hybrid TAN for this data set (see Table <ref type="table" target="#tab_1">1</ref>), seem to indicate that in this domain attributes are not modeled well by Gaussians.</p><p>As a further illustration of this, we show in Figure <ref type="figure">4</ref> the estimate of the joint density of "Calcium" and "Magnesium" in both networks (given a particular value for the class), as well as the training data from which both estimates were learned. As we can see, most of the training data is centered at one point (roughly, when M = 3:5 and C = 8), but there is fair dispersion of data points when M = 0. In the Gaussian case, C is modeled by a mixture of two Gaussians (centered on 8:3 and 11:8, where the former has most of the weight in the mixture), and M is modeled as a linear function of C with a fixed variance. Thus, we get a sharp "bump" at the main concentration point on the low ridge in Figure <ref type="figure">4a</ref>. On the other hand, in the hybrid model, for each attribute, we model the probability in each bin by a truncated Gaussian. In this case, C is partitioned into three bins and M into two. This model results in the discontinuous density function we see in Figure <ref type="figure">4b</ref>. As we can see, the bump at the center of concentration is now much wider, and the whole region of dispersion corresponds to a low, but wide, "tile" (in fact, this tile is a truncated Gaussian with a large variance). In these plots, each point represents a data set, and the coordinates correspond to the prediction error of each of the methods compared. Points below the diagonal line correspond to data sets where the y axis method is more accurate, and points above the diagonal line correspond to data sets where the x axis method is more accurate. In (b), the dashed lines connect points that correspond to the same data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL EVALUATION</head><p>We ran our experiments on the 23 data sets listed in Table <ref type="table" target="#tab_1">1</ref>.</p><p>All of these data sets are from the UCI repository <ref type="bibr" target="#b15">[16]</ref>, and are accessible at the MLC++ ftp site. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each data set. We estimate the prediction accuracy of each classifier as well as the variance of this accuracy by using the MLC++ system <ref type="bibr" target="#b13">[14]</ref>. Accuracy was evaluated using 5-fold cross validation (using the methods described in <ref type="bibr" target="#b12">[13]</ref>). Since we do not currently deal with missing data, we removed instances with missing values from the data sets. To construct discretizations, we used a variant of the method of Fayyad and Irani <ref type="bibr" target="#b6">[7]</ref>, using only the training data, in the manner described in <ref type="bibr" target="#b3">[4]</ref>. These preprocessing stages were carried out by the MLC++ system. We note that experiments with the various learning procedures were carried out on exactly the same training sets and evaluated on exactly the same test sets. Table <ref type="table" target="#tab_1">1</ref> summarizes the accuracies of the learning procedures we have discussed in this paper: As we can see in Figure <ref type="figure" target="#fig_2">5</ref>(a), neither the discrete TAN (Disc) nor the mixture of Gaussians TAN (Mix) outperforms the other. In some domains, such as "anneal-U" and "glass," the discretized version clearly performs better; in others, such as "balance-scale," "hayes-roth," and "iris," the semiparametric version performs better. Note that the latter three data sets are all quite small. So, a reasonable hypothesis is that the data is too sparse to learn good discretizations. On the other hand, as we can see in Figure <ref type="figure" target="#fig_2">5</ref>(b), the hybrid method performs at roughly the same level as the best of either Mix or Disc approaches. In this plot, each pair of connected points describes the accuracy results achieved by Disc and Mix for a single data set. Thus, the best accuracy of these two methods is represented by the lower point on each line. As we can see, in most data sets the hybrid method performs roughly at the same level as these lower points. In addition, in some domains such as "glass2," "hayes-roth," and "hepatitis" the ability to model more complex interactions between the different continuous and discrete attributes results in a higher prediction accuracy. Finally, given the computational cost involved in using EM to fit the mixture of Gaussians we include the accuracy of H/Gauss so that the benefits of using a mixture model can be evaluated. At the same time, the increase in prediction accuracy due to the dual representation can be evaluated by comparing to Gauss.</p><p>Due to the fact that H/Mix increases the number of parameters that need to be fitted, feature selection techniques are bound to have a noticeable impact. This is evident in the results obtained for H/Mix-FS which, as mentioned above, supports a primitive form of feature selection (see Figure <ref type="figure" target="#fig_2">5</ref>(c)). These results indicate that we may achieve better performance by incorporating a feature selection mechanism into the classifier. Clearly, we expect that such a combination would perform better than this simple form of feature selection. We leave this as a topic for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>The contributions of this work are twofold. First, we extend the TAN classifier to directly model continuous attributes by parametric and semiparametric methods. We use standard procedures to estimate each of the conditional distributions, and then combine them in a structure learning phase by maximizing the likelihood of the TAN model. The resulting procedure preserves the attractive properties of the original TAN classifier-we can learn the best model in polynomial time. Of course, one might extend TAN to use other parametric families (e.g., Poisson distributions) or other semiparametric methods, (e.g., kernel-based methods). The general conclusion we draw from these extensions is that if the assumptions embedded in the parametric forms "match" the domain, then the resulting TAN classifier generalizes well and will lead to good prediction accuracy. We also note that it is straightforward to extend the procedure to select, at learning time, a parametric form from a set of parametric families.</p><p>Second, we introduced a new method to deal with different representations of continuous attributes within a single model. This method enables our model learning procedure (in this case, TAN) to automate the decision as to which representation is most useful in terms of providing information about other attributes. As we showed in our experiments, the learning procedure managed to make good decisions on these issues and achieve performance that roughly as good as both the purely discretized and the purely continuous approaches.</p><p>This method can be extended in several directions. For example, to deal with several discretizations of the same attributes in order to select the granularity of discretization that is most useful for predicting other attributes. Another direction involves adapting the discretization to the particular edges that are present in the model. As argued Friedman and Goldszmidt <ref type="bibr" target="#b8">[9]</ref>, it is possible to discretize attributes to gain the most information about the neighboring attributes. Thus, we might follow the approach in <ref type="bibr" target="#b8">[9]</ref> and iteratively readjust the structure and discretization to improve the score. Finally, it is clear that this hybrid method is applicable not only to classification, but also to density estimation and related tasks using general Bayesian networks. We are currently pursuing these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The three possible ways of placing an edge from fB; B g into fA; A g. The parameterization of possible arcs are as follows: B ! A is a discrete model, both B ! A and B ! A are continuous models (e.g., Gaussians), A ! A is a truncated continuous model (e.g., truncated Gaussian), and A ! A is a deterministic model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: A hybrid TAN model learned for the data set "glass2." For clarity, the edges from the class to all the attributes are not shown. The attributes marked with asterisks ( ) correspond to the discretized representation. Dotted boxes mark two versions of the same attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scatter plots comparing the performance (a) of Disc (x axis) vs. Mix (y axis), (b) of H/Mix (x axis) vs. Disc and Mix (y axis), and (c) of H/Mix (x axis) vs. H/Mix-FS (y axis). In these plots, each point represents a data set, and the coordinates correspond to the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>Disc-TAN classifier based on prediscretized attributes; (2) Gauss-TAN classifier using Gaussians for the continuous attributes and multinomials for the discrete ones; (3) Mix-TAN classifier using mixtures of Gaussians for the continuous attributes; (4) H/Gauss-hybrid TAN classifier enabling the dual representation and using Gaussians for the continuous version of the attributes; (5) H/Mix-hybrid TAN classifier using mixtures of Gaussian for the continuous version of the attributes; and (6) H/Mix-FS-same as H/Mix but incorporating a primitive form of feature selection. The discretization procedure often removes attributes by discretizing them into one interval. Thus, these attributes are ignored by the discrete version of TAN. H/Mix-FS imitate this feature selection by also ignoring the continuous version of the attributes removed by the discretization procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental Results. The first four column describe the name of the data sets, the number of continuous and discrete attributes, and the number of instances. The remaining columns report percentage classification error and std. deviations from 5-fold cross validation of the tested procedures (see text).</figDesc><table><row><cell></cell><cell></cell><cell>Attr.</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction Errors</cell><cell></cell><cell></cell></row><row><cell>Data set</cell><cell>C</cell><cell>D</cell><cell>Size</cell><cell>Disc</cell><cell>Gauss</cell><cell>Mix</cell><cell>H/Gauss</cell><cell>H/Mix</cell><cell>H/Mix-FS</cell></row><row><cell>anneal-U</cell><cell>6</cell><cell>32</cell><cell>898</cell><cell>2.45 +-1.01</cell><cell>23.06 +-3.49</cell><cell>7.46 +-3.12</cell><cell>10.91 +-1.79</cell><cell>4.12 +-1.78</cell><cell>4.34 +-1.43</cell></row><row><cell>australian</cell><cell>6</cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Some of the discrete attributes do not appear in the figure, since they were discretized into one bin.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Anne Urban for help with the experiments. M. Goldszmidt and T. Lee were supported in part by DARPA's High Performance Knowledge Bases program under SPAWAR contract N66001-97-C-8548. N. Friedman was supported in part by ARO under grant DAAH04-96-1-0341.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Info. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">of Information Theory</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised discretization of continuous features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Even</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-interval discretization of continuousvalued attributes for classification learning</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI &apos;93</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discretization of continuous attributes while learning Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: a unification for discrete and Gaussian domains</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MLC++: A machine learning library in C++</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6&apos;th Inter. Conf. on Tools with AI</title>
		<meeting>6&apos;th Inter. Conf. on Tools with AI</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphical models for associations between variables, some of which are qualitative and some quantitative</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/˜mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding optimal branching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
