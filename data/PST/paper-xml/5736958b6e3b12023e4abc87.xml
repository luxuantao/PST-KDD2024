<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anti-Spoofing for Text-Independent Speaker Verification: An Initial Database, Comparison of Countermeasures, and Human Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhizheng</forename><surname>Wu</surname></persName>
							<email>zhizheng.wu@ed.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Phillip</forename><forename type="middle">L</forename><surname>De Leon</surname></persName>
							<email>pdeleon@nmsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cenk</forename><surname>Demiroglu</surname></persName>
							<email>cenk.demiroglu@ozyegin.edu.tr</email>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Khodabakhsh</surname></persName>
							<email>alikhodabakhsh@gmail.com</email>
						</author>
						<author>
							<persName><roleName>Fellow IEEE</roleName><forename type="first">Simon</forename><surname>King</surname></persName>
							<email>simon.king@ed.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daisuke</forename><surname>Saito</surname></persName>
							<email>dsksaito@gavo.t.u-tokyo.ac.jp</email>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Stewart</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
							<email>tomoki@icts.nagoya-u.ac.jp</email>
						</author>
						<author>
							<persName><forename type="first">Mirjam</forename><surname>Wester</surname></persName>
							<email>mwester@inf.ed.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
							<email>jyamagis@inf.ed.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centre for Speech Technology Research</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New Mexico State University (NMSU)</orgName>
								<address>
									<postCode>88003</postCode>
									<settlement>Las Cruces</settlement>
									<region>NM</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ozyegin University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">is with Information Technology Center</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anti-Spoofing for Text-Independent Speaker Verification: An Initial Database, Comparison of Countermeasures, and Human Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C110912B6528055468247C89F5D346D1</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2016.2526653, IEEE/ACM Transactions on Audio, Speech, and Language Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2016.2526653, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speaker verification</term>
					<term>speech synthesis</term>
					<term>voice conversion</term>
					<term>spoofing attack</term>
					<term>anti-spoofing</term>
					<term>countermeasure</term>
					<term>security</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a systematic study of the vulnerability of automatic speaker verification to a diverse range of spoofing attacks. We start with a thorough analysis of the spoofing effects of five speech synthesis and eight voice conversion systems, and the vulnerability of three speaker verification systems under those attacks. We then introduce a number of countermeasures to prevent spoofing attacks from both known and unknown attackers. Known attackers are spoofing systems whose output was used to train the countermeasures, whilst an unknown attacker is a spoofing system whose output was not available to the countermeasures during training. Finally, we benchmark automatic systems against human performance on both speaker verification and spoofing detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The task of automatic speaker verification (ASV), sometimes described as a type of voice biometrics, is to accept or reject a claimed identity based on a speech sample. There are two types of ASV system: text-dependent and textindependent. Text-dependent ASV assumes constrained word content and is normally used in authentication applications because it can deliver the high accuracy required. However, text-independent ASV does not place constraints on word content, and is normally used in surveillance applications. For example, in call-center applications 1,2 , a caller's identity can be verified during the course of a natural conversation without forcing the caller to speak a specific passphrase. Moreover, as such a verification process usually takes place under remote scenarios without any face-to-face contact, a spoofing attack -an attempt to manipulate a verification result by mimicking a target speaker's voice in person or by using computer-based techniques such as voice conversion or speech synthesisis a fundamental concern. Hence, in this work, we focus on spoofing and anti-spoofing for text-independent ASV.</p><p>Due to a number of technical advances, notably channel and noise compensation techniques, ASV systems are being widely adopted in security applications <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. A major concern, however, when deploying an ASV system, is its resilience to a spoofing attack. As identified in <ref type="bibr" target="#b7">[8]</ref>, there are at least four types of spoofing attack: impersonation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, replay <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, speech synthesis <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and voice conversion <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Among the four types of spoofing attack, replay, speech synthesis, and voice conversion present the highest risk to ASV systems <ref type="bibr" target="#b7">[8]</ref>. Although replay might be the most common spoofing technique which presents a risk to both text-dependent and textindependent ASV systems <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, it is not viable for the generation of utterances of specific content, such as would be required to maintain a live conversation in a callcenter application. On the other hand, open-source software for state-of-the-art speech synthesis and voice conversion is readily available (e.g., Festival <ref type="foot" target="#foot_2">3</ref> and Festvox<ref type="foot" target="#foot_3">4</ref> ), making these two approaches perhaps the most accessible and effective means to carry out spoofing attacks, and therefore presenting a serious risk to deployed ASV systems <ref type="bibr" target="#b7">[8]</ref>. For that reason, the focus in this work is only on those two types of spoofing attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speech Synthesis and Voice Conversion Spoofing</head><p>Many studies have reported and analysed the vulnerability of ASV systems to speech synthesis and voice conversion spoofing. The potential vulnerability of ASV to synthetic speech was first evaluated in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. An HMM-based speech synthesis system was used to spoof an HMM-based, text-prompted ASV system. They reported that the false acceptance rate (FAR) increased from 0% to over 70% under a speech synthesis spoofing attack. In <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, the vulnerability of two ASV systems -a GMM-UBM system (Gaussian mixture models with a universal background model), and an SVM system (support vector machine using a GMM supervector) -was assessed using a speaker-adaptive, HMMbased speech synthesizer. Experiments using the Wall Street Journal (WSJ) corpus (283 speakers) <ref type="bibr" target="#b23">[24]</ref> showed that FARs increased from 0.28% and 0.00% to 86% and 81% for GMM-UBM and SVM systems, respectively. These studies confirm the vulnerability of ASV systems to speech synthesis spoofing attack.</p><p>Voice conversion as a spoofing method has also been attracting increasing attention. The potential risk of voice conversion to a GMM ASV system was evaluated for the first time in <ref type="bibr" target="#b24">[25]</ref>, which used the YOHO database (138 speakers). In <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b16">[17]</ref>, text-independent GMM-UBM systems were assessed when faced with voice conversion spoofing on NIST speaker recognition evaluation (SRE) datasets. These studies showed an increase in FAR from around 10% to over 40% and confirmed the vulnerability of GMM-UBM systems to voice conversion spoofing attack.</p><p>Recent studies <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have evaluated more advanced ASV systems based on joint factor analysis (JFA), i-vectors, and probabilistic linear discriminative analysis (PLDA), on the NIST SRE 2006 database. The FARs of these systems increased five-fold from about 3% to over 17% under attacks from voice conversion spoofing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spoofing countermeasures</head><p>The vulnerability of ASV systems to spoofing attacks has led to the development of anti-spoofing techniques, often referred to as countermeasures. In <ref type="bibr" target="#b27">[28]</ref>, a synthetic speech detector based on the average inter-frame difference (AIFD) was proposed to discriminate between natural and synthetic speech. This countermeasure works well if the dynamic variation of the synthetic speech is different from that of natural speech; however, if global variance compensation is applied to the synthetic speech, the countermeasure becomes less effective <ref type="bibr" target="#b14">[15]</ref>.</p><p>In <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, a synthetic speech detector based on image analysis of pitch-patterns was proposed for human versus synthetic speech discrimination. This countermeasure was based on the observation that there can be artefacts in the pitch contours generated by HMM-based speech synthesis. Experiments showed that features extracted from pitch-patterns can be used to significantly reduce the FAR for synthetic speech. The performance of the pitch-pattern countermeasure was not evaluated for detecting voice conversion spoofing.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, a temporal modulation feature was proposed to detect synthetic speech generated by copy-synthesis. The modulation feature captures the long-term temporal distortion caused by independent frame-by-frame operations in speech synthesis. Experiments conducted on the WSJ database showed the effectiveness of the modulation feature when integrated with frame-based features. However, whether the detector is effective across a variety of speech synthesis and voice conversion spoofing attacks is unknown. Also using spectro-temporal information, a feature derived from local binary patterns <ref type="bibr" target="#b31">[32]</ref> was employed to detect voice conversion and speech synthesis attacks in <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>Phase-and modified group delay-based features have also been proposed to detect voice conversion spoofing <ref type="bibr" target="#b34">[35]</ref>. A cosine-normalised phase feature was derived from the phase spectrogram while the modified group delay feature contained both magnitude and phase information. Evaluation on the NIST SRE 2006 data confirmed the effectiveness of the proposed features. However, it remains unknown whether the phase-based features are also effective in detecting attacks from speech synthesisers using unknown vocoders. Another phase-based feature called the relative phase shift was proposed in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> to detect speech synthesis spoofing, and was reported to achieve promising performance for vocoders using minimum phase rather than natural phase.</p><p>In <ref type="bibr" target="#b37">[38]</ref>, an average pair-wise distance (PWD) between consecutive feature vectors was employed to detect voiceconverted speech, on the basis that the PWD feature is able to capture short-term variabilities, which might be lost during statistical averaging when generating converted speech. Although the PWD was shown to be effective against attacks from their own voice conversion system, this technique (which is similar to the AIFD feature proposed in <ref type="bibr" target="#b27">[28]</ref>) might not be an effective countermeasure against systems that apply global variance enhancement.</p><p>In contrast to the above methods focusing on discriminative features, a probabilistic approach was proposed in <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. This approach uses the same front-end as ASV, but treats the synthetic speech as a signal passed through a synthesis filter. Experiments on the NIST SRE 2006 database showed comparable performance to feature-based countermeasures. In this work, we focus on feature-based anti-spoofing techniques, as they can be optimised independently without rebuilding the ASV systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motivations and Contributions of this Work</head><p>In the literature, each study assumes a particular spoofing type (speech synthesis or voice conversion) and often just one variant (algorithm) of that type, then designs and evaluates a countermeasure for that specific, known attack. However, in practice it may not be possible to know the exact type of spoofing attack and therefore evaluations of ASV systems and countermeasures under a broad set of spoofing types are desirable. Most, if not all, previous studies have been unable to conduct a broader evaluation because of the lack of a standard, publicly-available spoofing database that contains a variety of spoofing attacks. To address this issue, we have previously developed a spoofing and anti-spoofing (SAS) database including both speech synthesis and voice conversion spoofing attacks <ref type="bibr" target="#b0">[1]</ref>. This database includes spoofing speech from two different speech synthesis systems and seven different voice conversion systems. Now, we first broaden the SAS database by including four more variants: three text-to-speech (TTS) synthesisers and one voice conversion system. They will be referred to as SS-SMALL-48, SS-LARGE-48, SS-MARY and VC-LSP 5 , and are described in Section II.A.</p><p>We also develop a joint speaker verification and countermeasure evaluation protocol, then refine that evaluation protocol to enable better generalisability of countermeasures developed using the database. We include contributions from both the speech synthesis and speaker verification communities. This database is offered as a resource for researchers investigating generalised spoofing and anti-spoofing methods 6 . We hope that the availability of a standard database will contribute to reproducible research 7 .</p><p>Second, with the SAS database, we conduct a comprehensive analysis of spoofing attacks on six different ASV systems. From this analysis we are able to determine which spoofing type and variant currently poses the greatest threat and how best to counter this threat. To the best of our knowledge, this study is the first evaluation of the vulnerability of ASV using such a diverse range of spoofing attacks and the most thorough analysis of the spoofing effects of speech synthesis and voice conversion spoofing systems under the same protocol.</p><p>Third, we present a comparison of several anti-spoofing countermeasures to discriminate between human and artificial speech. In our previous work, we applied cosine-normalised phase <ref type="bibr" target="#b34">[35]</ref>, modified group delay <ref type="bibr" target="#b34">[35]</ref> and segment-based modulation features <ref type="bibr" target="#b30">[31]</ref> to detect voice converted speech, and applied pitch pattern based features to detect synthetic speech <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In this work, we evaluate these countermeasures against both spoofing types and propose to fuse decisions at the score level in order to leverage multiple, complementary sources of information to create stronger countermeasures. We also extend the segment-based modulation feature to an utterance-level feature, to account for long-term variations.</p><p>Finally, we perform listening tests to evaluate the ability of human listeners to discriminate between human and artificial speech 8 . Although the vulnerability of ASV systems in the face of spoofing attacks is known, some questions still remain unanswered. These include whether human perceptual ability is important in identifying spoofing and whether humans can achieve better performance than automatic approaches in detecting spoofing attacks. In this work, we attempt to answer these questions through a series of carefully-designed listening tests. In contrast to the human assisted speaker recognition (HASR) evaluation <ref type="bibr" target="#b42">[43]</ref>, we consider spoofing attacks in 5 The four systems are new in this article while other systems have been published in a conference paper <ref type="bibr" target="#b0">[1]</ref>. SS-SMALL-48 and SS-LARGE-48 allow us to analyse the effect of sampling rates of spoofing materials. SS-MARY is useful to understand the effect of waveform concatenation-based speech synthesis spoofing. 6 Based on this database, a spoofing and countermeasure challenge <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> has already been successfully organised as a special session of INTER-SPEECH 2015. 7 The SAS corpus is publicly available: http://dx.doi.org/10.7488/ds/252 8 The preliminary version was published at INTERSPEECH 2015 <ref type="bibr" target="#b1">[2]</ref> where we focused on human and automatic spoofing detection performance on wideband and narrowband data. The current work benchmarks automatic systems against human performance on speaker verification and spoofing detection tasks.</p><p>speaker verification and conduct listening tests for spoofing detection, which was not considered in the HASR evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATABASE AND PROTOCOL</head><p>We extended our SAS database <ref type="bibr" target="#b0">[1]</ref> by including additional artificial speech. The database is built from the freely available Voice Cloning Toolkit (VCTK) database of native speakers of British English<ref type="foot" target="#foot_4">9</ref> . The VCTK database was recorded in a hemi-anechoic chamber using an omni-directional headmounted microphone (DPA 4035) at a sampling rate of 96 kHz. The sentences are selected from newspapers, and the average duration of each sentence is about 2 seconds.</p><p>To design the spoofing database, we took speech data from VCTK comprising <ref type="bibr" target="#b44">45</ref>  The small set consists of data only from Part A, while the large set comprises the data from Parts A and B together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spoofing systems</head><p>We implemented five speech synthesis (SS) and eight voice conversion (VC) spoofing systems, as summarised in Table <ref type="table" target="#tab_1">I</ref>. These systems were built using both open-source software (to facilitate reproducible research) as well as our own state-ofthe-art systems (to provide comprehensive results):</p><p>NONE: This is a baseline zero-effort impostor trial in which the impostor's own speech is used directly with no attempt to match the target speaker.</p><p>SS-LARGE-16: An HMM-based TTS system built with the statistical parametric speech synthesis framework described in <ref type="bibr" target="#b43">[44]</ref>. For speech analysis, the STRAIGHT vocoder with mixed excitation is used, which results in 60-dimensional Bark-Cepstral coefficients, log F 0 and 25-dimensional bandlimited aperiodicity measures <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Speech data from 257 (115 male and 142 female) native speakers of British English is used to train the average voice model. In the speaker adaptation phase, the average voice model is transformed using structural variational Bayesian linear regression <ref type="bibr" target="#b46">[47]</ref> followed by maximum a posteriori (MAP) adaptation, using the target speaker's data from Parts A and B. To synthesise speech, acoustic feature parameters are generated from the adapted HMMs using a parameter generation algorithm that considers global variance (GV) <ref type="bibr" target="#b47">[48]</ref>. An excitation signal is generated using mixed excitation and pitch-synchronous overlap and add <ref type="bibr" target="#b48">[49]</ref>, and used to excite a Mel-logarithmic spectrum approximation (MLSA) filter <ref type="bibr" target="#b49">[50]</ref> corresponding to the STRAIGHT Bark cepstrum, to create the final synthetic speech waveform. SS-LARGE-48: Same as SS-LARGE-16, except that 48 kHz sample rate waveforms are used for adaptation. The use of 48 kHz data is motivated by findings in speech synthesis that speaker similarity can be improved significantly by using data at a higher sampling rate <ref type="bibr" target="#b50">[51]</ref>.</p><p>SS-SMALL-16: Same as SS-LARGE-16, except that only Part A of the target speaker data is used for adaptation.</p><p>SS-SMALL-48: Same as SS-SMALL-16, except that 48 kHz sample rate waveforms are used to adapt the average voice.</p><p>SS-MARY: Based on the Mary-TTS 10 unit selection synthesis system <ref type="bibr" target="#b51">[52]</ref>. Waveform concatenation operates on diphone units. Candidate units for each position in the utterance are found using decision trees that query the linguistic features of the target diphone. A preselection algorithm is used to prune candidates that do not fit the context well. The target cost sums linguistic (target) and acoustic (join) costs. Candidate diphone and target diphone labels and their contexts are used to compute the linguistic sub-cost. Pitch and duration are used for the join cost. Dynamic programming is used to find the sequence of units with the minimum total target plus join cost. Concatenation takes place in the waveform domain, using pitch-synchronous overlap-add at unit boundaries.</p><p>VC-C1: The simplest voice conversion method, which modifies the spectral slope simply by shifting the first Mel-Generalised Cepstral coefficient (MGCs) <ref type="bibr" target="#b52">[53]</ref>. No other speaker-specific features are changed. The STRAIGHT 10 http://mary.dfki.de/ vocoder is used to extract MGCs, band aperiodicities (BAPs) and F 0 .</p><p>VC-EVC: A many-to-many eigenvoice conversion (EVC) system <ref type="bibr" target="#b53">[54]</ref>. The eigenvoice GMM (EV-GMM) is constructed from the training data of one pivot speaker in the ATR Japanese speech database <ref type="bibr" target="#b54">[55]</ref>, and 273 speakers (137 male, 136 female) from the JNAS database <ref type="foot" target="#foot_5">11</ref> . Settings are the same as in <ref type="bibr" target="#b55">[56]</ref>. The 272-dimensional weight vectors are estimated by using the Part A of the training data. STRAIGHT is used to extract 24-dimensional MGCs, 5 BAPs, and F 0 . The conversion function is applied only to the MGCs.</p><p>VC-FEST: The voice conversion toolkit provided by the open-source Festvox system. It is based on the algorithm proposed in <ref type="bibr" target="#b56">[57]</ref>, which is a joint density Gaussian mixture model with maximum likelihood parameter generation considering global variance. It is trained on the Part A set of parallel training data, keeping the default settings of the toolkit, except that the number of Gaussian components in the mixture distributions is set to 32.</p><p>VC-FS: A frame selection voice conversion system, which is a simplified version of exemplar-based unit selection <ref type="bibr" target="#b57">[58]</ref>, using a single frame as an exemplar and without a concatenation cost. We used the Part A set for training. The same features as in VC-C1 are used, and once again only the MGCs are converted.</p><p>VC-GMM: Another GMM-based voice conversion method very similar to VC-FEST but with some enhancements, which also uses the parallel training data from Part A. STRAIGHT is used to extract 24-dimensional MGCs, 5 BAPs, and F 0 . The search range for F 0 extraction is automatically optimized speaker by speaker to reduce errors. Two GMMs are trained for separately converting the 1 st through 24 th MGCs and 5 BAPs. The number of mixture components is set to 32 for MGCs and 8 for BAPs, respectively. GV-based post-filtering <ref type="bibr" target="#b58">[59]</ref> is used to enhance the variance of the converted spectral parameter trajectories.</p><p>VC-KPLS: Voice conversion using kernel partial least square (KPLS) regression <ref type="bibr" target="#b59">[60]</ref>, trained on the Part A parallel data. Three hundred reference vectors and a Gaussian kernel are used to derive kernel features and 50 latent components are used in the PLS model. Dynamic kernel features are not included, for simplicity. STRAIGHT is used to extract 24dimensional MGCs, 25 BAPs, and F 0 .</p><p>VC-TVC: Tensor-based arbitrary voice conversion (TVC) system <ref type="bibr" target="#b55">[56]</ref>. To construct the speaker space, the same Japanese dataset as in VC-EVC is used. The size of the weight matrices that represent each speaker is set to 48 × 80. The same part of the SAS database and the same features as in VC-EVC are used, and again only MGCs are converted, without altering other features.</p><p>VC-LSP: This system is also based on the standard GMMbased voice conversion method similar to VC-GMM using the parallel training data from Part A. STRAIGHT is used as the speech analysis-synthesis method. 24-dimensional line spectral pairs (LSPs) and their delta coefficients are used as the spectral features. A 16-component GMM is trained for the modelling of joint LSP feature vectors. For each component, the four blocks of its covariance matrix are set to be diagonal. No quality enhancement or post-filtering techniques are applied during the reconstruction of converted speech.</p><p>In addition to the above descriptions, for all the voice conversion approaches, F 0 is converted by a global linear transformation: simple mean-variance normalisation. In VC-KPLS, VC-EVC, VC-TVC, VC-FS and VC-C1, the source speaker BAPs are simply copied, without undergoing any conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speaker Verification and Countermeasure Evaluation Protocol</head><p>For the evaluation of ASV systems, enrolment data for each client (speaker) were selected from Part C under two conditions: 5-utterance or 50-utterance enrolments. For five utterances, this is about 5-10 seconds of speech while for 50 utterances it is about 1 minute of speech. The development set, used to tune the ASV system and decide thresholds, was taken from Part D and involves both genuine and impostor trials. All utterances from a client speaker in Part D were used as genuine trials, and this results in 1498 male and 1999 female genuine trials. For the impostor trials, ten randomly selected non-target speakers were used as impostors. All Part D utterances from a specific impostor were used as impostor trials against the client's model, leading to 12981 male and 17462 female impostor trials. The evaluation set is taken from Part E and is arranged into genuine and imposter trials in a similar way to the development set, with 4053 male and 5351 female genuine trials, and 32833 male and 46736 female impostor trials. A summary of the development and evaluation sets is shown in Table <ref type="table" target="#tab_2">II</ref>. We used the synthetic speech and voice conversion systems described above to generate artificial speech for both development and evaluation sets. During the execution of spoofing attacks, the transcript of an impostor trial was used as the textual input to each speech synthesis system, and the speech signal of the impostor trial was the input to each voice conversion system. As a result, the zero-effort impostor trial, the speech synthesis spoofed trial, and the voice conversion spoofed trial all have the same language content (i.e., word sequence). In this way, the number of spoofed trials of one spoofing system is exactly the same as the number of impostor trials presented in Table <ref type="table" target="#tab_2">II</ref>. This allows a fair comparison between non-spoofed and spoofed speaker verification results. Only five of the available spoofing systems were used during development, with all thirteen spoofing systems (Table <ref type="table" target="#tab_1">I</ref>) being run on the evaluation set. Hence, the number of total spoofed trials is 12981×5 and 17462×5 for males and females, respectively, for the development set, and 32833×13 and 46736×13 for male and female speakers, respectively, for the evaluation set. In the countermeasure evaluation protocol, we used a further 25 speakers' voices as training data and only implemented five attacks (as known attacks) on the training set. The 25 speakers do not appear in the development and evaluation sets for ASV, and this allows us to develop speaker-and gender-independent countermeasures. For countermeasure development and evaluation sets, the same speakers and same spoofed trials are used as those for ASV. This allows us to integrate countermeasures with ASV systems and to evaluate the integration performance. A summary of the countermeasure protocol is presented in Table <ref type="table" target="#tab_2">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPEAKER VERIFICATION SYSTEMS</head><p>We used three classical ASV systems: Gaussian Mixture Models with a Universal Background Model (GMM-UBM) <ref type="bibr" target="#b60">[61]</ref>, Joint Factor Analysis (JFA) <ref type="bibr" target="#b61">[62]</ref> and i-vector with Probabilistic Linear Discriminant Analysis (PLDA) <ref type="bibr" target="#b62">[63]</ref>. In this paper, we use PLDA to refer to this i-vector-PLDA system. Each system was implemented under the two enrolment scenarios: 5-utterance and 50-utterance enrolment. All systems used the same front-end to extract acoustic features: 19dimensional Mel-Frequency Cepstral Coefficients (MFCCs) plus log-energy with delta and delta-delta coefficients. By excluding the static energy feature (but retaining its delta and delta-delta), <ref type="bibr" target="#b58">59</ref>-dimensional feature vectors are obtained. To extract MFCCs, we applied a Hamming analysis window, the size of which is 25 ms with a 10-ms shift, and we employed a mel-filter bank with 24 channels. We note that C0 is not retained in the extracted MFCCs. In practice, the SPro toolkit <ref type="foot" target="#foot_6">12</ref>was used to extract MFCCs. The AudioSeg toolkit was used to perform voice activity detection (VAD) <ref type="bibr" target="#b63">[64]</ref>. GMM-UBM: with 512 Gaussian components in the UBM, and a client speaker model obtained by performing maximum a posteriori (MAP) adaptation, with the relevance factor set to 10. Only mean vectors were adapted, keeping diagonal covariance matrices and mixture weights the same as in the UBM.</p><p>JFA: using a UBM with the same 512 components as the GMM-UBM as well as eigenvoice and eigenchannel spaces with 300 and 100 dimensions, respectively. Cosine scoring was performed on the speaker variability vectors.</p><p>PLDA: a PLDA system operating in i-vector space. An i-vector is a low-dimensional vector to represent a speakerand channel-dependent GMM supervector M through a low rank matrix T , as M = m + T w, where m is a speakerand channel-independent supervector, which is realised by a UBM supervector in this work; T is also called the total variability matrix; and w is the i-vector. In this work, 400dimensional i-vectors were extracted with the maximum a posteriori (MAP) criterion and using the same UBM as the JFA system. Linear discriminant analysis (LDA) was first applied to reduce the i-vector dimension to 200. Then, ivectors were centred, length-normalised, and whitened. The whitening transformation was learned from i-vectors in the development set. After that, a Gaussian PLDA model was trained using the expectation-maximisation (EM) algorithm which was run for 20 iterations. The rank of the eigenspace (number of columns in the eigenmatrix) was set to 100. Scoring was done with a log-likelihood ratio test. In practice, the MSR Identity Toolbox <ref type="bibr" target="#b64">[65]</ref> was used to implement the PLDA system.</p><p>We used three WSJ databases (WSJ0, WSJ1, and WSJ-CAM) and the Resource Management database (RM1) for training the UBM, eigenspaces, and LDA. The statistics of the three databases are presented in Table <ref type="table" target="#tab_4">IV</ref>. The sampling rate of all four database is 16 kHz. We note that our preliminary experimental results suggested that WSJCAM was very useful for improving verification performance. The maximum likelihood criterion was employed to train the UBM and eigenspaces while the Fisher criterion was used to train LDA.</p><p>The 50 enrolment utterances were merged into 10 sessions (each being the concatenation of 5 utterances); either 1 or 10 of these sessions were used in enrolment, for the two enrolment scenarios. For PLDA, when using 10 enrolment sessions, ivectors were extracted from each session then averaged as suggested in <ref type="bibr" target="#b65">[66]</ref>; for JFA, all features from all sessions were merged. We denote the ASV systems with 5 enrolment utterances (presented as 1 session) as GMM-UBM-5, JFA-5 or PLDA-5 and those with 50 enrolment utterances (presented as 10 sessions) as GMM-UBM-50, JFA-50 or PLDA-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ANTI-SPOOFING COUNTERMEASURES</head><p>We now examine five countermeasures <ref type="foot" target="#foot_7">13</ref> , described below along with the features they are based on, and then propose a fusion of these countermeasures in order to learn complementary information and improve anti-spoofing performance.</p><p>Given a speech signal x(n), short-time Fourier analysis can be applied to transform the signal from the time domain to the frequency domain by assuming the signal is quasi-stationary within a short time frame, e.g., 25ms. The short-time Fourier transform of the speech signal can be represented as follows:</p><formula xml:id="formula_0">X(ω) = |X(ω)|e jϕ(ω) ,<label>(1)</label></formula><p>where X(ω) is the complex spectrum, |X(ω)| is the magnitude spectrum and ϕ(ω) is the phase spectrum. It is usual to define |X(ω)| 2 as the power spectrum, from which features that only contain magnitude information, e.g., MFCCs, can be derived. The proposed feature-based countermeasures are derived from the complex spectrum X(ω) that has two parts: a real part X R (ω) and an imaginary part X I (ω), and from which the phase spectrum ϕ(ω) can be obtained.</p><p>To extract frame-wise features, we employ a hamming window, the size of which is 25ms, with a 5ms shift. The FFT length is set to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cosine Normalised Phase Feature</head><p>Even though phase information is important in human speech perception <ref type="bibr" target="#b66">[67]</ref>, most speech synthesis and voice conversion systems use a simplified, minimum phase model which may introduce artefacts into the phase spectrum. The cosine normalised phase (CosPh) feature is derived from the phase spectrum, and can be used to discriminate between human and synthetic speech. The feature is computed as follows:</p><p>1) Unwrap the phase spectrum.</p><p>2) Compute the CosPh spectrum by applying the cosine function to the spectrum in 1) to normalise to [-1.0, 1.0]. 3) Apply a discrete cosine transform (DCT) to the spectrum in 2).</p><p>4) Keep the first 18 cepstral coeffcients, and compute their delta and delta-delta coeffcients as features. By normalizing the values of the unwrapped phase spectrum, we can simplify subsequent statistical modeling. We note that the motivation for applying the DCT is decorrelation and dimensionality reduction; C0 is not retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modified Group Delay Cepstral Feature</head><p>In addition to the artefacts in the phase spectrum, the statistical averaging inherent in parametric modeling of the magnitude spectrum may also introduce artefacts, such as oversmoothed spectral envelopes. The use of both phase and magnitude spectra can therefore be useful for detecting synthetic speech. The Modified Group Delay Cepstral Coefficients (MGDCCs) can be used to detect artefacts in both spectra of synthetic speech. The MGDCC feature has also been used in speech recognition <ref type="bibr" target="#b67">[68]</ref> and speaker verification <ref type="bibr" target="#b68">[69]</ref>. The MGDCCs are derived from the complex spectrum as follows:</p><p>1) Apply the fast Fourier transform (FFT) to a windowed speech signal, x(n) and nx(n) to compute X(ω) and Y (ω), respectively. Here nx(n) is the re-scaled signal of x(n). 2) Compute the cepstrally-smoothed power spectrum 14  |S(ω)| 2 of |X(ω)| 2 . 3) Compute the MGD spectrum (R and I denote the real and imaginary parts of the spectrum)</p><formula xml:id="formula_1">τ ρ (w) = X R (w)Y R (w) + Y I (w)X I (w) |S(w)| 2ρ .<label>(2)</label></formula><p>4) Reshape τ ρ (w) as</p><formula xml:id="formula_2">τ ρ,γ (w) = τ ρ (w) |τ ρ (w)| |τ ρ (w)| γ . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>5) Apply the DCT to τ ρ,γ (w) and keep the first 18 cepstral coefficients with their delta and delta-delta coefficients as MGDCC features. In ( <ref type="formula" target="#formula_1">2</ref>) and (3), ρ and γ are two weighting variables that control the shape of the MGD spectrum. We set ρ = 0.7 and γ = 0.2 based on the performance on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segment-Based Modulation Feature</head><p>In speech synthesis and voice conversion, the speech signal is usually divided into overlapping frames for modeling, and this frame-by-frame or state-by-state modeling may introduce artefacts in the temporal domain due to the independence assumptions made by the underlying statistical model. These temporal artefacts are evident in the modulation domain and can be used to detect synthetic and voice-converted speech. The Segment-based Modulation Feature (SMF) is extracted from the MGD cepstrogram based on our previous work <ref type="bibr" target="#b30">[31]</ref>. The procedure for computing the SMF is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> and described as follows: 14 Cepstrally-smoothed spectrum is obtained through the following steps: a) compute the log-amplitude spectrum from X(ω), and apply a median filter to smooth the spectrum; b) apply the DCT to the log spectrum and keep the first 30 cepstral coefficients; c) apply the inverse DCT to the cepstral coeffcients to obtain the cepstrally-smoothed spectrum S(ω).</p><p>1) Divide the 18-dimensional MGD spectrogram into overlapping segments using a 50-frame window with 20frame shift. 2) Apply mean and variance normalisation to the MGD trajectory of each dimension to make it have zero mean and unit variance <ref type="foot" target="#foot_8">15</ref> . 3) Take the FFT of the normalised 18-dimensional trajectories to compute modulation spectra. 4) Concatenate the modulation spectra in one cepstrogram segment into a supervector, and use this as the SMF feature vector. 5) Average all the SMF vectors of one utterance to get an average feature vector. This averaged feature vector will be used as the feature vector for the utterance. In practice, we used a 64-point FFT to extract a 32dimensional modulation spectrum for each MGD trajectory. Hence, the modulation supervector of each segment is 18 × 32 = 576. We pass this supervector to a support vector machine (SVM) for classification. In practice, we employed the LIBSVM toolkit <ref type="bibr" target="#b69">[70]</ref> to implement the SVM. We used a radial basis kernel, and set the penalty factor to 34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Utterance-Based Modulation Feature</head><p>To extract the segment-based modulation feature, a speech signal needs to be divided into short segments first and then the corresponding modulation features are extracted for each segment. An alternative approach is to extract modulation features at the utterance level, to obtain Utterance-based Modulation Features (UMFs).</p><p>The process to extract UMFs is similar to that of SMFs, but only steps 2 -4 are applied, without dividing the utterances into frames. In practice, we used a 1024-point FFT to extract the modulation spectrum for each MGD trajectory, then applied a DCT to the modulation spectrum, and after that kept the first 32 coefficients as features. Hence, the dimensionalities of UMF and SMF for each utterance are the same: 576. Again, we pass the feature vector to an SVM for classification. The configuration of the SVM here is the same as that for SMF in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pitch Pattern Feature</head><p>The prosody of synthetic speech is generally not the same as natural speech <ref type="bibr" target="#b70">[71]</ref> and therefore the pitch pattern is another good candidate feature for a countermeasure. The pitch pattern, φ[n, m], is calculated by dividing the short-range autocorrelation function, r[n, m] by a normalization function, p[n, m] which is proportional to the frame energy <ref type="bibr" target="#b71">[72]</ref> </p><formula xml:id="formula_4">φ[n, m] = r[n, m] p[n, m]<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">r[n, m] = m/2 k=-m/2 x[n + k -m/2]x[n + k + m/2],<label>(5)</label></formula><formula xml:id="formula_6">p[n, m] = 1 2 m/2 k=-m/2 x 2 [n + k -m/2] + 1 2 m/2 k=-m/2 x 2 [n + k + m/2],<label>(6)</label></formula><p>and n, m are the sample instant and lag, respectively, over which the autocorrelation is computed. The lag parameter is chosen such that pitch frequencies can be observed <ref type="bibr" target="#b71">[72]</ref>; in this work, we choose 32 ≤ m ≤ 320 for a sample rate of 16kHz.</p><p>Once the pitch pattern is computed, we segment it into a binary pitch pattern image through the rule</p><formula xml:id="formula_7">φ seg [n, m] = 1, φ[n, m] ≥ θ 0, φ[n, m] &lt; θ<label>(7)</label></formula><p>where θ is a threshold; we set θ = 1/ √ 2 for all n, based on preliminary results on the development set. An example pitch pattern image is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Extracting features from the pitch pattern is a two-step process: 1) computation of the pitch pattern; 2) image analysis. First, the pitch pattern is computed using (4) and segmented using <ref type="bibr" target="#b6">(7)</ref> to form a binary image. In the second step, image processing of the segmented binary pitch pattern is performed in order to extract the connected components (CCs), i.e., black regions in Fig. <ref type="figure" target="#fig_1">2</ref>. This processing includes determining the bounding box and area of a CC, which are then used to distinguish between two types of CC: pitch pattern connected components (PPCC) and irregularly-shaped components or artefacts.</p><p>The resulting CCs are then analysed and the mean pitch stability µ s , mean pitch stability range µ R , and time support (TS) of each CC are computed as in <ref type="bibr" target="#b28">[29]</ref>. The proposed image processing-based approach determines parameters on a perconnected component basis and then computes statistics over the connected components of the utterance. The six element utterance feature vector used for classification contains R and the TS of the artefacts, the number of artefacts, µ S and T S of the PPCC, and standard deviation of the TS of PPCC. Other utterance features were considered during the training and development stage but were found not to contribute to the classifier accuracy.</p><p>For the pitch pattern countermeasure, a maximum likelihood classifier based on the log-likelihoods computed from the utterance feature vectors was used for classification. During training, human and spoofing utterance feature vectors were modeled as multivariate Gaussian distributions with full covariance matrices. During testing, the utterance is determined to be human if the log-likelihood ratio is greater than a threshold calibrated to produce equal error rate (EER) on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Fused countermeasure</head><p>To benefit from the multiple feature-based countermeasures, we propose a fused countermeasure. In speaker verification, system fusion is one way to combine multiple individual speaker verification systems to achieve better performance <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>. A similar strategy can be applied for anti-spoofing, as each feature-based countermeasure discussed above has its own pros and cons. For example, the pitch pattern feature-based countermeasure is expected to work well in detecting waveform concatenation based spoofing attacks, while other countermeasures are expected to detect phase and temporal artefacts. It is expected the fused countermeasure can benefit from the pros of each individual countermeasure.</p><p>We perform linear fusion at the score level. We first train a linear fusion function on the development set which only contains known attacks, and then apply the fusion function on the evaluation scores; finally, the fused score is used to discriminate between human and spoofed speech. In practice, we used the BOSARIS Toolkit <ref type="foot" target="#foot_9">16</ref> to train the fusion function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metric</head><p>In both speaker verification and spoofing detection, there are two types of errors: 1) genuine or human speech is accepted as impostor or spoofed speech; 2) impostor or spoofed speech is accepted as genuine or human. The first type of error is a false rejection error, while the second type is a false acceptance. When the false acceptance rate (FAR) equals to the false rejection rate (FRR), we are at the equal error rate (EER) point. In this work, when reporting the false acceptance rates (FARs) and the false rejection rates (FRRs) for a specific spoofing algorithm, the decision threshold is set to achieve the EER operating point for that spoofing algorithm. When reporting overall spoofing performance, all the spoofed samples are pooled together and treated as one (unknown) spoofing algorithm when setting the threshold, because in practice one may not know the exact type of spoofing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spoofing ASV Systems without Countermeasures</head><p>We evaluated the performance of the ASV systems for the various synthetic speech and voice conversion variants described in Section II-A. Prior to the evaluation, the ASV decision threshold was set to the EER point on the development set, using only human speech.</p><p>Speaker verification results are presented in Table <ref type="table" target="#tab_5">V</ref>. The FARs for the baseline experiment, which uses only human speech, are low (as expected) because the SAS database has near-ideal recordings, free from channel and background noise. In particular, the lowest FARs for GMM, JFA and PLDA systems are 0.09%, 1.25% and 1.16%, respectively. Note that the short duration of the trials preculdes even lower FARs and FRRs.</p><p>Whilst the ASV systems achieve excellent verification performance, they are still vulnerable to spoofing. The simple VC-C1 spoofing attack, which only modifies the spectral slope of the source speaker, increases FAR for nearly every ASV system. The attacks using speech synthesis or voice conversion, with more advanced algorithms, lead to FARs as high as 99.95%. On average, speech synthesis leads to FARs of over 95% for male voices and over 80% for female voices, and more sophisticated voice conversion algorithms lead to FARs of close to 80% for both male and female voices. These observations are consistent with previous studies on clean speech <ref type="bibr" target="#b15">[16]</ref> and telephone quality speech <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and confirm the vulnerability of ASV systems to a diverse range of spoofing attacks. In general, our experiments suggest that it is easier to spoof male speakers than female speakers in the sense that the FARs for the various spoofing attacks for female speakers are generally lower than that for male speakers. We speculate that it is relatively harder to model female speech or perform female-to-female conversion due to the higher variability of female speech.</p><p>Although ASV systems that have more enrolment data available to them give lower FARs in the baseline case, they are not necessarily more resistant to spoofing attack. For example, under the VC-FEST attack, the FARs of JFA-5 and PLDA-5 male systems are 91.25% and 97.41%, respectively, and the FARs of JFA-50 and PLDA-50 are even higher at 97.71% and 99.54%, respectively. Similar patterns can be observed for other spoofing algorithms, as well as for female speech.</p><p>From the perspective of spoofing, the first interesting observation is that voice conversion is as effective at spoofing as speech synthesis, given the same amount of training data. Most of the speech synthesis systems used in this work require a large amount of data to train the average voice model, which is adapted to the target. On the other hand, most voice conversion algorithms, including VC-FEST, VC-GMM and VC-FS, only need source and target speech data to train their conversion functions. Voice conversion spoofing is sometimes even more effective than speech synthesis. It is worth highlighting that the publicly-available voice conversion toolkit VC-FEST is at least as effective as the other voice conversion and speech synthesis techniques.</p><p>The second interesting observation is that, although VC-TVC and VC-EVC use a Japanese database to train eigenvoices for adaptation to English data, these methods still increase FARs as much as the other variants. This suggests that attackers could use alternate speech resources, i.e. speech corpora in another language, if they cannot find enough resources for the target language.</p><p>The third observation is that the use of higher sampling rate training data in speech synthesis results in higher FARs of ASV systems. This suggests that such data includes more speaker-specific characteristics and that attackers can use this to conduct more effective spoofing if they have access to such data.</p><p>The last observation is that more training data can improve the effectiveness of speech synthesis and voice conversion spoofing systems. Comparing SS-SMALL-16k and SS-LARGE-16k, using 40 instead of 24 training utterances results in an increase of about 4% absolute FAR. In contrast, using more enrollment data for ASV systems does not seem to be helpful in defending against spoofing attacks (except VC-C1), although it does improve the baseline ASV performance without spoofing. We speculate that, as the spoofed speech sounds more like the target speaker, it will achieve higher likelihood scores under any target speaker model that has been trained using more enrollment data, and hence results in higher FARs. This also explains why ASV systems with more enrollment data succeed in defending against the VC-C1 attack, which can be easily distinguished by the human ear in terms of speaker similarity, as shown in Table <ref type="table" target="#tab_6">VIII</ref>.</p><p>Given the wide-ranging spoofing results in Table V and the above observations, it is clear that countermeasures are needed. So, we next present an evaluation and analysis of a range of countermeasures, including a proposed new fused countermeasure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of Stand-Alone Countermeasures</head><p>We conducted experiments to evaluate the performance of stand-alone countermeasures, i.e. their ability to discriminate between human and artificial speech. When training countermeasures, five of the spoofing systems listed in Table <ref type="table" target="#tab_1">I</ref>, were used: SS-SMALL-16, SS-LARGE-16, VC-C1, VC-FEST and VC-FS.</p><p>For MFC, CosPh, MGD and PP features, GMM-based maximum likelihood classifiers were employed, while for SMS and UMS features, SVM classifiers were used. Whilst many combinations of features and classifier could of course be imagined, these choices give us a representative range of countermeasures to compare. For each countermeasure, the detection threshold was set to achieve the EER point on the development set under all five known attacks, and then the countermeasure was applied to the evaluation set to compute the FARs shown in Table <ref type="table" target="#tab_6">VI</ref>. These results show that the frame-based features MFCC, CosPh and MGD achieve better performance than the long-term features SMS, UMS and PP. Even though the modulation features SMS and UMS are derived from the MGD features, they do not perform as well as frame-based MGD features. This observation is consistent with our previous work <ref type="bibr" target="#b30">[31]</ref>. In the database, due to the short duration of trials, long-term features generally only provide a rather small number of feature vectors per utterance.</p><p>In respect of the frame-based features, the MGD-based countermeasure achieves the best overall performance in terms of low FARs and works well at detecting most types of spoofed speech with the notable exception of the SS-MARY attack. The MGD features include both magnitude and phase spectrum information, whereas MFCCs only capture magnitude spectrum and CosPh only phase spectrum. With respect to long-term features, both SMS and UMS perform well at detecting statistical parametric speech synthesis spoofing, yet fail to detect most of the voice conversion algorithms or unit selection speech synthesis.</p><p>The pitch pattern countermeasure detects synthetic speech well, but does not detect some voice conversion speech such as that from VC-C1, VC-FEST, VC-KPLS and VC-LSP. This is probably due to the fact that speech synthesis usually predicts fundamental frequency (F0) from text (and so produces rather unnatural trajectories) whereas voice conversion usually copies a source speaker's F0 trajectories to generate a target speaker's voice. Hence, voice conversion introduces fewer pitch pattern artefacts than speech synthesis. We note that the pitch pattern countermeasure achieves the best performance of 1.96% FAR against the SS-MARY unit selection synthesis attack.</p><p>In general, most of the countermeasures achieve better performance for known attacks than for unknown attacks, as spoofing data from known attacks are available for training countermeasures and those from unknown attacks are not available to train the detectors. From the perspective of spoofing algorithms, SS-MARY is the most difficult to detect, and this is presumed to be due to the fact that it uses original waveforms to generate spoofed speech and thus introduces fewer artefacts when compared with other methods.</p><p>We also fused the six individual countermeasures at the score level to create a new countermeasure as detailed in Section IV-F. The linear combination weights for MFC, CosPh, MGD, SMS, UMS and PP countermeasures are 26.71, 9.56, 6.58, 0.53, -0.07 and 0.97, respectively. The results for this are presented in the last column of Table <ref type="table" target="#tab_6">VI</ref>. The fused countermeasure detects most spoofing attacks, achieving FARs under 1% against all but one spoofing method; it fails to detect SS-MARY. Although the PP countermeasure can discriminate extremely well between human and SS-MARY speech, this ability is not picked up by the fused countermeasure because PP has a low weight. This is because the weights were learned on the development set, which of course only contains known attacks (the first group of 5 countermeasures in Table <ref type="table" target="#tab_6">VI</ref>), but the PP countermeasure performs poorly on many of those known attacks, especially the voice conversion ones. Hence, it is given a low weight, and essentially ignored in the fused countermeasure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spoofing ASV Systems that Employ a Countermeasure</head><p>We conducted experiments to evaluate the overall performance of speaker verification systems that include a countermeasure. We only consider the proposed fused countermeasure here, because it exhibited better overall performance than any individual countermeasure. We integrated the fused countermeasure with each of the ASV systems as a postprocessing module -as illustrated in Fig. <ref type="figure">3</ref> -to reflect the practical use case in which a separately-developed standalone countermeasure is added to an already-deployed ASV system <ref type="bibr" target="#b15">[16]</ref> without significant modification of that system. Reject claimed identity Fig. <ref type="figure">3</ref>. A speaker verification system with an integrated countermeasure.</p><p>The integrated system only accepts a claimed identity if it is accepted by the speaker verification system and classified as human speech by the countermeasure <ref type="bibr" target="#b15">[16]</ref>.</p><p>A good countermeasure should reduce FARs by rejecting non-human speech. The FAR results of systems with an integrated countermeasure are presented in Table <ref type="table" target="#tab_6">VII</ref>. Comparing against the FARs of the ASV systems without a countermeasure in Table <ref type="table" target="#tab_5">V</ref>, we can make the following observations. First, the FARs of all ASV systems are reduced dramatically for both male and female speech, and go down from about 70%-100% to below 1% in the face of most types of spoofing attack. This indicates that the fused countermeasure can be effectively integrated with any ASV system without needing additional joint optimisation. Second, the integrated system is robust against attacks from various state-of-the-art statistical parametric speech synthesis and voice conversion systems. However, it is still vulnerable to the unit selection synthesis (SS-MARY) spoofing attack. This suggests that new countermeasures are needed specifically for waveform selectionbased spoofing attacks. Third, although our stand-alone ASV systems achieve better performance for male than for female speakers, the integrated systems work equally well for both. In contrast, others have reported integrated systems working better for male speakers than for female speakers <ref type="bibr" target="#b39">[40]</ref>.</p><p>In general, by using the proposed fused countermeasure, the FARs of ASV systems under spoofing attack are reduced significantly. This indicates that the countermeasure is effective in detecting spoofing attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Human Versus Machine</head><p>To complement the comparisons already presented, we now benchmark automatic (machine-based) methods against speaker verification by human listeners. To do this, three listening tests were conducted: two speaker verification tasks and one spoofing detection task. The first verification task contained only human speech signals, the second verification task contained human speech but all test signals were artificial (synthetic and voice-converted speech). The third task, a detection task, contained both human and artificial speech signals and the goal for the listener was to correctly discriminate these signals. All three tasks covered the 46 target speakers in the evaluation set of the SAS corpus.</p><p>In order to encourage listeners to engage with the tasks to the best of their ability, they were presented as role-play scenarios. The human listening tasks were designed to be as similar to the ASV tasks as possible (to facilitate direct comparisons), whilst taking into account listener constraints such as fatigue or memory limitations. Listening protocols were inspired by the ones used in <ref type="bibr" target="#b74">[75]</ref> and the experiments were carried out via a web browser. In total, 100 native English listeners took part in the experiments. They were seated in sound-isolated booths and listened to all samples using Beyerdynamic DT 770 PRO headphones. Each listener performed three tasks and, on average, it took about an hour to complete the experiment. We only report results from listeners who completed all sessions in each task.</p><p>Task 1: Speaker Verification of Human Speech: In the speaker verification task, listeners were asked to imagine they were responsible for giving people access to their bank accounts. They were informed that they would only have a short recording of a person's voice to base their judgement on. It was stressed that it was important to not give access to "impostors" but equally important that access was given to the "bank account holder".</p><p>The listeners were given five sentences from each target speaker to familiarise themselves with the voice. After listening to the training samples, they were given 21 trials to judge as "same" or "different." The trials were of samples which include a reference and a test sample. This was repeated for three different target speakers. In this task, each target speaker was judged by 5 listeners. The number of targets versus non-targets varied per speaker to keep listeners from keeping count for individual speakers. On average there were 10 targets and 11 non-targets per speaker. Genders were not mixed within a trial.</p><p>Listeners recognised impostors as genuine targets 2.39% of the time (FAR) while 9.38% of genuine trials were misclassified as impostors (FRR). Comparing with the baseline ASV performance in Table <ref type="table" target="#tab_5">V</ref>, the results demonstrate that the speaker verification performance of humans is not as good as that of the best automatic systems. For example, PLDA-5 gives a FAR around 1.5% for both male and female speakers. This finding is similar to that in <ref type="bibr" target="#b75">[76]</ref> for the NIST SRE 2008 dataset.</p><p>Task 2: Speaker Verification of Artificial Speech: In the second task, listeners were asked to decide whether an artificial voice <ref type="foot" target="#foot_10">17</ref> sounded like the original speaker's voice. The listeners informed that the artificial voice would sometimes sound quite degraded but were asked to ignore the degradations as much as possible. Additionally, they were told that there would be artificial voices that were supposed to sound like the intended speaker as well as artificial voices that were not supposed to match the original speaker. The task was framed as "Your challenge is to decide which of the artificial voices are based on the 'bank account holder's voice' and which are based on an 'impostor's voice.' "</p><p>As in the first task, listeners were given five natural speech samples from the intended speaker to familiarise themselves with the voice. After listening to these training samples, subjects were presented with pairs of reference and test samples to judge as "same" or "different." It was made clear to the listeners that the test sample would be an artificial voice. Each target speaker was judged by 5 listeners. For each target speaker there were 65 trials (13 systems, each presented 5 times). On average there were 39 targets and 26 non-targets per speaker. Once again gender was not mixed within any of the trials.</p><p>The results are presented in Table <ref type="table" target="#tab_6">VIII</ref> (second column). The acceptance rate is not directly comparable with the automatic results presented in Table V but the relative differences across spoofing algorithms are comparable <ref type="foot" target="#foot_11">18</ref> .</p><p>It can be observed that SS-MARY gives the highest acceptance rate (i.e., listeners said that it sounded like the original speaker), while VC-C1 gives the lowest acceptance rate -this pattern is similar to that in the ASV results where SS-MARY achieves relatively high FARs and VC-C1 relatively low FARs. The results also indicate that spoofing systems that use more training data generally achieve higher acceptance rates with human listeners, mirroring what we saw earlier in the ASV results in Section V-B. An interesting difference between the ASV and human listener results is that, for human listeners, the use of higher sampling rate speech by some spoofing systems (SS-SMALL-48, SS-LARGE-48) leads to a lower acceptance rate than for lower sampling rate training data (SS-SMALL-16, SS-LARGE-16). This suggests that, whilst these types of spoofing systems (SS: statistical parametric speech synthesis) are able to generate information above 8 kHz that contributes to improved naturalness <ref type="bibr" target="#b50">[51]</ref>, listeners judge it as being more dissimilar to the natural speaker. This similarity observation is different from that in <ref type="bibr" target="#b50">[51]</ref>, where speaker-dependent speech synthesis is examined. An informal listening test gives the impression that SS-LARGE-48/SS-SMALL-48 produces more natural speech than SS-LARGE-16/SS-SMALL-16, as expected. However, as the reference target speech is a clean recording without any distortion, we speculate that it is more challenging for listeners to decide on the speaker similarity of the poor quality, buzzy-sounding speech of SS-LARGE-16/SS-SMALL-16.</p><p>Task 3: Detection: In the final task, listeners were asked to judge whether a speech sample was a recording of a human voice, or a sample of an artificial voice. The challenge to the listeners was formulated as: "Imagine an impostor trying to gain access to a bank account by mimicking a person's voice using speech technology. You must not let this happen. Your challenge in this final section is to correctly tell whether or not the sample is of a human or of a machine."</p><p>Listeners were again given some training speech signals. They listened to five samples of human speech from one speaker (not present in the detection task) and five examples of artificial speech generated using five known spoofing systems.  <ref type="table" target="#tab_6">VI</ref>. For most spoofing systems, the automatic approaches give FARs below 1%, while human listeners have FARs above 4%. However, humans are much better than any of the automatic countermeasures (except PP) in detecting SS-MARY. Most of the countermeasures exhibit FARs in excess of 80% for SS-MARY, while the FAR of human listeners is only 8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION AND FUTURE WORK</head><p>In this section, we summarise the findings in this work, and also discuss some of its limitations. Both the findings and the limitations suggest areas needing further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Research Findings</head><p>The main findings from this study are:</p><p>• All three classical ASV systems: GMM-UBM, JFA and PLDA systems are vulnerable to all the spoofing methods considered, with the exception of VC-C1. This confirms the findings of previous studies that only considered one or two spoofing algorithms. This also shows the importance of developing spoofing countermeasures to secure ASV systems. • The effectiveness of speech synthesis and voice conversion spoofing are comparable. Previous studies employed various databases for each attack which made direct comparisons of effectiveness across attacks difficult or impossible. The standardised protocol that we propose here, using our SAS database, allows direct comparisons. • When higher sampling rate and/or more training data are available to train spoofing systems, FARs of ASV systems increase significantly, as expected. This indicates that ASV systems are more vulnerable to attackers who have access to better quality and/or greater quantity of training data. • Generally, the spoofing countermeasures proposed in this work perform well in detecting statistical parametric speech and voice conversion attacks. However, they mostly fail to detect rather straightforward waveform concatenation, as in the case of the SS-MARY attack.</p><p>Because SS-MARY directly concatenates waveforms in the time-domain, the resulting spoofed speech has no distortions in the phase domain (except perhaps at the concatenation points); so, phase-based countermeasures are not a good way to detect such a spoofing attack. • ASV systems have reached a point where they routinely outperform ordinary humans <ref type="foot" target="#foot_12">19</ref> on speaker recognition and spoofing detection tasks. However, humans are still better able to detect waveform concatenation. An obvious practical approach at the current time, for example in callcentre applications, would be to combine the decisions of both human and automatic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations and Future Directions</head><p>We suggest future work in ASV spoofing and countermeasures along the following lines:</p><p>• More diverse spoofing materials: The current SAS database is biased towards the STRAIGHT vocoder, and only one type of unit selection system was used to generate the waveform concatenation materials. Moreover, replay attack -which does not require any speech processing knowledge on the part of the attacker -was not considered here. A generalised countermeasure should be robust against all spoofing algorithms and any vocoder. The development of generalised countermeasures might be accelerated by collecting more diverse spoofing materials. As the amount of spoofing materials increases, ASV systems can access more representative prior information about spoofing, and the security of ASV systems should be enhanced as a result. • Truly generalised countermeasures: The proposed countermeasures did not generalise well to unknown attacks, and in particular to the SS-MARY attack. This is because the proposed countermeasures were biased towards detecting phase artefacts. To detect the SS-MARY attack or similar waveform concatenation attacks, we suggest further development of pitch pattern-based countermeasures. Discontinuity detection for concatenative speech synthesis <ref type="bibr" target="#b76">[77]</ref> might also be useful in inspiring novel countermeasures against such attacks. Lastly, novel system fusion methods might also be a way to implement generalised countermeasures. A good fusion method should be able to benefit from all the individual countermeasures. Our proposed fusion method failed to take advantage of the strengths of the pitch pattern countermeasure, for example.</p><p>• Noise or channel robustness: The work here delberately focussed on clean speech without significant noise or channel effects. To make the proposed countermeasures appropriate for practical applications, it would of course be important to take channel and noise issues into consideration. • Text-dependent ASV: The current work assumes textindependent speaker verification. To make systems suitable for other voice authentication applications, spoofing countermeasures for text-dependent ASV must also be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>All existing literature that we are aware of in the areas of ASV spoofing and anti-spoofing, report results for just one or two spoofing algorithms, and generally assumes prior knowledge of the spoofing algorithm(s) in order to implement matching countermeasures. As discussed in <ref type="bibr" target="#b7">[8]</ref>, the lack of a large-scale, standardised dataset and protocol was a fundamental barrier to progress in this area. We hope that this situation is now rectified, by our release of the standard dataset SAS, combined with the benchmark results presented in this paper.</p><p>To acheive this, speech synthesis, voice conversion, and speaker verification researchers worked together to develop state-of-the-art systems from which to generate spoofing materials, and thus to develop countermeasures. The SAS corpus developed in this work is publicly released under a CC-BY license <ref type="bibr" target="#b77">[78]</ref>. We hope that the availability of the SAS corpus will facilitate reproducible research and as a consequence drive forward the development of novel generalised countermeasures against speaker verification system spoofing attacks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The process to extract Segment-based Modulation Features (SMFs) from modified group delay cepstral features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>Example binary pitch pattern image illustrating pitch stability Sc, pitch stability range Rc, upper edge τ U , lower edge τ L , connected component time support, and artefacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF THE SPOOFING SYSTEMS USED IN THIS PAPER. MGC, BAP AND F 0 MEAN MEL-GENERALISED CEPSTRAL (MGC) COEFFICIENTS, BAND APERIODICITY (BAP) AND FUNDAMENTAL FREQUENCY (F 0 ).</figDesc><table><row><cell></cell><cell>Spoofing</cell><cell>Sampling</cell><cell># training</cell><cell></cell><cell></cell><cell>Background</cell><cell>Known or</cell><cell>Open source</cell></row><row><cell></cell><cell>Algorithm</cell><cell>Rate</cell><cell>utterances</cell><cell>Vocoder</cell><cell>Features</cell><cell>data required?</cell><cell>Unknown?</cell><cell>Toolkit?</cell></row><row><cell>SS-LARGE-16</cell><cell>HMM TTS</cell><cell>16k</cell><cell>40</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Known</cell><cell>Yes</cell></row><row><cell>SS-LARGE-48</cell><cell>HMM TTS</cell><cell>48k</cell><cell>40</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Unknown</cell><cell>Yes</cell></row><row><cell>SS-SMALL-16</cell><cell>HMM TTS</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Known</cell><cell>Yes</cell></row><row><cell>SS-SMALL-48</cell><cell>HMM TTS</cell><cell>48k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Unknown</cell><cell>Yes</cell></row><row><cell>SS-MARY</cell><cell>Unit Selection TTS</cell><cell>16k</cell><cell>40</cell><cell>None</cell><cell>Waveform</cell><cell>No</cell><cell>Unknown</cell><cell>Yes</cell></row><row><cell>VC-C1</cell><cell>C1 VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>No</cell><cell>Known</cell><cell>No</cell></row><row><cell>VC-EVC</cell><cell>Eigenvoice VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Unknown</cell><cell>No</cell></row><row><cell>VC-FEST</cell><cell>GMM VC</cell><cell>16k</cell><cell>24</cell><cell>MLSA</cell><cell>MGC, F 0</cell><cell>No</cell><cell>Known</cell><cell>Yes</cell></row><row><cell>VC-FS</cell><cell>Frame selection VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>No</cell><cell>Known</cell><cell>No</cell></row><row><cell>VC-GMM</cell><cell>GMM VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>No</cell><cell>Unknown</cell><cell>No</cell></row><row><cell>VC-KPLS</cell><cell>KPLS VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>No</cell><cell>Unknown</cell><cell>No</cell></row><row><cell>VC-LSP</cell><cell>GMM VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>LSP, F 0</cell><cell>No</cell><cell>Unknown</cell><cell>No</cell></row><row><cell>VC-TVC</cell><cell>Tensor VC</cell><cell>16k</cell><cell>24</cell><cell>STRAIGHT</cell><cell>MGC, BAP, F 0</cell><cell>Yes</cell><cell>Unknown</cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II NUMBER</head><label>II</label><figDesc>OF TRIALS IN THE DEVELOPMENT AND EVALUATION SETS.</figDesc><table><row><cell></cell><cell cols="2">Development</cell><cell cols="2">Evaluation</cell></row><row><cell></cell><cell>Male</cell><cell>Female</cell><cell>Male</cell><cell>Female</cell></row><row><cell>Target speakers</cell><cell>15</cell><cell>20</cell><cell>20</cell><cell>26</cell></row><row><cell>Genuine trials</cell><cell>1498</cell><cell>1999</cell><cell>4053</cell><cell>5351</cell></row><row><cell>Impostor trials</cell><cell>12981</cell><cell>17462</cell><cell>32833</cell><cell>46736</cell></row><row><cell>Spoofed trials</cell><cell>12981×5</cell><cell>17462×5</cell><cell>32833×13</cell><cell>46736×13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV STATISTICS</head><label>IV</label><figDesc>OF WALL STREET JOURNAL (WSJ0, WSJ0, WSJCAM) AND RESOURCE MANAGEMENT (RM) DATABASES USED TO TRAIN UBM AND</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">EIGENSPACES.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">WSJ0+WSJ1</cell><cell></cell><cell>RM</cell><cell cols="2">WSJCAM</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell>Male</cell><cell>Female</cell><cell>Male</cell><cell>Female</cell><cell>Male</cell><cell>Female</cell><cell>Male</cell><cell>Female</cell></row><row><cell>Speakers</cell><cell>149</cell><cell>152</cell><cell>108</cell><cell>52</cell><cell>76</cell><cell>59</cell><cell>333</cell><cell>264</cell></row><row><cell>Utterances</cell><cell>14900</cell><cell>15199</cell><cell>7881</cell><cell>3982</cell><cell>6697</cell><cell>5148</cell><cell>29478</cell><cell>24329</cell></row><row><cell>Hours</cell><cell>∼26.3</cell><cell>∼27.9</cell><cell>∼5.6</cell><cell>∼2.9</cell><cell>∼12.1</cell><cell>∼9.5</cell><cell>∼44</cell><cell>∼40.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V FALSE</head><label>V</label><figDesc>ACCEPTANCE RATES (FARS) IN %, ON THE EVALUATION SET FOR THE TWO VARIANTS (-5 AND -50) OF THREE SPEAKER VERIFICATION SYSTEMS BASED ON: A GAUSSIAN MIXTURE MODEL WITH UNIVERSAL BACKGROUND MODEL (GMM-UBM); JOINT FACTOR ANALYSIS (JFA); AND PROBABILISTIC LINEAR DISCRIMINANT ANALYSIS (PLDA). THE DECISION THRESHOLD IS SET TO THE EQUAL ERROR RATE (EER) POINT ON THE DEVELOPMENT SET.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Male</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Female</cell><cell></cell><cell></cell></row><row><cell></cell><cell>GMM-</cell><cell>GMM-</cell><cell>JFA-</cell><cell>JFA-</cell><cell>PLDA-</cell><cell>PLDA-</cell><cell>GMM-</cell><cell>GMM-</cell><cell>JFA-</cell><cell>JFA-</cell><cell>PLDA-</cell><cell>PLDA-</cell></row><row><cell>Spoofing</cell><cell>UBM-5</cell><cell>UBM-50</cell><cell>5</cell><cell>50</cell><cell>5</cell><cell>50</cell><cell>UBM-5</cell><cell>UBM-50</cell><cell>5</cell><cell>50</cell><cell>5</cell><cell>50</cell></row><row><cell>Baseline</cell><cell>4.05</cell><cell>0.09</cell><cell>2.76</cell><cell>1.25</cell><cell>1.41</cell><cell>1.16</cell><cell>11.10</cell><cell>0.66</cell><cell>6.24</cell><cell>2.47</cell><cell>1.52</cell><cell>0.99</cell></row><row><cell>SS-LARGE-16</cell><cell>79.86</cell><cell>97.86</cell><cell>88.62</cell><cell>96.17</cell><cell>93.45</cell><cell>97.76</cell><cell>90.13</cell><cell>89.34</cell><cell>84.31</cell><cell>84.65</cell><cell>86.04</cell><cell>95.95</cell></row><row><cell>SS-LARGE-48</cell><cell>97.35</cell><cell>99.95</cell><cell>97.62</cell><cell>98.93</cell><cell>99.12</cell><cell>99.09</cell><cell>98.52</cell><cell>99.28</cell><cell>90.58</cell><cell>94.28</cell><cell>94.80</cell><cell>98.39</cell></row><row><cell>SS-MARY</cell><cell>86.57</cell><cell>99.39</cell><cell>91.09</cell><cell>96.81</cell><cell>96.77</cell><cell>98.74</cell><cell>95.23</cell><cell>99.17</cell><cell>91.37</cell><cell>95.11</cell><cell>95.28</cell><cell>98.10</cell></row><row><cell>SS-SMALL-16</cell><cell>75.65</cell><cell>91.62</cell><cell>83.64</cell><cell>91.25</cell><cell>89.21</cell><cell>94.87</cell><cell>86.49</cell><cell>81.72</cell><cell>80.60</cell><cell>77.97</cell><cell>81.60</cell><cell>93.14</cell></row><row><cell>SS-SMALL-48</cell><cell>95.63</cell><cell>98.89</cell><cell>94.97</cell><cell>95.75</cell><cell>97.07</cell><cell>96.63</cell><cell>97.44</cell><cell>97.63</cell><cell>86.46</cell><cell>90.36</cell><cell>93.02</cell><cell>96.86</cell></row><row><cell>VC-C1</cell><cell>4.78</cell><cell>0.11</cell><cell>2.62</cell><cell>1.46</cell><cell>1.83</cell><cell>1.67</cell><cell>17.68</cell><cell>1.94</cell><cell>12.71</cell><cell>6.80</cell><cell>3.92</cell><cell>3.56</cell></row><row><cell>VC-EVC</cell><cell>50.64</cell><cell>56.63</cell><cell>43.38</cell><cell>58.52</cell><cell>69.84</cell><cell>79.50</cell><cell>71.60</cell><cell>67.45</cell><cell>67.82</cell><cell>66.50</cell><cell>72.14</cell><cell>79.10</cell></row><row><cell>VC-FEST</cell><cell>79.67</cell><cell>98.29</cell><cell>91.25</cell><cell>97.71</cell><cell>97.41</cell><cell>99.54</cell><cell>91.30</cell><cell>94.39</cell><cell>85.77</cell><cell>91.76</cell><cell>86.11</cell><cell>93.53</cell></row><row><cell>VC-FS</cell><cell>79.12</cell><cell>98.65</cell><cell>78.68</cell><cell>91.62</cell><cell>91.05</cell><cell>96.16</cell><cell>86.61</cell><cell>94.77</cell><cell>71.19</cell><cell>75.32</cell><cell>79.37</cell><cell>90.33</cell></row><row><cell>VC-GMM</cell><cell>76.03</cell><cell>97.35</cell><cell>89.14</cell><cell>96.22</cell><cell>95.10</cell><cell>98.70</cell><cell>91.94</cell><cell>97.53</cell><cell>85.72</cell><cell>92.93</cell><cell>90.57</cell><cell>97.42</cell></row><row><cell>VC-KPLS</cell><cell>59.60</cell><cell>72.76</cell><cell>61.92</cell><cell>82.90</cell><cell>81.17</cell><cell>89.31</cell><cell>77.30</cell><cell>72.96</cell><cell>70.99</cell><cell>71.72</cell><cell>80.87</cell><cell>86.32</cell></row><row><cell>VC-LSP</cell><cell>57.98</cell><cell>71.57</cell><cell>51.71</cell><cell>68.37</cell><cell>74.99</cell><cell>89.82</cell><cell>75.26</cell><cell>75.44</cell><cell>65.30</cell><cell>60.27</cell><cell>70.99</cell><cell>75.14</cell></row><row><cell>VC-TVC</cell><cell>58.64</cell><cell>70.94</cell><cell>63.28</cell><cell>78.75</cell><cell>80.20</cell><cell>87.14</cell><cell>77.16</cell><cell>75.37</cell><cell>70.87</cell><cell>71.41</cell><cell>74.83</cell><cell>82.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI SPOOFING</head><label>VI</label><figDesc>COUNTERMEASURE RESULTS IN TERMS OF FALSE ACCEPTANCE RATE (FAR) IN % ON THE EVALUATION SET. THE DECISION THRESHOLD IS SET TO THE EER POINT ON THE DEVELOPMENT SET. THE FIRST GROUP OF 5 ATTACK METHODS IS KNOWN AND THE REMAINING 8</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ARE UNKNOWN.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MFC</cell><cell>CosPh</cell><cell>MGD</cell><cell>SMS</cell><cell>UMS</cell><cell>PP</cell><cell>Fusion</cell></row><row><cell>SS-SMALL-16</cell><cell>0.01</cell><cell>1.41</cell><cell>0.10</cell><cell>5.31</cell><cell>8.43</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SS-LARGE-16</cell><cell>0.01</cell><cell>1.03</cell><cell>0.11</cell><cell>5.44</cell><cell>8.10</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>VC-C1</cell><cell>27.08</cell><cell>0.44</cell><cell>4.07</cell><cell>45.20</cell><cell>33.70</cell><cell>68.73</cell><cell>0.80</cell></row><row><cell>VC-FEST</cell><cell>0.84</cell><cell>21.89</cell><cell>4.61</cell><cell>37.76</cell><cell>39.75</cell><cell>60.56</cell><cell>0.43</cell></row><row><cell>VC-FS</cell><cell>0.10</cell><cell>0.04</cell><cell>0.07</cell><cell>4.18</cell><cell>4.80</cell><cell>7.66</cell><cell>0.00</cell></row><row><cell>SS-LARGE-48</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell><cell>0.62</cell><cell>0.46</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SS-MARY</cell><cell>89.30</cell><cell>92.76</cell><cell>93.92</cell><cell>81.81</cell><cell>87.91</cell><cell>1.96</cell><cell>97.76</cell></row><row><cell>SS-SMALL-48</cell><cell>0.00</cell><cell>0.01</cell><cell>0.00</cell><cell>0.71</cell><cell>0.43</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>VC-EVC</cell><cell>2.72</cell><cell>0.01</cell><cell>1.87</cell><cell>23.28</cell><cell>4.18</cell><cell>0.00</cell><cell>0.02</cell></row><row><cell>VC-GMM</cell><cell>1.61</cell><cell>19.68</cell><cell>4.37</cell><cell>37.93</cell><cell>33.08</cell><cell>9.86</cell><cell>0.79</cell></row><row><cell>VC-KPLS</cell><cell>1.15</cell><cell>0.06</cell><cell>0.54</cell><cell>21.08</cell><cell>7.56</cell><cell>68.64</cell><cell>0.00</cell></row><row><cell>VC-LSP</cell><cell>4.86</cell><cell>0.03</cell><cell>0.84</cell><cell>56.93</cell><cell>19.46</cell><cell>73.26</cell><cell>0.15</cell></row><row><cell>VC-TVC</cell><cell>2.97</cell><cell>0.02</cell><cell>1.58</cell><cell>23.11</cell><cell>5.31</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>known</cell><cell>5.61</cell><cell>4.96</cell><cell>1.79</cell><cell>19.58</cell><cell>18.95</cell><cell>27.39</cell><cell>0.25</cell></row><row><cell>unknown</cell><cell>12.83</cell><cell>14.07</cell><cell>12.89</cell><cell>30.68</cell><cell>19.80</cell><cell>19.22</cell><cell>12.34</cell></row><row><cell>all attacks</cell><cell>10.05</cell><cell>10.57</cell><cell>8.62</cell><cell>26.41</cell><cell>19.47</cell><cell>22.36</cell><cell>7.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>At this point, the listeners were informed that the training samples did not cover all possible types of artificial speech. In Task 3, there were 130 samples (65 human, 65 artificial (13 × 5)), and those samples were randomly selected from the evaluation set for each listener. 84 listeners participated in the test.The human detection results are presented in Table VIII (third column). In general, human listeners detect spoofing less well than most of the automatic approaches presented in Table</figDesc><table><row><cell></cell><cell>TABLE VIII</cell><cell></cell></row><row><cell cols="3">TASK 2 -SPEAKER VERIFICATION (ARTIFICIAL)-AND TASK 3 -SPOOFING</cell></row><row><cell></cell><cell cols="2">DETECTION-RESULTS.</cell></row><row><cell></cell><cell>Task 2:</cell><cell>Task 3:</cell></row><row><cell></cell><cell>Speaker Verification</cell><cell>Spoofing Detection</cell></row><row><cell></cell><cell>Acceptance rate</cell><cell>Detection Error Rate</cell></row><row><cell>Human</cell><cell>-</cell><cell>11.94</cell></row><row><cell>SS-SMALL-16</cell><cell>35.33</cell><cell>5.48</cell></row><row><cell>SS-SMALL-48</cell><cell>32.19</cell><cell>7.86</cell></row><row><cell>SS-LARGE-16</cell><cell>39.46</cell><cell>5.71</cell></row><row><cell>SS-LARGE-48</cell><cell>36.18</cell><cell>8.10</cell></row><row><cell>SS-MARY</cell><cell>76.07</cell><cell>8.10</cell></row><row><cell>VC-GMM</cell><cell>30.63</cell><cell>13.57</cell></row><row><cell>VC-KPLS</cell><cell>29.06</cell><cell>6.90</cell></row><row><cell>VC-TVC</cell><cell>20.51</cell><cell>5.48</cell></row><row><cell>VC-EVC</cell><cell>21.23</cell><cell>8.10</cell></row><row><cell>VC-FS</cell><cell>35.47</cell><cell>4.29</cell></row><row><cell>VC-C1</cell><cell>7.26</cell><cell>23.81</cell></row><row><cell>VC-FEST</cell><cell>28.63</cell><cell>6.90</cell></row><row><cell>VC-LSP</cell><cell>23.36</cell><cell>7.38</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.nuance.com/for-business/customer-service-solutions/ voice-biometrics/freespeech/index.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://youtu.be/kyPTGoDyd o</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.cstr.ed.ac.uk/projects/festival/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://festvox.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>http://www.milab.is.tsukuba.ac.jp/jnas/instruct.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>Available at: http://www.irisa.fr/metiss/guig/spro/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>The cosine normalised phase feature, modified group delay cepstral feature, segment-based modulation feature and pitch pattern feature based countermeasures have been presented in our previous conference papers<ref type="bibr" target="#b34">[35]</ref>,<ref type="bibr" target="#b30">[31]</ref>,<ref type="bibr" target="#b29">[30]</ref>. The current study examines the generalisation abilities of each individual countermeasure and their combination in the face of various spoofing attacks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>The motivation to perform mean-variance normalisation is to make the trajectory of each dimension in the same scale.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_9"><p>https://sites.google.com/site/bosaristoolkit/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_10"><p>Artificial was explained to the listeners as being "produced by a machine, computer-generated, for example a synthetic voice".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_11"><p>In Task 2, the acceptance rate means the percentage of genuine speech recognised as the original speaker. The genuine speech is artificial speech using the target speaker's voice as the reference for adaptation or voice conversion, and the impostor speech is also artificial speech but using a nontarget speakers voice as the reference for adaptation or voice conversion. When computing the acceptance rate, zero is used as the threshold. On the other hand, the FAR in TableIVis the percentage of spoofed trials accepted as genuine. When computing the FAR, the threshold is determined at the EER point on the non-spoofed trials.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_12"><p>It would be interesting in the future to use either 'super recognisers' or forensic speech scientists, if we could access sufficient numbers of such listeners.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGEMENTS</head><p>The authors would like to thank: Dr. Tomi Kinnunen and Dr. Nicolas Evans for their valuable comments which have improved the speaker verification and countermeasure protocols; Dr. Ling-Hui Chen and Ms. Li-Juan Liu for their assistance in generating spoofing materials; the three anonymous reviewers for their valuable comments and suggestions to improve the manuscript.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by EPSRC under Programme Grant EP/I031022/1 (Natural Speech Technology) and EP/J002526/1 (CAF) and by TUBITAK 1001 grant No 112E160. This article is an expanded version of <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SAS: A speaker verification spoofing database containing diverse attacks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khodabakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human vs machine spoofing detection on wideband and narrowband data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voice biometrics-the Asia Pacific experience</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometric Technology Today</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="10" to="11" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Talking passwords: voice biometrics for data access and security</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khitrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometric Technology Today</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="11" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voice biometrics: success stories, success factors and what&apos;s next</title>
		<author>
			<persName><forename type="first">B</forename><surname>Beranek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometric Technology Today</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9" to="11" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker verification makes its debut in smartphone</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society Speech and language Technical Committee Newsletter</title>
		<imprint>
			<date type="published" when="2013-02">February 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Surveying the development of biometric user authentication on mobile phones</title>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE Communications Surveys and Tutorials</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spoofing and countermeasures for speaker verification: a survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="130" to="153" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vulnerability of speaker verification to voice mimicking</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Intelligent Multimedia, Video and Speech Processing</title>
		<meeting>Int. Symposium on Intelligent Multimedia, Video and Speech essing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">I-vectors meet imitators: on vulnerability of speaker verification systems against voice mimicry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Laukkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic versus human speaker verification: The case of voice mimicry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Laukkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="13" to="31" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Preventing replay attacks on speaker verification systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Carnahan Conf. on Security Technology (ICCST)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study on replay attack and anti-spoofing for text-dependent speaker verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference</title>
		<meeting>Asia-Pacific Signal Information essing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the study of replay and voice conversion attacks to text-dependent speaker verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Multimedia Tools and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of the vulnerability of speaker verification to synthetic speech</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>De Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: the Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: the Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of speaker verification security and detection of HMMbased synthetic speech</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>De Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saratxaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2280" to="2290" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Artificial impostor voice transformation effects on false acceptance rates</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sedlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ambikairajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference</title>
		<meeting>Asia-Pacific Signal Information essing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voice transformation-based spoofing of text-dependent speaker verification systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aronowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voice conversion versus speaker verification: an overview</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the security of HMM-based speaker verification systems against imposture using synthetic speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hitotsumatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Speech Communication and Technology (Eurospeech)</title>
		<meeting>European Conference on Speech Communication and Technology (Eurospeech)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imposture using synthetic speech against speaker verification based on spectrum and pitch</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based csr corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An experimental study of speaker verification sensitivity to computer voice-altered imposters</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Pellom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voice forgery using ALISP: indexation in a client memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aversano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blouet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effect of speech transformation on impostor acceptance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A robust speaker verification system against imposture using an HMM-based speech synthesis system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Speech Communication and Technology (Eurospeech)</title>
		<meeting>European Conference on Speech Communication and Technology (Eurospeech)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synthetic speech discrimination using pitch pattern statistics derived from image analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>De Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance of ivector speaker verification and the detection of synthetic speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mcclanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>De Leon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Synthetic speech detection using temporal modulation feature</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new speaker verification spoofing countermeasure based on local binary patterns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vipperla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amehraye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amehraye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<meeting>Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A crossvocoder study of speaker independent synthetic speech detection using phase information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saratxaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward a universal synthetic speech spoofing detection using phase information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saratxaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="810" to="820" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spoofing countermeasures to protect automatic speaker verification from voice conversion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amehraye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Introducing i-vectors for joint anti-spoofing and speaker verification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint speaker verification and antispoofing in the i-vector space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="821" to="832" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ASVspoof 2015: Automatic speaker verification spoofing and countermeasures challenge evaluation plan</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<ptr target="http://www.zhizheng.org/papers/asvSpoofevalplan.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanilc ¸i</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human assisted speaker recognition in nist sre10</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brandschain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: the Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: the Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: possible role of a repetitive structure in sounds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masuda-Katsuse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheveigné</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The CSTR/EMIME HTS system for Blizzard Challenge 2010</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Blizzard Challenge</title>
		<meeting>Blizzard Challenge<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">2010. Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structural bayesian linear regression for hidden Markov models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="358" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A speech parameter generation algorithm considering global variance for HMM-based speech synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. &amp; Syst</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones</title>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Charpentier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="453" to="468" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An adaptive algorithm for Mel-cepstral analysis of speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple methods for improving speakersimilarity of HMM-based speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The German text-to-speech synthesis system MARY: A tool for research, development and teaching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trouvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mel-generalized cepstral analysis-a unified approach to speech spectral estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Spoken Language Processing (ICSLP)</title>
		<meeting>Int. Conf. on Spoken Language essing (ICSLP)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-parallel training for many-to-many eigenvoice conversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4822" to="4825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ATR Japanese speech database as a tool of speech recognition and synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sagisaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Katagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuwabara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="357" to="363" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Effects of speaker adaptive training on tensor-based arbitrary speaker conversion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2222" to="2235" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exemplarbased unit selection for voice conversion utilizing temporal information</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Implementation of computationally efficient real-time voice conversion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Banno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Voice conversion using dynamic kernel partial least squares regression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Helander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Silén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="806" to="817" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Joint factor analysis versus eigenchannels in speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1435" to="1447" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Probabilistic models for inference about identity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">AudioSeg: Audio segmentation toolkit, release 1.2</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRISA</title>
		<imprint>
			<date type="published" when="2010-01">January 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MSR Identity Toolbox v1. 0: A MATLAB toolbox for speaker recognition research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society Speech and language Technical Committee Newsletter</title>
		<imprint>
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">From single to multiple enrollment i-vectors: Practical plda scoring variants for speaker verification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Afanasyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="93" to="101" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Usefulness of phase spectrum in human speech perception</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Alsteris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Significance of the modified group delay feature in speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R R</forename><surname>Gadde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robustness of phase based features for speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Text-to-speech synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Discrimination method of synthetic speech using pitch frequency against synthetic speech falsification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ogihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shiozakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. on Fundamentals of Electronics, Communications and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="286" />
			<date type="published" when="2005-01">jan 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fusion of heterogeneous speaker recognition systems in the STBU submission for the NIST speaker recognition evaluation 2006</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2072" to="2084" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sparse classifier fusion for speaker verification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sedlák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1622" to="1631" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Speaker verification by human listeners: Experiments comparing human and machine performance using the NIST 1998 speaker evaluation data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Approaching human listener accuracy with modern speaker verification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nosratighods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Discontinuity detection in concatenated speech synthesis based on nonlinear speech analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klabbers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khodabakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.7488/ds/252</idno>
		<ptr target="http://dx.doi.org/10.7488/ds/252" />
		<title level="m">Spoofing and Anti-Spoofing (SAS) corpus v1.0</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
