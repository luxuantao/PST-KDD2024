<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement-Learning-Based Robust Controller Design for Continuous-Time Uncertain Nonlinear Systems Subject to Input Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Derong</forename><surname>Liu</surname></persName>
							<email>derong.liu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiong</forename><surname>Yang</surname></persName>
							<email>xiong.yang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ding</forename><surname>Wang</surname></persName>
							<email>ding.wang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qinglai</forename><surname>Wei</surname></persName>
							<email>qinglai.wei@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement-Learning-Based Robust Controller Design for Continuous-Time Uncertain Nonlinear Systems Subject to Input Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1E9611A26C2805018B0D0789072D3E9B</idno>
					<idno type="DOI">10.1109/TCYB.2015.2417170</idno>
					<note type="submission">received October 12, 2014; revised January 28, 2015; accepted February 13, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Approximate dynamic programming (ADP)</term>
					<term>neural networks (NNs)</term>
					<term>neuro-dynamic programming</term>
					<term>nonlinear systems</term>
					<term>optimal control</term>
					<term>reinforcement learning (RL)</term>
					<term>robust control</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The design of stabilizing controller for uncertain nonlinear systems with control constraints is a challenging problem. The constrained-input coupled with the inability to identify accurately the uncertainties motivates the design of stabilizing controller based on reinforcement-learning (RL) methods. In this paper, a novel RL-based robust adaptive control algorithm is developed for a class of continuous-time uncertain nonlinear systems subject to input constraints. The robust control problem is converted to the constrained optimal control problem with appropriately selecting value functions for the nominal system. Distinct from typical action-critic dual networks employed in RL, only one critic neural network (NN) is constructed to derive the approximate optimal control. Meanwhile, unlike initial stabilizing control often indispensable in RL, there is no special requirement imposed on the initial control. By utilizing Lyapunov's direct method, the closed-loop optimal control system and the estimated weights of the critic NN are proved to be uniformly ultimately bounded. In addition, the derived approximate optimal control is verified to guarantee the uncertain nonlinear system to be stable in the sense of uniform ultimate boundedness. Two simulation examples are provided to illustrate the effectiveness and applicability of the present approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>proposed approaches focused on designing the direct adaptive controller and tackling control constraints by employing compensator schemes <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It is often a rather challenging task to construct compensator schemes and Lyapunov functions for guaranteeing the stability of this kind of nonlinear systems.</p><p>In order to overcome the above difficulty, in this paper, we transform the robust control problem to a class of optimal control problems by properly selecting value functions for the nominal system. The optimal control theory has made significant progress in the past half century. The important and valuable insights into the optimal control theory were presented in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Up to now, optimal control problems for nonlinear systems have attracted extensive attentions. A core challenge of obtaining the solution of nonlinear optimal control problems is that it often falls to solve the Hamilton-Jacobi-Bellman (HJB) equation. Because the HJB equation is actually a nonlinear partial differential equation (PDE), it is usually intractable to solve by analytical methods. To cope with the problem, Bellman <ref type="bibr" target="#b6">[7]</ref> developed dynamic programming (DP) theory. Though DP is successfully applied to solve optimal control problems, it is implemented backward-in-time which often makes the computation untenable to be run with increasing dimension of nonlinear systems. Consequently, approximate DP (ADP) algorithms were introduced by Werbos <ref type="bibr" target="#b7">[8]</ref>. The ADP methods can give approximate solutions of the HJB equation forward-in-time by employing neural networks (NNs). After that, various ADP approaches were developed <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Nevertheless, most of ADP algorithms are either implemented offline by utilizing iterative schemes or they require a priori knowledge of system dynamics. Since the exact knowledge of nonlinear dynamic systems is often unavailable, these ADP algorithms are intractable to real-time control applications.</p><p>To address the above issues, reinforcement learning (RL) is introduced. RL is a class of methods employed in machine learning to revise the actions of an agent based on responses from its environment <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. A general structure used to implement RL algorithm is the actor-critic architecture, where the actor performs actions by interacting with its environment, and the critic evaluates actions and offers feedback information to the actor, leading to the improvement in performance of the subsequent actor <ref type="bibr" target="#b22">[23]</ref>. RL differs significantly from typical ADP methods in that there is no prescribed behavior or training model proposed.</p><p>Consequently, RL is often applied to adaptive optimal controller designs <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b36">[37]</ref>.</p><p>During the past several years, many researchers have paid their attentions to the applications of RL methods to constrained nonlinear optimal control problems <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b40">[41]</ref>. Abu-Khalaf and Lewis <ref type="bibr" target="#b37">[38]</ref> developed an offline RL-based algorithm to solve the HJB equation of optimal control of continuous-time (CT) nonlinear systems with saturating actuators. By using the algorithm, the actor and the critic were sequentially tuned and the solution of the HJB equation was successively approximated. After that, Modares et al. <ref type="bibr" target="#b38">[39]</ref> proposed a novel algorithm based on integral RL methods to synchronously tune the critic and the actor. Thereafter, Modares and Lewis <ref type="bibr" target="#b39">[40]</ref> applied the proposed algorithm to study the constrained-input optimal tracking control problems. It should be mentioned that the knowledge of internal dynamics is not required in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref> [that is, the knowledge of f (x) presented in <ref type="bibr" target="#b1">(2)</ref> is unknown]. Different from the algorithms proposed in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b40">[41]</ref> employed identifier NNs to remove the requirement of the knowledge of internal dynamics and developed a new RL-based algorithm to obtain the optimal control for nonlinear systems with unknown structures. From <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, one shall find that the uncertainties of nonlinear systems can be conquered by using integral RL algorithms or introducing identifier NNs.</p><p>A question to be asked: based on the above two approaches, could the robust control for uncertain nonlinear systems be derived from the optimal control solution with appropriate value functions for the original uncertain nonlinear systems, rather than for the nominal system? In fact, the above two methods cannot be used for the former case. By using integral RL algorithms, the system state needs to be reset at each iteration step and it gives rise to difficulties for stability analysis <ref type="bibr" target="#b41">[42]</ref>. On the other hand, the identifier NNs might not accurately obtain the information of the uncertainties, when the uncertainty terms contain noise or immeasurable perturbation. For these reasons, Adhyaru et al. <ref type="bibr" target="#b42">[43]</ref> transformed the robust control problem to the constrained optimal control problem by selecting a suitable value function for the nominal system. The algorithm developed in <ref type="bibr" target="#b42">[43]</ref> is constructed by utilizing the least squares method and performed offline. Meanwhile, the stability analysis of the closed-loop optimal control system is not addressed.</p><p>More recently, Jiang and Jiang <ref type="bibr" target="#b43">[44]</ref> developed a robust ADP algorithm to derive the robust control for a class of uncertain nonlinear systems. Based on the algorithm in <ref type="bibr" target="#b43">[44]</ref>, the robust control is obtained by getting the optimal control solution with the infinite horizon cost for the original uncertain nonlinear systems, which is an advantage over the algorithm given in <ref type="bibr" target="#b42">[43]</ref>. Nevertheless, similar to the algorithms presented in <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b42">[43]</ref>, the algorithm developed in <ref type="bibr" target="#b43">[44]</ref> also requires the initial stabilizing control. To the best of our knowledge, there is no general method proposed to derive such a control law. From a mathematical point of view, the initial stabilizing control is actually a suboptimal control. The suboptimal control is intractable to obtain, since it is often impossible to solve the nonlinear PDEs analytically. Accordingly, the initial stabilizing control is a rather restrictive condition. Recently, Dierks and Jagannathan <ref type="bibr" target="#b44">[45]</ref> provided a way to relax the requirement of initial stabilizing control under a single online approximator-based framework. However, the control constraints are not taken into consideration. In real engineering applications, ignoring the actuators' limitation may give rise to undesirable transient response, and cause system instability. In addition, the developed algorithm is not utilized to derive robust control for CT nonlinear systems with unknown perturbation.</p><p>Motivated by the aforementioned work, in this paper, a novel RL-based robust adaptive control algorithm is developed for constrained-input CT nonlinear systems in the presence of unknown perturbation. The robust control problem is transformed to a constrained optimal control problem with properly selecting value functions for the nominal system. Distinct from traditional action-critic dual networks employed in RL, only one critic NN is constructed to derive the approximate optimal control. Meanwhile, unlike initial stabilizing control often indispensable in RL, there is no special requirement imposed on the initial control. By using Lyapunov's direct method, the closed-loop optimal control system and the estimated weights of the critic NN are proved to be uniformly ultimately bounded (UUB). In addition, the derived approximate optimal control is verified to guarantee the uncertain nonlinear system to be stable in the sense of uniform ultimate boundedness.</p><p>The main contributions of this paper include the following. 1) To the best of authors' knowledge, it is the first time that, by using RL methods, a critic NN is constructed to derive the robust control of constrained-input uncertain nonlinear CT systems without the requirement of the initial stabilizing control. 2) Unlike <ref type="bibr" target="#b38">[39]</ref> ignoring the higher-order terms of Taylor series in the stability analysis, in this paper, we take these terms into consideration. The higher-order terms often have a close connection with stability analysis (see Fact 1 in subsequent section). It will be more reasonable to take them into account for stability analysis. 3) In comparison with <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b43">[44]</ref>, a clear advantage of the developed algorithm in this paper is that a simpler algorithm architecture is constructed, that is, only one critic NN is employed. In this sense, the complexity of the computation is reduced. The rest of this paper is organized as follows. In Section II, we present the problem statement and preliminaries. In Section III, we provide the nominal system for uncertain nonlinear systems and show that the robust control problem can be transformed to a constrained optimal control problem. In Section IV, we design an online RL-based control scheme to derive the approximate solution of the HJB equation. In Section V, we develop the stability analysis. In Section VI, two examples are given to illustrate the theoretical results. Finally, in Section VII, we give several concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>R represents the set of all real numbers. R m denotes the Euclidean space of all real m-vectors. R n×m denotes the space of all n × m real matrices. I m represents m × m identity matrix. T is the transposition symbol.</p><p>is a compact set of R n , C m ( ) represents the class of functions having continuous mth derivative on . When ξ is a vector, ξ denotes the Euclidean norm of ξ . When A is a matrix, A denotes the two-norm of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM STATEMENT AND PRELIMINARIES</head><p>Consider the uncertain nonlinear CT system described by</p><formula xml:id="formula_0">ẋ(t) = f (x(t)) + g(x(t))u(x(t)) + f (x(t)) (1)</formula><p>with the state x(t) ∈ ⊂ R n and the control u(x) ∈ A, and</p><formula xml:id="formula_1">A = {u|u ∈ R m , |u i | ≤ κ, i = 1, . . . , m},</formula><p>where κ &gt; 0 is the saturating bound. f (x) ∈ R n and g(x) ∈ R n×m are known functions, and f (x) ∈ R n is an unknown perturbation. For convenience of later analysis, we provide the following assumptions, which have been used in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b45">[46]</ref>, and <ref type="bibr" target="#b46">[47]</ref>.</p><p>Assumption 1: The perturbation term f (x) satisfies the matching condition. That is, f (x) = g(x)d(x), where d(x) ∈ R m is an unknown function bounded by a known function d M (x), i.e., d(x) ≤ d M (x). In addition, d(0) = 0 and d M (0) = 0.</p><p>Assumption 2: f (x) + g(x)u is Lipschitz continuous on the compact set containing the origin, such that system (1) is stabilizable on . Moreover, f (0) = 0.</p><p>Assumption 3: The control matrix g(x) is known and bounded, i.e., there exist constants g m and g M (0 &lt; g m &lt; g M ) such that g m ≤ g(x) ≤ g M , for every x ∈ .</p><p>For system <ref type="bibr" target="#b0">(1)</ref>, in order to successfully tackle the robust control problem, one needs to derive a feedback control u(x) ∈ A, such that the closed-loop system is stable with the unknown term d(x). It is generally difficult to directly design such a controller, for the control is constrained and the uncertainty term d(x) is involved.</p><p>In this paper, we shall demonstrate that the robust control problem can be converted into the constrained optimal control problem with appropriately selecting value functions for the nominal system. Then, by solving the constrained optimal control problem, we can obtain a robust controller to guarantee system (1) to be stable in the sense of uniform ultimate boundedness (as for the definition of uniform ultimate boundedness, readers can refer to <ref type="bibr" target="#b47">[48]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NOMINAL SYSTEMS AND PROBLEM TRANSFORMATION</head><p>This section consists of two parts. First, the HJB equation for the constrained nominal system is developed. Then, we verify that the robust control for system (1) can be obtained by the optimal control for the constrained nominal system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. HJB Equation for Constrained Nominal Systems</head><p>The nominal system [i.e., system (1) without uncertainty] is described by</p><formula xml:id="formula_2">ẋ(t) = f (x(t)) + g(x(t))u(x(t)) (2)</formula><p>with u(x) ∈ A ⊂ R m . It is desired to obtain the control policy u(x) which minimizes the infinite horizon value function</p><formula xml:id="formula_3">V(x(t)) = ∞ t d 2 M x(s) + r x(s), u(s) ds (s ≥ t) (3)</formula><p>where r(x, u) = x T Qx + W(u), Q is a symmetric positive definite matrix and W(u) is positive definite. In order to overcome bounded controls in system (2), inspired by the work of <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, we define W(u) as</p><formula xml:id="formula_4">W(u) = 2κ u 0 ψ -1 (υ/κ) T Rdυ = 2κ m i=1 u i 0 ψ -1 (υ i /κ)r i dυ i where ψ -1 (υ/κ) = [ψ -1 (υ 1 /κ), . . . , ψ -1 (υ m /κ)] T , R = diag{r 1 , . . . , r m } with r i &gt; 0, i = 1, . . . , m, ψ ∈ R m , ψ -T denotes (ψ -1 ) T , and ψ(•) is a bounded one-to- one function satisfying |ψ(•)| ≤ 1 and belonging to C p (p ≥ 1) and L 2 ( ) note that ψ(ν) ∈ L 2 ( ) means that ψ T (ν)ψ(ν)dν 1/2 &lt; ∞ and ψ T (ν)ψ(ν)</formula><p>dν is the Lebesgue integral on <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Meanwhile, ψ(•) is a monotonic odd function with its derivative bounded by a constant ψ M , i.e., dψ(ς Let A ( ) be the set of admissible control <ref type="bibr" target="#b49">[50]</ref>. Given a control u(x) ∈ A ( ), if the associated value function V(x) ∈ C 1 ( ), then the infinitesimal version of (3) is the so-called Lyapunov equation</p><formula xml:id="formula_5">)/dς ≤ ψ M , ∀ς ∈ R. It should be emphasized that W(u) is positive definite since ψ -1 (•)</formula><formula xml:id="formula_6">V T x f (x) + g(x)u + d 2 M (x) + r(x, u) = 0<label>( 4 )</label></formula><p>where V x ∈ R n denotes the partial derivative of V(x) with respect to x, and V(0) = 0. Define the Hamiltonian for the control u(x) ∈ A ( ) and the value function V(x) as</p><formula xml:id="formula_7">H(x, V x , u) = V T x f (x) + g(x)u + d 2 M (x) + r(x, u). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>The optimal value function V * (x) ∈ C 1 ( ) is given as</p><formula xml:id="formula_9">V * (x(t)) = min u∈A ( ) ∞ t d 2 M x(s) + r x(s), u(s) ds (6)</formula><p>with V * (0) = 0. Then, the optimal cost V * (x) can be obtained by solving the HJB equation</p><formula xml:id="formula_10">min u∈A ( ) H x, V * x , u = 0. (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Suppose that the minimum value on the left-hand side of (7) exists and is unique. Then, the closed-form expression for the optimal control is derived as</p><formula xml:id="formula_12">u * (x) = -κ tanh 1 2κ g T (x)V * x . (<label>8</label></formula><formula xml:id="formula_13">) IEEE TRANSACTIONS ON CYBERNETICS</formula><p>Substituting ( <ref type="formula" target="#formula_12">8</ref>) into <ref type="bibr" target="#b6">(7)</ref>, we obtain the HJB equation for the nonlinear systems as</p><formula xml:id="formula_14">V * x T f (x) -2κ 2 A T (x) tanh(A(x)) + d 2 M (x) + x T Qx + 2κ -κ tanh(A(x)) 0 tanh -T (υ/κ)dυ = 0 (9)</formula><p>where <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>, we know 2κ</p><formula xml:id="formula_15">A(x) = (1/2κ)g T (x)V * x . Denote A(x) = [A 1 (x), . . . , A m (x)] T ∈ R m with A i (x) ∈ R, i = 1, . . . , m. By</formula><formula xml:id="formula_16">-κ tanh(A(x)) 0 tanh -T (υ/κ)dυ = 2κ 2 A T (x) tanh(A(x)) + κ 2 m i=1 ln 1 -tanh 2 (A i (x)) .</formula><p>Then, the HJB equation ( <ref type="formula">9</ref>) can be rewritten as</p><formula xml:id="formula_17">V * x T f (x) + d 2 M (x) + x T Qx + κ 2 m i=1 ln 1 -tanh 2 (A i (x)) = 0. (<label>10</label></formula><formula xml:id="formula_18">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problem Transformation</head><p>In this section, we establish a theorem to show that the robust control for system (1) can be obtained by the optimal control solution for system (2) with the value function <ref type="bibr" target="#b2">(3)</ref>.</p><p>Theorem 1: Consider the nominal system described by (2) with the value function <ref type="bibr" target="#b2">(3)</ref>. Let Assumptions 1-3 hold. Then, the optimal control u * (x) developed in (8) ensures system (1) to be stable in the sense of uniform ultimate boundedness.</p><p>Proof: Let V * (x) and u * (x) be the optimal value given in (6) and the optimal control derived in <ref type="bibr" target="#b7">(8)</ref>, respectively. According to the definition of V * (x) given in <ref type="bibr" target="#b5">(6)</ref>, we can obtain that V * (x) &gt; 0 for ∀x = 0 and V * (x) = 0 ⇔ x = 0. Taking the derivative of V * (x) along the system trajectory ẋ = f (x) + g(x)u * + f (x), we have</p><formula xml:id="formula_19">V * (x) = V * T x f (x) + g(x)u * + V * T x f (x). (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>Using ( <ref type="formula">9</ref>), we obtain</p><formula xml:id="formula_21">V * T x f (x) + g(x)u * = -d 2 M (x) -x T Qx -2κ m i=1 u * i 0 tanh -1 (υ i /κ)dυ i (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where</p><formula xml:id="formula_23">u * = u * 1 , . . . , u * m T with u * i ∈ R, i = 1, . . . , m. Observe that (8) yields V * T x g(x) = -2κ tanh -T (u * /κ).</formula><p>Then, based on Assumption 1, we get</p><formula xml:id="formula_24">V * T x f (x) = -2κ tanh -T (u * /κ)d(x). (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>Substituting ( <ref type="formula" target="#formula_21">12</ref>) and ( <ref type="formula" target="#formula_24">13</ref>) into <ref type="bibr" target="#b10">(11)</ref>, we derive</p><formula xml:id="formula_26">V * (x) = -d 2 M (x) -x T Qx + £ 1 (x) -2κ tanh -T u * /κ d(x) (14) with £ 1 (x) = -2κ m i=1 u * i 0 tanh -1 (υ i /κ)dυ i .</formula><p>Let</p><formula xml:id="formula_27">τ i = tanh -1 (υ i /κ), i = 1, . . . , m.</formula><p>Then, by applying variable substitution methods <ref type="bibr" target="#b50">[51]</ref> to £ 1 (x), we have</p><formula xml:id="formula_28">£ 1 (x) = -2κ 2 m i=1 tanh -1 (u * i /κ) 0 τ i 1 -tanh 2 (τ i ) dτ i = 2κ 2 m i=1 tanh -1 (u * i /κ) 0 τ i tanh 2 (τ i )dτ i -κ 2 m i=1 tanh -1 u * i /κ 2 . (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>Note that</p><formula xml:id="formula_30">m i=1 tanh -1 (u * i /κ) 2 = tanh -T u * /κ tanh -1 u * /κ . (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>Then, by using ( <ref type="formula" target="#formula_28">15</ref>) and ( <ref type="formula" target="#formula_30">16</ref>), ( <ref type="formula">14</ref>) can be represented as</p><formula xml:id="formula_32">V * (x) = -d 2 M (x) -x T Qx + d T (x)d(x) -d(x) + κ tanh -1 (u * /κ) T × d(x) + κ tanh -1 (u * /κ) + £ 2 (17) with £ 2 (x) = 2κ 2 m i=1 tanh -1 (u * i /κ) 0 τ i tanh 2 (τ i )dτ i .</formula><p>Using the integral mean-value theorem <ref type="bibr" target="#b50">[51]</ref>, we have</p><formula xml:id="formula_33">£ 2 (x) = 2κ 2 m i=1 tanh -1 u * i /κ θ i tanh 2 (θ i ) (<label>18</label></formula><formula xml:id="formula_34">)</formula><p>where θ i ∈ R is selected between 0 and tanh -1 (u * i /κ), i = 1, . . . , m. From the expression of £ 2 (x) given in <ref type="bibr" target="#b17">(18)</ref>, one can easily derive that £ 2 (x) &gt; 0.</p><p>Because u * is an admissible control for nominal system (2) with the value function (3), by using the definition of admissible control <ref type="bibr" target="#b49">[50]</ref>, one can derive that V * (x) is finite for arbitrary x ∈ . Moreover, one can conclude that V *</p><p>x is bounded. Without loss of generality, we denote that V *</p><p>x is bounded by δ M &gt; 0, i.e., V *</p><p>x ≤ δ M . Accordingly, by using Assumption 3 and ( <ref type="formula" target="#formula_30">16</ref>), and observing that 0</p><formula xml:id="formula_35">&lt; tanh 2 (θ i ) ≤ 1, we obtain £ 2 (x) ≤ 2κ 2 m i=1 tanh -1 (u * i /κ)θ i ≤ 2κ 2 tanh -T u * /κ tanh -1 u * /κ = 1 2 V * T x g(x)g T (x)V * x ≤ 1 2 g 2 M δ 2 M . (<label>19</label></formula><formula xml:id="formula_36">)</formula><p>Combining <ref type="bibr" target="#b16">(17)</ref> with <ref type="bibr" target="#b18">(19)</ref> and using Assumption 1, we derive</p><formula xml:id="formula_37">V * (x) ≤ -d 2 M (x) -x T Qx + d T (x)d(x) -d(x) + κ tanh -1 u * /κ T × d(x) + κ tanh -1 u * /κ + 1 2 g 2 M δ 2 M ≤ -λ min (Q) x 2 + 1 2 g 2 M δ 2 M</formula><p>where λ min (Q) denotes the minimum eigenvalue of the matrix Q. Noting that Q is positive definite, we obtain</p><formula xml:id="formula_38">λ min (Q) &gt; 0.</formula><p>Consequently, V * (x) &lt; 0 as long as the state x(t) is out of the compact set</p><formula xml:id="formula_39">x = x : x ≤ g M δ M √ 2λ min (Q) .</formula><p>This shows that V * (x) is a Lyapunov function for system <ref type="bibr" target="#b0">(1)</ref> with the control u * , whenever x(t) lies outside the compact set x . Therefore, the optimal control u * developed in <ref type="bibr" target="#b7">(8)</ref> guarantees the trajectory of system (1) to be UUB. Remark 1: The optimal value V * (x) is often considered to be a smooth function <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b40">[41]</ref>. It implies that V * (x) ∈ C 1 ( ). Therefore, by functional analysis <ref type="bibr" target="#b48">[49]</ref>, we obtain that V *</p><p>x is bounded on . This verifies that there exists a constant δ M &gt; 0 such that V *</p><p>x ≤ δ M . In addition, should be selected large enough to make max{ x , x} ⊆ , where x is given in subsequent (60). In this sense, x will remain in .</p><p>According to Theorem 1, the robust control for system (1) can be obtained by solving the optimal control problem ( <ref type="formula">2</ref>) and (3). In other words, we need to get the solution of the HJB equation <ref type="bibr" target="#b9">(10)</ref>. Nevertheless, one shall find that ( <ref type="formula" target="#formula_17">10</ref>) is actually a nonlinear PDE with respect to V * (x), which is difficult to solve by analytical methods. To overcome the difficulty, an online RL-based optimal control scheme shall be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RL-BASED OPTIMAL CONTROL SCHEME</head><p>Two subsections are embodied in this section, including the introduction of policy iteration algorithm and the design of online NN-based optimal control scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Policy Iteration Algorithm</head><p>Step 1: Select a computation accuracy &gt; 0. Let j = 0 with V (0) (x) = 0. Then, begin with an initial admissible control policy u (0) (x).</p><p>Step 2: Get the value V ( j+1) (x) by solving the equation</p><formula xml:id="formula_40">V ( j+1) x T f (x) + g(x)u ( j) + d 2 M (x) + x T Qx -2κ u ( j) 0 tanh -T (υ/κ)dυ = 0.</formula><p>Step 3: Update the control policy using</p><formula xml:id="formula_41">u ( j+1) (x) = -κ tanh 1 2κ g T (x)V ( j+1)</formula><p>x .</p><p>Step 4: If V ( j+1) (x) -V ( j) (x) ≤ for every x ∈ , then stop and derive the approximate optimal control; otherwise, let j = j + 1 and go back to step 2. Based on the present algorithm, one can obtain that, for i → ∞, there exist V (i) (x) → V * (x) and u (i) (x) → u * (x). The convergence of the algorithm was shown in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Though the present policy iteration algorithm can be applied to solve <ref type="bibr" target="#b9">(10)</ref>, it is often implemented offline (see <ref type="bibr" target="#b37">[38]</ref>). On the other hand, it needs the initial admissible control. As mentioned before, it is a rather restrictive condition. To handle the above two problems, a novel online NN-based control algorithm shall be developed to solve <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Online NN-Based Control Design</head><p>In this section, a critic NN is constructed to approximate the value function. According to the universal approximation property of NNs, V * (x) given in ( <ref type="formula">6</ref>) can be represented by a single-layer NN on a compact set as</p><formula xml:id="formula_42">V * (x) = W T c σ (x) + ε(x) (<label>20</label></formula><formula xml:id="formula_43">)</formula><p>where</p><formula xml:id="formula_44">W c ∈ R N 0 is the ideal NN weight vector, σ (x) = [σ 1 (x), σ 2 (x), . . . , σ N 0 (x)] T ∈ R N 0 is the activation function with σ j (x) ∈ C 1 ( ) and σ j (0) = 0, the set {σ j (x)} N 0</formula><p>1 is often selected to be linearly independent, N 0 is the number of the neurons, and ε(x) is the NN function reconstruction error.</p><p>The derivative of V * (x) with respect to x is given as</p><formula xml:id="formula_45">V * x = ∇σ T (x)W c + ∇ε<label>(21)</label></formula><p>with ∇σ (x) = ∂σ (x)/∂x and ∇σ (0) = 0. Substituting ( <ref type="formula" target="#formula_45">21</ref>) into (10), we have</p><formula xml:id="formula_46">d 2 M (x) + x T Qx + W T c ∇σ f (x) + ∇ε T f (x) + κ 2 m i=1 ln 1 -tanh 2 ( 1i (x) + i (x)) = 0<label>(22)</label></formula><p>where</p><formula xml:id="formula_47">1 (x) = (1/2κ)g T (x)∇σ T W c , (x) = (1/2κ)g T (x)∇ε, and 1 (x) = [ 11 (x), . . . , 1m (x)] T with 1i (x) ∈ R, and<label>(</label></formula><formula xml:id="formula_48">x) = [ 1 (x), . . . , m (x)] T with i (x) ∈ R, i = 1, . . . , m.</formula><p>Using the mean-value theorem <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b21">(22)</ref> is represented as</p><formula xml:id="formula_49">d 2 M (x) + x T Qx + W T c ∇σ f (x) + κ 2 m i=1 ln 1 -tanh 2 1i (x) + ε HJB = 0 (<label>23</label></formula><formula xml:id="formula_50">)</formula><p>where ε HJB is the HJB approximation error <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and the expression is given as</p><formula xml:id="formula_51">ε HJB = ∇ε T f (x) + m i=1 2κ 2 ζ 1i tanh(ζ 2i ) tanh 2 (ζ 2i ) -1 i (x) with ζ 1i ∈ R selected between 1 -tanh 2 (A i (x))</formula><p>) and 1tanh 2 ( 1i (x)), and ζ 2i ∈ R chosen between A i (x) and 1i (x). Remark 2: It was shown in <ref type="bibr" target="#b37">[38]</ref> that ε HJB converges to zero as the number of neurons N 0 increases. In other words, for ∀ε h &gt; 0, there exists a positive N h (depending only on</p><formula xml:id="formula_52">ε h ) such that N 0 &gt; N h implies ε HJB ≤ ε h .</formula><p>Similarly, by using ( <ref type="formula" target="#formula_45">21</ref>) and the mean-value theorem, the optimal control (8) can be rewritten as</p><formula xml:id="formula_53">u * (x) = -κ tanh 1 (x) + ε u *<label>(24)</label></formula><p>where</p><formula xml:id="formula_54">ε u * = -1/2 1 -tanh 2 (ξ ) g T ∇ε with ξ ∈ R m selected between 1 (x) and A(x) and 1 = [1, . . . , 1] T ∈ R m .</formula><p>Because the ideal NN weight W c is typically unknown, (24) cannot be implemented in real-control process. Hence, we use a critic NN to approximate the value function given in <ref type="bibr" target="#b5">(6)</ref> as</p><formula xml:id="formula_55">V(x) = ŴT c σ (x) (<label>25</label></formula><formula xml:id="formula_56">)</formula><p>where Ŵc is the estimate of W c . Meanwhile, the estimation error for the weight is defined as</p><formula xml:id="formula_57">Wc = W c -Ŵc . (<label>26</label></formula><formula xml:id="formula_58">)</formula><p>Utilizing <ref type="bibr" target="#b24">(25)</ref>, the estimate of ( <ref type="formula" target="#formula_12">8</ref>) is derived as</p><formula xml:id="formula_59">û(x) = -κ tanh 1 2κ g T (x)∇σ T Ŵc . (<label>27</label></formula><formula xml:id="formula_60">)</formula><p>Combining ( <ref type="formula" target="#formula_7">5</ref>), <ref type="bibr" target="#b24">(25)</ref>, and ( <ref type="formula" target="#formula_59">27</ref>), we derive the approximate Hamiltonian as</p><formula xml:id="formula_61">H(x, Ŵc ) = d 2 M (x) + x T Qx + ŴT c ∇σ f (x) + κ 2 m i=1 ln 1 -tanh 2 ( 2i (x)) e (<label>28</label></formula><formula xml:id="formula_62">)</formula><p>where</p><formula xml:id="formula_63">2 (x) = (1/2κ)g T (x)∇σ T Ŵc , and 2 (x) = [ 21 (x), . . . , 2m (x)] T with 2i (x) ∈ R, i = 1, . . . , m.</formula><p>From ( <ref type="formula" target="#formula_49">23</ref>) and ( <ref type="formula" target="#formula_61">28</ref>), we have</p><formula xml:id="formula_64">e = -WT c ∇σ f (x) + m i=1 κ 2 ( 2i ) -( 1i ) -ε HJB (29)</formula><p>with</p><formula xml:id="formula_65">( ιi ) = ln 1 -tanh 2 ( ιi (x)) , ι = 1, 2.</formula><p>Observe that, for ∀ ιi (x) ∈ R, ( ιi ) can be represented as <ref type="bibr" target="#b40">[41]</ref> (</p><formula xml:id="formula_66">ιi ) = -2 ln 1 + exp -2 ιi (x)sgn( ιi (x)) -2 ιi (x)sgn( ιi (x)) + ln 4</formula><p>where sgn( ιi (x)) ∈ R is a sign function <ref type="bibr" target="#b50">[51]</ref>. Note that</p><formula xml:id="formula_67">m i=1 ( ιi ) = -2 m i=1 ln 1 + exp -2 ιi (x)sgn( ιi (x))</formula><p>-2 T ι (x)sgn( ι (x)) + m ln 4. (30) Therefore, using <ref type="bibr" target="#b28">(29)</ref> and <ref type="bibr" target="#b29">(30)</ref>, we get</p><formula xml:id="formula_68">e = 2κ 2 T 1 (x)sgn( 1 (x)) -T 2 (x)sgn( 2 (x)) -WT c ∇σ f (x) + κ 2 -ε HJB = κ W T c ∇σ g(x)sgn( 1 (x)) -ŴT c ∇σ g(x)sgn( 2 (x)) -WT c ∇σ f (x) + κ 2 -ε HJB = -WT c ∇σ f (x) -κ∇σ g(x)sgn( 2 (x)) + ρ(x)<label>(31)</label></formula><p>where</p><formula xml:id="formula_69">= 2 m i=1 ln 1 + exp -2 1i (x)sgn( 1i (x)) 1 + exp -2 2i (x)sgn( 2i (x)) ρ(x) = κW T c ∇σ g(x) sgn( 1 (x)) -sgn( 2 (x)) + κ 2 -ε HJB .</formula><p>To derive the minimum value of e, it is desired to choose Ŵc to minimize the squared residual error E = (1/2)e T e. By utilizing the gradient descent algorithm, the weight tuning law for the critic NN is generally given as <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> </p><formula xml:id="formula_70">Ẇc = - γ (1 + φ T φ) 2 ∂E ∂ Ŵc = - γ φ (1 + φ T φ) 2 e (<label>32</label></formula><formula xml:id="formula_71">)</formula><p>where φ = ∇σ f (x) + g(x)û , γ &gt; 0 is a design constant, and the term (1 + φ T φ) 2 is employed for normalization.</p><p>However, there exist two issues about the tuning rule <ref type="bibr" target="#b31">(32)</ref>. 1) Based on <ref type="bibr" target="#b31">(32)</ref>, the initial admissible control for systems ( <ref type="formula">2</ref>) and ( <ref type="formula">3</ref>) is required, for guaranteeing the validity of policy iteration algorithms presented in aforementioned literature. As stated before, the initial admissible control is actually a suboptimal control of system (2) with (3). The suboptimal control is intractable to obtain because it needs to get the analytical solution of the PDE (10). 2) By utilizing <ref type="bibr" target="#b31">(32)</ref>, if the initial control is not admissible, then tuning the critic NN alone might not guarantee the stability of the closed-loop system during the learning process of NNs. To tackle the above two issues, the weight update law for the critic NN should be redefined. Prior to proceeding, we provide a assumption as follows. The assumption is a common technique, which has been used in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and <ref type="bibr" target="#b51">[52]</ref>.</p><p>Assumption 4: J(x(t)) is a continuously differentiable radially unbounded Lyapunov function candidate such that J(x(t)) = J T</p><p>x f (x) + g(x)u * &lt; 0 with J x the partial derivative of J(x) with respect to x. Moreover, there exists a symmetric positive definite matrix B(x) ∈ R n×n defined on such that</p><formula xml:id="formula_72">J T x f (x) + g(x)u * = -J T x B(x)J x . (<label>33</label></formula><formula xml:id="formula_73">)</formula><p>Remark 3: f (x) + g(x)u * is often assumed to be bounded by a positive constant on the compact set <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>. That is, for every x ∈ , there exists a constant &gt; 0 such that f (x) + g(x)u * ≤ . To relax the condition, we assume that f (x) + g(x)u * is bounded by a function with respect to x. Because J x is a function with respect to x, without loss of generality, we assume that f (x) + g(x)u * ≤ η J x (η &gt; 0). In this sense, we derive that J T</p><p>x f (x) + g(x)u * ≤ η J x 2 . Observing that J T</p><p>x f (x) + g(x)u * &lt; 0, one shall find that (33) defined as in Assumption 4 is reasonable. In addition, J(x(t)) is usually derived through properly selecting functions, such as polynomials.</p><p>Based on Assumption 4 and aforementioned analyzes, we develop a novel weight update law for the critic NN as</p><formula xml:id="formula_74">Ẇc = -γ φ d 2 M (x) + x T Qx + ŴT c ∇σ f (x) + κ 2 m i=1 ln 1 -tanh 2 ( 2i (x)) + γ 2 (x, û)∇σ g(x) I m -B( 2 (x)) g T (x)J x + γ κ∇σ g(x) tanh( 2 (x)) -sgn( 2 (x)) ϕ T m s Ŵc -P 2 -P 1 ϕ T Ŵc (<label>34</label></formula><formula xml:id="formula_75">)</formula><p>where φ = φ/m 2 s , ϕ = φ/m s , m s = 1 + φ T φ, and B( 2 (x)) = diag tanh 2 ( 2i (x)) , i = 1, . . . , m, J x is defined as in Assumption 4, P 1 and P 2 are tuning parameters with suitable dimensions, and (x, û) is a sign function given as</p><formula xml:id="formula_76">(x, û) = 0, if J T x f (x)) + g(x)û &lt; 0 1, otherwise. (<label>35</label></formula><formula xml:id="formula_77">)</formula><p>Remark 4: Several notes about <ref type="bibr" target="#b33">(34)</ref> are listed as follows.</p><p>1) The first term given in <ref type="bibr" target="#b33">(34)</ref> shares the same feature with <ref type="bibr" target="#b31">(32)</ref>, which is employed to minimize the objective function E = (1/2)e T e.</p><p>2) The second term provided in ( <ref type="formula" target="#formula_74">34</ref>) is used to guarantee the stability of the closed-loop system during the NN learning process. We denote the derivative of the Lyapunov function candidate for system (2) with the control (27) as</p><formula xml:id="formula_78">= J T x f (x) -κg(x) tanh( 2 (x)) .</formula><p>If the closed-loop system is unstable, then we can obtain &gt; 0. In order to keep the closed-loop system stable, we just need to make &lt; 0. Using the gradient descent method, we have</p><formula xml:id="formula_79">-γ ∂ ∂ Ŵc = -γ ∂ J T x f (x) -κg(x) tanh( 2 (x)) ∂ Ŵc = γ ∂ 2 ∂ Ŵc T • ∂ κJ T x g(x) tanh( 2 (x)) ∂ 2 (x) = γ 2 ∇σ g(x) I m -B( 2 (x)) g T (x)J x (<label>36</label></formula><formula xml:id="formula_80">)</formula><p>where</p><formula xml:id="formula_81">B( 2 (x)) = diag tanh 2 ( 2i (x)) , i = 1, . . . , m.</formula><p>Equation <ref type="bibr" target="#b35">(36)</ref> indicates the reason that we employ the second term given in <ref type="bibr" target="#b33">(34)</ref>. Actually, by the definition of (x, û) given in <ref type="bibr" target="#b34">(35)</ref>, we find that if there exists &lt; 0 (that is, the closed-loop system is stable), then (x, û) = 0 and the second term in <ref type="bibr" target="#b33">(34)</ref> disappears. If the closed-loop system is unstable, then (x, û) = 1 and the second term in <ref type="bibr" target="#b33">(34)</ref> [that is <ref type="bibr" target="#b35">(36)</ref>] works. Based on <ref type="bibr" target="#b33">(34)</ref>, it makes no requirement of the initial stabilizing control for system <ref type="bibr" target="#b1">(2)</ref>. This property will be illustrated in numerical simulation.</p><p>3) The last term given in ( <ref type="formula" target="#formula_74">34</ref>) is a robust term, which is used for stability analysis in the subsequent discussion. 4) If selecting proper P i (i = 1, 2) such that P 2 = P 1 ϕ T , then, by <ref type="bibr" target="#b33">(34)</ref>, we have Ẇc = 0 when x = 0. In this case, V(x) will no longer be updated. However, the optimal control might not be achieved at finite time t f which makes x(t f ) = 0. To avoid this case, persistency of excitation (PE) condition is required. Observing the expression of φ given in <ref type="bibr" target="#b31">(32)</ref> and using <ref type="bibr" target="#b26">(27)</ref>, we obtain that ∇σ f (x) = φ + κ∇σ g(x) tanh( 2 (x)). Then, based on ( <ref type="formula" target="#formula_57">26</ref>), ( <ref type="formula" target="#formula_61">28</ref>), <ref type="bibr" target="#b30">(31)</ref>, and (34), we derive</p><formula xml:id="formula_82">Ẇc = γ ϕ m s -WT c φ + κ WT c ∇σ g(x)F(x) + ρ(x) - γ 2 (x, û)∇σ g(x) I m -B( 2 (x)) g T (x)J x + γ κ∇σ g(x)F(x) ϕ T m s Ŵc + P 2 -P 1 ϕ T Ŵc (37)</formula><p>where</p><formula xml:id="formula_83">F(x) = sgn( 2 (x)) -tanh( 2 (x)).</formula><p>Traditionally, for utilizing RL approaches, a second NN called the action NN is introduced to approximate the control policy <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, in this paper, the action NN is not required. The reasons are as follows.</p><p>1) As pointed out in <ref type="bibr" target="#b22">[23]</ref>, the action NN is mainly employed to avoid the need for knowledge of the internal dynamics f (x). Nevertheless, both the knowledge of f (x) and g(x) is assumed to be available in our case. 2) From ( <ref type="formula" target="#formula_55">25</ref>) and ( <ref type="formula" target="#formula_59">27</ref>), we can find that the value function shares the same weight Ŵc with the control policy. Therefore, if the value function can be approximated by the critic NN given in ( <ref type="formula" target="#formula_55">25</ref>), then the control policy is obtained via <ref type="bibr" target="#b26">(27)</ref>. In other words, the action NN can be replaced with <ref type="bibr" target="#b26">(27)</ref>. Based on the above analyzes, the schematic of the developed control algorithm is shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. STABILITY ANALYSIS</head><p>In this section, we present our main results via Lyapunov's direct method. Before proceeding, we provide the following two assumptions, which have been used in <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>.</p><p>Assumption 5: The ideal NN weight W c is bounded by a known constant W M &gt; 0, i.e., W c ≤ W M . There exist known constants b ε &gt; 0 and b εx &gt; 0 such that ε(x) &lt; b ε , ∇ε(x) &lt; b εx , for every x ∈ . In addition, ε u * given in ( <ref type="formula" target="#formula_53">24</ref>) is bounded by a known constant b ε u * &gt; 0 over , i.e., ε u * ≤ b ε u * , for every x ∈ .</p><p>Assumption 6: There exist known constants b σ &gt; 0 and</p><formula xml:id="formula_84">b σ x &gt; 0 such that σ (x) ≤ b σ , ∇σ (x) ≤ b σ x , for every x ∈ . Let G( i ) = tanh( i (x)), i = 1, 2.</formula><p>By employing Taylor series, we have</p><formula xml:id="formula_85">G( 1 ) -G( 2 ) = ∂G( 2 ) ∂ 2 1 (x) -2 (x) + O ( 1 (x) -2 (x)) 2 = 1 2κ I m -B( 2 (x)) g T ∇σ T Wc + O ( 1 (x) -2 (x)) 2 (<label>38</label></formula><formula xml:id="formula_86">)</formula><p>where 2 is the higher-order terms of IEEE TRANSACTIONS ON CYBERNETICS the Taylor series <ref type="bibr" target="#b50">[51]</ref>. Then, by using <ref type="bibr" target="#b37">(38)</ref>, we derive</p><formula xml:id="formula_87">B( 2 (x)) = diag tanh 2 ( 2i (x)) , i = 1, . . . , m and O ( 1 (x) -2 (x))</formula><formula xml:id="formula_88">O ( 1 (x) -2 (x)) 2 = G( 1 ) -G( 2 ) + 1 2κ [B( 2 (x)) -I m ]g T ∇σ T Wc .</formula><p>Observing that tanh( i ) ≤ 1 (i = 1, 2), we can obtain a fact as follows. It should be noted that a similar fact has been stated in <ref type="bibr" target="#b52">[53]</ref>. Fact 1: For hyperbolic function tanh, the higher-order term in the Taylor series is bounded by</p><formula xml:id="formula_89">O ( 1 (x) -2 (x)) 2 ≤ c 1 + c 2 Wc</formula><p>where c i (i = 1, 2) are computable positive constants.</p><p>Theorem 2: Consider the nominal nonlinear CT system described by <ref type="bibr" target="#b1">(2)</ref> with the associated HJB equation <ref type="bibr" target="#b9">(10)</ref>. Let Assumptions 2-6 hold and take the control input for system <ref type="bibr" target="#b1">(2)</ref> as given in <ref type="bibr" target="#b26">(27)</ref>. Meanwhile, let the critic NN weight tuning law be described by <ref type="bibr" target="#b33">(34)</ref>. Then, the function J x and the critic NN weight estimation error Wc are guaranteed to be UUB.</p><p>Proof: Consider the Lyapunov function candidate</p><formula xml:id="formula_90">L(t) = L 1 (x) + 1 2 WT c γ -1 Wc (<label>39</label></formula><formula xml:id="formula_91">)</formula><p>where L 1 (x) = J(x) with J(x) given in Assumption 4.</p><p>Taking the time derivative of (39), we have</p><formula xml:id="formula_92">L(t) = J T x f (x) + g(x)û + ẆT c γ -1 Wc = J T x f (x) -κg(x) tanh( 2 (x)) + ẆT c γ -1 Wc . (<label>40</label></formula><formula xml:id="formula_93">)</formula><p>Using <ref type="bibr" target="#b36">(37)</ref>, the last term of (40) can be represented as</p><formula xml:id="formula_94">ẆT c γ -1 Wc = -WT c φ + κ WT c ∇σ gF(x) + ρ(x) ϕ T m s Wc - 1 2 x, û J T x g(x) I m -B( 2 (x)) g T (x) × ∇σ T Wc + κ WT c ∇σ g(x)F(x) ϕ T m s Ŵc + WT c P 2 Ŵc -P 1 ϕ T Ŵc = -WT c ϕϕ T Wc + α(x)ϕ T Wc + WT c β(x) - 1 2 x, û J T x g(x) I m -B( 2 (x)) g T (x) × ∇σ T Wc + WT c P 2 Ŵc -P 1 ϕ T Ŵc<label>(41)</label></formula><p>where</p><formula xml:id="formula_95">α(x) = ρ(x)/m s and β(x) = κ∇σ g(x)F(x) (ϕ T /m s )W c .</formula><p>By the definition of Wc given in <ref type="bibr" target="#b25">(26)</ref>, we derive the last term in <ref type="bibr" target="#b40">(41)</ref> as</p><formula xml:id="formula_96">WT c P 2 Ŵc -P 1 ϕ T Ŵc = WT c P 2 W c -WT c P 2 Wc -WT c P 1 ϕ T W c + WT c P 1 ϕ T Wc . Let Y T = WT c ϕ WT c .</formula><p>Then, (41) can be rewritten as</p><formula xml:id="formula_97">ẆT c γ -1 Wc = -Y T MY + Y T N - 1 2 (x, û)J T x g(x) × I m -B( 2 (x)) g T (x)∇σ T Wc<label>(42)</label></formula><p>where</p><formula xml:id="formula_98">M = ⎡ ⎢ ⎣ I - 1 2 P T 1 - 1 2 P 1 P 2 ⎤ ⎥ ⎦, N = α(x) β(x) + P 2 W c -P 1 ϕ T W c .</formula><p>Substituting ( <ref type="formula" target="#formula_97">42</ref>) into (40) and choosing P i (i = 1, 2) such that the matrix M is positive definite, we have</p><formula xml:id="formula_99">L(t) ≤ J T x f (x) + g(x)û -λ min (M) Y 2 - 1 2 (x, û)J T x g(x)[I m -B( 2 (x))] × g T (x)∇σ T Wc + ζ N Y (<label>43</label></formula><formula xml:id="formula_100">)</formula><p>where λ min (M) denotes the minimum eigenvalue of M, and</p><formula xml:id="formula_101">ζ N is an upper bound of N , i.e., N ≤ ζ N .</formula><p>Based on (x, û) given in <ref type="bibr" target="#b34">(35)</ref>, we divide (43) into the following two cases for discussion.</p><p>Case I ( (x, û) = 0): In this sense, the first term in (43) is negative. Since x &gt; 0 is guaranteed by adding the PE signal, one can obtain that there exists a positive constant μ such that 0 &lt; μ ≤ ẋ implies J T</p><p>x ẋ ≤ -J x μ &lt; 0 based on Archimedean property of R <ref type="bibr" target="#b50">[51]</ref>. Then, ( <ref type="formula" target="#formula_99">43</ref>) is developed as</p><formula xml:id="formula_102">L(t) ≤ J x ẋ -λ min (M) Y 2 + ζ N Y ≤ -J x μ + 1 4 ζ 2 N /λ min (M) -λ min (M) Y - 1 2 ζ N /λ min (M) 2 . (<label>44</label></formula><formula xml:id="formula_103">)</formula><p>Thus, (44) yields L(t) &lt; 0 as long as one of the following conditions holds:</p><formula xml:id="formula_104">J x &gt; ζ 2 N 4μλ min (M) D 1 , or Y &gt; ζ N λ min (M)</formula><p>. <ref type="bibr" target="#b44">(45)</ref> Noticing that Y ≤ 1 + ϕ 2 Wc and observing the fact that ϕ ≤ (1/2), we can derive Y ≤ ( √ 5/2) Wc . Then, by using <ref type="bibr" target="#b44">(45)</ref>, we obtain</p><formula xml:id="formula_105">Wc &gt; 2ζ N √ 5λ min (M) D 2 .</formula><p>Case II ( (x, û) = 1): In this circumstance, the first term in ( <ref type="formula" target="#formula_99">43</ref>) is nonnegative. It implies that the control (27) might not stabilize system (2). Then, by using ( <ref type="formula" target="#formula_59">27</ref>), <ref type="bibr" target="#b42">(43)</ref> becomes</p><formula xml:id="formula_106">L(t) ≤ J T x f (x) -κJ T x g(x) tanh( 2 (x)) + 1 2κ I m -B( 2 (x)) g T (x)∇σ T Wc -λ min (M) Y 2 + ζ N Y . (<label>46</label></formula><formula xml:id="formula_107">)</formula><p>Utilizing <ref type="bibr" target="#b37">(38)</ref>, we get</p><formula xml:id="formula_108">tanh( 2 (x)) + 1 2κ I m -B( 2 (x)) g T (x)∇σ T Wc = tanh( 1 (x)) -O ( 1 (x) -2 (x)) 2 . (<label>47</label></formula><formula xml:id="formula_109">)</formula><p>Substituting ( <ref type="formula" target="#formula_108">47</ref>) into (46) and using <ref type="bibr" target="#b23">(24)</ref>, we have</p><formula xml:id="formula_110">L(t) ≤ J T x f (x) + g(x)u * -J T x g(x)ε u * -κJ T x g(x)O ( 1 (x) -2 (x)) 2 -λ min (M) Y 2 + ζ N Y . (<label>48</label></formula><formula xml:id="formula_111">)</formula><p>Using Assumptions 3-5 and Fact 1, ( <ref type="formula" target="#formula_110">48</ref>) is developed as</p><formula xml:id="formula_112">L(t) ≤ J T x f (x) + g(x)u * -J T x g(x)ε u * -κJ T x g(x)O ( 1 (x) -2 (x)) 2 -λ min (M) Y 2 + ζ N Y ≤ -λ min (B(x)) J x 2 + g M (κc 1 + b ε u * ) J x + g M κc 2 J x Wc -λ min (M) Y 2 + ζ N Y . (<label>49</label></formula><formula xml:id="formula_113">)</formula><p>For every i ∈ (0, 1), i = 1, 2, let 1 + 2 = 1. Then, (49) can be represented as</p><formula xml:id="formula_114">L(t) ≤ -1 λ min (B(x)) J x 2 + g M (κc 1 + b ε u * ) J x -2 λ min (B(x)) J x - g M κc 2 Wc 2 2 λ min (B(x)) 2 + (g M κc 2 ) 2 4 2 λ min (B(x)) Wc 2 -λ min (M) Y 2 + ζ N Y . (<label>50</label></formula><formula xml:id="formula_115">)</formula><p>Noticing that Wc 2 ≤ Y 2 , we develop (50) as</p><formula xml:id="formula_116">L(t) ≤ -1 λ min (B(x)) J x 2 + g M (κc 1 + b ε u * ) J x -λ min (M) - (g M κc 2 ) 2 4 2 λ min (B(x)) Y 2 + ζ N Y = -1 λ min (B(x)) J x - g M (κc 1 + b ε u * ) 2 1 λ min (B(x)) 2 - 4 2 λ min (B(x)) Y - 2 2 λ min (B(x))ζ N 2 + g 2 M (κc 1 + b ε u * ) 2 4 1 λ min (B(x)) + 2 λ min (B(x))ζ 2 N (<label>51</label></formula><formula xml:id="formula_117">)</formula><p>where</p><formula xml:id="formula_118">= 4 2 λ min (B(x))λ min (M) -g 2 M κ 2 c 2 2 .</formula><p>Observe that the value of depends on the parameters 2 , B(x), and P i (i 1, 2). Hence, the value of can be kept positive by properly selecting these parameters.</p><p>For convenience, we denote</p><formula xml:id="formula_119">N = g 2 M (κc 1 + b ε u * ) 2 4 1 λ min (B(x)) + 2 λ min (B(x))ζ 2 N .</formula><p>Then, (51) implies L(t) &lt; 0 as long as one of the following conditions holds:</p><formula xml:id="formula_120">J x &gt; g M (κc 1 + b ε u * ) 2 1 λ min (B(x)) + N 1 λ min (B(x)) D 1 or Y &gt; 2 2 λ min (B(x))ζ N + 4 2 λ min (B(x))N . (<label>52</label></formula><formula xml:id="formula_121">)</formula><p>Observe that Y ≤ ( √ 5/2) Wc . Then, by using (52), we have</p><formula xml:id="formula_122">Wc &gt; 4 2 λ min (B(x))ζ N √ 5 + 4 2 λ min (B(x))N 5 D 2 .</formula><p>Combining cases I and II and using the standard Lyapunov extension theorem <ref type="bibr" target="#b52">[53]</ref>, we can obtain that J x is UUB with ultimate bound D 1 (or D 1 ) and the weight estimation error Wc is also UUB with ultimate bound D 2 (or D 2 ).</p><p>Remark 5: Note that given in <ref type="bibr" target="#b30">(31)</ref> satisfies that ∈ (-m ln 4, m ln 4). By Assumptions 3, 5, and 6, we derive that ρ(x) given in <ref type="bibr" target="#b30">(31)</ref> is bounded. Meanwhile, by ϕ and m s given in <ref type="bibr" target="#b33">(34)</ref>, we can obtain ϕ ≤ 1/2 and 1/m s ≤ 1. Therefore, N given in ( <ref type="formula" target="#formula_97">42</ref>) is bounded.</p><p>Remark 6: Because J(x) given in Assumption 4 is often obtained by selecting polynomials, one can derive that J x is also a polynomial with respect to x. Since Theorem 2 has verified that J x is UUB, one can easily obtain that the trajectory of the closed-loop system is also UUB.</p><p>Next, we develop a theorem to show rigorously that the closed-loop system is stable in the sense of uniform ultimate boundedness during NN learning process.</p><p>Theorem 3: Consider system (2) with associated HJB equation <ref type="bibr" target="#b8">(9)</ref>. Let Assumptions 3, 5, and 6 hold and let the control input be given in <ref type="bibr" target="#b26">(27)</ref> for system <ref type="bibr" target="#b1">(2)</ref>. Meanwhile, the weight update law for the critic NN is given in <ref type="bibr" target="#b33">(34)</ref>. Then, the closedloop system is guaranteed to be UUB with the ultimate bound ˜ x given in the subsequent (60).</p><p>Proof: Consider the Lyapunov function candidate V * (x) given in <ref type="bibr" target="#b5">(6)</ref>. Taking the derivative of V * (x) along the system trajectory ẋ = f (x) + g(x)û, we get</p><formula xml:id="formula_123">V * (x) = V * T x f (x) + V * T x g(x)û. (<label>53</label></formula><formula xml:id="formula_124">)</formula><p>From ( <ref type="formula" target="#formula_12">8</ref>) and ( <ref type="formula">9</ref>), we have</p><formula xml:id="formula_125">V * x T f (x) = -V * T x g(x)u * -d 2 M (x) -x T Qx -2κ u * 0 tanh -T (υ/κ)dυ. (<label>54</label></formula><formula xml:id="formula_126">)</formula><p>Substituting ( <ref type="formula" target="#formula_125">54</ref>) into (53), we derive</p><formula xml:id="formula_127">V * (x) = V * T x g(x) û -u * -x T Qx -d 2 M (x) -2κ u * 0 tanh -T (υ/κ)dυ. (<label>55</label></formula><formula xml:id="formula_128">)</formula><p>Combining <ref type="bibr" target="#b19">(20)</ref> with <ref type="bibr" target="#b54">(55)</ref> and observing the fact that 2κ</p><formula xml:id="formula_129">u * 0 tanh -T (υ/κ)dυ is positive definite, we have V * (x) ≤ -x T Qx + W T c ∇σ g(x) û -u * + ∇ε T g(x) û -u * . (<label>56</label></formula><formula xml:id="formula_130">)</formula><p>On the other hand, by using ( <ref type="formula" target="#formula_53">24</ref>), <ref type="bibr" target="#b26">(27)</ref>, and (38), we get</p><formula xml:id="formula_131">û -u * = κ tanh( 1 (x)) -tanh( 2 (x)) -ε u * = 1 2 I m -B( 2 (x)) g T ∇σ T Wc + κO ( 1 (x) -2 (x)) 2 -ε u * .</formula><p>Then, employing Assumptions 5 and 6 and Fact 1, we derive</p><formula xml:id="formula_132">û -u * ≤ g M b σ x + κc 2 Wc + κc 1 + b ε u * . (<label>57</label></formula><formula xml:id="formula_133">)</formula><p>By Theorem 2, we know that Wc is UUB with ultimate bound D 2 (or D 2 ). Let M = max{D 2 , D 2 }. From (57), we have</p><formula xml:id="formula_134">û -u * ≤ g M b σ x + κc 2 M + κc 1 + b ε u * T 1 . (58) IEEE TRANSACTIONS ON CYBERNETICS</formula><p>Hence, by using (58) and Assumptions 3, 5, and 6, (56) becomes</p><formula xml:id="formula_135">V * (x) ≤ -λ min (Q) x 2 + g M (W M b σ x + b εx )T 1 . (59)</formula><p>Therefore, (59) yields V * (x) &lt; 0 as long as x is out of</p><formula xml:id="formula_136">˜ x = x : x ≤ g M (W M b σ x + b εx )T 1 λ min (Q) . (<label>60</label></formula><formula xml:id="formula_137">)</formula><p>According to the standard Lyapunov extension theorem <ref type="bibr" target="#b52">[53]</ref>, this verifies that the trajectory of the closed-loop system is UUB. That is, the closed-loop system is stable in the sense of uniform ultimate boundedness during NN learning process.</p><p>Corollary 1: The control û given in <ref type="bibr" target="#b26">(27)</ref> can approximate the optimal control u * within a finite bound T 1 given in (58). Meanwhile, V(x) given in <ref type="bibr" target="#b24">(25)</ref> will be close to the optimal value V * (x) within a finite bound T 2 given in (61).</p><p>Proof: The first part of Corollary 1 has been proved in Theorem 3. The second part of Corollary 1 is derived as follows. Using ( <ref type="formula" target="#formula_42">20</ref>), <ref type="bibr" target="#b24">(25)</ref>, and Assumptions 5 and 6, we have</p><formula xml:id="formula_138">V -V * ≤ b σ M + b ε u * T 2 (<label>61</label></formula><formula xml:id="formula_139">)</formula><p>where M is given in (58). Remark 7: Noticing the expressions of D 2 and D 2 , we find that M can be kept small by selecting proper parameters (e.g., λ min (M) is large enough). In addition, as pointed out in <ref type="bibr" target="#b53">[54]</ref> and <ref type="bibr" target="#b54">[55]</ref>, if the number of neurons N 0 goes to infinity, there exist ε → 0 and ∇ε → 0. Hence, ε u * can be arbitrarily small when N 0 is large enough. That is, b ε u * can be kept sufficiently small. Therefore, T 2 given in (61) can be made very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SIMULATION RESULTS</head><p>In this section, two examples are provided to illustrate the effectiveness of the developed theoretical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Example 1</head><p>Consider the uncertain CT linear system given by</p><formula xml:id="formula_140">ẋ = Ax + B u(x) + qx 1 sin 5 (x 2 ) cos 2 (x 3 )<label>(62)</label></formula><p>where</p><formula xml:id="formula_141">A = ⎡ ⎣ -1.01887 0.90506 -0.00215 0.82225 -1.07741 -0.17555 0 0 -1 ⎤ ⎦ , B = ⎡ ⎣ 0 0 1 ⎤ ⎦ with the state x = [x 1 , x 2 , x 3 ] T ∈ R 3 , the control u ∈ A = {u ∈ R : |u| ≤ 1}</formula><p>, and q is an unknown parameter. The term d(x) = qx 1 sin 5 (x 2 ) cos 2 (x 3 ) reflects the uncertainty of system (62).</p><p>For simplicity of discussion, we assume that q ∈ [-2, 2] and d M (x) = 2 x . The nominal system is ẋ = Ax + Bu with A and B given in (62). The corresponding value function is  The activation function for the critic NN is chosen with N 0 = 6 neurons as</p><formula xml:id="formula_142">V(x) = ∞ 0 4 x 2 + x T Qx + W(u) dt where Q = I 3 and W(u) = 2κ u 0 tanh -T (υ/κ)dυ.</formula><formula xml:id="formula_143">σ (x) = x 2 1 , x 2 2 , x 2 3 , x 1 x 2 , x 1 x 3 , x 2 x 3 T and Ŵc = [W c1 , W c2 , . . . , W c6 ] T is the critic NN weight.</formula><p>It should be emphasized that choosing the proper neurons for NNs is still an open question <ref type="bibr" target="#b55">[56]</ref>. In this example, the number of neurons is obtained by computer simulations. We find that selecting six neurons in the hidden layer for the critic NN can lead to satisfactory simulation results. The initial weight for the critic NN is chosen to be zero, and the initial state is set to be x 0 = [1, -1, 0.5] T . The parameters are designed as κ = 1, γ = 0.95. To guarantee the PE condition, a small exploratory signal n(t) = sin 2 (t) cos(t) + sin 2 (2t) cos(0.1t)+sin 2 (1.2t) cos(0.5t)+sin 5 (t)+sin 2 (1.12t)+ cos(2.4t) sin 3 (2.4t) is added to the control u(t) for the first 60 s. The developed control algorithm is implemented by using <ref type="bibr" target="#b26">(27)</ref> and <ref type="bibr" target="#b33">(34)</ref>.</p><p>The computer simulation results are shown in Figs. <ref type="figure" target="#fig_2">2</ref><ref type="figure" target="#fig_3">3</ref><ref type="figure" target="#fig_4">4</ref><ref type="figure" target="#fig_5">5</ref>. Fig. <ref type="figure" target="#fig_2">2</ref> illustrates the evolution of the nominal system state x  during the critic NN learning process. Fig. <ref type="figure" target="#fig_3">3</ref> indicates the convergence of the critic NN weights. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the critic NN weights converge to W c = [0.3871, 0.7402, 2.6356, 0.4915, -1.3225, -1.4737] T . Fig. <ref type="figure" target="#fig_4">4</ref> presents the control input. Fig. <ref type="figure" target="#fig_5">5</ref> shows the trajectories of system (62) under approximate optimal control. From Figs. <ref type="figure" target="#fig_2">2</ref><ref type="figure" target="#fig_3">3</ref><ref type="figure" target="#fig_4">4</ref>, one can find that the developed adaptive control guarantees that all signals in the closed-loop optimal control system are UUB. Moreover, from Fig. <ref type="figure" target="#fig_3">3</ref>, one can find the PE condition ensures the critic NN weight to be convergent in approximately 60 s. By Corollary 1, the critic is considered to be convergent to the approximate optimal value. Then, we apply the derived approximate optimal control to system (62). Fig. <ref type="figure" target="#fig_5">5</ref> shows the approximate optimal control can guarantee system (62) to be stable in the sense of uniform ultimate boundedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Example 2</head><p>Consider the uncertain CT nonlinear system given by <ref type="bibr" target="#b42">[43]</ref>  where</p><formula xml:id="formula_144">ẋ = f (x) + g(x) u(x) + qx 1 sin x 2<label>(63)</label></formula><formula xml:id="formula_145">f (x) = x 1 + x 2 -x 1 x 2 1 + x 2 2 -x 1 + x 2 -x 2 x 2 1 + x 2 2 , g(x) = 0 1 with the state x = [x 1 , x 2 ] T ∈ R 2 , the control u ∈ A = {u ∈ R : |u| ≤ 1}</formula><p>, and q is an unknown parameter. The term d(x) = qx 1 sin x 2 gives rise to the uncertainty of system (63). For simplicity of discussion, in this example, we assume that q ∈ [-1.4, 1.4] and d M (x) = 1.4 x . The nominal system is ẋ = f (x) + g(x)u with f (x) and g(x) given in (63). The corresponding value function is</p><formula xml:id="formula_146">V(x) = ∞ 0 2 x 2 + x T Qx + W(u) dt</formula><p>where Q = I 2 and W(u) = 2κ u 0 tanh -T (υ/κ)dυ. The activation function for the critic NN is chosen with N 0 = 24 neurons as σ (x) = x 2 1 , x The initial weight for the critic NN is selected to be zero (i.e., the initial control u = 0), and the initial system state is set to be x 0 = [0.5, -0.5] T . It is significant to point out that, in this circumstance, the initial control cannot stabilize system (63). To illustrate this fact, we present Fig. <ref type="figure" target="#fig_6">6</ref> (since the closed-loop system turns out to be periodic oscillation, we provide the trajectory of the system for the first 50 s). It also verifies that there is no requirement of the initial stabilizing control for implementing the new developed algorithm. The other parameters and the exploratory signal are chosen to be the same as in Example 1. The exploratory signal is added to u(t) for the first 600 s.   The computer simulation results are illustrated in Figs. 7-10. Fig. <ref type="figure" target="#fig_7">7</ref> shows the evolution of the nominal system state x during the critic NN learning process. Fig. <ref type="figure" target="#fig_8">8</ref> presents the convergence curves of the first eight weights of the critic NN. In fact, the weights of the critic NN converge to W c = [2.5849, 2.0037, 0.6158, 1.1825, 1.5860, -0.1390, 0.6583, -0.5108, 0.6364, 0.6695, -0.1333, 0.3175, -0.1374, 0.3578, -0.2267, 0.3878, 0.2951, -0.0874, 0.1738, -0.0517, 0.1243, -0.0437, 0.1497, -0.0854] T . Fig. <ref type="figure" target="#fig_9">9</ref> shows the control input. Fig. <ref type="figure" target="#fig_10">10</ref> indicates the trajectories of system (63) under approximate optimal control. From Figs. 7-9, one can find that the developed adaptive control guarantees that all signals in the closed-loop optimal control system are UUB. In addition, from Fig. <ref type="figure" target="#fig_8">8</ref>, one can find that the PE condition ensures the weight to be convergent in approximately 600 s. By Corollary 1, the critic is considered to be convergent to the approximate optimal value. Then, we apply the derived approximate optimal control to system (63). Fig. <ref type="figure" target="#fig_10">10</ref> shows that the derived optimal control can keep system (63) stable in the sense of uniform ultimate boundedness. Furthermore, it should be emphasized that, the present algorithm for deriving optimal control differs significantly from the algorithms proposed in <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and <ref type="bibr" target="#b42">[43]</ref>. In our case, no initial stabilizing control is required. This feature has been shown by Fig. <ref type="figure" target="#fig_8">8</ref>, where the initial weight for the critic NN can be zeros. In this situation, the closed-loop system is unstable (see Fig. <ref type="figure" target="#fig_6">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have developed a novel RL-based robust control algorithm for constrained-input uncertain nonlinear CT systems by solving constrained optimal control problems. The present algorithm employs a single critic NN to obtain the approximate optimal control, which guarantees the uncertain nonlinear CT system to be stable in the sense of uniform ultimate boundedness. By using the developed algorithm, no initial stabilizing control is required. A limitation of the present method is that the prior knowledge of f (x) and g(x) is required to be available. In our future work, we will relax the restrictive condition. In addition, if system (1) is nonaffine, then it will be a rather challenging task to design a robust controller. Therefore, we shall also focus on developing RL-based robust control algorithms for unknown nonaffine nonlinear CT systems in the presence of input constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is a monotonic odd function and R is positive definite. Without loss of generality, in this paper, we choose ψ(•) = tanh(•) and R = I m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of the developed control algorithm.</figDesc><graphic coords="7,313.00,53.56,249.60,173.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Evolution of nominal system state x(t) for NN learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence of the critic NN weight Ŵc .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Control input u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Trajectories of system (62) under approximate optimal control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Trajectories of system (63) with initial control u = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Evolution of nominal system state x(t) for NN learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Convergence of the critic NN weight Ŵc .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Control input u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Trajectories of system (63) under approximate optimal control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[W c1 , W c2 , . . . , W c24 ] T is the critic NN weight.Similar to Example 1, the number of neurons is also obtained by computer simulations.</figDesc><table><row><cell cols="2">2 2 , x 1 x 2 , x 4 1 , x 4 2 , x 3 1 x 2 , x 2 1 x 2 2 , x 1 x 3 2 , x 6 1</cell></row><row><cell>x 6 2 , x 5 1 x 2 , x 4 1 x 2 2 , x 3 1 x 3 2 , x 2 1 x 4 2 , x 1 x 5 2 , x 8 1 , x 8 2</cell><cell></cell></row><row><cell>x 7 1 x 2 , x 6 1 x 2 2 , x 5 1 x 3 2 , x 4 1 x 4 2 , x 3 1 x 5 2 , x 2 1 x 6 2 , x 1 x 7 2</cell><cell>T</cell></row><row><cell>and Ŵc =</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61034002, Grant 61233001, Grant 61273140, Grant 61304086, and Grant 61374105, in part by the Beijing Natural Science Foundation under Grant 4132078, and in part by the Early Career Development Award of the State Key Laboratory of Management and Control for Complex Systems. This paper was recommended by Associate Editor D. Wang.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Control Systems With Actuator Saturation: Analysis and Design</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhauser</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neuro-Fuzzy Control of Industrial Systems With Actuator Nonlinearities</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selmic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>SIAM Press</publisher>
			<pubPlace>Philadelphia, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural network control of a class of nonlinear systems with actuator saturation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selmic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="156" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust distributed model predictive control of constrained continuous-time nonlinear systems: A robustness constraint approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1673" to="1678" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Applied Optimal Control: Optimization, Estimation and Control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Syrmos</surname></persName>
		</author>
		<title level="m">Optimal Control</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m">Dynamic Programming</title>
		<meeting><address><addrLine>Princeton, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton Univ. Press</publisher>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond regression: New tools for prediction and analysis in the behavioral sciences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Appl. Math</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<pubPlace>Harvard University, Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adaptive Dynamic Programming for Control: Algorithms and Stability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finite-approximation-errorbased discrete-time iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2820" to="2833" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An iterative adaptive dynamic programming algorithm for optimal control of unknown discrete-time nonlinear systems with constrained inputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="331" to="342" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="634" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Goal representation heuristic dynamic programming on maze navigation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2038" to="2050" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive learning in tracking control based on the dual critic network design</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="913" to="928" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-optimal control for nonzero-sum differential games of continuous-time nonlinear systems using singlenetwork ADP</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="216" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural-network-based constrained optimal control scheme for discrete-time switched nonlinear system using dual heuristic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="839" to="849" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Infinite horizon self-learning optimal control of nonaffine discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="866" to="879" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-sum two-player game theoretic formulation of affine nonlinear discrete-time systems using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehraeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dierks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1641" to="1655" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An optimal approximate dynamic programming algorithm for concave, scalar storage problems with vector-valued controls</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2995" to="3010" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting approximate dynamic programming and its convergence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heydari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2733" to="2743" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement Learning-An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Application of reinforcement learning algorithms for the adaptive computation of the smoothing parameter for probabilistic neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kusy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zajdel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="105" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning for partially observable dynamic processes: Adaptive dynamic programming using measured output data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Near-optimal online control of uncertain nonlinear continuous-time systems based on concurrent learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07">Jul. 2014</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Off-policy reinforcement learning for H ∞ control design</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal control for unknown discrete-time nonlinear Markov jump systems using adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2141" to="2155" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforcement learning design-based adaptive tracking control with less learning parameters for nonlinear discrete-time MIMO systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="176" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning for continuous-time input-affine nonlinear systems with simultaneous invariant explorations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Policy iteration algorithm for online design of robust control for a class of continuous-time nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="627" to="632" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online actor-critic algorithm to solve the continuous-time infinite horizon optimal control problem</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="878" to="888" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data-driven robust approximate optimal tracking control for unknown general nonlinear systems using adaptive dynamic programming method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2226" to="2236" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel actor-critic-identifier architecture for approximate optimal control of uncertain nonlinear systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="92" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Approximate Dynamic Programming for Feedback Control</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural-network-based online optimal control for uncertain non-linear continuous-time systems with control constraints</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theory Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2037" to="2047" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discrete-time online learning control for a class of unknown nonaffine nonlinear systems using reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continuous-time Q-learning for infinite-horizon discounted cost linear quadratic regulator problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aurangzeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="176" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network HJB approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="791" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1780" to="1792" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reinforcement learning for adaptive optimal control of unknown continuous-time nonlinear systems with input constraints</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="553" to="566" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2699" to="2704" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bounded robust control of nonlinear systems using neural network-based HJB solution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Adhyaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust adaptive dynamic programming and feedback stabilization of nonlinear systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="882" to="893" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimal control of affine nonlinear continuous-time systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dierks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Control Conf</title>
		<meeting>Amer. Control Conf<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jun./Jul. 2010</date>
			<biblScope unit="page" from="1568" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Continuous state feedback guaranteeing uniform ultimate boundedness for uncertain dynamic systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Corless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leitmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1139" to="1144" />
			<date type="published" when="1981-10">Oct. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An optimal control approach to robust control of robot manipulators</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot. Autom</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Khalil</surname></persName>
		</author>
		<title level="m">Nonlinear Systems, 3rd ed</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<title level="m">Functional Analysis, 2nd ed</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Galerkin approximations of the generalized Hamilton-Jacobi-Bellman equation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2159" to="2177" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<title level="m">Principles of Mathematical Analysis</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural networkbased optimal adaptive output feedback control of a helicopter UAV</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zargarzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1061" to="1073" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Neural Network Control of Robot Manipulators and Nonlinear Systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yesildirek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Taylor and Francis</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multilayer feedforward neural networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The Method of Weighted Residuals and Variational Principles</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A single network adaptive critic (SNAC) architecture for optimal control synthesis for a class of nonlinear systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1648" to="1660" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
