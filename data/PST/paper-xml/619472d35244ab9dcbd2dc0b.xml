<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARN LOCALLY, CORRECT GLOBALLY: A DISTRIBUTED ALGORITHM FOR TRAINING GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Morteza</forename><surname>Ramezani</surname></persName>
							<email>morteza@cse.psu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mahmut</forename><forename type="middle">T</forename><surname>Kandemir</surname></persName>
							<email>kandemir@cse.psu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Sivasubramaniam</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<postCode>16802</postCode>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARN LOCALLY, CORRECT GLOBALLY: A DISTRIBUTED ALGORITHM FOR TRAINING GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes in a graph, and the privacy concern due to the centralized storage and model learning have spurred the need to design an effective distributed algorithm for GNN training. However, existing distributed GNN training methods impose either excessive communication costs or large memory overheads that hinders their scalability. To overcome these issues, we propose a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, we propose to apply Global Server Corrections on the server to refine the locally learned models. We rigorously analyze the convergence of distributed methods with periodic model averaging for training GNNs and show that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error. However, this residual error can be eliminated by utilizing the proposed global corrections to entail fast convergence rate. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) have achieved impressive results across numerous graph-based applications, including social networks <ref type="bibr" target="#b13">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b8">Deng et al., 2019)</ref>, recommendation systems <ref type="bibr" target="#b37">(Ying et al., 2018;</ref><ref type="bibr" target="#b35">Wang et al., 2018)</ref>, and drug discovery <ref type="bibr" target="#b11">(Fout et al., 2017;</ref><ref type="bibr" target="#b9">Do et al., 2019;</ref><ref type="bibr" target="#b12">Ghorbani et al., 2022;</ref><ref type="bibr" target="#b10">Faez et al., 2021)</ref>. Despite their recent success, effective training of GNNs on large-scale real-world graphs, such as Facebook social network <ref type="bibr" target="#b1">(Boldi &amp; Vigna, 2004)</ref>, remains challenging. Although several attempts have been made to scale GNN training by sampling techniques <ref type="bibr" target="#b13">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b42">Zou et al., 2019;</ref><ref type="bibr" target="#b39">Zeng et al., 2020;</ref><ref type="bibr" target="#b4">Chiang et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2018;</ref><ref type="bibr" target="#b40">Zhang et al., 2021;</ref><ref type="bibr" target="#b27">Ramezani et al., 2020)</ref>, they are still inefficient for training on extremely large graphs, due to the unique structure of GNNs and the limited memory capacity/bandwidth of current servers. One potential solution to tackle these limitations is employing distributed training with data parallelism, which have become almost a de facto standard for fast and accurate training for natural language processing <ref type="bibr" target="#b25">(Lin et al., 2021;</ref><ref type="bibr" target="#b14">Hard et al., 2018)</ref> and computer vision <ref type="bibr" target="#b2">(Bonawitz et al., 2019;</ref><ref type="bibr" target="#b21">Konečnỳ et al., 2018)</ref>. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, moving from single machine to multiple machines reduces the training time and alleviates the memory burden on each machine. Besides, scaling the training of GNNs with sampling techniques can result in privacy concerns: existing sampling-based methods require centralized data storage and model learning, which could result in privacy concerns in real-world scenarios <ref type="bibr" target="#b29">(Shin et al., 2018;</ref><ref type="bibr" target="#b36">Wu et al., 2021)</ref>. Fortunately, the privacy in distributed learning can be preserved by avoiding mutual access to data between different local machines, and using only a trusted third party server to access the entire data.</p><p>Nonetheless, generalizing the existing data parallelism techniques of classical distributed training to the graph domain is non-trivial, which is mainly due to the dependency between nodes in a graph. For example, unlike solving image classification problems where images are mutually independent, such that we can divide the image dataset into several partitions without worrying about the dependency between images; GNNs are heavily relying on the information inherent to a node and its neighboring nodes. As a result, partitioning the graph leads to subgraphs with edges spanning subgraphs (cut-edges), which will cause information loss and hinder the performance of the model <ref type="bibr" target="#b0">(Angerd et al., 2020)</ref>. To cope with this problem, <ref type="bibr" target="#b26">(Md et al., 2021;</ref><ref type="bibr" target="#b18">Jiang &amp; Rumi, 2021;</ref><ref type="bibr" target="#b0">Angerd et al., 2020)</ref> propose to transfer node features and <ref type="bibr" target="#b41">(Zheng et al., 2020;</ref><ref type="bibr" target="#b33">Tripathy et al., 2020;</ref><ref type="bibr" target="#b28">Scardapane et al., 2020)</ref> propose to transfer both the node feature and its hidden embeddings between local machines, both of which can cause significant storage/communication overhead and privacy concerns <ref type="bibr" target="#b29">(Shin et al., 2018;</ref><ref type="bibr" target="#b36">Wu et al., 2021)</ref>.  <ref type="bibr" target="#b7">et al., 2012;</ref><ref type="bibr" target="#b24">Li et al., 2020b)</ref>) suffers from significant accuracy drop and cannot achieve the same accuracy as the single machine training, even by increasing the number of communication. However, Global Graph Sampling (GGS) can successfully reach the baseline by considering the cut-edges and allowing feature transfer, at the cost of significant communication overhead, and potential violation of privacy.</p><p>In this paper, we propose a communication-efficient distributed GNN training method, called Learn Locally, Correct Globally (LLCG). To reduce the communication overhead, inspired by the recent success of the distributed optimization with periodic averaging <ref type="bibr" target="#b30">(Stich, 2019;</ref><ref type="bibr" target="#b38">Yu et al., 2019)</ref> To get a deeper understanding on the necessity of Global Server Correction, we provide the first theoretical analysis on the convergence of distributed training for GNNs with periodic averaging. In particular, we show that solely averaging the local machine models and ignoring the global graph structure will suffer from an irreducible residual error, which provides sufficient explanation on why Parallel SGD with Periodic Averaging can never achieve the same performance as the model trained on a single machine in Figure <ref type="figure" target="#fig_1">2 (a)</ref>. Then, we theoretically analyze the convergence of our proposal LLCG . We show that by carefully choosing the number of global correction steps, LLCG can overcome the aforementioned residual error and enjoys O 1/ √ P T convergence rate with P local machines and T iterations of gradient updates, which matches the rate of <ref type="bibr" target="#b38">(Yu et al., 2019)</ref> on a general (not specific for GNN training) non-convex optimization setting. Finally, we conduct comprehensive evaluations on real-world graph datasets with ablation study to validate the effectiveness of LLCG and its improvements over the existing distributed methods.</p><p>Related works. Recently, several attempts have been made on distributed GNN training. According to how they deal with the input/hidden feature of nodes that are associated with the cut-edges (i.e., the edges spanning subgraphs of each local machine), existing methods can be classified into two main categories: (1) Input feature only communication-based methods: In these methods, each local machine receives the input features of all nodes required for the gradient computation from other machines, and trains individually. However, since the number of required nodes grows exponentially with the number of layers, these methods suffer from a significant communication and storage overhead. To alleviate these issues, <ref type="bibr" target="#b26">(Md et al., 2021)</ref> proposes to split the original graph using a min-cut graph partition algorithm that can minimize the number of cut-edges. <ref type="bibr" target="#b18">(Jiang &amp; Rumi, 2021)</ref> proposes to use importance sampling to assign nodes on the local machine with a higher probability. <ref type="bibr" target="#b0">(Angerd et al., 2020)</ref> proposes to sample and save a small subgraph from other local machines as an approximation of the original graph structure. Nonetheless, these methods are limited to a very shallow GNN structure and suffer from significant performance degradation when the original graph is dense. (2) Input and hidden feature communication-based methods: These methods propose to communicate hidden features in addition to the input node features. Although these methods reduce the number of transferred bytes during each communication round (due to the smaller size of hidden embedding and less required nodes features), the number of communication rounds grows linearly as the number of layers, and are prone to more communication delay. To address these issues, in addition to optimal partitioning of the graph, <ref type="bibr" target="#b41">(Zheng et al., 2020)</ref> proposes to use sparse embedding to reduce the number of bytes to communicate and <ref type="bibr" target="#b33">(Tripathy et al., 2020)</ref> proposes several graph partitioning techniques to diminish the communication overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND PROBLEM FORMULATION</head><p>In this section, we start by describing Graph Convolutional Network (GCN) and its training algorithm on a single machine, then formulate the problem of distributed GCN training. Note that we use GCN with mean aggregation for simplicity, however, our discussion is also applicable to other GNN architectures, such as SAGE <ref type="bibr" target="#b13">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref>, ResGCN <ref type="bibr" target="#b22">(Li et al., 2019)</ref> and APPNP <ref type="bibr" target="#b20">(Klicpera et al., 2019)</ref>.</p><p>Training GCN on a single machine. Here, we consider the semi-supervised node classification in an undirected graph G(V, E) with N = |V| nodes and |E| edges. Each node v i ∈ V is associated with a pair (x i , y i ), where x i ∈ R d is the input feature vector, y i ∈ R |C| is the ground truth label, and C is the candidate labels in the multi-class classifications. Besides, let X = [x 1 , . . . , x N ] ∈ R N ×d denote the input node feature matrix. Our goal is to find a set of parameters θ = {W ( ) } L =1 by minimizing the empirical loss L(θ) over all nodes in the training set, i.e.,</p><formula xml:id="formula_0">L(θ) = 1 N i∈V φ(h (L) i , y i ),<label>h</label></formula><formula xml:id="formula_1">( ) i = σ 1 |N (v i )| j∈N (vi) h ( −1) j W ( ) ,<label>(1)</label></formula><p>where φ(•, •) is the loss function (e.g., cross entropy loss), σ(•) is the activation function (e.g., ReLU), and N (v i ) is the neighborhood of node v i . In practice, we can update the model parameters by the stochastic gradient computed on a sampled mini-batch (using full-neighbors) by</p><formula xml:id="formula_2">∇L(θ, ξ) = 1 B i∈ξ ∇φ(h (L) i , y i ),<label>(2)</label></formula><p>where ξ denotes an i.i.d. sampled mini-batch of size B and we have E[ ∇L(θ, ξ)] = ∇L(θ).</p><p>Distributed GCN training with periodic averaging. In this paper, we consider the distributed learning setting with P local machines and a single parameter server. The original input graph G is partitioned into P subgraphs, where G p (V p , E p ) denotes the subgraph on the p-th local machine with N p = |V p | nodes, and X p ∈ R Np×d as the input feature of all nodes in V p located on the p-th machine. Then, the full-batch local gradient ∇L local p (θ p ) is computed as where</p><formula xml:id="formula_3">∇L local p (θ p ) = 1 N p i∈Vp ∇φ(h (L) i , y i ), h ( ) i = σ 1 |N p (v i )| j∈Np(vi) h ( −1) j W ( ) p ,<label>(3</label></formula><formula xml:id="formula_4">θ p = {W ( ) p } L =1 is the model parameters on the p-th local machine, N p (v i ) = {v j |(v i , v j ) ∈ E p }</formula><p>is the local neighbors of node v i on the p-th local machine. When the graph is large, the computational complexity of forward and backward propagation could be very high. One practical solution is to compute the stochastic gradient on a sampled mini-batch with neighbor sampling, i.e., At the beginning of the training phase, all local models θ t p are far from the optimal solution and will receive a gradient ∇L ), such that we can chose a larger local epoch size to reduce the number of communications, without worrying about the divergence of local models. By doing so, after total number of T = R r=1 Kρ r iterations, LLCG only requires R = log ρ T K rounds of communications. Therefore, compared to the fully-synchronous method, we can significantly reduce the total number of communications from O(T ) to O(log ρ T K ).</p><formula xml:id="formula_5">∇L local p (θ p , ξ p ) = 1 B p i∈ξp ∇φ( h(L) i , y i ), h( ) i = σ 1 | Ñp (v i )| j∈ Ñp(vi) h( −1) j W ( ) p , (4) where ξ p is an i.i.d. sampled mini-batch of B p nodes, Ñp (v i ) ⊂ N (v i ) is the sampled neighbors.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GLOBAL SERVER CORRECTION</head><p>The design of the global server correction is to ensure that the trained model not only learns from the data on each local machine, but also learns the global structure of the graph, thus reducing the information loss caused by graph partitioning and avoiding cut-edges. Before the correction, the server receives the locally trained models from all local machines (line 10) and applies model parameter averaging (line 12). Next, S server correction steps are applied on top of the averaged model (line 13 to 18). During the correction, the server first constructs a mini-batch ξ t using fullneighbors<ref type="foot" target="#foot_0">1</ref> (line 15), compute the stochastic gradient ∇L( θt , ξ t ) on the constructed mini-batch by Eq. 2 (line 16) and update the averaged model θt for S iterations (line 17). The number of correction steps S<ref type="foot" target="#foot_1">2</ref> depends on the heterogeneity among the subgraphs on each local machine: the more heterogeneous the subgraphs are, the more correction steps are required to better refine the averaged model and reduce the divergence across the local models. Note that, the heterogeneity is minimized when employing GGS (Figure <ref type="figure" target="#fig_1">2</ref>) with the local machines having access to the full graph, as a result. However, GGS requires sampling from the global graph and communication at every iteration, which results in additional overhead and lower efficiency. Instead, in LLCG we are trading computation on the server for the costly feature communication, and only requires periodic communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>In this section, we provide the convergence analysis on the distributed training of GCN under two different settings, i.e., with and without server correction. We first introduce the notations and assumptions for the analysis (Section 4.1). Then, we show that periodic averaging of local machine models alone and ignoring the global graph structure will suffer from an irreducible residual error (Section 4.2). Finally, we show that this residual error can be eliminated by running server correction steps after each periodic averaging step on the server (Section 4.3). </p><formula xml:id="formula_6">∇ℒ 𝜃 ≠ 0.5× ∇ℒ ! "#$%" 𝜃 + ∇ℒ &amp; "#$%" (𝜃) 𝔼 - ∇ℒ ! "#$%" (𝜃) ≠ ∇ℒ ! "#$%" 𝜃 𝔼 - ∇ℒ &amp; "#$%" (𝜃) ≠ ∇ℒ &amp; "#$%" (𝜃) ∇L(θ) = ∇L full 1 (θ) + ∇L full 2 (θ) 2 ∇L(θ) = ∇L local 1 (θ) + ∇L local 2 (θ) 2 E ∇L local 1 (θ, ξ1) = ∇L local 1 (θ), E ∇L local 1 (θ, ξ2) = ∇L local 2 (θ) ∇L local 2 (θ, ξ2) ∇L local 1 (θ, ξ1) ∇L local 1 (θ) ∇L local 2 (θ) ∇L full 2 (θ) ∇L full 1 (θ)</formula><p>\mathcal{N}(v_i) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NOTATIONS AND ASSUMPTIONS</head><p>Let us first recall the notations defined in Section 2, where L(θ) denotes the global objective function computed using the all node features X and the original graph G, L p (θ) denotes the local objective function computed using the local node features X p and local graph G p , θ t p denotes the model parameters on the p-th local machine at the t-th step, and θt = 1 P P p=1 θ t p denotes the virtual averaged model at the t-th step. In the non-convex optimization, our goal is to show the expected gradient of the global objective on the virtual averaged model parameters E[ ∇L( θt ) 2 ] decreases as the number of local machines P and the number of training steps T increase. Besides, we introduce ∇L full p (θ) as the gradient computed on the p-th local machine but have access the full node features X and the original graph structure G as</p><formula xml:id="formula_7">∇L full p (θ) = 1 |V p | i∈Vp ∇φ(h (L) i , y i ),<label>h</label></formula><formula xml:id="formula_8">( ) i = σ 1 |N (v i )| j∈N (vi) h ( −1) j W ( ) p . (5)</formula><p>Please refer to Figure <ref type="figure" target="#fig_4">3</ref> for an illustration of different gradient computations. Besides, we introduce local-global gradient discrepancy as</p><formula xml:id="formula_9">κ 2 = κ 2 A + κ 2 X , where κ 2 A = max p∈[P ] { ∇L local p (θ) − ∇L full p (θ) 2 }</formula><p>is the maximum difference between the gradient computed on the local machine with and without having access to the global graph structure, which is mainly due to fact that the local machines are oblivious to the full graph information; and κ 2 X = max p∈[P ] { ∇L full p (θ) − ∇L(θ) 2 } is the maximum difference between the gradient computed using the local node and all nodes, which is mainly due to the heterogeneity of the node features on each local machine, and we have κ 2 X = 0 if the nodes are i.i.d. sampled to each local machine. Notice that local-global gradient discrepancy κ 2 plays an important role in our theoretical results.</p><p>For the convergence analysis, we make the following standard assumptions.</p><p>Assumption 1 The stochastic gradient on the p-th local machine (with neighbor sampling) has stochastic gradient variance bounded by σ 2 var and stochastic gradient bias bounded by σ 2 bias , i.e.,</p><formula xml:id="formula_10">E[ ∇L local p (θ; ξ) − E[ ∇L local p (θ; ξ)] 2 ] ≤ σ 2 var , E[ E[ ∇L local p (θ; ξ)] − ∇L local p (θ) 2 ] ≤ σ 2 bias . Assumption 2</formula><p>The stochastic gradient for global server correction (with full neighbors) has stochastic gradient variance bounded by</p><formula xml:id="formula_11">σ 2 global , i.e., E[ ∇L full p (θ; ξ) − ∇L full p (θ)] 2 ] ≤ σ 2 global .</formula><p>The existence of stochastic gradient bias and variance in sampling-based GNN training have been studied in <ref type="bibr" target="#b5">(Cong et al., 2020;</ref><ref type="bibr">2021)</ref>, where <ref type="bibr" target="#b6">(Cong et al., 2021)</ref> further quantify the stochastic gradient bias and variance as a function of the number of GCN layers. In particular, they show that the existence of σ 2 bias is due to neighbor sampling and non-linear activation, and we have σ 2 bias = 0 if all neighbors are used or the non-linear activation is removed. The existence of σ 2 var is because we are sampling mini-batches to compute the stochastic gradient on each local machine during training. As the mini-batch size increases, σ 2 var will be decreasing, and we have σ 2 var = 0 when using full-batch.  Note that we choose GGS as a reasonable representation for most existing proposals <ref type="bibr" target="#b26">(Md et al., 2021;</ref><ref type="bibr" target="#b41">Zheng et al., 2020;</ref><ref type="bibr" target="#b33">Tripathy et al., 2020)</ref> for distributed GNN training, since these methods have very close communication cost and also require a large cluster of machines to truly show their performance improvement. We also use PSGD-PA as a lower bound for communication size, which is widely used in traditional distributed training and similar to the one used in <ref type="bibr" target="#b0">(Angerd et al., 2020;</ref><ref type="bibr" target="#b18">Jiang &amp; Rumi, 2021)</ref>. However, we did not specifically include these methods in our results since we could not reproduce their results in our settings. Please refer to Appendix A for a detailed description of implementation, hardware specification and link to our source code. Datasets and evaluation metric. We compare LLCG and other baselines on real-world semisupervised node classification datasets, details of which are summarized in Table <ref type="table">2</ref> in the Appendix. The input graphs are splitted into multiple subgraphs using METIS before training, then the same set of subgraphs are used for all baselines. For training, we use neighborhood sampling <ref type="bibr" target="#b13">(Hamilton et al., 2017)</ref> with 10 neighbors sampled per node and ρ = 1.1 for LLCG. For a fair comparison, we chose the base local update step K such that LLCG has the same number of local update steps as PSGD-PA. During evaluation, we use full-batch without sampling, and report the performance on the full graph using AUC ROC and F1 Micro as the evaluation metric. Unless otherwise stated, we conduct each experiment five times and report the mean and standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PRIMARY RESULTS</head><p>In this section, we compare our proposed LLCG algorithm with baselines on four datasets. Due to space limitations we defer the detailed discussion on additional datasets to the Appendix A.4.</p><p>LLCG requires same number of communications. It can be seen that PSGD-PA suffers from performance drop compared to other two methods, due to the residual error we discussed in Section 4, while both GGS and LLCG perform well and can achieve the expected accuracy. Note that the performance drop of PSGD-PA can vary across different datasets; in some cases such as Reddit, PSGD-PA can significantly hurt the accuracy, while on other datasets the gap is smaller. Nevertheless, LLCG can always close the gap between PSGD-PA and GGS with minimal overhead. LLCG convergences as fast as GGS. To represent the effect of communication on the real-time convergence, in Figure <ref type="figure">4</ref> (e) and 4 (f), we plot the global training loss (training loss computed on the full-graph on the server) after each communication round. Similar to the accuracy score, the training loss is also computed on the server averaged (and corrected, in case of LLCG) global model. These results clearly indicate that LLCG can improve the convergence over PSGD-PA, while it shows a similar convergence speed to GGS. LLCG exchanges data as little as PSGD-PA. Figure <ref type="figure">4</ref> (g) and 4 (h) show the relation between global validation accuracy with the average size (volume) of communication in bytes. As expected, this figure clearly shows the effectiveness of LLCG . On the one hand, LLCG has a similar amount of communication volume as PSGD-PA but can achieve a higher accuracy. On the other hand, LLCG requires significantly less amount of communication volume than GGS to achieve the same accuracy, which leads to slower training time in real world settings. LLCG works with various GNN models and aggregations. We evaluate four popular GNN models, used in recent graph learning literature: <ref type="bibr">GCN Kipf &amp; Welling (2017)</ref>, <ref type="bibr">SAGE Hamilton et al. (2017)</ref>, <ref type="bibr">GAT Velickovic et al. (2018)</ref> and <ref type="bibr">APPNP Klicpera et al. (2019)</ref>. In Table <ref type="table" target="#tab_3">1</ref>, we summarize the test score and average communication size (in MB) on different datasets for a fixed number of communication rounds. Note that we only include the results for the aggregation methods (GCN or SAGE) that have higher accuracy for the specific datasets, details of which can be found in Appendix A.2. As shown here, LLCG can consistently improve the test accuracy for all different models compared to PSGD-PA, while the communication size is significantly lower than GGS, since LLCG only needs to exchange the model parameters. Effect of local epoch size. Figure <ref type="figure">5</ref> compares the effect of various values of local epoch size K ∈ {1, 4, 16, 64, 128} for fixed ρ and S on the OGB-Arxiv dataset. When using fully synchronous with K = 1, the model suffers from very slow convergence and needs more communications. Further increasing the K to larger values can speed up the training; however, we found a diminishing return point for K &gt; 128 in this dataset and extremely large K in general. Effect of sampling in local machines. In Figure <ref type="figure" target="#fig_6">6</ref>, we report the validation scores per round of communication to compare the effect of neighborhood sampling at local machines. We can observe that when the neighborhood sampling size is reasonably large (i.e., 20%), the performance is very similar to full neighborhood training. However, reducing the neighbor sampling ratio to 5% could result in a larger local stochastic gradient bias σ 2 bias , which requires using more correction steps (S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUDING REMARKS</head><p>In this paper, we propose a novel distributed algorithm for training Graph Neural Networks (GNNs).</p><p>We theoretically analyze various GNN models and discover that, unlike the traditional deep neural networks, due to inherent data samples dependency in GNNs, naively applying periodic parameter averaging leads to a residual error and current solutions to this issue impose huge communication overheads. Instead, our proposal tackles these problems by applying correction on top of locally learned models, to infuse the global structure of the graph back into the network and avoid any costly communication. In addition, through extensive empirical analysis, we support our theoretical findings and demonstrate that LLCG can achieve high accuracy without additional communication costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the speedup and the memory consumption of distributed multimachine training and centralized single machine training on the Reddit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of (a) the validation F1score and (b) the average data communicated per round (in bytes and log-scale) for two different distributed GNN training settings, including Parallel SGD with Periodic Averaging (PSGD-PA) where the cut-edges are ignored and only the model parameters are transferred and Global Graph Sampling (GGS), where the cut-edges are considered and the node features of the cut-edges are transferred to the corresponding local machine, on the Reddit dataset using 8 machines. To better understand the challenge of distributed GNN training, we compare the validation F1score in Figure 2 (a) and the average data communicated per round in Figure 2 (b) for two different distributed GNN training methods on the Reddit dataset. On the one hand, we can observe that when ignoring the cut-edges, Parallel SGD with Periodic Averaging (PSGD-PA<ref type="bibr" target="#b7">(Dean et al., 2012;</ref><ref type="bibr" target="#b24">Li et al., 2020b)</ref>) suffers from significant accuracy drop and cannot achieve the same accuracy as the single machine training, even by increasing the number of communication. However, Global Graph Sampling (GGS) can successfully reach the baseline by considering the cut-edges and allowing feature transfer, at the cost of significant communication overhead, and potential violation of privacy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>An illustration of distributed GCN training with Parallel SGD with Periodic Averaging (PSGD-PA) is summarized in Algorithm 1. Before training, the server maintains a global model θ0 and each local machine keeps a local copy of the same model θ 0 p . During training, the local machine first updates the local model θ t p using the stochastic gradient ∇L local p (θ t p , ξ t p ) computed by Eq. 4 for K iterations (line 8), then sends the local model θ t p to the server (line 10). At each communication step, the server collects and averages the model parameters from the local machines (line 12) and send the averaged model θ t+1 p back to each local machine.Limitations. Although PSGD-PA can significantly reduce the communication overhead by transferring the locally trained models instead of node feature/embeddings (refer to Figure2 (b)), it suffers from performance degeneration due to ignorance of the cut-edges (refer to Figure2 (a)). In the next section, we introduce a communication-efficient algorithm LLCG that does not suffer from this issue, and can achieve almost the same performance as training the model on a single machine.3 PROPOSED ALGORITHM: LEARN LOCALLY CORRECT GLOBALLYIn this section, we describe Learn Locally, Correct Globally (LLCG) for distributed GNN training. LLCG includes two main phases, local training with periodic model averaging and global server correction, to help reduce both the number of required communications and size of transferred data, without compromising the predictive accuracy. We summarize the details of LLCG in Algorithm 2. 3.1 LOCAL TRAINING WITH PERIODIC MODEL AVERAGING At the beginning of a local epoch, each local machine receives the latest global model parameters from the server (line 3). Next, each local machine runs Kρ r iterations to update the local model (line 4 to 9), where K and ρ are the hyper-parameters that control the local epoch size. Note that instead of using a fixed local epoch size as Algorithm 1, we choose to use exponentially increasing local epoch size in LLCG with ρ &gt; 1. The reasons are as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of notations ∇L local p (θ), ∇L local p (θ, ξ p ), and ∇L full p (θ) on two local machines, where the blue node and green circles represent nodes on different local machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 2 Figure 4 :</head><label>24</label><figDesc>Figure 4: Comparing LLCG against PSGD-PA and GGS on real-world datasets. We show the global validation score in terms of the number of communications in (a,b,c,d), the training loss per round of communications in (e,f ), and the global validation score per bytes of exchanged data in (g,h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: Effect of local epoch size (K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we propose Local Training with Periodic Averaging: where each local machine first locally trains a GNN model by ignoring the cut-edges, then sends the trained model to the server for periodic model averaging, and receive the averaged model from server to continue the training. By doing so we eliminate the features exchange phase between server and local machines, but it can result in a significant performance degradation due to the lack of the global graph structure and the dependency between nodes among different machines. To compensate for this error, we propose a Global Server Correction scheme to take advantage of the available global graph structure on the server and refine the averaged locally learned models before sending it back to each local machine. Notice that without Global Server Correction, LLCG is similar to PSGD-PA as introduced in Figure2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Algorithm 1 Distributed GCN training with "Parallel SGD with Periodic Averaging" Input: Global parameters θ0 , local parameters θ 0 p = θ0 , time-step t = 0, learning rate η. 1: for r ← 1 to R do 2: for p ← 1 to P do in parallel Server returns trained GCN model with mint E[ ∇L( θt ) 2 ].</figDesc><table><row><cell>3:</cell><cell cols="4">Parallel training on local machines Local machine p receives the global parameters θ t p ← θt . Communication</cell></row><row><cell>4:</cell><cell>for k ← 1 to K do</cell><cell></cell><cell></cell></row><row><cell>5:</cell><cell>t ← t + 1.</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell cols="4">Local machine p constructs the mini-batch ξ t p with neighbor sampling.</cell></row><row><cell>7:</cell><cell cols="3">Local machine p computes the stochastic gradients ∇L local p (θ t p , ξ t p ).</cell></row><row><cell>8:</cell><cell cols="2">Local machine p updates the local parameter by θ t+1 p</cell><cell cols="2">= θ t p − η ∇L local p (θ t p , ξ t p ).</cell></row><row><cell>9:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell>Local machine p sends the local parameters θ t+1 p</cell><cell cols="3">to the server. Communication</cell></row><row><cell>11: 12:</cell><cell cols="3">end for Server updates the global parameters by parameter averaging θt+1 = 1 P</cell><cell>P p=1 θ t+1 p</cell><cell>.</cell></row><row><cell cols="2">13: end for</cell><cell></cell><cell></cell></row><row><cell cols="2">Output:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>local p (θ t p , ξ t p ) computed by Eq. 4. Using a smaller local update step at the early stage guarantees each local model does not diverge too much from each other before the model averaging step at the server side (line 12). However, towards the end of the training, all local Algorithm 2 Distributed GCN training by "Learn Locally, Correct Globally" Input: Global parameters θ0 , local parameters θ 0 p , time-step t = 0, local step size hyperparameters K, ρ, and learning rate γ, η 1: for r ← 1 to R do 2: for p ← 1 to P do in parallel Server return GCN model with trained mint E[ ∇L( θt ) 2 ]</figDesc><table><row><cell>3: 4:</cell><cell cols="4">Parallel training on local machine Local machine p receives the global parameters θ t Communication p ← θt for k ← 1 to Kρ r do</cell></row><row><cell>5:</cell><cell>t ← t + 1</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell cols="4">Local machine p constructs the mini-batch ξ t p with neighbor sampling</cell></row><row><cell>7:</cell><cell cols="4">Local machine p computes stochastic gradients ∇L local p (θ t p , ξ t p )</cell></row><row><cell>8:</cell><cell cols="2">Local machine p updates model parameter by θ t+1 p</cell><cell cols="2">= θ t p − η ∇L local p (θ t p , ξ t p )</cell></row><row><cell>9:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell>Local machine p sends the local parameters θ t+1 p</cell><cell cols="2">to the server</cell><cell>Communication</cell></row><row><cell>11: 12:</cell><cell cols="4">end for Server updates the global parameters using parameter averaging θt+1 = 1 P</cell><cell>P p=1 θ t+1 p</cell></row><row><cell>13:</cell><cell>for s ← 1 to S do</cell><cell></cell><cell></cell><cell>Server Correction</cell></row><row><cell>14:</cell><cell>t ← t + 1</cell><cell></cell><cell></cell></row><row><cell>15:</cell><cell cols="2">Server constructs a mini-batch ξ t with full-neighbors</cell><cell></cell></row><row><cell>16:</cell><cell cols="2">Server computes the stochastic gradient ∇L( θt , ξ t )</cell><cell></cell></row><row><cell>17:</cell><cell cols="4">Server updates the global parameters by θt+1 = θt − γ ∇L( θt , ξ t )</cell></row><row><cell>18:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="2">19: end for</cell><cell></cell><cell></cell></row><row><cell cols="3">Output: models θ t p will receive relatively smaller gradient ∇L local p (θ t p , ξ t p</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of performance and the average Megabytes of node representation/feature communicated per round on various datasets.</figDesc><table><row><cell></cell><cell>Method</cell><cell>No.</cell><cell>GCN / SAGE</cell><cell>GAT</cell><cell>APPNP</cell></row><row><cell></cell><cell></cell><cell>Comm.</cell><cell cols="3">Performance Avg. MB Performance Avg. MB Performance Avg. MB</cell></row><row><cell>Flickr (F1-score)</cell><cell>PSGD-PA GGS LLCG</cell><cell>50</cell><cell>49.08±0.27 12.57 51.22±0.13 1849.32 50.38±0.20 12.57</cell><cell>51.56±0.28 4.24 52.41±0.29 1895.61 52.01±0.33 4.24</cell><cell>50.81±0.48 8.40 51.33±0.33 1897.82 51.15±0.25 8.40</cell></row><row><cell>OGB-Proteins (ROC-AUC)</cell><cell>PSGD-PA GGS LLCG</cell><cell>100</cell><cell>72.85±0.70 6.20 74.78±0.36 922.42 73.92±0.45 6.20</cell><cell>64.95±1.01 3.14 68.11±0.60 912.79 67.62±0.58 3.14</cell><cell>71.10±0.79 7.31 71.29±0.31 917.20 71.18±0.43 7.31</cell></row><row><cell>OGB-Arxiv (F1-score)</cell><cell>PSGD-PA GGS LLCG</cell><cell>100</cell><cell>69.43±0.21 3.55 70.51±0.26 3391.03 70.21±0.13 3.55</cell><cell>69.88±0.18 3.59 70.82±0.23 3396.79 70.58±0.37 3.59</cell><cell>68.48±0.17 7.71 69.01±0.10 3394.33 68.73±0.29 7.71</cell></row><row><cell>Reddit (F1-score)</cell><cell>PSGD-PA GGS LLCG</cell><cell>75</cell><cell>71.17±1.06 14.83 94.77±0.20 3798.81 94.67±0.15 14.83</cell><cell>70.57±1.24 7.48 95.03±0.48 3805.28 94.73±0.23 7.48</cell><cell>83.48±0.81 11.63 95.23±0.22 3770.46 94.64±0.17 11.63</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that using full neighbors is required for the server correction but not the local machines</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In practice, we found S = 1 or S = 2 works well on most datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and NSF grants 1909004, 1714389, 1912495,  1629915, 1629129, 1763681, 2008398.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We provide a GitHub repository in Appendix A including all code and scripts used in our experimental studies. This repository includes a README.md file, explaining how to install and prepare the code and required packages. Detailed instruction on how to use the partitioning scripts is provided for various datasets. In addition, we provide several configuration files (under scripts/configs) folder for different hyper-parameters on each individual dataset, and a general script (scripts/run-config.py) to run and reproduce the results with these configurations. Details of various models and parameters used in our evaluation studies can also be found in Appendix A.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a biased estimation of the global full-batch gradient, i.e., 1 P P p=1 ∇L local p (θ) = ∇L(θ). This is because each local machine does not have access to the original input graph and full node feature matrix. Note that the aforementioned equivalence is important for the classifcal distributed training analysis <ref type="bibr" target="#b7">Dean et al. (2012)</ref>; <ref type="bibr" target="#b38">Yu et al. (2019)</ref>. (2) The expectation of the local stochastic gradient is a biased estimation of the local full-batch gradient i.e., E[ ∇L local p (θ, ξ p )] = ∇L local p (θ). This is because the stochastic gradient on each local machine is computed by using neighbor sampling, which has been studied in <ref type="bibr" target="#b6">(Cong et al., 2021)</ref>. 8LP 3/4 , then for any T ≥ L 2 P steps of gradient updates we have</p><p>Theorem 1 implies that, by carefully choosing the learning rate η and the local step size K, the gradient norm computed on the virtual averaged model is bounded by</p><p>T 3/4 ) communication rounds, but suffers from an irreducible residual error upper bound O(κ 2 + σ 2 bias ). In the next section, we show that this residual error can be eliminated by applying server correction. Theorem 2 implies that, by carefully choosing the learning rates γ and η, the local step size hyperparameters K, ρ, and the number of global correction steps S, after T steps (R rounds of communication), employing parameter averaging with Global Server Correction, we have the norm of gradient bounded by O(1/ √ P T ), without suffering the residual error that exists in the naive parameter averaging (in Theorem 1). Besides, the server correction step size is proportional to the scale of κ 2 and local stochastic gradient bias σ 2 bias . The larger κ 2 and σ 2 bias , the more corrections are required to eliminate the residual error. However, in practice, we observe that a very small number of correction steps (e.g., S = 1) performs well, which minimizes the computation overhead on the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Real-world simulation. In a real-world distributed setting, the server and local machines are located on different machines, connected through the network <ref type="bibr" target="#b23">(Li et al., 2020a)</ref>. However, for our experiments, we only have access to a single machine with multiple GPUs. As a result, we simulate a real-world distributed learning scenario, such that each GPU is responsible for the computation of two local machines (8 in total) and the CPU acts as the server. For these reasons, in our evaluations, we opted to report the communication size and number of communication rounds, instead of the wall-clock time, which can show the benefit of distributed training. We argue that these are acceptable measures in real-world scenarios as well since the two main factors in distributed training are initializing connection overhead and bandwidth <ref type="bibr" target="#b33">(Tripathy et al., 2020)</ref>. Baselines. To illustrate the effectiveness of LLCG, we setup two general synchronized distributed training techniques as the our baseline methods, namely "Parallel SGD with Parameter Averaging" (PSGD-PA) and "Global Graph Sampling" (GGS), as introduced in Figure <ref type="figure">2</ref>, where the cut-edges in PSGD-PA are ignored and only the model parameters are transferred, but the cut-edges in GGS are considered and the node features of the cut-edges are transferred to the corresponding machine.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Distributed training of graph convolutional networks using subgraph approximation</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Angerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04930</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The webgraph framework I: compression techniques</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
		<idno type="DOI">10.1145/988672.988752</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on World Wide Web</title>
				<meeting>the 13th international conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004-05-17">2004. May 17-20, 2004. 2004</date>
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards federated learning at scale: System design</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Van Overveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Roselander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems<address><addrLine>MLSys; Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03-31">2019. 2019. March 31 -April 2, 2019. mlsys.org, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330925</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">August 4-8, 2019. 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahmut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">August 23-27, 2020. 2020</date>
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the importance of sampling in learning graph convolutional networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Ramezani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02696</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03">2012. December 3-6, 2012. 2012</date>
			<biblScope unit="page" from="1232" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning dynamic context graphs for predicting social events</title>
		<author>
			<persName><forename type="first">Songgaojun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ning</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330919</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">August 4-8, 2019. 2019</date>
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph transformation policy network for chemical reaction prediction</title>
		<author>
			<persName><forename type="first">Kien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330958</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">August 4-8, 2019. 2019</date>
			<biblScope unit="page" from="750" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep graph generators: A survey</title>
		<author>
			<persName><forename type="first">Faezeh</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassaman</forename><surname>Ommi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdieh</forename><surname>Soleymani Baghshah</surname></persName>
		</author>
		<author>
			<persName><surname>Hamid R Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="106675" to="106702" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ra-gcn: Graph convolutional network for disease prediction problems with imbalanced data</title>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soleymani</forename><surname>Mahdieh</surname></persName>
		</author>
		<author>
			<persName><surname>Baghshah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Hamid R Rabiee</surname></persName>
		</author>
		<author>
			<persName><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102272</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03604</idno>
		<title level="m">Federated learning for mobile keyboard prediction</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
				<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Communication-efficient sampling for distributed training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masuma</forename><surname>Akter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07706</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00936</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="9266" to="9275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15704</idno>
		<title level="m">Pritam Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the convergence of fedavg on non-iid data</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. OpenReview.net, 2020b</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hulin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Avestimehr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08815</idno>
		<title level="m">Fednlp: A research platform for federated learning in natural language processing</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Vasimuddin</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guixiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanarayan</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06700</idno>
		<title level="m">Distgnn: Scalable distributed training for large-scale graph neural networks</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gcn meets gpu: Decoupling &quot;when to sample&quot; from &quot;how to sample</title>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Ramezani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><forename type="middle">Di</forename><surname>Lorenzo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06281</idno>
		<title level="m">Distributed graph convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Privacy enhanced matrix factorization for recommendation with local differential privacy</title>
		<author>
			<persName><forename type="first">Hyejin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbum</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1770" to="1782" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local SGD converges fast and communicates little</title>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adaptive graph diffusion networks with hop-wise attention</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15024</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with self-label-enhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reducing communication in graph neural network training</title>
		<author>
			<persName><forename type="first">Alok</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydin</forename><surname>Buluc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03300</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219869</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. August 19-23. 2018. 2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fedgnn: Federated graph neural network for privacy-preserving recommendation</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04925</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219890</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. August 19-23. 2018. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5693" to="5700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A biased graph neural network sampler with near-optimal regret</title>
		<author>
			<persName><forename type="first">Qingru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01089</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Distdgl: Distributed graph neural network training for billion-scale graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05337</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="11247" to="11256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
