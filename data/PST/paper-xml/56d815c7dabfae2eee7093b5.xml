<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Task-Dependent Distributed Representations by Backpropagation Through Structure C h r i s t o p h Goller</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Automated Reasoning Group Neural Information Processing Department Computer Science Institute Computer Science</orgName>
								<orgName type="institution">Technical University Munich University of Ulm</orgName>
								<address>
									<postCode>D-80290, D-89069</postCode>
									<settlement>Miinchen, Ulm</settlement>
									<country>Germany Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Task-Dependent Distributed Representations by Backpropagation Through Structure C h r i s t o p h Goller</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E4E4EEA89A984A8A5B02740044AA36DF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we got superior results on that domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introdluction</head><p>In the late eighties connectionism had been blamed of being unable to represent and process complex composite symbolic structures like trees, graphs, lists and terms. One of the most convincing counterexamples to this criticism was given with the Recursive Autoassociative Memory (RAAM) <ref type="bibr" target="#b6">[7]</ref>, a method for generating fixed-width distributed representations for variable-sized recursive data structures.</p><p>There have been several publications showing the appropriateness of representations produced by the RAAM for subsequent classification tasks [3, 61 and also for more complex tasks even with structured output 12, 41. Our approach, however, is different. We present a simple architecture together with a novel supervised learning scheme that we call backpropagation through structure (BPTS) in analogy to backpropagation through time for recurrent networks <ref type="bibr">[lo]</ref>. It allows us to generate distributed representations for symbolic structures which are exclusively tuned for the intended task.</p><p>The next two sections describe the proposed architecture and the corresponding learning scheme in detail. Section 4 presents experimental results on a set of basic classification tasks (detectors) on logical terms. We show that all classification problems can be solved with smaller networks, less training epochs and better classification results than with using the ordinary RAAM learning scheme <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Labeling RAAM</head><p>The architecture we use is inspired by the Labeling RAAM (LRAAM) [SI, an extension of $he RAAM <ref type="bibr" target="#b6">[7]</ref> model that learns fixed-width distributed representations for labeled variable-sized recursive data structures. As the most general example for such structures, a labeled directed graph will be used. The general structure for an LRAAM is that of a three-layer feedforward network (see The idea is to obtain a compressed representation (hidden layer activation) for a node of a l a b m d directed graph by allocating a part of the input (output) of the network to represent the label and the rest to represent its subgraphs with a fixed maximum number of subgraphs IC using a special representation for the empty subgraph. The network is trained by backpropagation in an autoassociative way using the compressed representations recursively. As the representations are consistently updated during the training, the training set is dynamic (moving target), starting with randomly chosen representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Folding Architecture</head><p>For reasons of clarity and simplicity we will concentrate on inference tasks consisting of the classification of logical terms in the following. However note, that our architecture together with the corresponding training procedure (Section 3 ) could be easily extended to handle inference tasks of more complex nature.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> (left side) shows the folding architecture we use. The first two layers occupy the role of the standard LRAAM encoder part, the hidden units are connected to a simple sigmoid feedforward layer, in our case just one unit for classification. For the classification of a new structure, the network is virtually unfolded (just as in Figure <ref type="figure" target="#fig_2">3</ref>) to compute the structure's representation, which is then used as input for the classifier. This would be done in the same way for LRAAM-derived representations. However, we use the virtual unfolding of the network also for the learning phase and propagate the classification error through the whole virtually unfolded network, instead of using a decoder part for finding unique representations. The idea behind this is, that for many inference tasks unique representations are not necessary as long as the information needed for the inference task is represented properly. Our learning scheme is completely supervised without moving targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Backpropagation Through Structure</head><p>The principle idea of using recursive backpropagation-like learning procedures for symbolic tree-processing has been mentioned first in [l, 91. We assume in the following the reader to be familiar with the standard backpropagation algorithm (BP) and its variant backpropagation through time (BPTT) <ref type="bibr">[lo]</ref> that is used to train recurrent network models. For the sake of brevity we will only be able to give a sketch of the underlying principles of our approach. Refer to 151 for a detailed discussion and formal specification of the given algorithms.</p><p>Let us first have a closer look on the representation of structures we want to process. All kinds of recursive symbolic data structures we aim at can be mapped onto labeled directed acyclic graphs (DAGS). For example choosing a DAG-representation for a set of logical terms (see Figure <ref type="figure" target="#fig_1">2</ref>)-that allows to represent different occurrences of a (sub-)structure only as one nodemay lead to a considerable (even exponential) reduction of space complexity. But this complexity reduction also holds for the time complexity of training. For the LRAAM training as well as for BPTS (as we will show in the following two sections) the number of forward and backward phases per epoch is linear to the number of nodes in the DAG-representation of the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BPTS for flrees</head><p>For reasons of simplicity we first restrict our considerations to tree-like structures. In the fomoard phase the encoder (Figure <ref type="figure" target="#fig_0">1</ref>, left) is used to compute a representation for a given tree in the same way as in the plain LRAAM. The following metaphor helps us to explain the backward phase. Imagine the encoder of the folding architecture is virtually unfolded (with copied weights) according to the tree structure (see Figure <ref type="figure" target="#fig_2">3</ref>). Now the error passed from the classifier to the hidden layer is propagated through the unfolded encoder network. With a similar argument as for BPTT <ref type="bibr">(lo]</ref> we can see, that the exact gradient is computed. The exact formulation is given below:</p><p>For each (sub-)tree t , t.z E Rn+km is the input vector of the encoder, &amp; E R" the vector of (error-) deltas for t's representation, and (t.z, t') the projection of t.z onto t's subtree t'. Let further W be the encoder matrix, f' the derivative of the transfer function and 0 the multiplication of two vectors by components.</p><p>AW is calculated as sum over all (sub-)trees (1). The 6 ; for each subtree t' is calculated by propagating</p><p>the &amp; of the one definite parent node t oft' back according to (2): AW = ~C g ( t . z ) ~</p><p>(1)</p><formula xml:id="formula_0">t 6 1 = (W'&amp; 0 f'(t.z), t')<label>(2)</label></formula><p>For each (sub-)tree in the training set one epoch requires exactly one forward and one backward phase through the encoder. The training set is static (no moving target).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BPTS for DAGS</head><p>However, if we use a DAG-representation and represent a (sub-)structure t only as one node independently from the number of its occurrences, then there may be different &amp; in (1) and (2) for each occurrence of t.</p><p>We call this situation a delta conflict.</p><p>Suppose the (sub-)structures ti and t j are identical. Of course this means that corresponding substructures within ti and t j are identical too. This clearly gives us ti.x = tj.z, but we may have For calculating AW we only need the sum of 6 ; ' and 6 ; . This is shown by the following transformation of (1) which holds because of the linearity of matrix multiplication: t/3 f/2 g/2 i / l j / l instances of t(a,i(X),f(b,Y)), a/0 b/O C/O d/O g(i(f(b,X)),j(Y)) or (259,141) (1394,776) (7,7) g(i(X),i(f(b,Y))) t / 3 f/2 g/2 i / l j / l instances of f(j(X),f(a,Y)), i(j(f(a,Y))) occur somewhere The &amp;I'S of corresponding children t' oft, and t, can be calculated more efficiently too, by propagating the sum of 6yx and 6 ; back in (2). A similar transformation (linearity of n, @ and matrix multiplication) for <ref type="bibr" target="#b1">(2)</ref> shows this.</p><formula xml:id="formula_1">a/O b/O C/O d/O g(f(a,Y),i(X)</formula><p>Summing up all different 6's coming from each occurrence of a (sub-)structure is a correct (steepest gradient) solution of the delta conflict and enables a very efficient implementation of BPTS for DAGS. We just have to organize the nodes of the training set in a topological order. The forward phase starts with the leaf-nodes and proceeds according to the reverse ordering -ensuring that representations for identical substructures have to be computed only once. The backward phase follows the topological order beginning at the root-nodes. In this way the 6's of all occurrences of a node are summed up before that node is processed. Again for each node in the training set one epoch requires exactly one forward and one backward phase through the encoder. Similar to standard BP, BPTS can be used in batch or in online mode. BPTSbatch updates the weights after the whole training set has been presented. By the optimization techniques discussed above (&amp;summation and DAG-representation) each node has to be processed only once per epoch. This does not hold for online mode because the weights are updated immediately after one structure has been presented and therefore substructures have to be processed for each occurrence separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The characteristics of each term classification problem we used to evaluate our approach are summarized in Table <ref type="table">1</ref>. The set of problems is an extension of the one used in <ref type="bibr" target="#b5">[6]</ref>. They range from the detection of a certain subterm to the satisfaction of a specific unification pattern. The second column reports the set of symbols (with associated arity) compounding the terms, the third column shows the rule(s) used to generate the positive examples with upper case letters representing all-quantified logical variables. The fourth column reports the number of terms in the training and test set respectively, the fifth column the number of subterms (using the DAG-representation as described in Section 3), and the last column the   Table <ref type="table">2</ref> shows the best results we have obtained. The different columns describe the problem class, the method used, the topology of the network ( + #L, number units in the labels, #H, number of units for the representations), training parameters ( a 7, learning parameter, +p, momentum), the performance ( * %Tr./%Ts. the percentage of terms of the training/test set correctly classified) and the number of learning epochs needed t o achieve these results. We have applied and compared BPTS in online (+ U) and batch (=s-0 ) mode on each problem instance. For online mode, the training set was presented in the same order every epoch. Only one learning parameter 7, one momentum p = 0.6 and one transfer function tanh was used throughout the whole three-layered network architecture. Since ' we have considered two-class decision problems the output unit was taught with values -1.0/1.0 for negative/positive membership. The classification performance measure was computed by fixing the decision boundary at 0. The performance of the folding architecture with BPTS is listed together with the results of a combined LRAAM-classifier2 order of magnitude less hidden units and by a learning procedure which needs more than one order of magnitude less training epochs to converge to the same performance. Beside complexity considerations (see section 3.2) BPTS-batch seems to have no significant advantages over BPTS-online at the current stage of our investigations based on the given experimental results.</p><p>Why is the folding architecture together with BPTS superior to the LRAAMC? Obviously, the LRAAMC has to solve the additional task of developing unique representations, i.e. to minimize the encoding-decoding error ( j % Dec.-Enc., in Table <ref type="table">2</ref>). This may be much more difficult and sometimes contradictory to the classification task. Furthermore, as the error from the classifier is not propagated recursively through the structure in the LRAAMC, the exact gradient is not computed and (in contrast to our approach) the representations of subterms are not optimized directly for the classification task concerning their parents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>By using the folding architecture together with BPTS the encoding of structures is melted with further inference into one single process. We have shown that our method is really superior over previously known approaches, at least on the set of term-classification problems presented here.</p><p>In order learn more about the generalization capabilities of the architecture it will be important to analyze the generated representations. We plan to experiment with more complex architectures (e.g. additional layers in the encoder or in the classifier) and with examples coming from "real" applications. One we are currently working on is a hybrid (symbolic/connectionist) reasoning system, in which our methods will be used to learn search control heuristics from examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The Folding Architecture (left side) and the standard LRAAM (right side).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Tree and DAG representation of a set of terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The Encoder unfolded by the structure f(X, g(a, Y)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of terms. For each problem about the same number of positive and negative examples is given. 130th positive and negative examples have been generated randomly. Training and test sets are disjoint and have been generated by the same algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table : 2</head><label>:</label><figDesc>: The best results obtained for each classification problem.</figDesc><table><row><cell>Method</cell><cell cols="2">Topology</cell><cell cols="2">Learning Par.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#epochs</cell></row><row><cell>A lraamc 0 batch</cell><cell>8 8</cell><cell>35 2</cell><cell>0.2 0.001</cell><cell cols="2">0.001 0.5 -0.6</cell><cell>4.25</cell><cell>100 99.61</cell><cell>98.58 100</cell><cell>11951 108</cell></row><row><cell>~o n l i n e</cell><cell>8</cell><cell>2</cell><cell>0.001</cell><cell></cell><cell>0.6</cell><cell></cell><cell>100</cell><cell>100</cell><cell>444</cell></row><row><cell>A</cell><cell>8</cell><cell>25</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>100</cell><cell>98.84</cell><cell>94.29</cell><cell>27796</cell></row><row><cell>0</cell><cell>8</cell><cell>5</cell><cell>0.001</cell><cell></cell><cell>0.6</cell><cell></cell><cell>97.69</cell><cell>95.71</cell><cell>5271</cell></row><row><cell></cell><cell>8</cell><cell>5</cell><cell>0.001</cell><cell></cell><cell>0.6</cell><cell></cell><cell>100</cell><cell>94.29</cell><cell>6925</cell></row><row><cell>A</cell><cell>6</cell><cell>35</cell><cell>0.2</cell><cell>0.06</cell><cell>0.5</cell><cell>100</cell><cell>97</cell><cell>93.98</cell><cell>10452</cell></row><row><cell>0</cell><cell>6</cell><cell>3</cell><cell>0.001</cell><cell></cell><cell>0.6</cell><cell></cell><cell>97</cell><cell>93.98</cell><cell>278</cell></row><row><cell></cell><cell>6</cell><cell>5</cell><cell>0.005</cell><cell></cell><cell>0.6</cell><cell></cell><cell>97.5</cell><cell>91.57</cell><cell>213</cell></row><row><cell>A</cell><cell>6</cell><cell>45</cell><cell>0.2</cell><cell cols="2">0.005 0.5</cell><cell>36.14</cell><cell>94.55</cell><cell>90.82</cell><cell>80000</cell></row><row><cell>0</cell><cell>6</cell><cell>3</cell><cell>0.005</cell><cell></cell><cell>0.6</cell><cell></cell><cell>95.56</cell><cell>92.86</cell><cell>4250</cell></row><row><cell></cell><cell>6</cell><cell>3</cell><cell>0.005</cell><cell></cell><cell>0.6</cell><cell></cell><cell>98.52</cell><cell>94.90</cell><cell>465</cell></row></table><note><p>% Dec.-Enc. % Tr. % Ts.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(LRAAMC) architecture (* A) whenever results for the LRAAMC were available<ref type="bibr" target="#b5">[6]</ref>.A more detailed discussion of the problem classes viewed in relation to the results can be found in[5].Our folding architecture supplied with BPTS leads to nearly the same or slightly better classification performance than the LRAAMC approach. But this has been achieved by a topology which uses one 'We define the depth of a term as the maximum number of edges between the root and leaf nodes in the term's LDAG-</figDesc><table><row><cell>instl</cell><cell>A</cell><cell>6</cell><cell>35</cell><cell>0.2</cell><cell cols="2">0.005 0.5</cell><cell>98.86</cell><cell>100</cell><cell>100</cell><cell>1759</cell></row><row><cell></cell><cell>0 B</cell><cell>6 6</cell><cell>3 3</cell><cell>0.001 0.001</cell><cell></cell><cell>0.6 0.6 -</cell><cell></cell><cell>100 100</cell><cell>100 100</cell><cell>37 68</cell></row><row><cell></cell><cell>A</cell><cell>6</cell><cell>35</cell><cell>0.2</cell><cell cols="2">0.005 0.5</cell><cell>8.97</cell><cell>100</cell><cell>100</cell><cell>6993</cell></row><row><cell>inst7 termoccl very long inst5 c c</cell><cell>0 m A 0 0 8 U</cell><cell cols="2">6 6 13 13 13 8 8 I 13 3 3 3 40 3 3 6 6</cell><cell>0.003 0.005 0.1 0.01 0.01 0.001 0.001 0.001</cell><cell>0.01</cell><cell>0.6 0.6 0.2 0.6 0.6 0.6 --0.6 0.6 -</cell><cell>1.05</cell><cell>100 100 100 100 100 94.21 99.64 99.29</cell><cell>100 100 100 100 100 93.62 98.33 94.17</cell><cell>150 27 6158 10 57 1558 3522 2263</cell></row><row><cell>simil neg. iinstoccl c</cell><cell>0</cell><cell>13 13 13</cell><cell>5 6 7</cell><cell>0.001 0.001 0.001</cell><cell></cell><cell>0.6 0.6 0.6</cell><cell></cell><cell>98.06 96.31 97.24</cell><cell>92.20 80.72 72.29</cell><cell>198 338 369</cell></row><row><cell>representation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">2LRAAMC uses a special dynamic pattern selection strategy for network training and applies different learning parameters</cell></row><row><cell cols="4">to the LRAAM (a 7) and the classifier ( 3 e ) .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>--</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was supported by the German Research Foundation (DFG) under grant No. Pa 268/10-1. Our thanks to Alessandro Sperduti for helpful comments and to Andreas Stolcke for providing us with his RAAM implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connectionist Parser with Recursive Sentence Structure and Lexical Disambiguation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the A A A I 92</title>
		<meeting>the A A A I 92</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring the Symbolic/Subsymbolic Continuum: A Case Study of RAAM</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Meeden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Symbolic and Connectionist Paradigms: Closing the Gap</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dinsmore</surname></persName>
		</editor>
		<imprint>
			<publisher>LEA Publishers</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoding Syntactical Trees with Labelling Recursive Auto-Associative Memory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cadoret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECAI 94</title>
		<meeting>the ECAI 94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="555" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Recursive Distributed Representations for Holistic Computation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chrisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="366" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning Task-Dependent Distributed Representations by Backpropagation Through Structure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
		<idno>AR-95-02</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Institut fur Informatik, Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">AR-report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations for the Classification of Terms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI</title>
		<meeting>the IJCAI</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recursive Distributed Representations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Encoding of Labeled Graphs by Labeling RAAM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<editor>NIPS 6, (J. D. Cowan, G. Tesauro</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tree Matching with Recursive Distributed Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. Rep. TR-</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Backpropagation Trough Time: What it Does and How to Do it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the and</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<meeting>the and<address><addrLine>Berkeley, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1994. 1992. Oct. 1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
		<respStmt>
			<orgName>International Computer Science Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
