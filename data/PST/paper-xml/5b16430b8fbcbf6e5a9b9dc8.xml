<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Image Saliency Detection with Gestalt-laws Guided Optimization and Visual Attention Based Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yijun</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinchang</forename><surname>Ren</surname></persName>
							<email>jinchang.ren@strath.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Genyun</forename><surname>Sun</surname></persName>
							<email>genyunsun@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Huimin</forename><surname>Zhao</surname></persName>
							<email>zhaohuimin@gpnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Marshall</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Unsupervised Image Saliency Detection with Gestalt-laws Guided Optimization and Visual Attention Based Refinement</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Unsupervised Image Saliency Detection with Gestalt-laws Guided</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Geosciences</orgName>
								<orgName type="institution">China University of Petroleum (East China)</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">The School of Computer Sciences</orgName>
								<orgName type="institution">Guangdong Polytechnic Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">The School of Automation</orgName>
								<orgName type="institution" key="instit2">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">China Academy of Science</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">The School of Computer Sciences</orgName>
								<orgName type="institution">Guangdong Polytechnic Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="department">School of Geosciences</orgName>
								<orgName type="institution">China University of Petroleum (East China)</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="department">School of Computer Sciences</orgName>
								<orgName type="institution">Guangdong Polytechnic Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="laboratory">Unsupervised Image Saliency Detection with Gestalt-laws Guided Optimization and Visual Attention Based Refinement Yijun Yan</orgName>
								<orgName type="institution">China Academy of Science</orgName>
								<address>
									<addrLine>Jinchang Ren, Junwei Han</addrLine>
									<settlement>Xi&apos;an, Genyun Sun, Huimin Zhao, Xuelong Li</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<address>
									<country>J Ren</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Image Saliency Detection with Gestalt-laws Guided Optimization and Visual Attention Based Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC99247AE3827AA118FB33FF3AEE1B68</idno>
					<idno type="DOI">10.1016/j.patcog.2018.02.004</idno>
					<note type="submission">Received date: 29 May 2017 Revised date: 16 January 2018 Accepted date: 2 February 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Background connectivity</term>
					<term>Gestalt laws guided optimization</term>
					<term>Image saliency detection</term>
					<term>Feature fusion</term>
					<term>Human vision perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual attention is a kind of fundamental cognitive capability that allows human beings to focus on the region of interests (ROIs) under complex natural environments. What kind of ROIs that we pay attention to mainly depends on two distinct types of attentional mechanisms. The bottom-up mechanism can guide our detection of the salient objects and regions by externally driven factors, i.e. color and location, whilst the top-down mechanism controls our biasing attention based on prior knowledge and cognitive strategies being provided by visual cortex. However, how to practically use and fuse both attentional mechanisms for salient object detection has not been sufficiently explored. To the end, we propose in this paper an integrated framework consisting of bottom-up and top-down attention mechanisms that enable attention to be computed at the level of salient objects and/or regions. Within our framework, the model of a bottom-up mechanism is guided by the gestalt-laws of perception. We interpreted gestalt-laws of homogeneity, similarity, proximity and figure and ground in link with color, spatial contrast at the level of regions and objects to produce feature contrast map. The model of top-down mechanism aims to use a formal computational model to describe the background connectivity of the attention and produce the priority map. Integrating both mechanisms and applying to salient object detection, our results have demonstrated that the proposed method consistently outperforms a number of existing unsupervised approaches on five challenging and complicated datasets in terms of higher precision and recall rates, AP (average precision) and AUC (area under curve) values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p> Two-stage refinement to show best among 10 state-of-the-art methods on 5 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>For human beings, our visual attention system is mainly made up by both bottom-up and top-down attention mechanisms that enable us to allocate to the most salient stimuli, location, or feature that evokes the stronger neural activation than others in the natural scenes <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Bottom-up attention helps us gather information from separated feature maps e.g. color or spatial measurements, which is then incorporated to a global contrast map representing the most salient objects/regions that pop out from their surroundings <ref type="bibr" target="#b10">[11]</ref>. Top-down attention modulates the bottom-up attentional signals and helps us voluntarily focus on specific targets/objects i.e. face and cars <ref type="bibr" target="#b14">[15]</ref>. However, due to the high level of subjectivity and lack of formal mathematical representation, it is still very challenging for computers to imitate the characteristics of our visual attention mechanisms. In <ref type="bibr" target="#b10">[11]</ref>, it is found that the two attentional functions have distinct neural mechanisms but constantly influence each other to attentions. To this end, we aim to build a cognitive framework where separated model for each attentional mechanism is integrated together to determine the visual attention refer to the salient object detection.</p><p>To extract features at the bottom level, color plays an important role since it is a central component of the human visual system, which also facilitates our capability for scene segmentation and visual memory <ref type="bibr" target="#b21">[22]</ref>. Color is particularly useful for object identification as it is invariant under different viewpoints. We can move or even rotate an object, yet the color we see seems unchanged due to the light reflected from the object into the retina remains the same. As a result, the salient regions/objects can be easily recognized intuitively for their high contrast to the surrounding background.</p><p>In addition to color features, our visual perception system is also sensitive to spatial signals, as the retinal ganglion cells can transmit the spatial information within natural images to the brain <ref type="bibr" target="#b24">[25]</ref>. As a result, our human beings pay Image LMLC <ref type="bibr" target="#b0">[1]</ref> HDCT <ref type="bibr" target="#b7">[8]</ref> RC <ref type="bibr" target="#b2">[3]</ref> GMR <ref type="bibr" target="#b11">[12]</ref> Proposed Ground truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 5 more attention to the objects and regions not only with dominant colors but also with close and compact spatial distributions. Therefore, the main objective of saliency detection is to computationally group the perceptual objects on the base of the way how our human visual perception system works.</p><p>Although color and spatial features have been widely used for salient object detection, the efficacy can still be fragile, especially in dealing with large objects and/or complicated background in the scenes <ref type="bibr" target="#b22">[23]</ref>. The salient object often cannot be extracted as a whole (see examples in Fig. <ref type="figure" target="#fig_0">1</ref>), though it is still relatively easily for our HVS to identify the full range of the salient objects. This shows a gap between existing approaches to an ideal one that can better exploit the potential of our HVS for more accurate salient object detection. To this end, we propose a Gestalt-law guided cognitive approach to calculate bottom-up attention. As gestalt-laws can characterize the capabilities of HVS to yield whole forms of objects from a group of simple and even unrelated visual elements <ref type="bibr" target="#b26">[27]</ref>, e.g. edges and regions, we aim to employ these laws to guide/improve the process of salient object detection.</p><p>For modelling top-down attention, Al-Aidroos et al <ref type="bibr" target="#b27">[28]</ref> proposed a theory named 'background connectivity' to describe the stimulus-evoked response of our visual cortex. It is found that focus on the scenes rather than objects may increase the background connectivity. Inspired by this theory, we employed a robust background detection model to represent the background connectivity of top-down attention in the images as post-processing to further refine the saliency maps detected using gestalt-laws guided processing. 3) We have carried out comprehensive experiments on five challenging and complex datasets and benchmarked with eight state-of-the-art saliency detection models, where useful discussions and conclusions are achieved.</p><p>The rest of this paper is organized as follows. Section 2 summarizes the related work on saliency detection. The proposed framework by combining bottom-up and top-down HVS mechanisms for saliency detection is presented in Section 3, where the implementation detail is discussed in Section 4. Section 5 presents the experimental results and performance analysis. Finally, some concluding remarks are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the past decades, a number of salient object detection methods have been developed to identify salient regions in terms of the saliency map and capture as much as possible human perceptual attention. In general saliency detection methods can be categorized into two classes, i.e. supervised and unsupervised approaches. Most supervised methods including those using deep learning <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> are able to obtain good saliency maps, where high performance computers even with particular graphic process units (GPU) are needed to cope with the lengthy training time. In addition, supervised methods may also suffer from lack of generality, especially when the training samples are limited and/or insufficiently representative. With deep learning, this drawback seems can be somehow overcome <ref type="bibr" target="#b31">[32]</ref>, yet at a cost of a large amount of data requested for training to learn the prior knowledge. On the contrary, it seems our human vision system can guide us to easily detect and recognize objects under complex scenes without supervision <ref type="bibr" target="#b33">[34]</ref>. To this end, in this paper we focus mainly on unsupervised saliency detection.</p><p>In a recent benchmark survey <ref type="bibr" target="#b34">[35]</ref>, quite a few unsupervised saliency detection methods are summarized and assessed, where the two main objectives of saliency detection are fixation prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> and salient object detection <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. In fixation prediction, it aims to predict eye's gaze or motion through detecting sparse blob-like salient regions <ref type="bibr" target="#b39">[40]</ref>, whilst salient object detection is to detect the salient objects/regions in the scene <ref type="bibr" target="#b40">[41]</ref>. According to the survey <ref type="bibr" target="#b34">[35]</ref>, much more salient object detection methods are proposed than those using eye fixation prediction, possibly due to their contributions to a wide range of applications including content-based image retrieval <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>,</p><p>image/video compression <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>, image quality assessment <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>, region of interest segmentation <ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>, and object detection <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Inspired by a biologically plausible architecture <ref type="bibr" target="#b5">[6]</ref> and the feature integration theory <ref type="bibr" target="#b56">[57]</ref>, Itti et al <ref type="bibr" target="#b3">[4]</ref> proposed an epic saliency detection model in 1998. With multiple image features extracted including luminance, color and edge orientation, the saliency map is generated by using center-surround difference across these features. In the following two decades, quite a few landmark saliency models are developed, which are briefly reviewed below.</p><p>Depending on whether a salient object is detected from pixels or regions, saliency detection techniques can be further categorized into two groups, i.e. pixel based and region based. Herein the main difference between the two groups is whether the image is segmented into regions for saliency detection, using either color quantization or pixel clustering. In <ref type="bibr" target="#b8">[9]</ref>, a contrast based saliency map is proposed, where the color difference between the pixel and its neighbors is determined to extract the attended areas using the fuzzy theory <ref type="bibr" target="#b57">[58]</ref>. In <ref type="bibr" target="#b9">[10]</ref> a biologically plausible bottom-up visual saliency model is presented based on the Markovian approach and mass concentration algorithm.</p><p>More recently, an efficient method is introduced in Achanta et al. <ref type="bibr" target="#b15">[16]</ref> to build high quality saliency maps using low-level features such as luminance and color in the L*a*b* color space. In <ref type="bibr" target="#b17">[18]</ref>, a salient region is detected by using the color difference between the pixels and their average value in the image, again in the L*a*b* color space. In <ref type="bibr" target="#b18">[19]</ref>,</p><p>instead of treating the whole image as the common surround for any given pixel, the saliency map is defined by using color difference between the given pixel and a local symmetric surround region. In Cheng et al <ref type="bibr" target="#b23">[24]</ref>, a global contrast based method is proposed to determine the saliency value. The color is quantized into a number of bins in the L*a*b* color space with the global color contrast measured between color bins. Furthermore, a color space smoothing process is also introduced to reduce quantization artefacts before assigning similar saliency value to similar color bins.</p><p>Although the aforementioned approaches are found to produce relatively good results on saliency detection, their robustness is limited when extending to large datasets due to increasing complexity of the scenes, especially the variations in terms of spatial size and layout between the salient objects and the image background. The reason here is, except for the color contrast, spatial contrast is also an important perception in our human visual system, regardless the extremely high computational cost for pixel-level saliency computation. To this end, region-based contrast and saliency detection has become increasingly popular in recent years, especially using the superpixel based approach. In <ref type="bibr" target="#b23">[24]</ref>, a region-level saliency map is proposed based on both color and spatial difference across the regions, where the spatial prior is used to highlight the salient regions. In <ref type="bibr" target="#b58">[59]</ref>, a contrast-based saliency estimation is proposed, where a given image is segmented into a number of homogeneous regions by using superpixel. The contrast and spatial</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>distribution of these regions are measured and smoothed by using high-dimensional Gaussian filters for saliency detection. In <ref type="bibr" target="#b13">[14]</ref>, a superpixel based saliency detection method is proposed, where color and spatial contrast across the superpixels are used for efficient saliency detection. In Kim et al <ref type="bibr" target="#b7">[8]</ref>, high-dimensional color transform is applied to over-segmented images for saliency detection.</p><p>In general, the whole process of bottom-up saliency detection can be divided into at least three stages, i.e.</p><p>pre-processing, feature based salient map generation, and post-processing. Apparently feature based salient map generation is the key in saliency models, where various color and spatial features are extracted and measured in determining the saliency maps. The pre-processing is often for spatial and illumination normalization, image enhancement and image segmentation (only for region-based approaches). The post-processing, on the contrary, serves mainly for normalization and/or fusion of saliency maps, where object prior is widely used in region-based approaches. One optional stage is to extract the binary template of the salient object via thresholding the salient map, where histogram based adaptive thresholding such as Otsu's approach <ref type="bibr" target="#b59">[60]</ref> is commonly used.</p><p>In Table <ref type="table" target="#tab_0">I</ref>, some typical unsupervised saliency detection approaches are summarized for comparison in terms of the features used and any adopted pre-processing and post-processing stages. First, color and spatial information is the most widely used features due to their importance in visual psychology <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. However, spatial features are excluded in some pixel-based approaches. For region-based approaches, color quantization and graph or superpixel based clustering is normally used. Combination of color and spatial features are then employed for saliency map </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-processing Features for initial saliency map generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-processing to refine the saliency map</head><p>Pixel-level saliency detection IT <ref type="bibr" target="#b3">[4]</ref> No Color, Intensity, Orientation Normalization of saliency map MZ <ref type="bibr" target="#b8">[9]</ref> Image resizing, Color quantization Color Refinement via Fuzzy growing GB <ref type="bibr" target="#b9">[10]</ref> No Color, Orientation, Intensity HC As shown in Table <ref type="table" target="#tab_0">I</ref>, the principle of gestalt laws has been reflected in many existing approaches. This includes not only the color and spatial features in representing the HVS but also their applications in other key stages.</p><p>Examples herein can be found in homogeneity based color quantization and pixel clustering in the pre-processing, similarity based feature measurement in saliency map generation and object-prior based grouping in post-processing.</p><p>In fact, the concept of gestalt laws was first introduced as a series of mechanisms to explain the human visual perception, dated back to 1920s in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, where Gestalt law based detection is proved to fit the human perception <ref type="bibr" target="#b62">[63]</ref>. Starting from low-level features of the salient stimuli that influence perception at the bottom level way up to high level cognition, gestalt can be naturally used to define bottom-up objects <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. Although the gestalt theories are gradually used in saliency detection models explicitly <ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref> or implicitly <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b68">69]</ref>, there is no specified structure to regulate the relationship between Gestalt law and saliency models. How these laws can be systemically applied for salient detection remains unexplored. As a result, we aim to further explore various aspects of gestalt laws when applying in a bottom-up model and also combined with the top-down model for feature contrast map generation, which is detailed in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED GLGOV FRAMEWORK FOR UNSUPERVISED SALIENCY DETECTION</head><p>A new saliency detection framework inspired by the Gestalt laws of HVS is proposed. The proposed framework contains six main modules, i.e. homogeneity, similarity and proximity, figure and ground, background connectivity, two stage refinement and performance evaluation. The overall diagram of our saliency detection framework is illustrated in Fig. <ref type="figure" target="#fig_5">2</ref> where corresponding gestalt laws and visual psychology used in different modules are specified and also detailed below.</p><p>The homogeneity module aims to group or cluster pixels into regions on the basis of human visual perception <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b69">70]</ref>. According to Gestalt law of homogeneity and the gestalt perceptual organization theory <ref type="bibr" target="#b61">[62]</ref>, if pixels have similar intensity, color, orientation or other features, they should be treated as homogeneity regions. To this end, there can be a number of homogeneity regions extracted in one image. Specifically, the simple linear iterative cluster (SLIC) method <ref type="bibr" target="#b70">[71]</ref> is used to separate the image into regions namely superpixels, where the color quantization is applied to reduce the number of colors based on color homogeneity. neighboring superpixels provided that their colors are similar to each other. As such, a smoothing process should be introduced to refine the extracted saliency map, which will be defined in two stage refinement.</p><p>The saliency point detection module is to coarsely split the whole image into foreground and background based on a convex hull formed by the detected saliency points. Inspired by human visual psychology <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, the greyscale image and color image are treated differently. Herein, we choose luminance-based operator for greyscale images and</p><p>Color Boosted Harris (CBH) operator for color images for detection of saliency points <ref type="bibr" target="#b71">[72]</ref>. As this module is relative In figure and ground module, Gestalt law of figure and ground is applied to suppress the background and highlight the foreground objects. According to HVS, human's perception is comprised by objects and their surrounding background under observation, where salient objects can be automatically highlighted from the suppressed background. To this end, superpixel level color quantization, adaptive thresholding and a saliency points detection method <ref type="bibr" target="#b71">[72]</ref> are well combined together for background suppression.</p><p>The background connectivity module is to calculate the background probability of each super-pixels. Inspired by background connectivity theory <ref type="bibr" target="#b72">[73]</ref>, attention to the scene or background of the image will increase the background connectivity in our visual cortex. Herein, we employ a component from the background detection method <ref type="bibr" target="#b19">[20]</ref> to represent the background connectivity mathematically.</p><p>In the two-stage refinement module, the first refinement helps to smooth Gestalt law guided saliency maps by considering all superpixels from the foreground or background as well as fusion with the feature contrast map and background suppression map. The result from the first refinement will be fused with the background probability map derived from the background connectivity module and further smoothed in the second refinement process. Finally, the final saliency map is determined.</p><p>In the performance evaluation module, comprehensive experiments are carried out against a number of state-of-the-art methods on several widely used publicly available databases. Some widely used evaluation criteria such as PR curve, ROC curve, AUC and AP value are used for quantitative assessment. Detailed results are reported in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAIL OF THE PROPOSED GLGOV FRAMEWORK</head><p>In this section, the implementation of the proposed saliency detection framework is detailed in five stages, i.e.</p><p>homogeneity, similarity and proximity, figure and ground, background connectivity and two-stage refinement below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Homogeneity</head><p>For a color image in three channels, it usually contains thousands of pixels with up to possible values, where pixel-based saliency detection suffers from high computational cost. Therefore, we use two operations to reduce the dimension of the image and simplify the problem. First, based on Gestalt law of homogeneity, similar pixels spatially</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T 12</formula><p>close to each other in the image can be considered as a whole. To this end, we use SLIC <ref type="bibr" target="#b70">[71]</ref> to segment the image into a number of regions called superpixels which generally has inner color consistency and a compact shape with decent boundary adherence. As suggested by <ref type="bibr" target="#b7">[8]</ref>, we set the number of superpixels n to 500. Secondly, using the same scheme as suggested in <ref type="bibr" target="#b13">[14]</ref>, we uniformly quantize each color channel into q bins in order to reduce the number of </p><formula xml:id="formula_2">colors</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity and Proximity</head><p>In human vision perception, usually we are aware of the image regions that have high contrast to their surroundings. According to the Gestalt law of proximity and similarity, HVS tends to perceive elements that are close or similar to each other in a group. This actually explains that, at the superpixel-level, those image regions are normally grouped by spatial closeness and color similarity of superpixels. In addition, those superpixels should have similar level of low inner contrast and highly different from the surrounding superpixels. In this section, we presented in detail the proposed saliency object detection method based on color and spatial contrast at superpixel level.</p><p>First, for two superpixels and , their color distance is defined as the Euclidean distance between the mean colors of the two superpixels and scaled within the range of [0,1]. Here the color distance measures the similarity of the color appearance of the two superpixels.</p><p>In addition to the color based similarity, the gestalt law of proximity for the two superpixels and is denoted as a spatial proximity distance , which is defined as the Euclidean distance between the centroids of the two superpixels. The larger the is, the higher proximity between and is.The spatial proximity distance can be further normalized within [0, 1] as follows:</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T 13<label>(1)</label></formula><p>where refer respectively to the minimum and the maximum of all possible values of as determined from superpixels of the given image.</p><p>Based on the defined color distance and the normalized spatial proximity distance , the global color contrast for a superpixel is defined by</p><formula xml:id="formula_4">∑ (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where n is the number of superpixels, and is the normalized area of the superpixel where a larger one contributes more to the global color contrast value of , where the sum of is 1.</p><p>Similarly, we define the global spatial contrast value for a superpixel below:</p><formula xml:id="formula_6">∑ ∑ ∑<label>(3)</label></formula><p>where is the weight of the spatial layout, which is defined as the minimum distance from the centroid of superpixel to any of the four image borders; is the inter superpixel color similarity defined below, where refer respectively to the color histogram of the two superpixels and with as the index for the color bins.</p><formula xml:id="formula_7">∑ | |<label>(4)</label></formula><p>Unlike conventional histogram intersection method, we introduce the term ( | |) rather than to measure the similarity of two histogram bins based on their frequencies. For example, given two pairs of histogram bins: , and , conventional histogram intersection method will produce the same result of 0.2, which fails to reflect the difference between the histogram bins. On the contrary, in our modified definition, the similarity terms for the two cases become 0.9 and 0.4, respectively, which improves the similarity measurement and makes it more consistent to human perceptions than conventional histogram intersection approach.</p><p>For simplicity, we define as a combined measurement of color similarity and spatial proximity between two superpixels and :</p><formula xml:id="formula_8">A C C E P T E D M A N U S C R I P T<label>14</label></formula><formula xml:id="formula_9">( )<label>(5)</label></formula><p>Accordingly, Eq. ( <ref type="formula" target="#formula_6">3</ref>) can be simplified as</p><formula xml:id="formula_10">∑ ∑<label>(6)</label></formula><p>Based on both the global color contrast and the global spatial contrast , the feature contrast value for superpixel is defined as</p><formula xml:id="formula_11">(7)</formula><p>Although a salient object usually has a high contrast to the background, it may contain some low-contrast parts. As a result, the salient object cannot be detected as a whole as these low-contrast parts will be missed due to their small sizes and low saliency values. To tackle these problems, a smoothing process (detailed in 4.5) is applied to filter such superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Figure and Ground</head><p>Although the image background can be very complicated and even similar to the foreground, in most cases people can still recognize the salient objects without difficulty, and this can be explained by the Gestalt law of figure-ground that human's perception is the fusion of observed objects and their surrounding background. To this end, we can enhance the contrast of salient superpixels for their easier detection by suppressing the saliency value for the superpixels in the background. Based on gestalt law of similarity and proximity, salient objects in superpixel level image are composed of several superpixels with similar color appearance and close spatial distance. This actually indicates that salient objects are formed by several superpixels in a compact manner yet the distribution of background superpixels tends to be more dispersive. Based on this assumption, in this subsection, we introduce a novel background suppression model, the spatial distribution of superpixels is utilized to highlight the objects and suppress the background.</p><p>According to the dominant color theory <ref type="bibr" target="#b73">[74]</ref>, images are usually quantized into up to 8 dominant colors. Similarly, for a given superpixel level image, the colors of all superpixels can also be divided into Herein the color boosted Harris point (CBHP) operator <ref type="bibr" target="#b71">[72]</ref> is employed to determine the coarse regions of foreground and background. For color or greyscale images, the CBHP operator and luminance operator are respectively applied to detect the salient points, where a convex hull is computed to enclose all these salient points. All superpixels within the convex hull are defined as the foreground and the remaining superpixels as the background (see for example in Fig. <ref type="figure" target="#fig_11">2 module 4</ref>). Let and denote the mean color (with three components) of the foreground and the background . To maximize the color contrast, in each color component the threshold T (.) is adaptively defined as ( ).</p><p>With the adaptively determined eight dominant colors above, we merge similar colors for simplicity below. We iteratively examine each pair of the extracted dominant colors and merge them if their Euclidean distance is less than a threshold . For each image, the threshold is adaptively decided as half of the Euclidean distance between and , and the upper limit of is set as 15.</p><p>Let be two color clusters, they can be merged by the weighted average agglomerative procedure <ref type="bibr" target="#b74">[75]</ref>:</p><formula xml:id="formula_12">(8)</formula><p>where denote respectively the numbers of pixels in . For each superpixel-level image, we can usually extract 3-8 dominant colors, located in background or foreground. In other words, we may have up to 16 color clusters obtained, where half of them are from the foreground group and others from the background group. As the spatial information is excluded in color quantization, within each color cluster the superpixels are not necessarily spatially grouped together. This means that each color cluster may contain several separated objects composed of one or more superpixels. As a result, the superpixel-level image has now become object-level image.</p><p>For each color cluster in either or , denote as its the geographic center. The spatial compactness term of is defined as the average distance between and each object it contains:</p><formula xml:id="formula_13">∑ ‖ ‖ (9a) ∑ ‖ ‖ (9b)</formula><p>where , denotes the number of objects in the cluster and refers to the geographic center of the object .</p><p>Similar to in Eq. ( <ref type="formula" target="#formula_3">1</ref>), can be scaled into [0, 1] by <ref type="bibr" target="#b9">(10)</ref> where is either or ; and refer respectively to the minimum and the maximum of all spatial compactness values of .</p><p>By applying the spatial compactness, the salient objects can be highlighted. Herein we introduce a new background correlation term to further suppress the background and enhance the salient objects. This term is used to measure the connection degree between an object O and the image borders, where a large value indicates a high degree of overlapping between O and the image borders. As a result, the object O will be more likely be classified into the background as we assume the salient object should be away from the image borders. The background correlation term is defined as <ref type="bibr" target="#b10">(11)</ref> where denotes the total number of superpixels in O, among which is the number of superpixels in O that directly connects to the image borders. Using the maximum and the minimum of all possible values of , denoted as and , we can obtain normalized within [0,1] below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>̅̅̅ (12)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Background connectivity</head><p>In this subsection, an effective method of background detection <ref type="bibr" target="#b19">[20]</ref> is employed to extract the background connectivity in a numerical value by determining the probability of each superpixels being the background. For a given superpixel , the associated background probability can be determined by:</p><formula xml:id="formula_14">√ (13) ∑ ( ( ) )<label>(14)</label></formula><formula xml:id="formula_15">∑ ( ) ( )<label>(15)</label></formula><p>where is set to 10, is 1 for superpixels on the image boundary and 0 otherwise as suggested in <ref type="bibr" target="#b64">[65]</ref>, and is the number of superpixels. ( ) is the geodesic distance between any two superpixels, which is calculated by accumulating the weights , measured using the Euclidean distance along the shortest path formed by a sequence of adjacent superpixels between and on the graph, and is the total number of superpixels in the determined path.</p><formula xml:id="formula_16">( ) ∑<label>(16)</label></formula><p>To scale within [0,1], the parameter is set to 1, and the normalization process is defined as:</p><p>(17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Two-stage refinement</head><p>In the first stage refinement, the feature contrast map, spatial compactness term and background correlation terms will be smoothed separately and fused together to generate initial saliency map.</p><p>For each superpixel, its saliency value is replaced using the weighted average of the saliency values of its surrounding superpixels. Similar to image filtering, the process applied to the superpixel level image can also smooth the saliency values for more consistent detection of salient objects. In <ref type="bibr" target="#b23">[24]</ref>, a linear varying smoothing operator is employed to smooth the color contrast in the image. However, this smoothing procedure fails to address the spatial factor. In our approach, we improve this procedure with Gestalt law of similarity and proximity, and apply it to superpixel level image filtering. For robustness, for a given superpixel , we choose nearest neighbors for weighted average to refine the feature contrast value of locally as follows:</p><formula xml:id="formula_17">∑ (<label>18</label></formula><formula xml:id="formula_18">)</formula><p>where ∑ is the sum of color similarity and spatial proximity between and its k nearest neighbors.</p><p>For the input image in Fig. <ref type="figure" target="#fig_8">3</ref> For all the superpixels in , they will be assigned the same spatial compactness term as determined in Section 4.3. The lower this value is, the higher likeness the salient object it has. Although can measure the spatial compactness of all superpixels within , it cannot differ between them even for incorrectly detected foreground superpixels. To overcome this drawback, using the similar process in Eq. ( <ref type="formula" target="#formula_17">18</ref>), the Gestalt laws of similarity and proximity are applied to smooth the spatial compactness term for each superpixel as follows:</p><formula xml:id="formula_19">∑ ( ( )) ( )<label>(19)</label></formula><p>where again n is the number of superpixels, ∑ is the sum of color similarity and spatial proximity between and other superpixels. Note that, in global refinement all superpixels rather than neighboring superpixels are selected for weight average because the spatial compactness term has been extended from superpixel-level to object-level. In addition, the background correlation term can also be globally refined by</p><formula xml:id="formula_20">∑ ( ( )) ̅̅̅<label>(20)</label></formula><p>Finally, the background suppression map is determined by using the conjunction of both the spatial compactness term and background correlation term below:</p><formula xml:id="formula_21">(21)</formula><p>Based on Eq.( <ref type="formula" target="#formula_17">18</ref>) and Eq. ( <ref type="formula">21</ref>), our initial saliency map is formed by</p><formula xml:id="formula_22">(22)</formula><p>In the second refinement stage, the initial saliency map and the background probability map are fused by adopting a cost function <ref type="bibr" target="#b19">[20]</ref> to optimize the whole procedure. Let denote the final saliency value determined for the superpixel , the cost function is given by:</p><formula xml:id="formula_23">[∑ ∑ ∑ ( ) ]<label>(23)</label></formula><formula xml:id="formula_24">( ) (<label>24</label></formula><formula xml:id="formula_25">)</formula><p>Note that is set to 10 as defined in Eq. ( <ref type="formula" target="#formula_8">14</ref>). As seen in Fig. <ref type="figure" target="#fig_8">3 (d-e</ref>), by adding the two-stage refinement, the salient object can be significantly highlighted whilst the saliency value for the background is effectively suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>For performance evaluation of our proposed saliency detection method, in total 10 state-of-the-art algorithms are used for benchmarking, as listed below by the first letter of the name of methods. They are selected for two main reasons, i.e. high citation and wide acknowledgement in the community and/or newly presented in the last 3-5 years.</p><p>Introduction to the datasets and criteria used for evaluation as well as relevant results and discussions are presented in detail in this section.</p><p> Bayesian saliency via low and mid-level cues (LMLC) <ref type="bibr" target="#b0">[1]</ref>  Dense and sparse reconstruction (DSR) <ref type="bibr" target="#b16">[17]</ref>  Graph-based manifold ranking (GMR) <ref type="bibr" target="#b11">[12]</ref> A</p><formula xml:id="formula_26">C C E P T E D M A N U S C R I P T 20  Minimum barrier (MB+) [2]</formula><p> Region based contrast (RC) <ref type="bibr" target="#b2">[3]</ref>  Salient region detection via high dimension color transform (HDCT) <ref type="bibr" target="#b7">[8]</ref>  Superpixel based saliency (SP) <ref type="bibr" target="#b13">[14]</ref>  Robust background detection (RBD) <ref type="bibr" target="#b19">[20]</ref>  Multiscale Deep Features (MDF) <ref type="bibr" target="#b22">[23]</ref>  Deep Contrast Learning (DCL) <ref type="bibr" target="#b25">[26]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset description</head><p>In our experiments, five publicly available datasets including MSRA10K, DUTOMRON, THUR15K, ECSSD and PASCAL-S are employed for performance assessment.</p><p>The MSRA10K dataset <ref type="bibr" target="#b2">[3]</ref> contains 10000 images with pixel-level salient object labeling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b75">76]</ref>. This database has various image categories such as animals, flowers, humans and natural scenes, et al, where most images contain only one salient object. Given the large size and wide variety of contents, this dataset is very challenging for testing the efficacy of relevant saliency detection approaches. The THUR15K dataset <ref type="bibr" target="#b76">[77]</ref> consists of 15000 images, which are divided into five categories: butterfly, coffee mug, dog jump, giraffe, and plane. Since it does not contain a salient region labeled for every image, we only use those labeled images (6232 in total) in our experiment for testing. The DUTOMRON dataset <ref type="bibr" target="#b11">[12]</ref> is a very challenging database that contains 5166 images. Each image has one or more saliency objects and complex background. The ground truth is labeled by several experienced participants who are familiar with the goal of saliency detection.</p><p>The ECSSD dataset <ref type="bibr" target="#b77">[78]</ref> and the PASCAL-S dataset <ref type="bibr" target="#b78">[79]</ref> are two other challenging databases. ECSSD has 1000 semantically meaningful images with the complicated background, which is also widely used for saliency detection <ref type="bibr" target="#b34">[35]</ref>. PASCAL-S has 850 natural images and contains people, animal, vehicles, and indoor objects. This dataset is widely used to recognize an object from a number of visual object classes in the real world <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation criteria</head><p>For quantitative performance assessment of the proposed saliency detection algorithm, several commonly used metrics are adopted in our experiments, which include the precision-recall curve (PR), average precision (AP),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 21 receiver operating characteristics (ROC) curve and area under the ROC curve (AUC). By varying a threshold from 1</p><p>to 255 and applying it to the determined saliency map, a series of binary images indicating the detected saliency objects can be produced, from which the PR, ROC curve and AUC can be obtained for quantitative assessment.</p><p>The PR curve is formed by the true positive rate (TPR, also namely recall) versus positive predictive value (PPV, also namely precision) and the ROC curve is formed by the false positive rate (FPR) versus TPR. The three rates including TPR, PPV and FPR are determined by , where , , and respectively refer to the number of correctly detected foreground pixels of the salient object, incorrectly detected foreground pixels (false alarms), correctly detected background pixels (non-objects) and incorrectly detected background pixels (or missing pixels from the object). Specifically, these four numbers can be calculated by comparing the binary masks of the detected image and the ground truth.</p><p>In addition, the F-measure defined below is also used for comprehensive performance assessment:</p><p>where the parameter is set to 0.3 to combine the precision and the recall rate as suggested in <ref type="bibr" target="#b17">[18]</ref>. As can be seen, most of these benchmarking methods fail to highlight the objects as a whole or with a high contrast. However, our proposed method can successfully suppress the background regions and maintain the boundaries for the salient object due to the gestalt law guided cognitive framework. In addition, since the object can be well highlighted, this can further facilitate some potential applications e.g. classification of butterflies (rows 7 and 8) and flowers (row 6) and recognition of people (rows 3 and 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Assessment of the obtained saliency maps</head><p>For quantitative assessment, the results are compared using AP, AUC measurement and running time for each test image in Table <ref type="table" target="#tab_3">II</ref>. In total there are 10 approaches benchmarked with ours in   For unsupervised approaches, the proposed approach yields the highest AUC in all the five datasets, followed by DSR, though the AP value from our approach is slightly less than those of DSR except the MSRA10K dataset.</p><p>However, our approach is 3.5 times faster than DSR. This has validated the efficacy of the proposed approach, especially the gestalt laws and background connectivity used in guiding the process of saliency detection. Although MB+ is the third or fourth place in this group of experiments, the extremely high efficiency makes it a good candidate for particular applications, i.e. online processing. It is worth noting that although RC, GMR and RBD have very low running time, the AUC and AP measurements they achieve on these datasets seem inferior. For RC, the reason for the degraded performance is mainly due to the hard constraints it used to reduce the saliency value near the image borders. These seem to work fine in the MSRA10K dataset. However, given a large amount of images along with large variations in terms of image contents and complex background such as PASCAL-S, these constraints become less effective in refining the detected saliency maps. Similar to GMR, it works well on MSRA 10K and ECSSD, but fails to process complicated images such as THUR15K and DUTOMRON. For RBD, due to the lack of effective foreground detection, its precision isn't good enough which also leads to its inferior segmentation performance (Table <ref type="table" target="#tab_5">IV</ref>). For HDCT, the high dimensional color transform used increases the success of foreground and background separation but increase the running time as well. Moreover, as aforementioned, this method does not totally fit the HVS and results in undesirable results when both the foreground and background contain the same color elements. For LMLC and SP, regardless of the running time, their performance seems to be quite low due to the lack of effective post-processing to further refine the detected saliency maps. Thanks to our cognitive framework, the way our proposed method detects the salient object fit human visual attention very well. This has helped us suppress the background and highlight the main objects more effectively. Therefore, the results of several datasets show much less inconsistency. It is believed that our computation cost can be significantly improved by transplanting the code from MATLAB to C++ implementation. To further evaluate the performance of these approaches, we plot in Fig. <ref type="figure">5</ref> the PR curves and the ROC curves for the results obtained from the five datasets. For better visual effect, we only compare in Fig. <ref type="figure">5</ref> the results from the unsupervised approaches, as the advantages of deep learning based supervised methods have been discussed</p><p>according to the results in Table <ref type="table" target="#tab_3">II</ref>. As seen in Fig. <ref type="figure">5</ref>, our approach almost outperforms all other unsupervised methods, especially on the MSRA10K and ECSSD datasets, yet the performance on the rest three datasets appears quite comparable to DSR. Although the curves from MB+ are close to those from our GLGOV approach, the much lower AP as shown in Table <ref type="table" target="#tab_3">II</ref> indicates more false alarms in the detected results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Key component analysis</head><p>In this subsection, we discuss the effect of several key components in the proposed method, where all the evaluations are carried out on the MSRA 10k dataset due to its popularity. As the proposed GLGOV framework is actually a multi-stage approach, in the following we assess the contributions of three major components of our algorithm, which include the feature contrast map FC, initial saliency map after the first refinement ISA, and the final saliency map with the second refinement FSA.</p><p>Fig. <ref type="figure" target="#fig_13">6</ref> shows the ROC curves obtained from these three settings, where the AUC measurements are given and compared in Table <ref type="table" target="#tab_4">III</ref>. For the feature contrast map FC, the result with an AUC at 93.3% seems undesirable. After applying the figure and ground in the first-stage refinement, the AUC reaches 95.58% with an increase of 2.28%. By further adding background connectivity model for the second-stage refinement, the AUC becomes 96.67%, i.e. an additional gain of 1.09%. Meanwhile, the running time has been increased from 1.16s to 1.47s and 1.68s after introducing the first and second stage refinement, respectively. This has clearly demonstrates the contribution of the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>key components in our proposed GLGOV framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Validation on image segmentation</head><p>Based on the determined saliency maps, the salient objects can be extracted as binary masks, which can be further applied for performance assessment of the saliency detection approaches. Herein the OTSU approach <ref type="bibr" target="#b59">[60]</ref> is used for adaptive thresholding to generate the binary masks of salient objects. For quantitative performance assessment, we calculate the average F-measure of all the test images over their ground truth maps and report the results in Table <ref type="table" target="#tab_5">IV</ref>.</p><p>As can be seen, our proposed GLGOV model consistently produces the highest F-measure on the MSRA10K, PASCAL-S and the ECSSD datasets in comparison to other unsupervised peers, and also the second best on the DUTOMRON and THUR15K datasets after DSR. If taking the five datasets as a whole, our proposed approach outperforms the second best, DSR, 1.5% and 1.1% in terms of the overall and average F-measure, respectively.</p><p>It is worth noting that the deep learning based supervised approaches are unsurprisingly high and surpass all unsupervised ones. Nevertheless, there is still some space for further improvement of their learning strategies. For example, the F-measure from MDF is slightly less than our approach on the MSRA10K dataset, which shows that training on 2500 images seems insufficient to fully learn the characteristics of 10k images. This drawback is possibly overcome in DCL, as DCL seems more effective than MDF in the tested two datasets, ECSSD and DUTOMRON.</p><p>Again, it shows that the performance of deep learning based approaches can be very sensitive to the learning strategy used, regardless the extremely high computational resources and computational cost needed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Inspired by both Gestalt laws optimization and background connectivity theory, in this paper, we proposed GLGOV as a cognitive framework to combine bottom-up and top-down vision mechanisms for unsupervised saliency detection. Experimental results over five publicly available datasets have shown that our method helps to produce the best overall accuracy and average accuracy when benchmarking with a number of state-of-the-art unsupervised techniques. Additional assessments in terms of the PR curve, ROC curve, F-measure, AUC and AP have also verified the efficacy of the proposed approach.</p><p>The most important finding in this paper is the efficacy of bottom-up and top-down mechanisms for saliency detection, which are actually guided by necrologies such as Gestalt laws and background connectivity. On the one hand, the aim of saliency detection is to enable computers to recognize the salient object like human. On the other hand, Gestalt laws are the main theories that describe the mechanism of HVS, whilst background connectivity can reflect our visual cortex reaction to stimuli. As such, these necrologies can be well introduced into the process of and support the modelling of saliency detection. Our outcomes showed that with the guidance of necrologies, the proposed unsupervised saliency methodology consistently produces good results on different datasets. Although there is still some gap to deep learning based supervised approaches, unsupervised approach may supplement in cases there are no sufficient training samples and/or with limited computational resources.</p><p>For future work, we will focus on more in-depth guidance from Gestalt laws on saliency detection, where the laws of closure and continuity can be injected to further improve the performance. Texture feature and deep learning models will also be considered for saliency detection beyond color contrast where semi-supervised or weakly supervised learning can be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Highlights</head><label>1</label><figDesc>Gestalt laws guided saliency detection via characterizing HVS and forming objects.  Smooth at superpixel and object levels by fusing bottom-up and top-down mechanisms;  Background suppression with background correlation term &amp; spatial compactness term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Three examples of salient objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 1 ) 2 )</head><label>112</label><figDesc>Fig.1shows several examples in which the salient objects contain poor color and/or spatial contrasts. As such, conventional approaches either fails to detect the object as a whole or results in massive false alarms. Within the proposed cognitive framework, salient objects can be successfully detected whilst the false alarms are significantly suppressed. Descriptions of the proposed salient model and its implementation are detailed in Sections 3-4.The main contributions of this paper can be highlighted as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>proximity module, color contrast and spatial contrast across superpixels are measured for extraction of the feature contrast map and smoothing process. Based on Gestalt laws of similarity and proximity, elements/objects tend to be perceived as a whole in cognition if they are close enough to each other and/or share similar appearance of visual features. In other words, if a superpixel with a low saliency value is surrounded by those with high saliency values, this superpixel should be assigned with a high saliency value as determined by its</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed framework in six modules guided by various gestalt laws (specified in brackets) and visual psychology.</figDesc><graphic coords="11,83.73,46.86,404.83,400.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>8 coarse clusters in the L*a*b* color space, where each of the three individual color components is divided into two parts by a threshold . For each of the eight color clusters, the extracted dominant color is denoted as ̅ ̅ ̅ , , where ̅ is the A C C E P T E D M A N U S C R I P T 15 average value in one of the three color components L*, a* and b*, and will be used as the center for the corresponding color cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a), Fig.3(b)shows the contrast-based saliency map obtained from Eq.<ref type="bibr" target="#b6">(7)</ref>.Due to the shadow-caused low contrast parts in the flower, the saliency map actually contains many low saliency valued superpixels. This will inevitably affect the successful detection of the salient object from the image. As a result, the smoothing process and background suppression model is applied with the results shown in Fig.3(c). After smoothing, the saliency map has been improved in several ways. First, the low saliency values from the salient object have been enhanced. Second, the saliency values for all the superpixels become more consistent, which are actually raised. This shows that the proposed smoothing procedure can not only filtering low-contrast defects but also normalize the overall saliency values. Consequently, the quality of the adjusted saliency map is significantly improved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of proposed saliency computation: (a) The original image, (b) Feature contrast map, (c) background suppression map, (d) first-stage refinement, (e) final saliency map after second-stage refinement, and (f) the ground truth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>performance of the proposed GLGOV method, we show comprehensive comparison results using the PR curve and AUC values on all the five datasets. For subjective assessment, several typical examples with either large objects or complicated backgrounds are shown in Fig. 4 for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison. The ground truth (GT) in shown in the second column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>25 ForFig. 5 .</head><label>255</label><figDesc>Fig. 5. PR and ROC curve.</figDesc><graphic coords="26,146.50,469.21,141.51,106.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results of GLGOV with various settings on the MSRA10K dataset.</figDesc><graphic coords="27,285.24,319.19,221.32,183.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="24,50.84,111.31,474.21,239.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="28,68.13,487.22,445.76,208.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I Overview</head><label>I</label><figDesc></figDesc><table /><note><p>of some popular unsupervised saliency detection models</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to generate a new image histogram H with bins. Herein we define q=12 instead of 16, as this change has little effect to the experimental results yet the number of colors and associated computation cost has been significantly reduced, i.e. proportionally to the reduction from to .From the quantized image, bins of dominant colors are selected to cover more than 95% of the image pixels. The</figDesc><table /><note><p>rest color bins that cover less than 5% pixels are merged into the selected dominant color bins based on minimum Euclidean distance criterion. An example of the color quantization is shown in Module 4, Fig. 2. After superpixel-based image segmentation and color quantization, each superpixel has its histogram normalized by ∑ , where k is the index of the color bins.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>TableII, where the first eight are unsupervised, and the last two are supervised ones using deep learning. All the approaches are tested on a computer with Intel Dual Core i5-4210U 1.7 GHz CPU and 4GB RAM, where for consistency GPU is absent for deep learning based approaches. Due to hardware configuration reasons, we cannot implement DCL hence we use their published saliency maps which are only available for two datasets, i.e. ECSSD and DUTOMRON.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II AUC</head><label>II</label><figDesc>, AP and running time measurement (top two unsupervised methods are highlighted in red and green). *: deep learning based method</figDesc><table><row><cell>Method</cell><cell>Time (s)</cell><cell cols="2">MSRA10K AUC AP</cell><cell cols="2">PASCAL-S AUC AP</cell><cell>ECSSD AUC AP</cell><cell cols="2">DUTOMRON AUC AP</cell><cell cols="2">THUR15K AUC AP</cell><cell>Overall(OA) AUC AP</cell><cell>Average(AG) AUC AP</cell></row><row><cell>GLGOV</cell><cell>1.68</cell><cell cols="5">0.967 0.888 0.868 0.680 0.915 0.773</cell><cell>0.902</cell><cell>0.539</cell><cell cols="2">0.905 0.580</cell><cell>0.930 0.715</cell><cell>0.911 0.692</cell></row><row><cell>MB+ [2]</cell><cell>0.05</cell><cell cols="5">0.955 0.835 0.859 0.633 0.906 0.695</cell><cell>0.893</cell><cell>0.484</cell><cell cols="4">0.900 0.542 0.921 0.665 0.903 0.638</cell></row><row><cell>RC [3]</cell><cell>0.25</cell><cell cols="5">0.936 0.838 0.707 0.348 0.893 0.733</cell><cell>0.893</cell><cell>0.503</cell><cell cols="4">0.896 0.568 0.905 0.669 0.865 0.598</cell></row><row><cell>LMLC [1]</cell><cell>140</cell><cell cols="5">0.936 0.721 0.793 0.516 0.850 0.570</cell><cell>0.818</cell><cell>0.374</cell><cell cols="4">0.853 0.447 0.878 0.557 0.850 0.526</cell></row><row><cell>HDCT [8]</cell><cell>4.02</cell><cell cols="5">0.941 0.784 0.807 0.628 0.868 0.710</cell><cell>0.867</cell><cell>0.506</cell><cell cols="4">0.878 0.541 0.899 0.648 0.872 0.634</cell></row><row><cell>DSR [17]</cell><cell>5.85</cell><cell cols="5">0.959 0.878 0.866 0.699 0.915 0.788</cell><cell>0.900</cell><cell>0.578</cell><cell cols="4">0.902 0.612 0.925 0.729 0.908 0.711</cell></row><row><cell>GMR [12]</cell><cell>0.5</cell><cell cols="5">0.954 0.882 0.860 0.680 0.894 0.748</cell><cell>0.894</cell><cell>0.544</cell><cell>0.886</cell><cell>0.57</cell><cell cols="2">0.893 0.706 0.873 0.681</cell></row><row><cell>RBD [20]</cell><cell>0.25</cell><cell cols="5">0.944 0.876 0.822 0.663 0.890 0.763</cell><cell>0.854</cell><cell>0.526</cell><cell cols="4">0.856 0.579 0.874 0.643 0.846 0.612</cell></row><row><cell>SP [14]</cell><cell>1.2</cell><cell cols="5">0.923 0.810 0.780 0.576 0.848 0.682</cell><cell>0.837</cell><cell>0.475</cell><cell cols="4">0.843 0.518 0.917 0.701 0.898 0.644</cell></row><row><cell>MDF* [23]</cell><cell>200</cell><cell cols="5">0.973 0.871 0.908 0.762 0.941 0.829</cell><cell>0.917</cell><cell>0.649</cell><cell cols="4">0.921 0.611 0.943 0.746 0.932 0.745</cell></row><row><cell>DCL* [26]</cell><cell>75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.971 0.897</cell><cell>0.934</cell><cell>0.675</cell><cell>-</cell><cell>-</cell><cell cols="2">0.940 0.711 0.952 0.786</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III AUC</head><label>III</label><figDesc>values and running time for our GLGOV approach under various settings on the MSRA 10K dataset.</figDesc><table><row><cell>Components</cell><cell>FC</cell><cell>ISA</cell><cell>FSA</cell></row><row><cell>AUC</cell><cell cols="3">0.9330 0.9558 0.9667</cell></row><row><cell>Time (s/image)</cell><cell>1.16</cell><cell>1.47</cell><cell>1.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV F</head><label>IV</label><figDesc>-measure of segmented results (top two unsupervised methods are highlighted in red and green). *: deep learning based</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>method Method MSRA10K PASCAL-S ECSSD DUTOMRON THUR15K Overall(OA) Average(AG)</head><label></label><figDesc></figDesc><table><row><cell>Proposed</cell><cell>0.8810</cell><cell>0.6625</cell><cell>0.7592</cell><cell>0.6041</cell><cell>0.6042</cell><cell>0.7320</cell><cell>0.7022</cell></row><row><cell>RC [3]</cell><cell>0.8395</cell><cell>0.4191</cell><cell>0.7277</cell><cell>0.5647</cell><cell>0.5947</cell><cell>0.6926</cell><cell>0.6291</cell></row><row><cell>HDCT [8]</cell><cell>0.8348</cell><cell>0.6074</cell><cell>0.7105</cell><cell>0.5947</cell><cell>0.5881</cell><cell>0.7017</cell><cell>0.6671</cell></row><row><cell>LMLC [1]</cell><cell>0.7501</cell><cell>0.5328</cell><cell>0.5880</cell><cell>0.4340</cell><cell>0.4816</cell><cell>0.5930</cell><cell>0.5573</cell></row><row><cell>SP [14]</cell><cell>0.8106</cell><cell>0.5689</cell><cell>0.6737</cell><cell>0.5367</cell><cell>0.5456</cell><cell>0.6640</cell><cell>0.6271</cell></row><row><cell>GMR [12]</cell><cell>0.8521</cell><cell>0.6467</cell><cell>0.7425</cell><cell>0.5959</cell><cell>0.5924</cell><cell>0.7133</cell><cell>0.6859</cell></row><row><cell>DSR [17]</cell><cell>0.8374</cell><cell>0.6476</cell><cell>0.7388</cell><cell>0.6205</cell><cell>0.6114</cell><cell>0.7174</cell><cell>0.6911</cell></row><row><cell>MB+ [2]</cell><cell>0.8484</cell><cell>0.6614</cell><cell>0.7226</cell><cell>0.5939</cell><cell>0.5977</cell><cell>0.7124</cell><cell>0.6848</cell></row><row><cell>RBD [20]</cell><cell>0.8610</cell><cell>0.654</cell><cell>0.7178</cell><cell>0.6011</cell><cell>0.5851</cell><cell>0.7156</cell><cell>0.6838</cell></row><row><cell>MDF* [23]</cell><cell>0.8794</cell><cell>0.7346</cell><cell>0.8173</cell><cell>0.6948</cell><cell>0.6429</cell><cell>0.7670</cell><cell>0.7538</cell></row><row><cell>DCL* [26]</cell><cell>-</cell><cell>-</cell><cell>0.8968</cell><cell>0.7375</cell><cell>-</cell><cell>0.7633</cell><cell>0.8172</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was partially supported by the Natural Science Foundation of China (61672008, 61772144), the Natural Science Foundation of Guangdong Province (2016A030311013, 2015A030313672), Guangdong Provincial Application-oriented Technical Research and Development Special fund project (2016B010127006, 2017A050501039), and International Scientific and Technological Cooperation Projects of Education Department of Guangdong Province (2015KGJHZ021).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian saliency via low and mid level cues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1404" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis And Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guided Search 2.0 A revised model of visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="238" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Matters of intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="115" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural mechanisms of selective visual attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient Region Detection via High-Dimensional Color Transform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on Multimedia</title>
		<meeting>the eleventh ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention: Different processes and overlapping neural systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Katsuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Constantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Neuroscientist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="509" to="521" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superpixel-based saliency detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual attention: Bottom-up versus top-down</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Egeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="850" to="R852" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Salient region detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wils</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency detection using maximum symmetric surround</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cortical mechanisms of colour vision</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="563" to="572" />
			<date type="published" when="2003-07">Jul 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep CNN features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient coding of spatial information in the primate retina</title>
		<author>
			<persName><forename type="first">E</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greschner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="16256" to="16264" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Donahoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology: The Science of Behavior</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Pearson Education Canada</publisher>
			<pubPlace>Ontario, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Top-down attention switches coupling between low-level and high-level areas of human visual cortex</title>
		<author>
			<persName><forename type="first">N</forename><surname>Al-Aidroos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Turk-Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="14675" to="14680" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HARF: Hierarchy-associated rich features for salient object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Saliency detection via combining region-level and pixel-level predictions with CNNS</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="809" to="825" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DISC: Deep image saliency computing via progressive representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1135" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Complex networks driven salient region detection based on superpixel segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aksac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ozyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alhajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bottom-up saliency detection with sparse representation of learnt texture atoms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="348" to="360" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diversity induced matrix decomposition model for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="253" to="267" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning feature fusion strategies for various image types to detect salient objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hollitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="106" to="120" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Experimental Analysis of Saliency Detection with Respect to Three Saliency Levels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention-driven image interpretation with application to image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1604" to="1621" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Internet visual media processing: a survey with graphics and vision applications</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual attention guided bit allocation in video compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The JPEG2000 still image coding system: an overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skodras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1103" to="1127" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Does where you gaze on an image affect your perception of quality? Applying visual attention to image quality metric</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ninassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barbba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual attention in objective image quality assessment: based on eye-tracking data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heynderickx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="971" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency-based image quality assessment criterion</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Intelligent Computing Theories and Applications. With Aspects of Theoretical and Methodological Issues</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1124" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised salient object segmentation based on kernel density estimation and two-phase graph cut</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1275" to="1289" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference onComputer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fusing disparate object signatures for salient object detection in video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="285" to="299" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An object-oriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2009" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<title level="m">Fuzzy sets and fuzzy logic</title>
		<imprint>
			<publisher>Prentice hall New Jersey</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Untersuchungen zur Lehre von der Gestalt. II</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wertheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="301" to="350" />
			<date type="published" when="1923">1923</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Laws of organization in perceptual forms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wertheimer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Computational gestalts and perception thresholds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology-Paris</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Object-based attention in Chinese readers of Chinese words: Beyond Gestalt principles</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="945" to="949" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast and bottom-up object detection, segmentation, and evaluation using Gestalt principles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="3423" to="3428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A two-stage approach to saliency detection in images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Gestalt principles for attention and segmentation in natural and artificial vision systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bergström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA 2011 Workshop on Semantic Perception, Mapping and Exploration (SPME)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gestalt saliency: Salient region detection based on gestalt principles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improved saliency detection based on superpixel clustering and saliency propagation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Vision science: Photons to phenomenology</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Boosting color saliency in image feature detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attentional modulation of background connectivity between ventral visual cortex and the medial temporal lobe</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Córdova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tompary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Turk-Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of learning and memory</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mpeg-7 visual part of experimentation model version 9</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeannin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cieplinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISO/IEC JTC1/SC</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>WG</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A fast MPEG-7 dominant color extraction with new similarity measure for image retrieval</title>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to Detect A Salient Object</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xi'an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Salientshape: Group saliency in image collections</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
