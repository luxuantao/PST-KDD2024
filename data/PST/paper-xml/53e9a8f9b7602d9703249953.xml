<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intent-Aware Search Result Diversif cation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rodrygo</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
							<email>rodrygo@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intent-Aware Search Result Diversif cation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0EE8413BAE9DD05916320C341BC4153F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-retrieval models Algorithms</term>
					<term>Experimentation</term>
					<term>Performance Web Search</term>
					<term>Relevance</term>
					<term>Diversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search result diversification has gained momentum as a way to tackle ambiguous queries. An effective approach to this problem is to explicitly model the possible aspects underlying a query, in order to maximise the estimated relevance of the retrieved documents with respect to the different aspects. However, such aspects themselves may represent information needs with rather distinct intents (e.g., informational or navigational). Hence, a diverse ranking could benefit from applying intent-aware retrieval models when estimating the relevance of documents to different aspects. In this paper, we propose to diversify the results retrieved for a given query, by learning the appropriateness of different retrieval models for each of the aspects underlying this query. Thorough experiments within the evaluation framework provided by the diversity task of the TREC 2009 and 2010 Web tracks show that the proposed approach can significantly improve state-of-the-art diversification approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>User queries often carry some degree of ambiguity <ref type="bibr" target="#b38">[38]</ref>. On the one hand, genuinely ambiguous queries (e.g., 'zeppelin') have multiple interpretations (e.g., 'airship', 'band '). On the other hand, even those queries with a single, clearly defined interpretation might still be underspecified, as it is not clear which aspects of this interpretation the user is actually interested in (e.g., 'led zeppelin'... 'website' ? 'downloads' ? 'biography' ? 'albums' ? 'reunion' ?) <ref type="bibr" target="#b13">[13]</ref>.</p><p>Search result diversification has recently gained attention as a means to tackle query ambiguity. Instead of trying to identify the 'correct' interpretation behind a query, the idea is to diversify the search results, in the hope that different users will find at least one of these results to be relevant to their information need <ref type="bibr" target="#b1">[1]</ref>. Differently from the traditional assumption of independent document relevance <ref type="bibr" target="#b30">[30]</ref>, diversification approaches typically consider the relevance of a document in light of the other retrieved documents. For instance, once users have found the information they are seeking, it is questionable whether documents with the same (or very similar) information will still be of any use <ref type="bibr" target="#b13">[13]</ref>.</p><p>An effective approach to diversifying search results is to explicitly account for the various aspects <ref type="foot" target="#foot_0">1</ref> underlying an ambiguous query <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref>. By doing so, the problem becomes to select a ranking of documents that collectively provide the maximum relevance with respect to these different aspects. In a real scenario, however, the actual aspects of a query are not known, nor is the relevance of each retrieved document to each query aspect determined with certainty. Moreover, the relevance of a document to a query aspect may depend on the intent<ref type="foot" target="#foot_1">2</ref> underlying this aspect (e.g., informational or navigational <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b31">31]</ref>). Additionally, different aspects could feasibly represent information needs with different intents. For instance, 'website', 'downloads', and 'biography' arguably represent navigational, transactional, and informational aspects of the query 'led zeppelin', respectively. Similarly, 'albums' could exemplify a typed query aspect, representing an information need for a list of entities. In the same vein, 'reunion' might denote a question-answering aspect of the query 'led zeppelin', regarding whether the legendary rock band have any plans of reuniting in the near future.</p><p>Queries with different intents have been shown to benefit from different retrieval models <ref type="bibr" target="#b22">[22]</ref>. Likewise, we hypothesise that explicit diversification approaches may benefit from accounting for the intents of different query aspects. For instance, relevance estimations with respect to the 'website' aspect of the query 'led zeppelin' could be arguably improved by applying a retrieval model suitable for navigational queries. In this paper, we propose a novel diversification approach, aimed at learning the appropriateness of multiple intent-aware retrieval models for each aspect. As a result, the relevance of a document to multiple aspects-i.e., its diversity-can be estimated more effectively.</p><p>We thoroughly evaluate our approach in the context of the diversity task of the TREC 2009 and 2010 Web tracks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. In particular, we investigate learning strategies that either select the most appropriate retrieval model or merge multiple retrieval models for each query aspect. The results of our investigations attest the effectiveness of both strategies within our proposed intent-aware approach for diversifying Web search results, with significant improvements on top of state-of-the-art diversification approaches.</p><p>The remainder of this paper is organised as follows. Section 2 describes related work on search result diversification and search intents. Section 3 further details our main contributions. Section 4 describes our approach for leveraging intent-aware models for diversification. Sections 5 and 6 detail the experimental setup and the evaluation of our approach. Finally, Section 7 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we provide background on the search result diversification problem and related approaches to this problem. We then set the grounds for our proposed approach by reviewing related work on the use of search intents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Result Diversif cation</head><p>Most of the existing diversification approaches are somehow inspired by the work of Carbonell and Goldstein <ref type="bibr" target="#b5">[5]</ref>. The basic idea of their Maximal Marginal Relevance (MMR) method is to iteratively select a document with the highest similarity to the query and lowest similarity to the already selected documents, in order to promote novelty. Subsequent implementations of this idea include the approach of Zhai et al. <ref type="bibr" target="#b41">[41]</ref> to model relevance and novelty within a risk minimisation framework. In particular, they promote documents with highly divergent language models from those of the already selected documents. Chen and Karger <ref type="bibr" target="#b10">[10]</ref> proposed a probabilistic approach to the related problem of finding at least one relevant result for a given query, by choosing documents under the assumption that those already chosen are not relevant to the query. More recently, Wang and Zhu <ref type="bibr" target="#b39">[39]</ref> proposed to diversify a document ranking as a means to reduce the risk of overestimating its relevance. In their work, two documents are compared based on the correlation of their relevance scores.</p><p>By assuming that similar documents will cover similar aspects, the aforementioned approaches only consider the aspects underlying a query implicitly. An alternative approach consists of explicitly modelling these aspects <ref type="bibr" target="#b36">[36]</ref>. For instance, Agrawal et al. <ref type="bibr" target="#b1">[1]</ref> proposed the IA-Select algorithm for search result diversification. It employs a classification taxonomy over queries and documents to iteratively promote documents that share a high number of classes with the query, while demoting those documents with classes already well represented in the ranking. Similarly, Carterette and Chandar <ref type="bibr" target="#b7">[7]</ref> proposed a probabilistic approach to maximise the coverage of the retrieved documents with respect to the aspects of a query, by modelling these aspects as topics identified from the top ranked documents. Recently, Santos et al. <ref type="bibr" target="#b33">[33]</ref> introduced the xQuAD probabilistic framework for search result diversification, which explicitly represents different query aspects as 'sub-queries'. They defined a di-versification objective based on the estimated relevance of documents to multiple sub-queries, as well as on the relative importance of each sub-query in light of the initial query.</p><p>Since our goal is to produce intent-aware relevance estimations given an explicit representation of query aspects, our approach is also set in the context of explicit diversification. Accordingly, in Section 6, we use both IA-Select <ref type="bibr" target="#b1">[1]</ref> and xQuAD <ref type="bibr" target="#b33">[33]</ref> as a basis for evaluating our approach. In particular, these two approaches represent the state-of-theart in explicit search result diversification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intents in Information Retrieval</head><p>Different information retrieval tasks have benefited from taking into account the intent of a query (e.g., informational, navigational, or transactional <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b31">31]</ref>). These approaches can be generally categorised based on whether or not they rely on the classification of queries into predefined intents.</p><p>Query intent detection approaches first classify a query with respect to a predefined set of intents. A retrieval model specifically trained for the predicted intent is then applied to retrieve documents for the query. For instance, Kang and Kim <ref type="bibr" target="#b22">[22]</ref> showed that queries of different intents can benefit from the application of intent-specific retrieval models. A major shortcoming of this approach, however, is the limited accuracy of existing intent detection mechanisms <ref type="bibr" target="#b17">[17]</ref>.</p><p>Instead of classifying a query into a predefined target intent, an alternative is to identify similar queries from a training set, and then to apply a retrieval model appropriate for this set. This approach has an advantage over a classification of queries based on a fixed set of intents, as queries of the same intent often benefit from different retrieval models <ref type="bibr" target="#b17">[17]</ref>. For example, Geng et al. <ref type="bibr" target="#b20">[20]</ref> proposed an instance-based learning approach using k-nearest neighbour (k-NN) classification to improve Web search effectiveness. In their approach, a k-NN classifier is used to identify training queries similar to an unseen query. A retrieval model is then learned based on the identified queries and applied to the unseen query. A more general approach was proposed by Peng et al. <ref type="bibr" target="#b27">[27]</ref>. In their work, multiple ranking functions are chosen from a pool of candidate functions, based on their performance on training queries similar to an unseen query.</p><p>Our approach is similar in spirit to the approaches of Kang and Kim <ref type="bibr" target="#b22">[22]</ref>, Geng et al. <ref type="bibr" target="#b20">[20]</ref>, and Peng et al. <ref type="bibr" target="#b27">[27]</ref>. However, while these approaches focused on inferring the intent of a query, we target the problem of inferring the intent underlying different aspects of this query. Besides this difference in granularity, our intent-aware approach tackles a different search scenario, namely, search result diversification.</p><p>In a similar vein, Santos et al. <ref type="bibr" target="#b34">[34]</ref> proposed a selective diversification approach, aimed at tailoring a diversification strategy to the ambiguity level of different queries. In particular, given an unseen query, their approach learns a trade-off between relevance and diversity, based on optimal trade-offs observed for similar training queries. As a result, their approach effectively determines when to diversify the results for an unseen query, and also by how much. Our proposed approach also differs from the approach of Santos et al. <ref type="bibr" target="#b34">[34]</ref>, in that ours focuses on selecting appropriate retrieval models for different query aspects, as opposed to the query itself. More importantly, their approach is orthogonal to ours. In essence, instead of determining when to diversify the results for a given query, we tackle the problem of how to diversify these results given the identified aspects of this query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONTRIBUTIONS OF THIS PAPER</head><p>The major contributions of this paper are:</p><p>1. A novel aspect intent-aware diversification approach, aimed at predicting, for each identified query aspect, the appropriateness of different retrieval models.</p><p>2. A thorough evaluation of the proposed approach within the standard experimentation paradigm of the diversity task of the TREC 2009 and 2010 Web tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INTENT-AWARE SEARCH RESULT DIVERSIFICATION</head><p>As discussed in Sections 1 and 2, different aspects of an ambiguous query can have rather different intents. To illustrate this, consider topic #1 from the diversity task of the TREC 2009 Web track <ref type="bibr" target="#b11">[11]</ref>, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In this example, different aspects of the query 'obama family tree' are represented as a set of sub-topics, identified from the query log of a commercial search engine. Moreover, these sub-topics represent aspects with an informational ('inf') or a navigational ('nav') intent, as judged by TREC assessors.</p><p>&lt;topic number="1" type="faceted"&gt; &lt;query&gt;obama family tree&lt;/query&gt; &lt;description&gt; Find information on President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc. &lt;/description&gt; &lt;subtopic number="1" type="nav"&gt; Find the TIME magazine photo essay "Barack Obama's Family Tree". &lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt; Where did Barack Obama's parents and grandparents come from? &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; Find biographical information on Barack Obama's mother. &lt;/subtopic&gt; &lt;/topic&gt; Inspired by related work in Section 2.2, we hypothesise that diversification approaches can benefit from retrieval models targeted to the intents of different query aspects. For instance, for the query exemplified in Figure <ref type="figure" target="#fig_0">1</ref>, a diversification approach could leverage a navigational intentaware model for the first query aspect, and an informational intent-aware model for the second and third aspects.</p><p>In this work, we propose a supervised learning approach for estimating the appropriateness of multiple intent-aware retrieval models for each query aspect. Given a query q, our goal is to maximise the diversity of the retrieved documents with respect to the aspects underlying this query. Without loss of generality, following an explicit diversification strategy, we can quantify the diversity of a document d given a query q and the other retrieved documents S as the expected relevance of d with respect to the aspects of q, denoted A(q): P(d|S, q) = X a∈A(q) P(a|q) P(d|S, q, a),</p><p>where P(a|q) captures the relative importance of each aspect a given the query q, and P(d|S, q, a) denotes the probability of the document d being relevant to this aspect, given how well the documents in S already satisfy this aspect.</p><p>Equation ( <ref type="formula" target="#formula_0">1</ref>) can be seen as a canonical formulation of the objective functions deployed by different explicit diversification approaches in the literature <ref type="bibr" target="#b34">[34]</ref>. In particular, these approaches differ primarily in how they represent the set of aspects associated with a query, and in how they estimate the relevance of each document to every identified query aspect. For instance, Agrawal et al. <ref type="bibr" target="#b1">[1]</ref> rely on the toplevel categories from the Open Directory Project (ODP) <ref type="foot" target="#foot_2">3</ref>for representing query and document classes, and integrate relevance and classification scores for ranking documents. Santos et al. <ref type="bibr" target="#b33">[33]</ref> exploit query reformulations from commercial search engines as representations of the different aspects of a query, and directly estimate the relevance of documents to these aspects. Our proposed approach is agnostic to any particular mechanism for generating explicit query aspect representations. Indeed, our only assumption is that different aspects may convey different user intents.</p><p>In the interest of keeping the description of our approach general, in the remainder of this section, we adopt an abstract view of aspects and intents. <ref type="foot" target="#foot_3">4</ref> In particular, to formalise our approach, we further derive Equation (1), by marginalising P(d|S, q, a) over a target set of intents I:</p><formula xml:id="formula_1">P(d|q, S) = X a∈A(q) P(a|q) X i∈I P(i|a) P(d|S, q, a, i),<label>(2)</label></formula><p>where P(i|a) denotes the probability that the aspect a of the initial query q conveys the intent i. Accordingly, P(d|S, q, a, i) denotes the relevance of the document d in light of the other retrieved documents S, the query q, the aspect a, and the intent i. Once again, without loss of generality, assuming that different aspects are equally probable (i.e., P(a|q) = 1 |A(q)| , ∀a ∈ A(q)), <ref type="foot" target="#foot_4">5</ref> our task becomes two-fold:</p><p>1. To infer the probability P(i|a) of each intent i ∈ I given a query aspect a ∈ A(q); 2. To learn an appropriate retrieval model P(d|S, q, a, i) for each predicted intent i ∈ I.</p><p>In Section 4.1, we propose a classification approach for the first task. For the second task, as described in Section 4.2, we resort to learning-to-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inferring the Query Aspect Intents</head><p>In order to infer the probability of different intents for a query aspect, we propose a linear classification approach. In particular, given a query aspect a, our goal is to estimate the probability of an intent i ∈ I as:</p><formula xml:id="formula_2">P(i|a) = f (w • xa),<label>(3)</label></formula><p>where xa is a feature vector representing the aspect a, and w is a weight vector, which is learned from labelled training data. The function f maps the dot product of the weight and feature vectors into the desired prediction outcome. Alternative regimes for instantiating the function f are described in Section 4.1.1. Section 4.1.2 describes our choices for labelling training data. Lastly, Section 4.1.3 defines the classification space considered in this paper, and describes the query aspect features leveraged for this classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Classif cation Regimes</head><p>We propose two alternative regimes for instantiating the function f in Equation ( <ref type="formula" target="#formula_2">3</ref>): model selection and model merging. The model selection regime employs a hard classification approach <ref type="bibr" target="#b40">[40]</ref>. In particular, this approach treats different intents as mutually exclusive, hence assigning each aspect a single (i.e., the most likely) intent. For instance, for a target set of intents I = {i1, i2, i3}, a possible selection outcome could be: P(i1|a) = 1, P(i2|a) = 0, P(i3|a) = 0. In this example, the aspect a would be associated with its most likely intent, i1, and only the retrieval model P(d|S, q, a, i1) would have an impact on the estimated relevance of document d to the aspect a. This classification regime resembles the selective retrieval approaches described in Section 2.2, except that the most appropriate model is selected at the aspect level (as opposed to the query level).</p><p>Our second regime, model merging, provides a relaxed alternative to model selection. In particular, it deploys a soft classification approach, in order to obtain a full probability distribution over the considered intents <ref type="bibr" target="#b40">[40]</ref>. For the above example, a possible outcome of the model merging classification could be P(i1|a) = 0.6, P(i2|a) = 0.3, P(i3|a) = 0.1. In this case, the estimated relevance of a document d to the aspect a would be determined by a linear combination: P(d|S, q, a) = 0.6 × P(d|S, q, a, i1) + 0.3 × P(d|S, q, a, i2) + 0.1 × P(d|S, q, a, i3).</p><p>Different classifiers can be deployed to implement both the model selection and model merging regimes. Further details about the specific classifiers that enable both regimes in our investigations are provided in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Classif cation Labels</head><p>In order to determine the ground-truth intent for different query aspects, we investigate two alternatives. The first one is based on the direct judgement by humans, who base their assessment solely on the observed aspects. However, the differences between query aspects may go beyond their apparent characteristics. For instance, aspects with the same judged intent could still benefit from leveraging different retrieval models <ref type="bibr" target="#b17">[17]</ref>. Additionally, judging the intent of different aspects may be costly for large training datasets.</p><p>To overcome these limitations, we propose a second alternative for automatically labelling training aspects. Given a training query q with aspects A(q) and a set of target intents I, with |A| = k and |I| = p, we devise an oracle selection mechanism. In particular, this oracle mechanism always chooses the best out of the p k possible selections for the k aspects of q, according to a diversity evaluation metric (e.g., ERR-IA <ref type="bibr" target="#b9">[9]</ref>, or any of the metrics described in Section 5.4). Although estimating this oracle may be infeasible for large values of k, it can be easily estimated for most practical settings. For instance, the maximum number of aspects per query in the TREC 2009 and 2010 Web tracks is k = 8. Moreover, if many more aspects were available for a particular query, less plausible aspects could be discarded without much loss. Indeed, this is precisely what leading Web search engines do when displaying only the top suggestions for a user query, which have been shown to deliver an effective diversification performance <ref type="bibr" target="#b33">[33]</ref>. Finally, it is worth noting that this entire labelling process is conducted offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Classif cation Features</head><p>So far, we have intentionally described our approach without a strict definition of the classification space. This abstract view of classification instances as query aspects demonstrates the generality of our proposed approach, and its applicability to different explicit diversification approaches in the literature (e.g., <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref>). In particular, our proposed approach is not bound to any particular query aspect representation. In fact, any aspect representation that portrays the multitude of information needs underlying an ambiguous query <ref type="bibr" target="#b35">[35]</ref> is potentially applicable, as different information needs can convey different user intents.</p><p>Nonetheless, to enable our investigations in Section 6, we follow Santos et al. <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref> and adopt a concrete representation of query aspects as 'sub-queries'. In particular, a sub-query can be seen as a keyword-based representation of the information need expressed by a query aspect. In our experiments, we consider two mechanisms for generating sub-queries, as described in Section 5.1. Additionally, limited by the TREC Web test collection used in our experiments <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, we restrict the space of target intents to navigational and informational ones. Based on this representation of aspects and intents, and inspired by research on related query analysis tasks, we devise a large feature set for sub-query intent classification. In particular, these include features computed from the words in the sub-query itself, as well as from the top documents retrieved for this sub-query. In total, we devise 838 features, based on 21 different feature classes. These features are described on the left side of Table <ref type="table" target="#tab_0">1</ref>, and organised into three groups:</p><p>Query log features (LOG). Query logs provide valuable evidence for discriminating between informational and navigational intents. To exploit such evidence, we compute several sub-query features based on the 15-million query MSN Search 2006 Query Log. For instance, we count the raw frequency of sub-queries, as navigational sub-queries are generally more popular than informational ones. Likewise, informational sub-queries intuitively require more effort from the users while inspecting the retrieved results. We quantify this in terms of the number of examined results and the time spent in doing so, as well as the click entropy <ref type="bibr" target="#b15">[15]</ref>.</p><p>Query performance predictors (QPP). The intent of a sub-query may be reflected not only on the sub-query itself, but also on the documents retrieved for this sub-query. For instance, a low coherence of the top-retrieved documents could indicate a sub-query with an informational intent. This, in turn, can reflect on the performance of this subquery when used in a retrieval system. To exploit this intuition, we build upon a large body of research on query performance prediction <ref type="bibr" target="#b6">[6]</ref> and leverage both pre-and postretrieval predictors as sub-query features. In particular, the former are solely based on statistics of the sub-query terms, while the latter also leverage information from the documents retrieved for the sub-query.</p><p>Taxonomy-based features (TAX). Informational needs are intuitively broader than navigational ones, in terms of the concepts they cover. To quantify this intuition, we devise different features based on concepts from two different taxonomies derived from Wikipedia: categories and named entities. For the latter, we consider entities of four types: people, organisations, products, and locations. In partic- ular, we represent the documents retrieved for each subquery in the space of the concepts from these different taxonomies. 6 Based on this representation, we compute various distributional features, such as the average number of retrieved concepts, the average distance between pairs of documents, and the concept entropy of the entire retrieved list. Additionally, we also quantify the number of ambiguous entities among the top documents retrieved for a sub-query. Our intuition is that the presence of such entities further indicates the broadness of the sub-query <ref type="bibr" target="#b37">[37]</ref>.</p><p>Most of these features are extracted in multiple variants. For instance, retrieval-based features are computed based on five different approaches, as implemented by the Terrier IR platform <ref type="bibr" target="#b25">[25]</ref>: Okapi BM25, the Divergence From Randomness (DFR) DPH and PL2 models, a language modelling (LM) approach with Dirichlet smoothing, and a count of the number of matching query terms. Additionally, these features are estimated at six rank cutoffs: 1, 3, 5, 10, 50, and 100. Finally, distributional features (e.g., the number of concepts across the retrieved documents) are summarised using up to four different statistics: mean, standard deviation, median, and maximum. Altogether, these amount to the grand total of 838 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Intent-Aware Retrieval Models</head><p>In Section 4.1.1, we proposed two regimes for inferring an intent distribution P(i|a) for each aspect a. In this section, we propose a learning-to-rank approach for producing bined with the features selected before it. From the table, we observe that the top selected features are generally intuitive. For instance, DPH (which is used to generate the initial sample of documents for learning) is the top feature for both models. Likewise, as expected, various URL and link analysis features (e.g., URLWiki, URLLength, Absorbing, InvPageRank, Inlinks) are ranked high in the navigational model. Besides producing intuitive intent-aware models, we believe that our data-driven approach based on a large set of features provides a more robust alternative to hand-picking features traditionally associated with a particular intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary</head><p>In this section, we have introduced a novel supervised learning approach for diversifying the search results in light of the intents of different aspects of an ambiguous query. To enable our investigations in Section 6, we have instantiated our intent-aware diversification approach in light of two target aspect intents, namely, informational and navigational. Given these intents, we have described large feature sets for both inferring the intent distribution of different aspects (Section 4.1), as well as for learning the corresponding intent-aware retrieval models (Section 4.2). Although the choice of appropriate feature sets naturally depends on how learning instances (i.e., aspects) and labels (i.e., intents) are represented <ref type="bibr" target="#b40">[40]</ref>, it is worth reiterating that our approach is agnostic to these representations. While instantiating it for a different aspect representation or a different set of intents may require devising different features, no modification to the approach itself would be necessary. Moreover, although motivated by the learning tasks at hand, both feature sets in Table <ref type="table" target="#tab_0">1</ref> comprise features deployed for a variety of different purposes in the literature. As a result, we believe they might be useful for deploying our approach with target intents beyond the two considered in our current investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL SETUP</head><p>In the next section, we investigate our intent-aware diversification approach proposed in Section 4. In particular, we aim to answer two main research questions:</p><p>1. Can we improve diversification performance with our intent-aware model selection regime?</p><p>2. Can we improve diversification performance with our intent-aware model merging regime?</p><p>These questions are investigated in Sections 6.1 and 6.2, respectively. In the remainder of this section, we detail the experimental setup that supports these investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Collection, Queries, and Sub-Queries</head><p>Our analysis is conducted within the standard experimentation paradigm provided by the diversity task of the TREC 2009 and 2010 Web tracks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>-henceforth denoted WT09 and WT10 tasks, respectively. As a document collection, we consider the category-B ClueWeb09 dataset, as used in these tasks. This collection comprises 50 million English documents, aimed to represent the first tier of a commercial search engine index. In our experiments, we index this collection using Terrier <ref type="bibr" target="#b25">[25]</ref>, after applying Porter's English weak stemmer and without removing stopwords.</p><p>The WT09 and WT10 tasks comprise 50 and 48 queries, respectively. As mentioned in Section 4.1.3, for each of these 98 queries, we generate two sets of sub-queries, in order to provide alternative aspect representations for our investigations. The first sub-query set is based on the official sub-topics identified by TREC assessors for each of these queries. In particular, each WT09 query has an average of 3.54 informational and 1.32 navigational aspects, as judged by TREC assessors. For the WT10 queries, these numbers become 2.84 and 1.50, respectively. As TREC only provides a natural language description for each sub-topic, we obtain a shorter, keyword-like version using Amazon's Mechanical Turk. This step was necessary to make these sub-topics better resemble real Web search requests, so as to enable their matching in our query log. Note that this procedure by no means interfere with our conclusions, as these keyword-like sub-topics are uniformly deployed for all tested approaches.</p><p>Using the official TREC Web track sub-topics as a subquery set has two main advantages. Firstly, as discussed in Section 4.1.2, they provide judged intent labels for each subquery, which can be contrasted to our proposed performanceoriented labelling of training data. Secondly and most important, they provide a controlled environment for evaluating the effectiveness of our approach while isolating the impact of any particular aspect representation. In addition to this 'ground-truth' sub-query set, we also evaluate our approach using an alternative sub-query set. Following Santos et al. <ref type="bibr" target="#b33">[33]</ref>, for each of the 98 queries, we obtain up to 13 query suggestions from a commercial search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Diversif cation Approaches</head><p>In Section 6, we apply our intent-aware model selection and model merging regimes to two diversification approaches: IA-Select <ref type="bibr" target="#b1">[1]</ref> and xQuAD <ref type="bibr" target="#b33">[33]</ref>. In particular, both approaches instantiate the general explicit diversification objective described in Equation ( <ref type="formula" target="#formula_0">1</ref>), and hence can directly leverage our intent-aware aspect relevance estimations. Additionally, as discussed in Section 2.1, these approaches are representative of the state-of-the-art in search result diversification. Indeed, a variant of xQuAD was among the top performing approaches in the diversity task of both TREC 2009 and 2010 <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. In our investigations, both IA-Select and xQuAD diversify the top 1000 documents retrieved by DPH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Classif cation Approaches</head><p>In Section 4.1, we introduced two regimes for leveraging the inferred intents of different aspects: model selection and model merging. The model selection regime builds upon a hard classification of intents. To enable this regime, we deploy two alternative classifiers. Firstly, we train a support vector machine (SVM) classifier with a polynomial kernel through a sequential minimal optimisation <ref type="bibr" target="#b40">[40]</ref>. Our second Table <ref type="table">3</ref>: Diversification performance of IA-Select and xQuAD using informational or navigational models uniformly (UNI) or selectively (SEL) according to the WT09 topics, with the WT10 topics used for training.</p><p>classifier performs a multinomial logistic regression with a ridge estimator <ref type="bibr" target="#b40">[40]</ref>. In both cases, the single most likely intent is chosen for each aspect, in a typical selective fashion.</p><p>In order to enable our second regime, model merging, we fit the output of the SVM classifier to a logistic regression model, hence obtaining a full probability distribution over intents for each aspect underlying the query <ref type="bibr" target="#b40">[40]</ref>. In order to cope with the high dimensionality of our sub-query feature set, classification is performed after a dimensionality reduction via principal component analysis <ref type="bibr" target="#b40">[40]</ref>. All classification tasks are performed using the Weka suite. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation and Training Procedure</head><p>We report our results based on the official evaluation metrics in the diversity task of the TREC 2010 Web track <ref type="bibr" target="#b12">[12]</ref>: ERR-IA <ref type="bibr" target="#b9">[9]</ref>, α-nDCG <ref type="bibr" target="#b13">[13]</ref>, NRBP <ref type="bibr" target="#b14">[14]</ref>, and MAP-IA <ref type="bibr" target="#b1">[1]</ref>. The first three metrics implement a cascade user model, which penalises redundancy by assuming an increasing probability that users will stop inspecting the results as they find their desired information. The fourth metric is based on a simpler model, which rewards a high coverage of query aspects, without directly penalising redundancy.</p><p>Our evaluation ensures a complete separation between training and test settings. In particular, we use the WT09 and WT10 queries interchangeably as training and test, in a cross-year evaluation fashion (i.e., we train on WT09 and test on WT10, and vice versa). This training procedure renders our results on the WT10 queries directly comparable to those of participant systems in TREC 2010. For the reported results on the WT09 queries, however, we note that TREC 2009 participant systems naturally did not have access to WT10 queries for training. This training procedure is used for the classification approaches described in 8 http://www.cs.waikato.ac.nz/ml/weka/ Section 5.3, as well as for xQuAD's diversification trade-off parameter λ <ref type="bibr" target="#b33">[33]</ref>. As for IA-Select, it is a parameter-free diversification approach, and hence requires no training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate our intent-aware diversification approach, in order to answer the two research questions stated in Section 5. In particular, Section 6.1 investigates the effectiveness of our model selection regime, while Section 6.2 analyses the effectiveness of the model merging regime. Both regimes were described in Section 4.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Intent-Aware Model Selection</head><p>Our primary goal in this experiment is to assess the effectiveness of our model selection regime for search result diversification. As described in Section 4.1.1, this regime selects the most likely between an informational and a navigational intent-aware retrieval model for each aspect. As a baseline, we consider a simple regime that uniformly deploys one of the informational or navigational models for all aspects, regardless of the intents of these aspects. To validate our findings, as described in Section 5.2, we test both our model selection regime as well as the baseline uniform regime applied to two diversification approaches: IA-Select and xQuAD. Additionally, these diversification approaches are deployed using two different aspect representations: the official TREC Web track sub-topics and query suggestions from a search engine, as discussed in Section 5.1. Moreover, we test variants of our model selection regime. Each variant is denoted Sel(C,L), where C and L denote a classifier and a set of classification training labels, respectively. In particular, C can be one of three classifiers: an oracle (ora), which simulates a perfect classification accuracy, and the logistic regression (log) and support vector machine (svm)  <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref> compare the aforementioned variants of our model selection regime to the baseline uniform regime on the WT09 and WT10 queries, respectively, in terms of the four metrics described in Section 5.4: ERR-IA@20, α-nDCG@20, NRBP, and MAP-IA. In parentheses, we show the percent improvement of each variant of our model selection regime compared to the uniform regime that uses the informational (Uni(inf)) or the navigational (Uni(nav)) model, respectively. Significant improvements are measured by the Wilcoxon signed-rank test. In particular, the symbols ( ) and △ (▽) denote a significant increase (decrease) at the p &lt; 0.01 and p &lt; 0.05 levels, respectively. Lastly, the bottom row in Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref> shows the performance of the top performing category-B system in the WT09 and WT10 tasks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, respectively, hence providing a further reference value for evaluating our intent-aware approach.</p><p>From Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref>, we first note that the uniform application of an informational or a navigational model provides a strong baseline performance. The uniform application of the navigational model, in particular, performs at least comparably to the best performing TREC system in both the WT09 and WT10 tasks. To see whether our model selection regime can improve upon this strong baseline, we first look at the performance of this regime using an oracle classifier. The results show that a massive improvement can be attained by selecting the most appropriate model for each aspect (as opposed to uniformly using a single model for all aspects) for both IA-Select and xQuAD, when the performance-oriented labels (perf) are used for training. On the WT09 queries (Table <ref type="table">3</ref>), compared to the strongest uniform setting (i.e., Uni(nav)) in terms of ERR-IA@20, improvements can be as high as 29.1% for IA-Select and 19.4% for xQuAD using the Web track sub-topics as sub-queries, and are always significant. When using query suggestions, the potential improvements are 38.3% and 30.8%, respectively. On the WT10 queries (Table <ref type="table" target="#tab_3">4</ref>), similar figures are observed: 32.3% and 24.5% gain for IA-Select and xQuAD, respectively, using the Web track sub-topics; 30.1% and 37.8% using query suggestions. Once again, all improvements are statistically significant. Human judgements, in contrast, provide a supotimal labelling criterion, as denoted by the lower performance attained when using the judg labels. Indeed, even an oracle classifier, which always choses the correct intent according to these judgements (i.e., the Sel(ora,judg) regime), cannot improve over applying the navigational model uniformly. As discussed in Section 4.1.2, this further confirms our intuition that the appropriateness of an intent-aware retrieval model for a given aspect cannot be effectively judged purely on the basis of the apparent characteristics of this aspect.</p><p>Besides showing a strong potential for improving diversification performance, as demonstrated using an oracle classifier (i.e., the Sel(ora,perf) regime), our intent-aware approach is also effective in a practical deployment based on standard classifiers. Indeed, Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref> show that our model selection regime using both logistic regression (log) and support vector machine (svm) classifiers with perf labels always improves compared to a uniform regime, often significantly. For instance, on the WT09 queries (Table 3) and considering the Web track sub-topics as aspect representations, compared to the stronger Uni(nav) baseline, improvements in terms of ERR-IA@20 are as high as 11.1% for IA-Select (Sel(log,perf)) and 7.9% for xQuAD (Sel(log,perf)). On the WT10 queries (Table <ref type="table" target="#tab_3">4</ref>), improvements are as high as 8.8% for IA-Select (Sel(log,perf)) Table <ref type="table">5</ref>: Diversification performance of IA-Select and xQuAD using informational or navigational models selectively (SEL) or through merging (MRG). WT09 and WT10 results are shown on the top and bottom halves, respectively. As in Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref>, WT09 results are trained on WT10 topics, and vice versa. and 6.6% for xQuAD (Sel(svm,perf)). Similar improvements across the other reported metrics are consistently observed. When query suggestions are used as aspect representations, although improvements are less pronounced, they are consistent and can still be significant.</p><p>Overall, the results in this section answer our first research question, by showing that diversification performance can be significantly improved by leveraging the most appropriate intent-aware retrieval model for each query aspect. Our model selection regime using performance-oriented classification labels is particularly effective, significantly improving upon a uniform regime comparable to the top performing systems of the TREC 2009 and 2010 Web tracks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. Furthermore, the consistency of our observations for two state-of-the-art diversification approaches and according to multiple evaluation metrics attests the robustness of the model selection regime. In the next section, we contrast this regime against the alternative model merging regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Intent-Aware Model Merging</head><p>After demonstrating the effectiveness of selecting a single model for each query aspect, in this experiment, we investigate whether deploying a model merging regime could bring further improvements. For this investigation, we focus our attention to the TREC Web track sub-topics as an aspect representation. As discussed in Section 5.1, this representation allows for assessing the effectiveness of our merging regime across the two proposed training labelling alternatives, judg and perf. The results based on query suggestions using perf labels lead to identical conclusions and are hence omitted for brevity. In particular, Table <ref type="table">5</ref> shows the diversification performance of IA-Select and xQuAD under the model merging regime (Mrg), in contrast to their performance under the model selection regime (Sel), which serves as our baseline in this investigation. Similarly to Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref>, percent differences between these two regimes are shown in parentheses, alongside one of the aforementioned symbols to denote the significance (or lack thereof) of such differences. As discussed in Section 5.3, both regimes are based on predictions made by an SVM classifier. In particular, the model merging regime is enabled by fitting the SVM predictions to a logistic regression model.</p><p>From Table <ref type="table">5</ref>, we observe that the model merging regime can improve upon the model selection regime in most cases, particularly on the WT10 queries (bottom half of Table <ref type="table">5</ref>). However, the merging regime can also underperform compared to the selection regime, when perf labels are used for IA-Select and xQuAD, on WT09 and WT10, respectively. Nevertheless, significant differences are only observed when IA-Select is deployed under the Mrg(svm,judg) regime on the WT10 queries. These results answer our second research question, by showing that merging multiple intent-aware models can be at least as effective as selecting the single most likely model. Moreover, we believe that the merging regime can offer additional benefits for an intent-aware diversification. For one, it can help attenuate the harm of selecting the wrong model for a particular sub-query. Additionally, it provides a natural upper-bound for the selection regime. Indeed, model selection is a special instance of model merging, with a mutually exclusive probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, we have introduced a novel intent-aware approach for search result diversification. Given the possible intents underlying the aspects of a query, our approach learns the appropriateness of retrieval models targeted to each of these intents. These models are then leveraged selectively or combined in a merging fashion in order to refine the estimation of the relevance of the retrieved documents with respect to each query aspect. In particular, our approach builds upon a general explicit diversification model, which makes it seamlessly deployable by existing approaches in the literature. Indeed, thorough experiments in the context of the TREC 2009 and 2010 Web tracks demonstrate that our approach is general and significantly improves the effectiveness of two state-of-the-art diversification approaches.</p><p>Our data-driven approach for learning both intent-aware retrieval models and their appropriateness for a given aspect opens up promising directions. In particular, the full potential of our approach could be further exploited by building upon a larger pool of intent-aware retrieval models (e.g., with, say, a transactional model <ref type="bibr" target="#b4">[4]</ref>), as well as features capturing dependencies between the retrieved documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TREC 2009 Web track, topic #1, along with its corresponding sub-topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>All features used in this work. Sub-query features (left side) are used for inferring the intents of different query aspects. Document features (right side) are used to produce intent-aware learned models.</figDesc><table><row><cell cols="2">Sub-Query Features</cell><cell></cell><cell></cell><cell>Document Features</cell><cell></cell></row><row><cell>Group Feature Class</cell><cell>Description</cell><cell cols="2">Total Group Feature Class</cell><cell>Description</cell><cell>Total</cell></row><row><cell>LOG ClickCount</cell><cell>No. of clicks</cell><cell>120 WM</cell><cell>BM25</cell><cell>BM25 score [25]</cell><cell>5</cell></row><row><cell>LOG ClickEntropy</cell><cell>URL-level click entropy [15]</cell><cell>3 WM</cell><cell>DPH</cell><cell>DPH score [25]</cell><cell>5</cell></row><row><cell>LOG HostEntropy</cell><cell>Host-level click entropy</cell><cell>2 WM</cell><cell>LM</cell><cell>LM score (Dirichlet) [25]</cell><cell>5</cell></row><row><cell cols="2">LOG QueryFrequency No. of occurrences</cell><cell>4 WM</cell><cell>PL2</cell><cell>PL2 score [25]</cell><cell>5</cell></row><row><cell>LOG ResultCount</cell><cell>No. of examined results</cell><cell>3 WM</cell><cell>MQT</cell><cell>No. of matching query terms</cell><cell>5</cell></row><row><cell cols="2">LOG SessionDuration Session duration (in sec.)</cell><cell>3 FM</cell><cell>BM25F</cell><cell>BM25F score [25]</cell><cell>1</cell></row><row><cell>QPP AvICTF</cell><cell>Pre-retrieval predictor [21]</cell><cell>1 FM</cell><cell>PL2F</cell><cell>PL2F score [25]</cell><cell>1</cell></row><row><cell>QPP AvIDF</cell><cell>Pre-retrieval predictor [21]</cell><cell>1 DM</cell><cell>MRF</cell><cell>MRF proximity score [25]</cell><cell>8</cell></row><row><cell>QPP AvPMI</cell><cell>Pre-retrieval predictor [21]</cell><cell>1 DM</cell><cell>pBiL</cell><cell>DFR pBiL proximity score [25]</cell><cell>8</cell></row><row><cell>QPP ClarityScore</cell><cell>Post-retrieval predictor [19]</cell><cell>30 LA</cell><cell>Absorbing</cell><cell>Absorbing Model score [28]</cell><cell>1</cell></row><row><cell>QPP EnIDF</cell><cell>Pre-retrieval predictor [21]</cell><cell>1 LA</cell><cell>Edgerecip</cell><cell>No. of reciprocal links [3]</cell><cell>1</cell></row><row><cell>QPP Gamma</cell><cell>Pre-retrieval predictor [21]</cell><cell>2 LA</cell><cell>Inlinks</cell><cell>No. of inlinks</cell><cell>1</cell></row><row><cell cols="2">QPP QueryDifficulty Post-retrieval predictor [2]</cell><cell>30 LA</cell><cell>Outlinks</cell><cell>No. of outlinks</cell><cell>1</cell></row><row><cell cols="2">QPP QueryFeedback Post-retrieval predictor [42]</cell><cell>30 LA</cell><cell>InvPageRank</cell><cell>PageRank transposed score</cell><cell>2</cell></row><row><cell>QPP QueryScope</cell><cell>Pre-retrieval predictor [21]</cell><cell>1 LA</cell><cell>PageRank</cell><cell>PageRank score [26]</cell><cell>1</cell></row><row><cell>QPP TermCount</cell><cell>No. of terms</cell><cell>1 SP</cell><cell>SpamFusion</cell><cell>Spam likelihood [16]</cell><cell>1</cell></row><row><cell>QPP TokenCount</cell><cell>No. of tokens</cell><cell>1 URL</cell><cell>URLDigits</cell><cell>No. of digits in domain and host</cell><cell>2</cell></row><row><cell>TAX ConceptCosine</cell><cell>Cosine over concepts [37]</cell><cell>4 URL</cell><cell cols="2">URLComponents No. of host/path/query components</cell><cell>3</cell></row><row><cell>TAX ConceptCount</cell><cell>No. of concepts [34]</cell><cell>360 URL</cell><cell>URLLength</cell><cell>Length of host/path/query string</cell><cell>3</cell></row><row><cell cols="2">TAX ConceptEntropy Entropy over concepts [37]</cell><cell>120 URL</cell><cell>URLType</cell><cell>Root, subroot, path, file</cell><cell>1</cell></row><row><cell>TAX DisambSenses</cell><cell>No. of disamb. senses [32]</cell><cell>120 URL</cell><cell>URLWiki</cell><cell>Whether URL is from Wikipedia</cell><cell>1</cell></row><row><cell>GRAND TOTAL</cell><cell></cell><cell>838</cell><cell></cell><cell></cell><cell>61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top 10  selected features in the two intentaware retrieval models used in this paper.</figDesc><table><row><cell cols="2">Informational</cell><cell cols="2">Navigational</cell></row><row><cell>Feature</cell><cell cols="2">MAP Feature</cell><cell>MAP</cell></row><row><cell>1 DPH</cell><cell cols="2">0.2614 DPH</cell><cell>0.2110</cell></row><row><cell>2 URLDigits</cell><cell cols="3">0.2752 MRF (body) 0.2273</cell></row><row><cell>3 PL2 (title)</cell><cell cols="3">0.2819 BM25 (title) 0.2408</cell></row><row><cell>4 BM25F</cell><cell cols="2">0.2915 URLWiki</cell><cell>0.2517</cell></row><row><cell>5 pBiL (body)</cell><cell cols="2">0.2963 MQT</cell><cell>0.2592</cell></row><row><cell cols="3">6 pBiL (anchor) 0.2985 URLLength</cell><cell>0.2629</cell></row><row><cell>7 Edgerecip</cell><cell cols="2">0.3001 Absorbing</cell><cell>0.2666</cell></row><row><cell>8 LM (title)</cell><cell cols="3">0.3010 InvPageRank 0.2695</cell></row><row><cell cols="3">9 MQT (body) 0.3017 Inlinks</cell><cell>0.2718</cell></row><row><cell>10 MQT</cell><cell cols="3">0.3026 pBiL (body) 0.2738</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Diversification performance of IA-Select and xQuAD using informational or navigational models uniformly (UNI) or selectively (SEL) according to the WT10 topics, with the WT09 topics used for training. classifiers described in Section 5.3. As for the classification labels L, as described in Section 4.1.2, we consider both human judgements (judg) as well as the selection with best diversification performance (perf) on the training data.Tables</figDesc><table><row><cell></cell><cell>Regime</cell><cell>ERR-IA@20</cell><cell>α-nDCG@20</cell><cell>NRBP</cell><cell>MAP-IA</cell></row><row><cell></cell><cell>DPH</cell><cell>0.1952</cell><cell>0.2620</cell><cell>0.1509</cell><cell>0.0469</cell></row><row><cell></cell><cell>+IA-Select Uni(inf)</cell><cell>0.2485</cell><cell>0.3261</cell><cell>0.2011</cell><cell>0.0717</cell></row><row><cell></cell><cell>+IA-Select Uni(nav)</cell><cell>0.2866</cell><cell>0.3465</cell><cell>0.2490</cell><cell>0.0729</cell></row><row><cell></cell><cell>+IA-Select Sel(ora,judg)</cell><cell>0.2829 (13.8,-1.3)</cell><cell>0.3496 (7.2,0.9)</cell><cell>0.2428 (20.7,-2.5)</cell><cell>0.0763 (6.4,4.7)</cell></row><row><cell></cell><cell>+IA-Select Sel(log,judg)</cell><cell>0.2797 (12.6,-2.4)</cell><cell>0.3442 (5.6,-0.7)</cell><cell>0.2396 (19.1,-3.8)</cell><cell>0.0730 (1.8,0.1)</cell></row><row><cell>WT10 sub-topics</cell><cell>+IA-Select Sel(svm,judg) +IA-Select Sel(ora,perf) +IA-Select Sel(log,perf) +IA-Select Sel(svm,perf) +xQuAD Uni(inf) +xQuAD Uni(nav) +xQuAD Sel(ora,judg) +xQuAD Sel(log,judg)</cell><cell cols="4">0.2897 (16.6△,1.1) 0.3791 (52.6 ,32.3 ) 0.4228 (29.7 ,22.0 ) 0.3491 (73.6 ,40.2 ) 0.0859 (19.8 ,17.8 ) 0.3535 (8.4,2.0) 0.2485 (23.6△,-0.2) 0.0750 (4.6,2.9) 0.3117 (25.4 ,8.8 ) 0.3710 (13.8 ,7.1 ) 0.2734 (36.0 ,9.8△) 0.0773 (7.8,6.0△) 0.3044 (22.5 ,6.2) 0.3638 (11.6△,5.0) 0.2667 (32.6 ,7.1) 0.0765 (6.7,4.9) 0.2472 0.3241 0.2007 0.0715 0.2905 0.3479 0.2535 0.0754 0.2699 (9.2,-7.1) 0.3408 (5.2,-2.0) 0.2245 (11.9,-11.4) 0.0782 (9.4,3.7) 0.2708 (9.5,-6.8) 0.3346 (3.2,-3.8) 0.2333 (16.2,-8.0) 0.0743 (3.9,-1.5)</cell></row><row><cell></cell><cell>+xQuAD Sel(svm,judg)</cell><cell>0.2913 (17.8△,0.3)</cell><cell>0.3512 (8.4,0.9)</cell><cell>0.2546 (26.9△,0.4)</cell><cell>0.0793 (10.9,5.2)</cell></row><row><cell></cell><cell>+xQuAD Sel(ora,perf)</cell><cell cols="4">0.3616 (46.3 ,24.5 ) 0.4119 (27.1 ,18.4 ) 0.3294 (64.1 ,29.9 ) 0.0864 (20.8 ,14.6 )</cell></row><row><cell></cell><cell>+xQuAD Sel(log,perf)</cell><cell>0.3090 (25.0 ,6.4)</cell><cell>0.3664 (13.1△,5.3)</cell><cell>0.2726 (35.8 ,7.5)</cell><cell>0.0791 (10.6,4.9)</cell></row><row><cell></cell><cell>+xQuAD Sel(svm,perf)</cell><cell>0.3098 (25.3 ,6.6)</cell><cell>0.3680 (13.5△,5.8)</cell><cell>0.2707 (34.9 ,6.8)</cell><cell>0.0798 (11.6△,5.8)</cell></row><row><cell></cell><cell>+IA-Select Uni(inf)</cell><cell>0.2468</cell><cell>0.3135</cell><cell>0.2053</cell><cell>0.0652</cell></row><row><cell>Query suggestions</cell><cell>+IA-Select Uni(nav) +IA-Select Sel(ora,perf) +IA-Select Sel(log,perf) +IA-Select Sel(svm,perf) +xQuAD Uni(inf) +xQuAD Uni(nav) +xQuAD Sel(ora,perf) +xQuAD Sel(log,perf)</cell><cell cols="4">0.2826 0.3677 (49.0 ,30.1 ) 0.4174 (33.1 ,22.1 ) 0.3343 (62.8 ,36.2 ) 0.0774 (18.7 ,14.3 ) 0.3419 0.2454 0.0677 0.2942 (19.2 ,4.1) 0.3523 (12.4△,3.0) 0.2575 (25.4△,4.9) 0.0715 (9.7 ,5.6△) 0.2945 (19.3 ,4.2) 0.3530 (12.6 ,3.2) 0.2586 (26.0 ,5.4) 0.0722 (10.7△,6.6△) 0.2456 0.3110 0.2025 0.0600 0.2579 0.3211 0.2174 0.0597 0.3554 (44.7 ,37.8 ) 0.4078 (31.1 ,27.0 ) 0.3210 (58.5 ,47.7 ) 0.0768 (28.0 ,28.6 ) 0.2743 (11.7△,6.4) 0.3375 (8.5△,5.1) 0.2349 (16.0△,8.0) 0.0716 (19.3 ,19.9 )</cell></row><row><cell></cell><cell>+xQuAD Sel(svm,perf)</cell><cell>0.2805 (14.2 ,8.8△)</cell><cell>0.3435 (10.5△,7.0)</cell><cell>0.2418 (19.4 ,11.2)</cell><cell>0.0730 (21.7 ,22.3 )</cell></row><row><cell></cell><cell cols="2">WT10 best (uogTrB67xS) [12] 0.2981</cell><cell>0.4178</cell><cell>0.2616</cell><cell>0.0737</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Unless otherwise noted, through the rest of this paper, we refer to query 'interpretations' and 'aspects' indistinctly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Agrawal et al. [1]  use 'intents' in the sense of what we call 'interpretations'. We believe our choice is more appropriate in light of the established nomenclature in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.dmoz.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>A concrete instantiation of query aspects and their possible intents is discussed in Section 4.1.3.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For alternatives on how to estimate the likelihood of different query aspects, we refer the reader to<ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diversifying search results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query difficulty, robustness and selective application of query expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">U</forename><surname>Bordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Link-based characterization and detection of Web spam</title>
		<author>
			<persName><forename type="first">L</forename><surname>Becchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<editor>AIRWeb</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A taxonomy of Web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating the query difficulty for information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">911</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic models of ranking novel documents for faceted topic retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1287" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Million Query track 2009 overview</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavluz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fangx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Less is more: Probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2009 Web track</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2010 Web track</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICTIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple approaches to analysing query diversity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abouammoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="734" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and effective spam filtering and re-ranking for large Web datasets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2004 Web track</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relevance weighting for query independent evidence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting query performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query dependent ranking using k-nearest neighbor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Query performance prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Query type classification for Web document retrieval</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic feature selection in the Markov random field model for information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Terrier: A high performance and scalable information retrieval platform</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR, OSIR Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the Web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to select a ranking function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="114" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The static absorbing model for the Web</title>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Eng</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="186" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LETOR: A benchmark collection for research on learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="346" to="374" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The probability ranking principle in IR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Doc</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding user goals in Web search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ambiguous queries: Test collections need more sense</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting query reformulations for Web search result diversification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selectively diversifying Web search results</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diversifying for multiple information needs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR, DDR Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Explicit search result diversification through sub-queries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="87" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Identification of ambiguous queries in Web search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="229" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ambiguous requests: Implications for retrieval tests, systems and theories</title>
		<author>
			<persName><forename type="first">K</forename><surname>Spärck-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="8" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Portfolio theory of information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Query performance prediction in web search environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
