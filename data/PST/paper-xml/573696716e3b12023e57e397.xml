<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kiran</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
							<email>kiran.raja@hig.no</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Norwegian Biometrics Laboratory</orgName>
								<orgName type="institution">Gjøvik University College</orgName>
								<address>
									<postCode>2802</postCode>
									<settlement>Gjøvik</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Norwegian Biometrics Laboratory</orgName>
								<orgName type="institution">Gjøvik University College</orgName>
								<address>
									<postCode>2802</postCode>
									<settlement>Gjøvik</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Vemuri</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Norwegian Biometrics Laboratory</orgName>
								<orgName type="institution">Gjøvik University College</orgName>
								<address>
									<postCode>2802</postCode>
									<settlement>Gjøvik</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Busch</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hochschule Darmstadt -CASED</orgName>
								<address>
									<addrLine>Haardtring 100</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nokia</forename><surname>Lumia</surname></persName>
						</author>
						<title level="a" type="main">Pattern Recognition Letters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3EFF503CC204D2A33F694D1F0F97C43E</idno>
					<idno type="DOI">10.1016/j.patrec.2014.09.006</idno>
					<note type="submission">Received 25 March 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visible iris recognition Deep sparse filtering Smartphone iris Noisy iris Biometrics</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Good biometric performance of iris recognition motivates it to be used for many large scale security and access control applications. Recent works have identified visible spectrum iris recognition as a viable option with considerable performance. Key advantages of visible spectrum iris recognition include the possibility of iris imaging in on-the-move and at-a-distance scenarios as compared to fixed range imaging in near-infrared light. The unconstrained iris imaging captures the images with largely varying radius of iris and pupil. In this work, we propose a new segmentation scheme and adapt it to smartphone based visible iris images for approximating the radius of the iris to achieve robust segmentation. The proposed technique has shown the improved segmentation accuracy up to 85% with standard OSIRIS v4.1. This work also proposes a new feature extraction method based on deep sparse f iltering to obtain robust features for unconstrained iris images. To evaluate the proposed segmentation scheme and feature extraction scheme, we employ a publicly available database and also compose a new iris image database. The newly composed iris image database (VSSIRIS) is acquired using two different smartphones -iPhone 5S and Nokia Lumia 1020 under mixed illumination with unconstrained conditions in visible spectrum. The biometric performance is benchmarked based on the equal error rate (EER) obtained from various state-of-art schemes and proposed feature extraction scheme. An impressive EER of 1.62% is obtained on our VSSIRIS database and an average gain of around 2% in EER is obtained on the public database as compared to the well-known state-of-art schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Numerous factors such as affordable cost and advanced features meeting the consumers' expectation have contributed to significant rise in the usage of smartphones. In addition recent advancements have resulted in smartphones with cameras at par with the imaging capabilities of dedicated cameras which further adds to the attracting features for owning a smartphone. With all the advanced functionality provided, smartphones can also be used as a personal security device for authentication or identification. At the same time, due to high amount of personal data, the smartphone itself has to be secured. These growing concerns regarding the protection of personal information and smartphone as a security device has resulted in various active research. Physical access can nowadays be granted by nearfield-communication (NFC) protocol for a smartphone, when it is positioned in the vicinity of gates to secured areas. Various online banking applications use smartphones as the authenticating device, while access to the token is at best controlled by a personal-identification-number (PIN). A more secure mode of authentication can be achieved by employing any biometric characteristic such as fingerprint, face, iris or palmprint. Motivated by the idea of using a smartphone for authentication, some of the key players in the smartphone market like Apple and Motorola have taken a step ahead to integrate fingerprint sensors into their smartphone products for authenticating the owner of device.</p><p>Along the same lines, facial biometrics (2D/3D) was implemented using data captured by smartphones <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref>. Smartphone based reallife verification using contactless capture of fingerprint for authentication was proposed recently <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. Contactless knuckleprint for identification <ref type="bibr" target="#b3">[4]</ref> also supports the argument of using smartphone as a biometric capture device.</p><p>A more robust and reliable biometric characteristic is the iris with its unique patterns. The unique patterns present in the iris texture result in information with high entropy <ref type="bibr" target="#b6">[7]</ref>. Owing to such high entropy, large scale systems such as India's Unique ID (UID) have employed iris biometrics. It has to be noted that the color of the iris differ based on the concentration of the melanin pigment. Higher concentration of melanin causes the eyes to be dark colored while the lower concentration results in lighter irises. Due to difficulty in resolving the texture pattern in the dark iris, traditional methods have employed near-infra-red (NIR) light for iris imaging. The visible spectrum iris</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: PATREC [m5G; <ref type="bibr">October 17, 2014;</ref><ref type="bibr">15:59]</ref> recognition has been explored in the recent years <ref type="bibr" target="#b17">[18]</ref>. Well known challenges like NICE II Iris biometrics competition <ref type="bibr" target="#b2">[3]</ref> reiterate the increasing interest in visible spectrum iris recognition. The possibility of imaging iris in visible spectrum further motivates the necessity to study the feasibility of smartphone based iris recognition. As any normal sensor, smartphones can be used as a standalone capture device to obtain iris images using built-in cameras of smartphones due to high quality of imaging. This work explores smartphone as a iris biometric sensor in detail. Particularly, this work contributes in three different aspects of smartphone based visible spectrum iris recognition, namely -(i). segmentation, (ii). feature extraction and (iii). construction of a new smartphone iris database in visible spectrum.</p><p>Traditional NIR iris recognition systems work with a known range of iris and pupil radius due to the restricted distance in the capture process. However, due to unconstrained nature of image capture in visible spectrum iris imaging, it is difficult to obtain the images of iris with known range of radius for iris and pupil. In this work, we propose an improved iris segmentation scheme for unconstrained iris acquisition in visible spectrum. We also present a novel feature extraction scheme for unconstrained visible spectrum iris recognition. The feature extraction scheme presented in this work relies on deep sparse filtering of the iris data followed by the pooling of the sparse filtered features. The feature data obtained from different sparse filters provide unique and robust features to obtain high recognition/verification rates. The proposed technique is evaluated on the publicly available smartphone iris dataset from BIPLab. 1 Further, the limited availability of datasets of iris images captured with smartphones in the visible spectrum has motivated us to compose a new database (i.e. the Gjøvik Visible Spectrum Smartphone Iris Database -VSSIRIS). In this work, we present the VSSIRIS database consisting of 560 iris images. We present an extensive analysis of the newly constructed database with state-of-the-art iris recognition techniques and compare them against the results obtained with the proposed feature extraction scheme.</p><p>The rest of the paper is organized as follows: Section 2 presents the details of the newly created iris database. Section 3 presents the details of the proposed iris recognition pipeline. Section 3.1 details the proposed method to estimate iris and pupil radius and Section 3.2 describes the proposed feature extraction technique. Section 4 provides the details of experimental protocols involved in evaluating the database using the new feature extraction technique. It also provides results obtained for various techniques along with the proposed feature extraction technique and finally, Section 5 presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Smartphone iris database</head><p>Owing to the difficulty in pattern visibility of the iris in visible spectrum light, iris recognition in visible spectrum had not been popular until the recent years <ref type="bibr" target="#b17">[18]</ref>. Advancements in the imaging sensors and algorithms to address the challenges in visible spectrum iris recognition have increased the interest of the research community to investigate more on visible spectrum iris recognition. Recent works in the visible spectrum iris recognition have demonstrated good performance in terms of verification accuracy and robustness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. However, there have not been many publications or reported works on smartphone based iris recognition which has resulted in non-availability of corresponding datasets except the database contributed by BIPLab. Motivated by this fact, this work intends to present a new database for visible spectrum iris research with iris images captured using two different smartphones. The VS-SIRIS database has been acquired with two most recent and popular 1 http://biplab.unisa.it/MICHE/index.html. phones -iPhone 5S and Nokia Lumia 1020. The images were captured using the rear camera of both smartphones. The specifications of the camera and operating environment are provided in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The images were acquired under the influence of mixed illumination constituted by artificial indoor illumination and natural daylight illumination. Thus, this database provides an opportunity to explore the challenges presented by the mixed illumination for iris recognition in visible spectrum.</p><p>As compared to the database provided by BIPLab <ref type="bibr" target="#b1">[2]</ref>, this database consists of images acquired from volunteers originating mostly from north European countries. Along with the various ethnicities provided in the BIPLab database, this database provides north European iris images which become important in measuring the performance of iris recognition systems with respect to different ethnicities. Using the two different databases, one can measure the robustness of various stages in an iris recognition pipeline such as segmentation and feature extraction and thus potentially identify dependencies with respect to the color of the iris or ethnicity of the person. Another important aspect is the choice of the smartphones employed in data collection for this database. This database is constructed using the most recent smartphones -iPhone 5S and Nokia Lumia 1020 providing a chance to benchmark them for iris recognition. Nokia Lumia 1020 provides the images in a spatial resolution of 41 mega pixels. To the best of our knowledge, this is the first work to employ these phones for iris recognition. Along with the other differences from BIPLab database, the VSSIRIS database is collected in the mixed illumination as compared to the images captured in BIPLab database under two different illumination conditions. Fig. <ref type="figure">1</ref> depicts few sample images from the new iris image database. The VSSIRIS database consists of images acquired from 28 subjects in a single session which constitutes to a total of 56 unique iris instances. Each unique iris instance is captured in 5 different presentations per device in a single session under semi-cooperation from the subjects and under unconstrained conditions. A total of 560 images are present in the disclosed database. The participants in the VSSIRIS database consist of various nationalities originating from eastern, northern and southern European countries. The distribution of the iris color and gender of the subjects in the database is provided in Fig. <ref type="figure" target="#fig_0">2</ref>. The subset of data under blue/green label indicates the group bearing the colors of iris such as light blue, dark blue, light green and dark green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>In the view of limited availability of any dataset for smartphone based visible spectrum iris, we intend to make the VSSIRIS database freely available for academic and research purposes. The database can be retrieved from http://www.nislab.no/biometrics_lab/vssiris_db/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed iris recognition pipeline</head><p>This section describes the components of the proposed iris recognition scheme. Fig. <ref type="figure">3</ref> illustrates various steps involved in the proposed iris recognition pipeline. Given an eye image, it is preprocessed to segment the iris and pupil boundary. The segmented iris region is normalized to a fixed dimension of 512 × 64 pixels. The normalized iris image is further used to extract robust iris features by employing the deepsparsef ilter responses and the feature vector is generated as outlined in the upcoming sections. The generated feature vectors are used for comparison with the sparse representation classification (SRC) method <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed segmentation scheme</head><p>This section provides the details of the proposed technique for segmentation of unconstrained iris images in visible spectrum. Due to the unconstrained nature of the capture process and large unrestricted field-of-view, an image acquired from a smartphone camera does not necessarily contain just the eye region. Presence of other background in the captured scene introduces the challenge to accurately seg-ment the iris pattern. Incorrect segmentation would degrade the iris recognition accuracy. Thus, in order to avoid segmentation failure of the iris region, it is essential to locate the eye region first. Based on the success of Haar cascade based object detectors, we have employed the eye detector available in Matlab <ref type="bibr" target="#b15">[16]</ref>. Fig. <ref type="figure">4</ref>(a) illustrates the input image and Fig. <ref type="figure">4</ref>(b) depicts the detected eye region using the employed Haar cascade eye detector.</p><p>After the localization of the eye region, the first step in iris recognition involves locating the iris and pupil boundary, which is popularly termed as segmentation. The region between the iris and pupil boundary is extracted and normalized to uniform size of fixed dimension. The location of the pupil and iris boundary still remains a challenge in unconstrained iris recognition, specifically when the samples are captured in the visible spectrum. One of the advantages of constrained iris acquisition or contact based iris acquisition comes with the known range of the iris diameter. This apriori information helps in achieving the segmentation successfully.</p><p>Most of the segmentation schemes work in the known range of the iris and pupil diameter. However, in an unconstrained condition, the range of the iris and pupil diameter varies largely. Some of the factors influencing them include the intensity of the illumination (i.e. incoming light on the eye), the optical resolution of the imaging device, the focal length and the distance of the camera from the capture subject. In order to overcome these challenges, the estimation of the iris diameter is obtained by exploring the supplementary information obtained from saliency maps of the localized eye image. Saliency maps provide sharp responses along the edges in any particular image. One of the ways to estimate a saliency map is to measure the variation in contrast. Since the eye region consists of edge information constituted by the iris-sclera boundary one can observe high contrast response corresponding to exactly iris region. The intensity change in the sclera and iris region further gives an indication for the approximate location of the iris along with the high intensity edge response that can be used to approximate the boundary of the iris region and approximate diameter. Based on these arguments, we have employed the saliency map estimation method proposed by <ref type="bibr" target="#b4">[5]</ref>. It can be observed from Fig. The change in contrast appears in many regions including the boundary of iris-sclera and also sclera-skin which occasionally results in a falsely estimated iris region. In order to overcome such challenges, our technique utilizes anisotropic diffusion to minimize the low contrast changes and thereby retaining only high contrast regions along with the boundary. Based on the reported accuracy of various methods for anisotropic diffusion, we have employed rotation invariant anisotropic diffusion to retain the boundaries and edges <ref type="bibr" target="#b27">[28]</ref>. Fig. <ref type="figure">4</ref>(d) shows the diffused saliency map which has retained the strong edge transitions while smoothing the local intensity changes.</p><p>To mitigate any false detection, the estimated iris region is correlated to a set of reference iris images from an independent set consisting of 20 cropped iris images obtained from UBIRIS v2 dataset <ref type="bibr" target="#b17">[18]</ref>. The reference images from this dataset serve as prototypes for various iris diameters. Each estimated region is correlated to the prototype set of images to obtain the correlation factor. The radius of the reference iris image having the highest correlation is used to provide the approximated iris radius. Fig. <ref type="figure">4</ref>(e) presents the located circular region in the image. The diameter of the approximated iris region is provided to the OSIRIS v4.1 <ref type="bibr" target="#b25">[26]</ref> segmentation technique for further processing and locating the exact iris and pupil boundary. OSIRIS v4.1 internally performs anisotropic diffusion on high resolution images to estimate the iris-pupil boundary. The diffused image is used to detect the coarse boundaries by employing Viterbi search algorithm <ref type="bibr" target="#b7">[8]</ref>.</p><p>The steps needed for approximating the iris radius are outlined in Algorithm ??. The reference image set is indicated by Ref and the i number of detected circles are indicated by C iD .</p><p>With the help of the proposed preprocessing method, the necessity for apriori knowledge of an iris diameter has been resolved. Thus, this work contributes to automate the standard segmentation procedure that is provided with OSIRIS v4.1 for unconstrained iris acquisitions in the visible spectrum. As compared to estimating the iris radius manually for every single image in unconstrained scenarios under varying resolutions and focus of different cameras, our proposed technique to approximate the radius has recorded the segmentation accuracy as given in Table <ref type="table" target="#tab_1">2</ref>. Note that the percentage is indicating the number of cases for which the automated segmentation operated with no error. The accuracy of the proposed improvement to segmentation technique is computed by manual inspection. The segmented iris is further normalized using the Daugman's rubber sheet model <ref type="bibr" target="#b5">[6]</ref>. The normalized iris is then processed to extract the features for verification.</p><p>The reader is advised to refer Section 4.1 to obtain the impact of the proposed segmentation scheme on iris recognition accuracy. Further, incorrect segmentation of the iris has been corrected manually to consider all the samples for the subsequent evaluation of the overall biometric system performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed feature extraction scheme</head><p>This section provides the details of the proposed feature extraction technique for iris images using deep sparse filtering. A brief summary of sparse filtering is outlined and the new feature extraction scheme based on deep sparse filtering for robust iris recognition is provided in upcoming sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Deep sparse filtering</head><p>Sparse filtering is a new and simple unsupervised algorithm to learn the number of specified features which does not explicitly attempt to model the distribution of data <ref type="bibr" target="#b16">[17]</ref>. Sparse filtering optimizes a simple cost function of sparsity using l 2normalized features. The key aspect in using sparse filtering is that unlike other algorithms in machine learning, sparse filtering does not necessarily include hyperparameter tuning and typically converges to optimal solution easily. The only parameter required in learning sparse filters is the number of features, as the sparse filters are learnt by optimizing sparsity in feature distribution.</p><p>Layers form the building blocks in learning deep sparse filters. In order to achieve deep sparse filtering, one has to employ more than one layer for learning. In this work, we learn the deep sparse filter consisting of two layers such that layer 1 is trained using 200,000 random patches of size 16 × 16 pixels from 4212 natural images <ref type="bibr" target="#b8">[9]</ref>. The patches of natural images are first normalized to obtain the absolute values of feature data. In learning techniques involving multi-layered learning approaches, output from one layer is provided as input to the subsequent layer. The number of layers can be varied to n number of layers for various tasks. The choice of two layered deep sparse filtering through canonical greedy layer-wise approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> is to learn more robust features with low computations <ref type="bibr" target="#b16">[17]</ref>. The sparse filtered features obtained as output from the layer 1 are normalized and provided to layer 2 using the feedforward network employing the soft-absolute function provided in Eq. ( <ref type="formula" target="#formula_0">1</ref>). We employ the same soft-absolute function in both layers.</p><formula xml:id="formula_0">f (i) j = + W T j X (i) 2 where = 10 -8<label>(1)</label></formula><p>where f (i) j represents the jth feature value corresponding to rows in the ith column. X represents the input vector and W represents the weights in the training network.</p><p>Fig. <ref type="figure">5</ref> presents the schematic processing of the sparse filter learning in our work. As described in Fig. <ref type="figure">5</ref>, the image patches are preprocessed and used to train the layer 1 and subsequently layer 2. In our work, the sparse filter is trained at layer 1 with 256 filters of dimension 16 × 16 features and at layer 2 with 256 sparse filters of 16 × 16 features. Fig. <ref type="figure">6</ref> illustrates some sample sparse filter features obtained in layer 2. We use the sparse filter features obtained from layer 2 to extract features from the iris images as explained in the next section.</p><p>Please cite this article as: K.B. <ref type="bibr">Raja</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Sparse feature vector for iris</head><p>As the iris pattern is known to consist of an unique texture pattern that is randomly formed, we propose to use sparse filters to extract robust and unique features. Robust feature extraction from the iris texture is important for the performance of an iris biometric system. Given the iris image, the normalization technique unwraps the circular iris region into a rectangular image using Daugman's rubber sheet model <ref type="bibr" target="#b5">[6]</ref>. Predominantly used dimensions of normalized iris image vary from 2048 × 1024 pixels to 512 × 64 pixels. With the larger dimension it becomes essential to extract more meaningful, fewer and unique features. In this work, we have employed a normalized iris dimension of 512 × 64 pixels. Under NIR imaging, iris features are generally obtained using 1D Gabor wavelets or 2D Gabor wavelets based features for successful recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref>. The visibility of the clear texture under NIR illumination contributes to obtain reliable features for recognition.</p><p>In our case of visible spectrum iris recognition, due to the unconstrained nature of the imaging process, the captured iris pattern may be influenced by a number of factors such as imaging device, the ability to resolve the texture based on the color of iris among and many other environmental factors. Thus, it becomes essential to obtain robust features from the captured iris images to perform reliable recognition. Considering the unconstrained nature of iris data along with the high dimension of data, sparse filtering can be successfully used to obtain more meaningful features.</p><p>In our work, we have employed 256 filters from layer 2. Each of the filters have a dimension of 16 × 16 pixels. When the iris image is convolved with the 256 filters of layer 2, a total of 256 response images are obtained. If the iris image is represented by I and the sparse filter is represented by S, the sparse filter response can be denoted as :</p><formula xml:id="formula_1">R = I * S (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where * represents the convolution operation. Considering 256 filters in our approach, we can adjust Eq. ( <ref type="formula" target="#formula_1">2</ref>) as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T(x, y)</head><formula xml:id="formula_3">= 1, if R(x, y) &gt; 0 0, otherwise<label>(4)</label></formula><p>Each of the binarized feature image T(x, y) are pooled in group of 8 images (i.e. the binarized feature matrices) to transform into a new feature domain. Considering a single pixel at a particular position (x, y), a set of binary values from images in the pool consisting of 8 matrices can be used to construct a binary code of 8 bits. Formulating it mathematically, for a pixel at position (x, y) for a set of 8 binary images, the pooled pixel feature encoded as gray value is presented as P(x, y): P(x, y) = 8 j=1 T j (x, y) × (2 (j-1) ); of features and thus cause the overhead for computation at the second level. In order to avoid this, our work proposes to use the histogram of each of the 32 images to form a feature vector. The histogram for a single image is given by :</p><formula xml:id="formula_4">H l = 255 m=0 (P l ) m forl = 1, 2, . . . 32<label>(6)</label></formula><p>The final feature vector denoted by F is formed by concatenating the histograms of all the 32 gray level response images. The final feature vector can be represented as F given by:</p><formula xml:id="formula_5">F = [H 1 , H 2 , . . . H 32 ]<label>( 7 )</label></formula><p>Thus each of the iris images is represented using 32 × 256 features obtained using the sparse filtering and histogram vector of pooled binary images. Further in our work, we normalize the histogram in order to obtain the final feature vector. Fig. <ref type="figure">8</ref> presents the schematic of the proposed iris feature extraction using deep sparse filtering. The final feature vector is used in conjunction with the sparse representation classification (SRC) <ref type="bibr" target="#b20">[21]</ref> to obtain the comparison scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature classification</head><p>The obtained features are further classified for verification. In this work, we have used sparse representation classification (SRC) to improve the recognition accuracy inspired by the success of SRC in other biometric applications <ref type="bibr" target="#b20">[21]</ref>. The distinct features obtained using our proposed scheme are classified by projecting them on l 1 normminimization via SPGL 1 solver based on a spectral gradient projection <ref type="bibr" target="#b20">[21]</ref>. The residual errors then correspond to the comparison scores (genuine and imposter scores) that could be used to compute the false match rate (FMR) and false non-match rate (FNMR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and discussion</head><p>This work has considered two publicly available smartphone iris database provided by BIPLab <ref type="bibr" target="#b1">[2]</ref> and VSSIRIS database (released with the publication of this paper) for an extensive evaluationof the proposed improvement to segmentation and to validate the efficiency of the newly proposed feature extraction scheme. All results on both databases are reported using the equal error rate (EER), which is a metric defined as a point for which the false match rate (FMR) equals the false non-match rate (FNMR) <ref type="bibr" target="#b10">[11]</ref>.</p><p>Seven well known feature extraction techniques predominantly used for iris recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> have been considered in this work. We compare the results of our proposed technique against these feature extraction techniques. Most of the mentioned techniques <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> are evaluated on both databases using the implementation obtained from USIT -University of Salzburg Iris Toolkit v1.0 <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance of proposed segmentation technique</head><p>In order to measure the impact of the proposed segmentation technique on the verification performance, we evaluate the verification accuracy in terms of equal error rate (EER) on both BIPLab database <ref type="bibr" target="#b1">[2]</ref> and VSSIRIS database. The EER reported in Table <ref type="table" target="#tab_3">3</ref> is obtained  using the proposed feature extraction technique explained in Section 3. Table <ref type="table" target="#tab_3">3</ref> provides the EER of the segmented iris images using standard OSIRIS v4.1 and proposed preprocessing to OSIRIS v4.1 for the unconstrained iris images in both the databases employed in this work. Fig. <ref type="figure" target="#fig_5">9</ref> provides the plots of detection error trade-off curves (DET) for images obtained from different phones. It can be observed that the proposed improvement to the segmentation scheme has reduced the EER significantly. For the sake of simplicity, the DETs of only four datasets have been presented in Fig. <ref type="figure" target="#fig_5">9</ref>. The reader is advised to refer the appendix for the DET plots of the other BIPLab datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on BIPLab database</head><p>The dataset provided by BIPLab <ref type="bibr" target="#b1">[2]</ref> consists of images captured using iPhone 5 and Samsung Galaxy S4. The images are captured using both frontal and rear camera in indoor and outdoor conditions. A subset consisting of 50 unique iris has been employed in this work to maintain the same number of iris in all different acquisition conditions and provide unbiased comparison.</p><p>Each unique iris instance has 4 samples and thus, this work adopts the leave-one-out approach by dividing the data in 1 : 3 ratio with 1 sample as reference and 3 as probe samples. The minimum score from three comparisons is used as the comparison score for the pair of probe and reference. The partition is continuously swapped to make each iris sample reference and probe at different instances. Further, the reference and probe partition is repeatedly changed m times with m = 10 under the leave-one-out cross-validation strategy. The final results are obtained by averaging the results obtained from all iterations of the leave-one-out approach. The results thus represent the mean value of all the 10 different trials taking care of statistical variations.</p><p>Table <ref type="table">4</ref> provides the EER scores obtained for the images in the database provided by BIPLab <ref type="bibr" target="#b1">[2]</ref>. The samples are identified as X_Y_Z where X refers to phone (I for iPhone 5 and S for Samsung Galaxy S4), Y symbolizes the illumination (I for indoor and O for outdoor) and Z denotes the camera (F for frontal and R for rear camera). For instance, I_O_F refers to iPhone-Outdoor-Frontal camera. It can be observed from Table <ref type="table">4</ref> that the proposed feature extraction technique performs well on the publicly available database validating the robustness of proposed method. On average, the obtained gain in performance is around 2% on all different cameras and illuminations. The obtained EER is presented in the plot provided by Figs. 10 and 11 for iPhone and Samsung phones respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on VSSIRIS database</head><p>We experimentally validate the proposed feature extraction technique on the newly constructed iris database VSSIRIS to measure its robustness. As described earlier, the database consists of 56 unique iris images accompanied with 5 samples for each unique iris Please cite this article as: K.B. <ref type="bibr">Raja</ref>  a Scores obtained using USIT v1.0 <ref type="bibr" target="#b23">[24]</ref>.  image. The database consists of images captured from two different smartphones. To provide the benchmark results, we evaluate the new database with all the popular feature extraction techniques in state-of-art schemes. We also evaluate the database on the proposed technique. We apply the leave-one-out cross-validation strategy with 10 iterations (m = 10). Due to availability of five samples for every iris instance, we divide the database in the ratio of 1 : 4 representing the ref erence : probe. We repeatedly swap the reference and probe within the samples and average the final results as mentioned in the previous section.</p><p>Table <ref type="table" target="#tab_5">5</ref> presents the results of all schemes on the newly constructed VSSIRIS database. It can be observed from the table that the proposed feature extraction technique has outperformed all the state-of-art techniques by providing the best EER of 1.62% for iPhone 5S images and 1.78% for Nokia 1020 images. Fig. <ref type="figure" target="#fig_11">12</ref> presents the plots of the obtained EER for various feature extraction schemes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work has investigated the use of smartphones as a biometric sensor for iris recognition. As approximating the radius of iris and pupil in the unconstrained iris images is a challenge for any of the segmentation methods in visible spectrum, this work has proposed a new algorithm for approximating the iris and pupil radius. The proposed radius approximation technique has been tested on the publicly available database <ref type="bibr" target="#b1">[2]</ref> and our newly created VSSIRIS database. The approximated iris radius is used as a priori information for standard OSIRIS segmentation scheme <ref type="bibr" target="#b25">[26]</ref>. The proposed method has reported an accuracy of around 75% in average with the best performance of 85% accuracy for iPhone 5S images.</p><p>Another important contribution of this work is in proposing a new feature extraction method based on deep sparse f iltering to obtain robust features. The proposed feature extraction method is extensively evaluated on our VSSIRIS database with a reported EER of 1.62% for iPhone and EER of 1.78% for Nokia phone. In order to prove the robustness of the proposed feature extraction technique, we have evaluated the proposed technique on the publicly available BIPLab <ref type="bibr" target="#b1">[2]</ref> database. The results on the available public database indicate the superior performance of the proposed method. An average gain of around 2% on the EER of all different smartphone images has been achieved over the state-of-art techniques.</p><p>In the view of limited availability of open iris databases acquired using the smartphones in visible spectrum, this work contributes a new database for the iris recognition research on smartphone in the visible spectrum. The new iris database consists of 56 unique iris with 5 samples for each eye. The samples are acquired in a real life scenario with mixed illumination under unconstrained conditions and thereby providing an opportunity to explore the challenges presented by mixed illumination, out-of-focus images and reflections.</p><p>Please cite this article as: K.B. <ref type="bibr">Raja</ref>    In summary, this work has contributed in three important stages of smartphone based visible spectrum iris recognition. This work supports the reproducible research by disseminating the entire database for non-profit research work. Further, the feature extraction technique shall be distributed as an executable for interested researchers to compare and benchmark any upcoming feature extraction algorithms on available databases that includes BIPLab database <ref type="bibr" target="#b1">[2]</ref> and our newly constructed VSSIRIS database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Color and gender distribution of samples in the database. (For interpretation of the reference to color in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Proposed iris recognition scheme.</figDesc><graphic coords="3,256.52,624.92,63.43,64.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Schematic of sparse filter feature learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>R</head><label></label><figDesc>Fig. 7 illustrates the sample response to eight different sparse filters. Fig. 7(a) provides the normalized iris image and Fig. 7(b) presents eight different responses. Since processing 256 responses represented by R i=1:256 at the feature level becomes tedious, this work proposes a simple way of binning the data to reduce the response images. Each of the response image is thresholded and binarized based on the pixel value. For a pixel at position (x, y) in the response image, the thresholded value is represented by T(x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 (Fig. 7 .Fig. 8 .</head><label>778</label><figDesc>Fig. 7(c) presents the pooled image formed by the 8 responses of sparse filtering. Similarly, pooling of 256 response images in groups of 8 images result in 32 gray level response images and are represented by P l=1:32 . The obtained 32 gray level images result in a huge number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance with the native OSIRIS segmentation and the improvement based on the proposed segmentation scheme obtained using the proposed feature extraction technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>DET curves for iPhone-Outdoor-Rear camera</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. DET curves obtained for various schemes applied on the iPhone images from BIPLab database [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Match Rate (%) Masek and Kovesi (2003) Ma et al. (2003) Ko et al. (2007) Rathgeb and Uhl (2010) Rathgeb and Uhl (2011) Daugman (2004) Raghavendra et al. (2013b) Proposed Scheme (c) DET curves for -Outdoor-Frontal camera Match Rate (%) Masek and Kovesi (2003) Ma et al. (2003) Ko et al. (2007) Rathgeb and Uhl (2010) Rathgeb and Uhl (2011) Daugman (2004) Raghavendra et al. (2013b) Proposed Scheme (d) DET curves for Outdoor-Rear camera</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. DET curves obtained for various schemes applied on the Samsung images from BIPLab database [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>DET curves for iPhone 5S images from the VSSIRIS database DET curves for Nokia images from the VSSIRIS database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. DET curves obtained for various schemes applied on the VSSIRIS database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Camera parameters of smartphones.</figDesc><table><row><cell>Parameters</cell><cell>iPhone 5S</cell><cell>Nokia Lumia 1020</cell></row><row><cell>Resolution</cell><cell>3264 × 2448 Pixels</cell><cell>7712 × 5360 pixels</cell></row><row><cell>Color representation</cell><cell>sRGB</cell><cell>sRGB</cell></row><row><cell>Bit depth</cell><cell>24</cell><cell>24</cell></row><row><cell>F-Stop</cell><cell>f/2.2</cell><cell>f/2.2</cell></row><row><cell>White balance</cell><cell>Auto</cell><cell>Auto</cell></row><row><cell>Flash</cell><cell>No flash</cell><cell>No flash</cell></row><row><cell>Metering mode</cell><cell>Average</cell><cell>Average</cell></row><row><cell>File format</cell><cell>JPEG</cell><cell>JPEG</cell></row><row><cell>Focus</cell><cell>Auto</cell><cell>Auto</cell></row><row><cell>Illumination</cell><cell>Mixed illumination</cell><cell>Mixed illumination</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Segmentation accuracy with the proposed radius approximation scheme.</figDesc><table><row><cell>Phone (camera)</cell><cell>Accuracy(%)</cell></row><row><cell cols="2">Database : BIPLab. University of Salerno)</cell></row><row><cell>iPhone Outdoor Rear</cell><cell>81</cell></row><row><cell>iPhone Outdoor Frontal</cell><cell>64</cell></row><row><cell>iPhone Indoor Rear</cell><cell>64.5</cell></row><row><cell>iPhone Indoor Frontal</cell><cell>76.5</cell></row><row><cell>Samsung Outdoor Rear</cell><cell>74.5</cell></row><row><cell>Samsung Outdoor Frontal</cell><cell>62</cell></row><row><cell>Samsung Indoor Rear</cell><cell>65</cell></row><row><cell>Samsung Indoor Frontal</cell><cell>77</cell></row><row><cell>Database : VSSIRIS</cell><cell></cell></row><row><cell>iPhone 5S</cell><cell>85</cell></row><row><cell>Nokia 1020</cell><cell>78.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>et al., Smartphone based visible iris recognition using deep sparse filtering, Pattern Recognition Letters (2014), http://dx.doi.org/10.1016/j.patrec.2014.09.006</figDesc><table><row><cell>JID: PATREC</cell><cell></cell><cell cols="4">ARTICLE IN PRESS</cell><cell cols="2">[m5G;October 17, 2014;15:59]</cell></row><row><cell>Patches from image</cell><cell>Normalize using l 2 norm patches</cell><cell>Sparse filter features using soŌ-absolute funcƟon</cell><cell>OpƟmize sparsity using l 1 penalty</cell><cell>Layer 1 Sparse Filter Features</cell><cell>Normalize sparse features</cell><cell>Sparse filter features using soŌ-absolute funcƟon</cell><cell>Layer 2 Sparse Filter Features</cell></row><row><cell cols="2">Preprocessing</cell><cell cols="2">Layer 1 Sparse Features</cell><cell></cell><cell></cell><cell>Layer 2 Sparse Features</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>EER (%) with the standard OSIRIS system and the improvement due to the proposed iris radius estimation. The performance is measured using the proposed feature extraction technique.</figDesc><table><row><cell>Phone (camera)</cell><cell></cell><cell>EER (%)</cell></row><row><cell></cell><cell>OSIRIS</cell><cell>Proposed improvement</cell></row><row><cell cols="2">Database : BIPLab. University of Salerno</cell><cell></cell></row><row><cell>iPhone Outdoor Rear</cell><cell>14.62</cell><cell>10.40</cell></row><row><cell>iPhone Outdoor Frontal</cell><cell>11.14</cell><cell>6.78</cell></row><row><cell>iPhone Indoor Rear</cell><cell>8.35</cell><cell>8.35</cell></row><row><cell>iPhone Indoor Frontal</cell><cell>3.74</cell><cell>4.16</cell></row><row><cell>Samsung Outdoor Rear</cell><cell>18.86</cell><cell>10.52</cell></row><row><cell>Samsung Outdoor Frontal</cell><cell>12.45</cell><cell>10.26</cell></row><row><cell>Samsung Indoor Rear</cell><cell>25.06</cell><cell>6.16</cell></row><row><cell>Samsung Indoor Frontal</cell><cell>8.40</cell><cell>4.47</cell></row><row><cell>Database : VSSIRIS</cell><cell></cell><cell></cell></row><row><cell>iPhone 5S</cell><cell>8.17</cell><cell>1.91</cell></row><row><cell>Nokia 1020</cell><cell>7.98</cell><cell>2.01</cell></row></table><note><p>Please cite this article as: K.B. Raja et al., Smartphone based visible iris recognition using deep sparse filtering, Pattern Recognition Letters (2014), http://dx.doi.org/10.1016/j.patrec.2014.09.006</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>et al., Smartphone based visible iris recognition using deep sparse filtering, Pattern Recognition Letters (2014), http://dx.doi.org/10.1016/j.patrec.2014.09.006</figDesc><table><row><cell>JID: PATREC</cell><cell cols="6">ARTICLE IN PRESS</cell><cell></cell><cell>[m5G;October 17, 2014;15:59]</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">EER (%) obtained for various schemes on BIPLab database [2].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Schemes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">EER (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Outdoor</cell><cell></cell><cell></cell><cell cols="2">Indoor</cell><cell></cell></row><row><cell></cell><cell>I_O_R</cell><cell>I_O_F</cell><cell>S_O_R</cell><cell>S_O_F</cell><cell>I_I_R</cell><cell>I_I_F</cell><cell>S_I_R</cell><cell>S_I_F</cell></row><row><cell>[6]</cell><cell>10.41</cell><cell>6.78</cell><cell>10.52</cell><cell>12.45</cell><cell>8.35</cell><cell>3.74</cell><cell>6.16</cell><cell>8.399</cell></row><row><cell>[15] a</cell><cell>24.01</cell><cell>21.86</cell><cell>20.83</cell><cell>21.22</cell><cell>13.90</cell><cell>17.01</cell><cell>17.26</cell><cell>18.83</cell></row><row><cell>[14] a</cell><cell>29.98</cell><cell>22.01</cell><cell>20.63</cell><cell>20.60</cell><cell>12.84</cell><cell>17.01</cell><cell>18.68</cell><cell>17.10</cell></row><row><cell>[13] a</cell><cell>21.78</cell><cell>18.11</cell><cell>18.05</cell><cell>17.70</cell><cell>11.29</cell><cell>14.58</cell><cell>14.68</cell><cell>14.06</cell></row><row><cell>[22] a</cell><cell>28.12</cell><cell>27.43</cell><cell>24.39</cell><cell>27.30</cell><cell>20.83</cell><cell>21.05</cell><cell>25.68</cell><cell>21.13</cell></row><row><cell>[22] a</cell><cell>30.39</cell><cell>26.76</cell><cell>26.04</cell><cell>31.09</cell><cell>26.31</cell><cell>26.84</cell><cell>33.17</cell><cell>30.43</cell></row><row><cell>[21]</cell><cell>8.57</cell><cell>7.82</cell><cell>6.29</cell><cell>8.11</cell><cell>0.48</cell><cell>2.07</cell><cell>4.18</cell><cell>4.23</cell></row><row><cell>Proposed scheme</cell><cell>6.25</cell><cell>4.18</cell><cell>2.06</cell><cell>6.27</cell><cell>6.25</cell><cell>0.02</cell><cell>3.96</cell><cell>2.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Biometric performance obtained for various schemes on the VSSIRIS database.</figDesc><table><row><cell>Schemes</cell><cell cols="2">EER (%)</cell></row><row><cell></cell><cell>iPhone 5S</cell><cell>Nokia 1020</cell></row><row><cell>[6]</cell><cell>3.62</cell><cell>3.52</cell></row><row><cell>[15] a</cell><cell>5.73</cell><cell>11.66</cell></row><row><cell>[14] a</cell><cell>7.89</cell><cell>13.88</cell></row><row><cell>[13] a</cell><cell>7.88</cell><cell>11.79</cell></row><row><cell>[22] a</cell><cell>16.26</cell><cell>24.16</cell></row><row><cell>[23] a</cell><cell>19.45</cell><cell>27.54</cell></row><row><cell>[21]</cell><cell>8.31</cell><cell>10.59</cell></row><row><cell>Proposed scheme</cell><cell>1.62</cell><cell>1.78</cell></row><row><cell cols="3">a Scores obtained using USIT v1.0 [24].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>et al., Smartphone based visible iris recognition using deep sparse filtering, Pattern Recognition Letters (2014), http://dx.doi.org/10.1016/j.patrec.2014.09.006</figDesc><table><row><cell>JID: PATREC</cell><cell>ARTICLE IN PRESS</cell><cell>[m5G;October 17, 2014;15:59]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to the anonymous reviewers for their constructive suggestions to improve the quality of the paper. Also we would like to thank the numerous volunteers that contributed to make the data collection at our campus possible.</p><p>The authors also wish to express thanks to Morpho (Safran Group) for supporting this work, and in particular to Morpho Research &amp; Technology team for the fruitful technical and scientific exchanges related to this particular work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">University of Salerno</title>
		<author>
			<persName><surname>Biplab</surname></persName>
		</author>
		<ptr target="http://biplab.unisa.it/MICHE/database/" />
	</analytic>
	<monogr>
		<title level="m">Mobile Iris CHallenge Evaluation (MICHE I and II)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The results of the nice. ii. Iris biometrics competition. Pattern Recognit</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167865511004144" />
	</analytic>
	<monogr>
		<title level="m">Noisy Iris Challenge Evaluation {II} -Recognition of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="965" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contactless finger knuckle identification using smartphones</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on BIOSIG 2012</title>
		<meeting>IEEE International Conference on BIOSIG 2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salient Object Detection and Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<idno>TPAMI-2011-10-0753</idno>
		<ptr target="http://mmcheng.net/salobj" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How iris recognition works</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probing the uniqueness and randomness of iriscodes: results from 200 billion iris pair comparisons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1927" to="1935" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Independent component filters of natural images compared with simple cells in primary visual cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Hateren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Schaaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. Lond. B Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ISO/IEC TC JTC1 SC37 Biometrics, Information Technology -Biometric Performance Testing and Reporting -Part 1: Principles and Framework</title>
		<author>
			<persName><surname>Iso/Iec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">19795-1:2006</date>
		</imprint>
	</monogr>
	<note>International Organization for Standardization and International Electrotechnical Committee</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust iris recognition using light field camera</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on The Colour and Visual Computing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel and efficient feature extraction method for iris recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="399" to="401" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Personal identification based on iris texture analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1519" to="1533" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Matlab source code for a biometric identification system based on iris patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Masek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kovesi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>The School of Computer Science and Software Engineering, The University of Western Australia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MATLAB, version 8.2.0 (R2013b) -Cascade Object Detector</title>
		<author>
			<persName><surname>Matlab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>The Mathworks Inc</publisher>
			<pubPlace>Natick, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Sparse filtering, in: NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1125" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The ubiris. v2: A database of visible wavelength iris images captured on-the-move and at-a-distance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1529" to="1535" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling-robust fingerprint verification with smartphone camera in real-life scenarios</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>IEEE Sixth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d face reconstruction and multimodal person identification from video captured using smartphone camera</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pflug</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technologies for Homeland Security</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="552" to="557" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining iris and periocular recognition using light field camera</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nd IAPR Asian Conference on Pattern Recognition (ACPR2013</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Secure iris recognition based on local intensity variations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-based biometric key generation for iris</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="389" to="397" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iris recognition: from segmentation to template security</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in information security</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fingerphoto recognition with smartphone cameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIOSIG-Proceedings of the International Conference of the</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A biometric reference system for iris</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dorizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia-Salicetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Othman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face authentication from cell phone camera images with illumination and temporal variations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qidwai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. C Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A scheme for coherence-enhancing diffusion filtering with optimized rotation invariance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2011.11.024</idno>
		<ptr target="http://dx.doi.org/http://dx.doi.org/10.1016/j.patrec.2011.11.024" />
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="2002">2002. 2014</date>
		</imprint>
	</monogr>
	<note>Pattern Recognition Letters</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
