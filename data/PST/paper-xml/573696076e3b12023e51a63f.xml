<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Limitations of Deep Learning in Adversarial Settings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
							<email>mcdaniel@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
							<email>jha@cs.wisc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Sciences Department</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">Carnegie Mellon University § United States</orgName>
								<address>
									<settlement>Adelphi</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
							<email>ananthram.swami.civ@mail.mil</email>
						</author>
						<title level="a" type="main">The Limitations of Deep Learning in Adversarial Settings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Accepted to the 1st IEEE European Symposium on Security &amp; Privacy, IEEE 2016. Saarbrucken, Germany.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Large neural networks, recast as deep neural networks (DNNs) in the mid 2000s, altered the machine learning landscape by outperforming other approaches in many tasks. This was made possible by advances that reduced the computational complexity of training <ref type="bibr" target="#b19">[20]</ref>. For instance, Deep learning (DL) can now take advantage of large datasets to achieve accuracy rates higher than previous classification techniques. In short, DL is transforming computational processing of complex data in many domains such as vision <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[37]</ref>, speech recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b31">[33]</ref>, language processing <ref type="bibr" target="#b12">[13]</ref>, financial fraud detection <ref type="bibr" target="#b22">[23]</ref>, and recently malware detection <ref type="bibr" target="#b13">[14]</ref>.</p><p>This increasing use of deep learning is creating incentives for adversaries to manipulate DNNs to force misclassification of inputs. For instance, applications of deep learning use image classifiers to distinguish inappropriate from appropriate content, and text and image classifiers to differentiate between SPAM and non-SPAM email. An adversary able to craft misclassified inputs would profit from evading detection-indeed such attacks occur today on non-DL classification systems <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In the physical domain, consider a driverless car system that uses DL to identify traffic signs <ref type="bibr" target="#b11">[12]</ref>. If slightly altering "STOP" signs causes DNNs to misclassify them, the car would not stop, thus subverting the car's safety. An adversarial sample is an input crafted to cause deep learning algorithms to misclassify. Note that adversarial samples are created at test time, after the DNN has been trained by the defender, and do not require any alteration of the training process. Figure <ref type="figure" target="#fig_0">1</ref> shows examples of adversarial samples taken from our validation experiments. It shows how an image originally showing a digit can be altered to force a DNN to classify it as another digit. Adversarial samples are created from benign samples by adding distortions exploiting the imperfect generalization learned by DNNs from finite training sets <ref type="bibr" target="#b3">[4]</ref>, and the underlying linearity of most components used to build DNNs <ref type="bibr" target="#b17">[18]</ref>. Previous work explored DNN properties that could be used to craft adversarial samples <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. Simply put, these techniques exploit gradients computed by network training algorithms: instead of using these gradients to update network parameters as would normally be done, gradients are used to update the original input itself, which is subsequently misclassified by DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1511.07528v1 [cs.CR] 24 Nov 2015</head><p>In this paper, we describe a new class of algorithms for adversarial sample creation against any feedforward (acyclic) DNN <ref type="bibr" target="#b29">[31]</ref> and formalize the threat model space of deep learning with respect to the integrity of output classification. Unlike previous approaches mentioned above, we compute a direct mapping from the input to the output to achieve an explicit adversarial goal. Furthermore, our approach only alters a (frequently small) fraction of input features leading to reduced perturbation of the source inputs. It also enables adversaries to apply heuristic searches to find perturbations leading to input targeted misclassifications (perturbing inputs to result in a specific output classification).</p><p>More formally, a DNN models a multidimensional function F : X → Y where X is a (raw) feature vector and Y is an output vector. We construct an adversarial sample X * from a benign sample X by adding a perturbation vector δ X solving the following optimization problem:</p><formula xml:id="formula_0">arg min δ X δ X s.t. F X + δ X = Y *<label>(1)</label></formula><p>where X * = X + δ X is the adversarial sample, Y * is the desired adversarial output, and • is a norm appropriate to compare the DNN inputs. Solving this problem is non-trivial, as properties of DNNs make it non-linear and non-convex <ref type="bibr" target="#b24">[25]</ref>. Thus, we craft adversarial samples by constructing a mapping from input perturbations to output variations. Note that all research mentioned above took the opposite approach: it used output variations to find corresponding input perturbations. Our understanding of how changes made to inputs affect a DNN's output stems from the evaluation of the forward derivative: a matrix we introduce and define as the Jacobian of the function learned by the DNN. The forward derivative is used to construct adversarial saliency maps indicating input features to include in perturbation δ X in order to produce adversarial samples inducing a certain behavior from the DNN. Forward derivatives approaches are much more powerful than gradient descent techniques used in prior systems. They are applicable to both supervised and unsupervised architectures and allow adversaries to generate information for broad families of adversarial samples. Indeed, adversarial saliency maps are versatile tools based on the forward derivative and designed with adversarial goals in mind, giving greater control to adversaries with respect to the choice of perturbations. In our work, we consider the following questions to formalize the security of DL in adversarial settings: <ref type="bibr" target="#b0">(1)</ref> "What is the minimal knowledge required to perform attacks against DL?", (2) "How can vulnerable or resistant samples be identified?", and (3) "How are adversarial samples perceived by humans?".</p><p>The adversarial sample generation algorithms are validated using the widely studied LeNet architecture (a pioneering DNN used for hand-written digit recognition <ref type="bibr" target="#b25">[26]</ref>) and MNIST dataset <ref type="bibr" target="#b26">[27]</ref>. We show that any input sample can be perturbed to be misclassified as any target class with 97.10% success while perturbing on average 4.02% of the input features per sample. The computational costs of the sample generation are modest; samples were each generated in less than a second in our setup. Lastly, we study the impact of our algorithmic parameters on distortion and human perception of samples. This paper makes the following contributions:</p><p>• We formalize the space of adversaries against classification DNNs with respect to adversarial goal and capabilities. Here, we provide a better understanding of how attacker capabilities constrain attack strategies and goals. • We introduce a new class of algorithms for crafting adversarial samples solely by using knowledge of the DNN architecture. These algorithms (1) exploit forward derivatives that inform the learned behavior of DNNs, and</p><p>(2) build adversarial saliency maps enabling an efficient exploration of the adversarial-samples search space. • We validate the algorithms using a widely used computer vision DNN. We define and measure sample distortion and source-to-target hardness, and explore defenses against adversarial samples. We conclude by studying human perception of distorted samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TAXONOMY OF THREAT MODELS IN DEEP LEARNING</head><p>Classical threat models enumerate the goals and capabilities of adversaries in a target domain <ref type="bibr" target="#b0">[1]</ref>. This section taxonimizes threat models in deep learning systems and positions several previous works with respect to the strength of the modeled adversary. We begin by providing an overview of deep neural networks highlighting their inputs, outputs and function. We then consider the taxonomy presented in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. About Deep Neural Networks</head><p>Deep neural networks are large neural networks organized into layers of neurons, corresponding to successive representations of the input data. A neuron is an individual computing unit transmitting to other neurons the result of the application of its activation function on its input. Neurons are connected by links with different weights and biases characterizing the strength between neuron pairs. Weights and biases can be viewed as DNN parameters used for information storage. We define a network architecture to include knowledge of the network topology, neuron activation functions, as well as weight and bias values. Weights and biases are determined during training by finding values that minimize a cost function c evaluated over the training data T . Network training is traditionally done by gradient descent using backpropagation <ref type="bibr" target="#b29">[31]</ref>.</p><p>Deep learning can be partitioned in two categories, depending on whether DNNs are trained in a supervised or unsupervised manner <ref type="bibr" target="#b27">[29]</ref>. Supervised training leads to models that map unseen samples using a function inferred from labeled training data. On the contrary, unsupervised training learns representations of unlabeled training data, and resulting DNNs can be used to generate new samples, or to automate feature engineering by acting as a pre-processing layer for larger DNNs. We restrict ourselves to the problem of learning multi-class classifiers in supervised settings. These DNNs are given an input X and output a class probability vector Y. Note that our work remains valid for unsupervised-trained DNNs, and leaves a detailed study of this issue for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing complexity</head><p>Decreasing knowledge <ref type="bibr" target="#b27">[29]</ref> ADVERSARIAL GOALS ADVERSARIAL CAPABILITIES Fig. <ref type="figure">2</ref>: Threat Model Taxonomy: our class of algorithms operates in the threat model indicated by a star. Figure <ref type="figure">3</ref> illustrates an example shallow feedforward neural network. <ref type="foot" target="#foot_0">1</ref> The network has two input neurons x 1 and x 2 , a hidden layer with two neurons h 1 and h 2 , and a single output neuron o. In other words, it is a simple multi-layer perceptron. Both input neurons x 1 and x 2 take real values in [0, 1] and correspond to the network input: a feature vector X = (x 1 , x 2 ) ∈ [0, 1] 2 . Hidden layer neurons each use the logistic sigmoid function φ : x → 1 1+e −x as their activation function. This function is frequently used in neural networks because it is continuous (and differentiable), demonstrates linear-like behavior around 0, and saturates as the input goes to ±∞. Neurons in the hidden layers apply the sigmoid to the weighted input layer: for instance, neuron h 1 computes h 1 (X) = φ (z h1 (X)) with z h1 (X) = w 11 x 1 +w 12 x 2 +b 1 where w 11 and w 12 are weights and b 1 a bias. Similarly, the output neuron applies the sigmoid function to the weighted output of the hidden layer where</p><formula xml:id="formula_1">z o (X) = w 31 h 1 (X) + w 32 h 2 (X) + b 3 .</formula><p>Weight and bias values are determined during training. Thus, the overall behavior of the network learned during training can be modeled as a function:</p><formula xml:id="formula_2">F : X → φ (z o (X)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial Goals</head><p>Threats are defined with a specific function to be protected/defended. In the case of deep learning systems, the integrity of the classification is of paramount importance. Specifically, an adversary of a deep learning system seeks to provide an input X * that results in an incorrect output classification. The nature of the incorrectness represents the Fig. <ref type="figure">3</ref>: Simplified Multi-Layer Perceptron architecture with input X = (x 1 , x 2 ), hidden layer (h 1 , h 2 ), and output o.</p><p>adversarial goal, as identified in the X-axis of Figure <ref type="figure">2</ref>. Consider four goals that impact classifier output integrity:</p><p>1) Confidence reduction -reduce the output confidence classification (thereby introducing class ambiguity) 2) Misclassification -alter the output classification to any class different from the original class 3) Targeted misclassification -produce inputs that force the output classification to be a specific target class. Continuing the example illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, the adversary would create a set of speckles classified as a digit. 4) Source/target misclassification -force the output classification of a specific input to be a specific target class.</p><p>Continuing the example from Figure <ref type="figure" target="#fig_0">1</ref>, adversaries take an existing image of a digit and add a small number of speckles to classify the resulting image as another digit.</p><p>The scientific community recently started exploring adversarial deep learning. Previous work on other machine learning techniques is referenced later in Section VII.</p><p>Szegedy et al., introduced a system that generates adversarial samples by perturbing inputs in a way that creates source/target misclassifications <ref type="bibr" target="#b34">[36]</ref>. The perturbations made by their work, which focused on a computer vision application, are not distinguishable by humans -for example, small but carefully-crafted perturbations to an image of a vehicle resulted in the DNN classifying it as an ostrich. The authors named this modified input an adversarial image, which can be generalized as part of a broader definition of adversarial samples. When producing adversarial samples, the adversary's goal is to generate inputs that are correctly classified (or not distinguishable) by humans or other classifiers, but are misclassified by the targeted DNN.</p><p>Another example is due to Nguyen et al., who presented a method for producing images that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type="bibr" target="#b28">[30]</ref>. For instance, they demonstrated how a DNN will classify a noise-filled image constructed using their technique as a television with high confidence. They named the images produced by this method fooling images. Here, a fooling image is one that does not have a source class but is crafted solely to perform a targeted misclassification attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial Capabilities</head><p>Adversaries are defined by the information and capabilities at their disposal. The following (and the Y-axis of Figure <ref type="figure">2</ref>) describes a range of adversaries loosely organized by decreasing adversarial strength (and increasing attack difficulty). Note that we only considers attack conducted at test time, any tampering of the training procedure is outside the scope of this paper.</p><p>Training data and network architecture -This adversary has perfect knowledge of the neural network used for classification. The attacker has to access the training data T , functions and algorithms used for network training, and is able to extract knowledge about the DNN's architecture F. This includes the number and type of layers, the activation functions of neurons, as well as weight and bias matrices. He also knows which algorithm was used to train the network, including the associated loss function c. This is the strongest adversary that can analyze the training data and simulate the deep neural network in toto.</p><p>Network architecture -This adversary has knowledge of the network architecture F and its parameter values. For instance, this corresponds to an adversary who can collect information about both (1) the layers and activation functions used to design the neural network, and (2) the weights and biases resulting from the training phase. This gives the adversary enough information to simulate the network. Our algorithms assume this threat model, and show a new class of algorithms that generate adversarial samples for supervised and unsupervised feedforward DNNs.</p><p>Training data -This adversary is able to collect a surrogate dataset, sampled from the same distribution that the original dataset used to train the DNN. However, the attacker is not aware of the architecture used to design the neural network. Thus, typical attacks conducted in this model would likely include training commonly deployed deep learning architectures using the surrogate dataset to approximate the model learned by the legitimate classifier.</p><p>Oracle -This adversary has the ability to use the neural network (or a proxy of it) as an "oracle". Here the adversary can obtain output classifications from supplied inputs (much like a chosen-plaintext attack in cryptography). This enables differential attacks, where the adversary can observe the relationship between changes in inputs and outputs (continuing with the analogy, such as used in differential cryptanalysis) to adaptively craft adversarial samples. This adversary can be further parameterized by the number of absolute or rate-limited input/output trials they may perform.</p><p>Samples -This adversary has the ability to collect pairs of input and output related to the neural network classifier. However, he cannot modify these inputs to observe the difference in the output. To continue the cryptanalysis analogy, this threat model would correspond to a known plaintext attack. These pairs are largely labeled output data, and intuition states that they would most likely only be useful in very large quantities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In this section, we present a general algorithm for modifying samples so that a DNN yields any adversarial output. We later validate this algorithm by having a classifier misclassify samples into a chosen target class. This algorithm captures adversaries crafting samples in the setting corresponding to the upper right-hand corner of Figure <ref type="figure">2</ref>. We show that knowledge of the architecture and weight parameters<ref type="foot" target="#foot_1">2</ref> is sufficient to derive adversarial samples against acyclic feedforward DNNs. This requires evaluating the DNN's forward derivative in order to construct an adversarial saliency map that identifies the set of input features relevant to the adversary's goal. Perturbing the features identified in this way quickly leads to the desired adversarial output, for instance misclassification. Although we describe our approach with supervised neural networks used as classifiers, it also applies to unsupervised architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Studying a Simple Neural Network</head><p>Recall the simple architecture introduced previously in section II and illustrated in Figure <ref type="figure">3</ref>. Its low dimensionality allows us to better understand the underlying concepts behind our algorithms. We indeed show how small input perturbations found using the forward derivative can induce large variations of the neural network output. Assuming that input biases b 1 , b 2 , and b 3 are null, we train this toy network to learn the AND function: the desired output is F(X) = x 1 ∧ x 2 with X = (x 1 , x 2 ). Note that non-integer inputs are rounded up to the closest integer, thus we have for instance 0.7 ∧ 0.3 = 0 or 0.8 ∧ 0.6 = 1. Using backpropagation on a set of 1,000 samples, corresponding to each case of the function (1∧1 = 1, 1 ∧ 0 = 0, 0 ∧ 1 = 0, and 0 ∧ 0 = 0), we train for 100 epochs using a learning rate η = 0.0663. The overall function learned by the neural network is plotted on Figure <ref type="figure" target="#fig_2">4</ref> for input values X ∈ [0, 1] 2 . The horizontal axes represent the 2 input dimensions x 1 and x 2 while the vertical axis represents the network output F(X) corresponding to X = (x 1 , x 2 ).</p><p>We are now going to demonstrate how to craft adversarial samples on this neural network. The adversary considers a legitimate sample X, classified as F(X) = Y by the network, X X* x2 Fig. <ref type="figure">5</ref>: Forward derivative of our simplified multi-layer perceptron according to input neuron x 2 . Sample X is benign and X * is adversarial, crafted by adding δ X = (0, δx 2 ). and wants to craft an adversarial sample X * very similar to X, but misclassified as F(X * ) = Y * = Y . Recall, that we formalized this problem as:</p><formula xml:id="formula_3">arg min δ X δ X s.t. F X + δ X = Y *</formula><p>where X * = X + δ X is the adversarial sample, Y * is the desired adversarial output, and • is a norm appropriate to compare points in the input domain. Informally, the adversary is searching for small perturbations of the input that will incur a modification of the output into Y * . Finding these perturbations can be done using optimization techniques, simple heuristics, or even brute force. However such solutions are hard to implement for deep neural networks because of non-convexity and non-linearity <ref type="bibr" target="#b24">[25]</ref>. Instead, we propose a systematic approach stemming from the forward derivative.</p><p>We define the forward derivative as the Jacobian matrix of the function F learned by the neural network during training. For this example, the output of F is one dimensional, the matrix is therefore reduced to a vector:</p><formula xml:id="formula_4">∇F(X) = ∂F(X) ∂x 1 , ∂F(X) ∂x 2<label>(2)</label></formula><p>Both components of this vector are computable using the adversary's knowledge, and later we show how to compute this term efficiently. The forward derivative for our example network is illustrated in Figure <ref type="figure">5</ref>, which plots the gradient for the second component ∂F(X) ∂x2 on the vertical axis against x 1 and x 2 on the horizontal axes. We omit the plot for</p><formula xml:id="formula_5">∂F(X) ∂x1</formula><p>because F is approximately symmetric on its two inputs, making the first component redundant for our purposes. This plot makes it easy to visualize the divide between the network's two possible outputs in terms of values assigned to the input feature x 2 : 0 to the left of the spike, and 1 to its left. Notice that this aligns with Figure <ref type="figure" target="#fig_2">4</ref>, and gives us the information needed to achieve our adversarial goal: find input perturbations that drive the output closer to a desired value.</p><p>Consulting Figure <ref type="figure">5</ref> alongside our example network, we can confirm this intuition by looking at a few sample points. Consider X = (1, 0.37) and X * = (1, 0.43), which are both located near the spike in Figure <ref type="figure">5</ref>. Although they only differ by a small amount (δx 2 = 0.05), they cause a significant change in the network's output, as F(X) = 0.11 and F(X * ) = 0.95.</p><p>Recalling that we round the inputs and outputs of this network so that it agrees with the Boolean AND function, we see that X* is an adversarial sample: after rounding, X * = (1, 0) and F(X * ) = 1. Just as importantly, the forward derivative tells us which input regions are unlikely to yield adversarial samples, and are thus more immune to adversarial manipulations. Notice in Figure <ref type="figure">5</ref> that when either input is close to 0, the forward derivative is small. This aligns with our intuition that it will be more difficult to find adversarial samples close to (1, 0) than (1, 0.4). This tells the adversary to focus on features corresponding to larger forward derivative values in a given input when constructing a sample, making his search more efficient and ultimately leading to smaller overall distortions. The takeaways of this example are thereby: (1) small input variations can lead to extreme variations of the output of the neural network, ( <ref type="formula" target="#formula_4">2</ref>) not all regions from the input domain are conducive to find adversarial samples, and (3) the forward derivative reduces the adversarial-sample search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalizing to Feedforward Deep Neural Networks</head><p>We now generalize this approach to any feedforward DNN, using the same assumptions and adversary model from Section III-A. The only assumptions we make on the architecture are that its neurons form an acyclic DNN, and each use a differentiable activation function. Note that this last assumption is not limiting because the back-propagation algorithm imposes the same requirement. In Figure <ref type="figure">6</ref>, we give an example of a feedforward deep neural network architecture and define some notations used throughout the remainder of the paper. Most importantly, the N -dimensional function F learned by the DNN during training assigns an output Y = F(X) when given an M -dimensional input X. We write n the number of hidden layers. Layers are indexed by k ∈ 0..n + 1 such that k = 0 is the index of the input layer, k ∈ 1..n corresponds to hidden layers, and k = n + 1 indexes the output layer.</p><p>Algorithm 1 shows our process for constructing adversarial samples. As input, the algorithm takes a benign sample X, a target output Y * , an acyclic feedforward DNN F, a maximum distortion parameter Υ, and a feature variation parameter θ. It returns new adversarial sample X * such that F(X * ) = Y * , and proceeds in three basic steps: (1) compute the forward derivative ∇F(X * ), (2) construct a saliency map S based on the derivative, and (3) modify an input feature i max by θ. This process is repeated until the network outputs Y * or the maximum distortion Υ is reached. We now detail each step.</p><p>1) Forward Derivative of a Deep Neural Network: The first step is to compute the forward derivative for the given sample X. As introduced previously, this is given by:</p><formula xml:id="formula_6">∇F(X) = ∂F(X) ∂X = ∂F j (X) ∂x i i∈1..M,j∈1..N<label>(3)</label></formula><p>This is essentially the Jacobian of the function corresponding to what the neural network learned during training. The forward derivative computes gradients that are similar to those computed for backpropagation, but with two important distinctions: we take the derivative of the network directly, rather </p><formula xml:id="formula_7">Input: X, Y * , F, Υ, θ 1: X * ← X 2: Γ = {1 . . . |X|} 3: while F(X * ) = Y * and ||δ X || &lt; Υ do 4:</formula><p>Compute forward derivative ∇F(X * )</p><p>5:</p><formula xml:id="formula_8">S = saliency_map (∇F(X * ), Γ, Y * ) 6: Modify X * imax by θ s.t. i max = arg max i S(X, Y * )[i] 7:</formula><p>δ X ← X * − X 8: end while 9: return X * than on its cost function, and we differentiate with respect to the input features rather than the network parameters. As a consequence, instead of propagating gradients backwards, we choose in our approach to propagate them forward, as this allows us to find input components that lead to significant changes in network outputs.</p><p>Our goal is to express ∇F(X * ) in terms of X and constant values only. To simplify our expressions, we now consider one element</p><formula xml:id="formula_9">(i, j) ∈ [1..M ] × [1.</formula><p>.N ] of the M × N forward derivative matrix defined in Equation <ref type="formula" target="#formula_6">3</ref>: that is the derivative of one output neuron F j according to one input dimension x i . Of course our results are true for any matrix element. We start at the first hidden layer of the neural network. We can differentiate the output of this first hidden layer in terms of the input components. We then recursively differentiate each hidden layer k ∈ 2..n in terms of the previous one:</p><formula xml:id="formula_10">∂H k (X) ∂x i = ∂f k,p (W k,p • H k−1 + b k,p ) ∂x i p∈1..m k (4)</formula><p>where H k is the output vector of hidden layer k and f k,j is the activation function of output neuron j in layer k. Each neuron p on a hidden or output layer indexed k ∈ 1..n + 1 is connected to the previous layer k − 1 using weights defined in vector W k,p . By defining the weight matrix accordingly, we can define fully or sparsely connected interlayers, thus modeling a variety of architectures. Similarly, we write b k,p the bias for neuron p of layer k. By applying the chain rule, we can write a series of formulae for k ∈ 2..n:</p><formula xml:id="formula_11">∂H k (X) ∂x i p∈1..m k = W k,p • ∂H k−1 ∂x i × ∂f k,p ∂x i (W k,p • H k−1 + b k,p )<label>(5)</label></formula><p>We are thus able to express ∂Hn ∂xi . We know that output neuron j computes the following expression:</p><formula xml:id="formula_12">F j (X) = f n+1,j (W n+1,j • H n + b n+1,j )</formula><p>Thus, we apply the chain rule again to obtain:</p><formula xml:id="formula_13">∂F j (X) ∂x i = W n+1,j • ∂H n ∂x i × ∂f n+1,j ∂x i (W n+1,j • H n + b n+1,j )<label>(6)</label></formula><p>In this formula, according to our threat model, all terms are known but one: ∂Hn ∂xi . This is precisely the term we computed recursively. By plugging these results for successive layers back in Equation <ref type="formula" target="#formula_13">6</ref>, we get an expression of component (i, j) of the DNN's forward derivative. Hence, the forward derivative ∇F of a network F can be computed for any input X by successively differentiating layers starting from the input layer until the output layer is reached. We later discuss in our methodology evaluation the computability of ∇F for stateof-the-art DNN architectures. Notably, the forward derivative can be computed using symbolic differentiation.</p><p>2) Adversarial Saliency Maps: We extend saliency maps previously introduced as visualization tools <ref type="bibr" target="#b32">[34]</ref> to construct adversarial saliency maps. These maps indicate which input features an adversary should perturb in order to effect the desired changes in network output most efficiently, and are thus versatile tools that allow adversaries to generate broad classes of adversarial samples.</p><p>Adversarial saliency maps are defined to suit problemspecific adversarial goals. For instance, we later study a network used as a classifier, its output is a probability vector across classes, where the final predicted class value corresponds to the component with the highest probability:</p><formula xml:id="formula_14">label(X) = arg max j F j (X)<label>(7)</label></formula><p>In our case, the saliency map is therefore based on the forward derivative, as this gives the adversary the information needed to cause the neural network to misclassify a given sample. More precisely, the adversary wants to misclassify a sample X such that it is assigned a target class t = label(X). To do so, the probability of target class t given by F, F t (X), must be increased while the probabilities F j (X) of all other classes j = t decrease, until t = arg max j F j (X). The adversary can accomplish this by increasing input features using the following saliency map S(X, t):</p><formula xml:id="formula_15">S(X, t)[i] =    0 if ∂Ft(X) ∂Xi &lt; 0 or j =t ∂Fj (X) ∂Xi &gt; 0 ∂Ft(X) ∂Xi j =t ∂Fj (X) ∂Xi otherwise (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where i is an input feature. The condition specified on the first line rejects input components with a negative target derivative or an overall positive derivative on other classes. Indeed, ∂Ft(X) ∂Xi should be positive in order for F t (X) to increase when feature X i increases. Similarly, j =t ∂Fj (X) ∂Xi needs to be negative to decrease or stay constant when feature X i is increased. The product on the second line allows us to consider all other forward derivative components together in such a way that we can easily compare S(X, t)[i] for all input features. In summary, high values of S(X, t)[i] correspond to input features that will either increase the target class, or decrease other classes significantly, or both. By increasing these features, the adversary eventually misclassifies the sample into the target class. A saliency map example is shown on Figure <ref type="figure">7</ref>.</p><p>It is possible to define other adversarial saliency maps using the forward derivative, and the quality of the map can have a large impact on the amount of distortion that Algorithm 1 introduces; we will study this in more detail later. Before moving on, we introduce an additional map that acts as a counterpart to the one given in Equation 8 by finding features that the adversary should decrease to achieve misclassification. The only difference lies in the constraints placed on the forward derivative values and the location of the absolute value in the second line:</p><formula xml:id="formula_17">S(X, t)[i] =    0 if ∂Ft(X) ∂Xi &gt; 0 or j =t ∂Fj (X) ∂Xi &lt; 0 ∂Ft(X) ∂Xi j =t ∂Fj (X) ∂Xi otherwise (9)</formula><p>3) Modifying samples: Once an input feature has been identified by an adversarial saliency map, it needs to be perturbed to realize the adversary's goal. This is the last step Fig. <ref type="figure">7</ref>: Saliency map of a 784-dimensional input to the LeNet architecture (cf. validation section). The 784 input dimensions are arranged to correspond to the 28x28 image pixel alignment. Large absolute values correspond to features with a significant impact on the output when perturbed.</p><p>in each iteration of Algorithm 1, and the amount by which the selected feature is perturbed (θ in Algorithm 1) is also problem-specific. We discuss in Section IV how this parameter should be set in an application to computer vision. Lastly, the maximum number of iterations, which is equivalent to the maximum distortion allowed in a sample, is specified by parameter Υ. It limits the number of features changed to craft an adversarial sample and can take any positive integer value smaller than the number of features. Finding the right value for Υ requires considering the impact of distortion on humans' perception of adversarial samples -too much distortion might cause adversarial samples to be easily identified by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATION OF THE APPROACH</head><p>We formally described a class of algorithms for crafting adversarial samples misclassified by feedforward DNNs using three tools: the forward derivative, adversarial saliency maps, and the crafting algorithm. We now apply these tools to a DNN used for a computer vision classification task: handwritten digit recognition. We show that our algorithms successfully craft adversarial samples from any source class to any given target class, which for this application means that any digit can be perturbed so that it is misclassified as any other digit.</p><p>We investigate a DNN based on the well-studied LeNet architecture, which has proven to be an excellent classifier for handwritten digits <ref type="bibr" target="#b25">[26]</ref>. Recent architectures like AlexNet <ref type="bibr" target="#b23">[24]</ref> or GoogLeNet <ref type="bibr" target="#b33">[35]</ref> are heavily reliant on convolutional layers introduced in the LeNet architecture, thus making LeNet a relevant DNN to validate our approach. We have no reason to believe that our method will not perform well on larger architectures. The network input is black and white images (28x28 pixels) of handwritten digits, which are flattened as Fig. <ref type="figure">8</ref>: Samples taken from the MNIST test set. The respective output vectors are: [0, 0, 0, 0, 0, 0, 0.99, 0, 0], [0, 0, 0.99, 0, 0, 0, 0, 0, 0], and [0, 0.99, 0, 0, 0, 0, 0, 0, 0], where all values smaller than 10 −6 have been rounded to 0. vectors of 784 features, where each feature corresponds to a pixel intensity taking normalized values between 0 and 1. This input is processed by a succession of a convolutional layer (20 then 50 kernels of 5x5 pixels) and a pooling layer (2x2 filters) repeated twice, a fully connected hidden layer (500 neurons), and an output softmax layer (10 neurons). The output is a 10 class probability vector, where each class corresponds to a digit from 0 to 9, as shown in Figure <ref type="figure">8</ref>. The network then labels the input image with the class assigned the maximum probability, as shown in Equation <ref type="formula" target="#formula_14">7</ref>. We train our network using the MNIST training dataset of 60,000 samples <ref type="bibr" target="#b26">[27]</ref>.</p><p>We attempt to determine whether, using the theoretical framework introduced in previous sections, we can effectively craft adversarial samples misclassified by the DNN. For instance, if we have an image X of a handwritten digit 0 classified by the network as label(X) = 0 and the adversary wishes to craft an adversarial sample X * based on this image classified as label(X * ) = 7, the source class is 0 and the target class is 7. Ideally, the crafting process must find the smallest perturbation δ X required to construct the adversarial sample X * = X + δ X . A perturbation is a set of pixel intensities -or input feature variations -that are added to X in order to craft X * . Note that perturbations introduced to craft adversarial samples must remain indistinguishable to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Crafting algorithm</head><p>Algorithm 2 shows the crafting algorithm used in our experiments, which we implemented in Python (see Appendix A for more information regarding the implementation). It is based on Algorithm 1, but several details have been changed to accommodate our handwritten digit recognition problem. Given a network F, Algorithm 2 iteratively modifies a sample X by perturbing two input features (i.e., pixel intensities) p 1 and p 2 selected by saliency_map. The saliency map is constructed and updated between each iteration of the algorithm using the DNN's forward derivative ∇F(X * ). The algorithm halts when one of the following conditions is met: (1) the adversarial sample is classified by the DNN with the target class t, (2) the maximum number of iterations max_iter has been reached, or (3) the feature search domain Γ is empty. The crafting algorithm is fine-tuned by three parameters:</p><p>• Maximum distortion Υ: this defines when the algorithm should stop modifying the sample in order to reach the ad-versarial target class. The maximum distortion, expressed as a percentage, corresponds to the maximum number of pixels to be modified when crafting the adversarial sample, and thus sets the maximum number of iterations max_iter (2 pixels modified per iteration) as follows:</p><formula xml:id="formula_18">max_iter = 784 • Υ 2 • 100</formula><p>where 784 = 28×28 is the number of pixels in a sample. • Saliency map: subroutine saliency_map generates a map defining which input features will be modified at each iteration. Policies used to generate saliency maps vary with the nature of the data handled by the considered DNN, as well as the adversarial goals. We provide a subroutine example later in Algorithm 3. • Feature variation per iteration θ: once input features have been selected using the saliency map, they must be modified. The variation θ introduced to these features is another parameter that the adversary must set, in accordance with the saliency maps she uses. The problem of finding good values for these parameters is a goal of our current evaluation, and is discussed later in Section V. For now, note that human perception is a limiting factor as it limits the acceptable maximum distortion and feature variation introduced. We now show the application of our framework with two different adversarial strategies. iter + + 14: end while 15: return X *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Crafting by increasing pixel intensities</head><p>The first strategy to craft adversarial samples is based on increasing the intensity of some pixels. To achieve this purpose, we consider 10 samples of handwritten digits from the MNIST test set, one from each digit class 0 to 9. We use this small subset of samples to illustrate our techniques. We scale up the evaluation to the entire dataset in Section V. Our goal is to report whether we can reach any adversarial target class for a given source class. For instance, if we are given a handwritten 0, we increase some of the pixel intensities to produce 9 adversarial samples respectively classified in each of the classes 1 to 9. All pixel intensities changed are increased by θ = +1. We discuss this choice of parameter in section V. We allow for an unlimited maximum distortion Υ = ∞. We simply measure for each of the 90 source-target class pairs whether an adversarial sample can be produced or not.</p><p>The adversarial saliency map used in the crafting algorithm to select pixel pairs that can be increased is an application of the map introduced in the general case of classification in Equation <ref type="formula" target="#formula_15">8</ref>. The map aims to find pairs of pixels (p 1 , p 2 ) using the following heuristic:</p><formula xml:id="formula_19">arg max (p1,p2)   i=p1,p2 ∂F t (X) ∂X i   × i=p1,p2 j =t ∂F j (X) ∂X i (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>where t is the index of the target class, the left operand of the multiplication operation is constrained to be positive, and the right operand of the multiplication operation is constrained to be negative. This heuristic, introduced in the previous section of this manuscript, searches for pairs of pixels producing an increase in the target class output while reducing the sum of the output of all other classes when simultaneously increased. The pseudocode of the corresponding subroutine saliency_map is given in Algorithm 3.</p><p>The saliency map considers pairs of pixels and not individual pixels because selecting pixels one at a time is too strict, and very few pixels would meet the heuristic search criteria described in Equation <ref type="formula" target="#formula_15">8</ref>. Searching for pairs of pixels is more likely to match the condition because one of the pixels can compensate a minor flaw of the other pixel. Let's consider a simple example: p 1 has a target derivative of 5 but a sum of other classes derivatives equal to 0.1, while p 2 as a target derivative equal to −0.5 and a sum of other classes derivatives equal to −6. Individually, these pixels do not match the saliency map's criteria stated in Equation <ref type="formula" target="#formula_15">8</ref>, but combined, the pair does match the saliency criteria defined in Equation <ref type="formula" target="#formula_19">10</ref>. One would also envision considering larger groups of input features to define saliency maps. However, this comes at a greater computational cost because more combinations need to be considered each time the group size is increased.</p><p>In our implementation of these algorithms, we compute the forward derivative of the network using the last hidden layer instead of the output probability layer. This is justified by the extreme variations introduced by the logistic regression computed between these two layers to ensure probabilities sum up to 1, leading to extreme derivative values. This reduces the quality of information on how the neurons are activated by different inputs and causes the forward derivative to loose accuracy when generating saliency maps. Better results are achieved when working with the last hidden layer, also made up of 10 neurons, each corresponding to one digit class 0 to 9. This justifies enforcing constraints on the forward derivative. Indeed, as the output layer used for computing the forward derivative does not sum up to 1, increasing F t (X) does not imply that j =t ∂F j (X) will decrease, and vice-versa.</p><p>Algorithm 3 Increasing pixel intensities saliency map ∇F(X) is the forward derivative, Γ the features still in the search space, and t the target class Input: ∇F(X), Γ, t 1: for each pair (p, q) ∈ Γ do</p><formula xml:id="formula_21">2: α = i=p,q ∂Ft(X) ∂Xi 3: β = i=p,q j =t ∂Fj (X) ∂Xi 4:</formula><p>if α &gt; 0 and β &lt; 0 and −α × β &gt; max then 5:</p><formula xml:id="formula_22">p 1 , p 2 ← p, q 6: max ← −α × β 7:</formula><p>end if 8: end for 9: return p 1 , p 2</p><p>The algorithm is able to craft successful adversarial samples for all 90 source-target class pairs. Figure <ref type="figure" target="#fig_0">1</ref> shows the 90 adversarial samples obtained as well as the 10 original samples used to craft them. The original samples are found on the diagonal. A sample on row i and column j, when i = j, is a sample crafted from an image originally classified as source class i to be misclassified as target class j.</p><p>To verify the validity of our algorithms, and more specifically of our adversarial saliency maps, we run a simple experiment. We run the crafting algorithm on an empty input (all pixels initially set to an intensity of 0) and craft one adversarial sample for each class from 0 to 9. The different samples shown in Figure <ref type="figure" target="#fig_4">9</ref> demonstrate how adversarial saliency maps are able to identify input features relevant to classification in a class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Crafting by decreasing pixel intensities</head><p>Instead of increasing pixel intensities to achieve the adversarial targets, the second adversarial strategy decreases pixel intensities by θ = −1. The implementation is identical to the exception of the adversarial saliency map. The formula is the same as previously written in Equation <ref type="formula" target="#formula_19">10</ref>but the constraints are different: the left operand of the multiplication operation is now constrained to be negative, and the right operand to be positive. This heuristic, also introduced in the previous section of this paper, searches for pairs of pixels producing an increase in the target class output while reducing the sum of the output of all other classes when simultaneously decreased. The algorithm is once again able to craft successful adversarial samples for all source-target class pairs. Figure <ref type="figure" target="#fig_5">10</ref> shows the 90 adversarial samples obtained as well as the 10 original samples used to craft them. One observation to be made is that the distortion introduced by reducing pixel intensities seems harder to detect by the human eye. We address the human perception aspect with a study later in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>We now use our experimental setup to answer the following questions: (1) "Can we exploit any sample?", (2) "How can we identify samples more vulnerable than others?" and (3) "How do humans perceive adversarial samples compared to DNNs?". Our primary result is that adversarial samples can be crafted reliably for our validation problem with a 97.10% success rate by modifying samples on average by 4.02%. We define a hardness measure to identify sample classes easier to exploit than others. This measure is necessary for designing robust defenses. We also found that humans cannot perceive the perturbation introduced to craft adversarial samples misclassified by the DNN: they still correctly classify adversarial samples crafted with a distortion smaller than 14.29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Crafting large amounts of adversarial samples</head><p>Now that we previously showed the feasibility of crafting adversarial samples for all source-target class pairs, we seek to measure whether the crafting algorithm can successfully handle large quantities of distinct samples of hand-written digits. That is, we now design a set of experiments to evaluate whether or not all legitimate samples in the MNIST dataset can be exploited by an adversary to produce adversarial samples. We run our crafting algorithm on three sets of 10,000 samples each extracted from one of the three MNIST training, validation, and test subsets <ref type="foot" target="#foot_2">3</ref> . For each of these samples, we craft 9 adversarial samples, each of them classified in one of the 9 target classes distinct from the original legitimate class. Thus, we generate 90,000 samples for each set, leading to a total of 270,000 adversarial samples. We set the maximum distortion to Υ = 14.5% and pixel intensities are increased by θ = +1. The maximum distortion was fixed after studying the effect of increasing it on the success rate τ . We found that 97.1% of the adversarial samples could be crafted with a distortion of less than 14.5% and observed that the success rate did not increase significantly for larger maximum distortions. Parameter θ was set to +1 after observing that decreasing it or giving it negative values increased the number of features modified, whereas we were interested in reducing the number of features altered during crafting. One will also notice that because features are normalized between 0 and 1, if we introduce a variation of θ = +1, we always set pixels to their maximum value 1. This justifies why in Algorithm 2, we remove modified pixels from the search space at the end of each iteration. The impact on performance is beneficial, as we reduce the size of the feature search space at each iteration. In other words, our algorithm performs a best-first heuristic search without backtracking.</p><p>We measure the success rate τ and distortion of adversarial samples on the three sets of 10,000 samples. The success rate τ is defined as the percentage of adversarial samples that were successfully classified by the DNN as the adversarial target class. The distortion is defined to be the percentage of pixels modified in the legitimate sample to obtain the adversarial sample. In other words, it is the percentage of input features modified in order to obtain adversarial samples. We compute two average distortion values: one taking into account all samples and a second one only taking into account successful samples, which we write ε. Figure <ref type="figure" target="#fig_0">11</ref> presents the results for the three sets from which the original samples were extracted. The results are consistent across all sets. On average, the success rate is τ = 97.10%, the average distortion of all adversarial samples is 4.44%, and the average distortion of successful adversarial samples is ε = 4.02%. This means that the average number of pixels modified to craft a successful adversarial sample is 32 out of 784 pixels. The first distortion figure is higher because it includes unsuccessful samples, for which the crafting algorithm used the maximum distortion Υ, but was unable to induce a misclassification. We also studied crafting of 9, 000 adversarial samples using the decreasing saliency map. We found that the success rate τ = 64.7% was lower and the average distortion ε = 3.62% slightly lower. Again, decreasing pixel intensities is less successful at producing the desired adversarial behavior than increasing pixel intensities. Intuitively, this can be understood because removing pixels reduces the information entropy, thus making it harder for DNNs to extract the information required to classify the sample. Greater absolute values of intensity variations are more confidently misclassified by the DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantifying hardness and building defense mechanisms</head><p>Looking at the previous experiment, about 2.9% of the 270, 000 adversarial samples were not successfully crafted. This suggests that some samples are harder to exploit than others. Furthermore, the distortion figures reported are averaged on all adversarial samples produced but not all samples require the same distortion to be misclassified. Thus, we now study the hardness of different samples in order to quantify these phenomena. Our aim is to identify which source-target class pairs are easiest to exploit, as well as similarities between distinct source-target class pairs. A class pair is a pair of a source class s and a target class t. This hardness metric allows us to lay ground for defense mechanisms.</p><p>1) Class pair study: In this experiment, we construct a deeper understanding of the crafting algorithm' success rate and average distortion for different source-target class pairs. We use the 90,000 adversarial samples crafted in the previous experiments from the 10,000 samples of the MNIST test set.</p><p>We break down the success rate τ reported in Figure <ref type="figure" target="#fig_0">11</ref> by source-target class pairs. This allows us to know, for a given source class, how many samples of that class were successfully misclassified in each of the target classes. In Figure <ref type="figure" target="#fig_6">12</ref>, we draw the success rate matrix indicating which pairs are most successful. Darker shades correspond to higher success rates. The rows correspond to the success rate per source class while the columns correspond to the success rate per target class. If one reads the matrix row-wise, it can be perceived that classes 0, 2, and 8 are hard to start with, while classes 1, 7, and 9 are easy to start with. Similarly, reading the matrix column-wise, one can observe that classes 1 and 7 are very hard to make, while classes 0, 8, and 9 are easy to make.</p><p>In Figure <ref type="figure" target="#fig_7">13</ref>, we report the average distortion ε of successful samples by source-target class pair, thus identifying class pairs requiring the most distortion to successfully craft adversarial  samples. Interestingly, classes requiring lower distortions correspond to classes with higher success rates in the previous matrix. For instance, the column corresponding to class 1 is associated with the highest distortions, and it was the column with the least success rates in the previous matrix. Indeed, the higher the average distortion of a class pair is, the more likely samples in that class pair are to reach the maximum distortion, and thus produce unsuccessful adversarial samples.</p><p>To better understand why some class pairs were harder to exploit, we tracked the evolution of class probabilities during the crafting process. We observed that the distortion required to leave the source class was higher for class pairs with high distortions whereas the distortion required to reach the target class, once the source class had been left, remained similar. This correlates with the fact that some source classes are more confidently classified by the DNN then others. 2) Hardness measure: Results indicating that some sourcetarget class pairs are not as easy as others lead us to question the existence of a measure quantifying the distance between two classes. This is relevant to a defender seeking to identify which classes of a DNN are most vulnerable to adversaries. We name this measure the hardness of a target class relatively to a given source class. It normalizes the average distortion of a class pair (s, t) relatively to its success rate:</p><formula xml:id="formula_23">H(s, t) = τ ε(s, t, τ )dτ<label>(11)</label></formula><p>where ε(s, t, τ ) is the average distortion of a set of samples for the corresponding success rate τ . In practice, these two quantities are computed over a finite number of samples by fixing a set of K maximum distortion parameter values Υ k in the crafting algorithm where k ∈ 1..K. The set of maximum distortions gives a series of pairs (ε k , τ k ) for k ∈ 1..K. Thus, the practical formula used to compute the hardness of a sourcedestination class pair can be derived from the trapezoidal rule:</p><formula xml:id="formula_24">H(s, t) ≈ K−1 k=1 (τ k+1 − τ k ) ε(s, t, τ k+1 ) + ε(s, t, τ k ) 2<label>(12)</label></formula><p>We computed the hardness values for all classes using a set of K = 9 maximum distortion values Υ ∈ {0.3, 1.3, 2.6, 5.1, 7.7, 10.2, 12.8, 25.5, 38.3}% in the algorithm. Average distortions ε and success rates τ are averaged over 9,000 adversarial samples for each maximum distortion value Υ. Figure <ref type="figure" target="#fig_8">14</ref> shows the hardness values H(s, t) for all pairs (s, t) ∈ {0..9} 2 . The reader will observe that the matrix has a shape similar to the average distortion matrix plotted on Figure <ref type="figure" target="#fig_7">13</ref>. However, the hardness measure is more accurate because it is plotted using a series of maximum distortions. 3) Adversarial distance: The measure introduced lays ground towards finding defenses against adversarial samples. Indeed, if the hardness measure were to be predictive instead of being computed after adversarial crafting, the defender could identify vulnerable inputs. Furthermore, a predictive measure applicable to a single sample would allow a defender to evaluate the vulnerability of specific samples as well as class pairs. We investigated several complex estimators including convolutional transformations of the forward derivative or Hessian matrices. However, we found that simply using a formulae derived from the intuition behind adversarial saliency maps gave enough accuracy for predicting the hardness of samples in our experimental setup.</p><p>We name this predictive measure the adversarial distance of sample X to class t and write it A(X, t). Simply put, it estimates the distance between a sample X and a target class t. We define the distance as:</p><formula xml:id="formula_25">A(X, t) = 1 − 1 M i∈0..M 1 S(X,t)[i]&gt;0<label>(13)</label></formula><p>where 1 E is the indicator function for event E (i.e., is 1 if and only if E is true). In a nutshell, A(X, t) is the normalized number of non-zero elements in the adversarial saliency map of X computed during the first crafting iteration in Algorithm 2. The closer the adversarial distance is to 1, the more likely sample X is going to be harder to misclassify in target class t. Figure <ref type="figure" target="#fig_9">15</ref> confirms that this formulae is empirically well-founded. It illustrates the value of the adversarial distance averaged per source-destination class pairs, making it easy to compare the average value with the hardness matrix computed previously after crafting samples. To compute it, we slightly altered Equation <ref type="formula" target="#formula_25">13</ref>to sum over pairs of features, reflecting the observations made during our validation process.  This notion of distance between classes intuitively defines a metric for the robustness of a network F against adversarial perturbations. We suggest the following definition :</p><formula xml:id="formula_26">R(F) = min (X,t) A(X, t)<label>(14)</label></formula><p>where the set of samples X considered is sufficiently large to represent the input domain of the network. A good approximation of the robustness can be computed with the training dataset. Note that the min operator used here can be replaced by other relevant operators, like the statistical expectation. The study of various operators is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Study of human perception of adversarial samples</head><p>Recall that adversarial samples must not only be misclassified as the target class by deep neural networks, but also visually appear (be classified) as the source class by humans. To evaluate this property, we ran an experiment using 349 human participants on the Mechanical Turk online service. We presented three original or adversarially altered samples from the MNIST dataset to human participants. To paraphrase, participants were asked for each sample: (a) 'is this sample a numeric digit?', and (b) 'if yes to (a) what digit is it?'. These two questions were designed to determine how distortion and intensity rates effected human perception of the samples.</p><p>The first experiment was designed to identify a baseline perception rate for the input data. The 74 participants were presented 3 of 222 unaltered samples randomly picked from the original MNIST data set. Respondents identified 97.4% as digits and classified the digits correctly 95.3% of the samples. Shown in Figure <ref type="figure" target="#fig_10">16</ref>, a second set of experiments attempted to evaluate how the amount of distortion (ε) impacts human perception. Here, 184 participants were presented with a total of 1707 samples with varying levels of distortion (and features altered with an intensity increase θ = +1). The experiments showed that below a threshold (ε = 14.29% distortion), participants were able to identify samples as digits (95%) and correctly classify them (90%) only slightly less accurately than the unaltered samples. The classification rate dropped dramatically (71%) at distortion rates above the threshold. A final set of experiments evaluate the impact of intensity variations (θ) on perception, as shown Figure <ref type="figure" target="#fig_11">17</ref>. The 203 participants were accurate at identifying 5, 355 samples as digits (96%) and classifying them correctly (95%). At higher absolute intensities (θ = −1 and θ = +1), specific digit classification decreased slightly (90.5% and 90%), but identification as digits was largely unchanged.</p><p>While preliminary, these experiments confirm that the overwhelming number of generated samples retain human recognizability. Note that because we can generate samples with less than the distortion threshold for the almost all of the input data, (ε ≤ 14.29% for roughly 97% in the MNIST data), we can produce adversarial samples that humans will mis-interpret-thus meeting our adversarial goal. Furthermore, altering feature distortion intensity provides even better results: at −0.7 ≤ θ ≤ +0.7, humans classified the sample data at essentially the same rates as the original sample data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>We introduced a new class of algorithms that systematically craft adversarial samples misclassified by a DNN once an adversary possesses knowledge of the DNN architecture. Although we focused our work on DL techniques used in the context of classification and trained with supervised methods, our approach is also applicable to unsupervised architectures. Instead of achieving a given target class, the adversary achieves a target output Y * . Because the output space is more complex, it might be harder or impossible to match Y * . In that case, Equation 1 would need to be relaxed with an acceptable distance between the network output F(X * ) and the adversarial target Y * . Thus, the only remaining assumption made in this paper is that DNNs are feedforward. In other words, we did not consider recurrent neural networks, with cycles in their architecture, as the forward derivative must be adapted to accommodate such networks.</p><p>One of our key results is reducing the distortion-the number of features altered-to craft adversarial samples, compared to previous work. We believe this makes adversarial crafting much easier for input domains like malware executables, which are not as easy to perturb as images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>. This distortion reduction comes with a performance cost. Indeed, more elaborate but accurate saliency map formulae are more expensive to compute for the attacker. We would like to emphasize that our method's high success rate can be further improved by adversaries only interested in crafting a limited number of samples. Indeed, to lower the distortion of one particular sample, an adversary can use adversarial saliency maps to fine-tune the perturbation introduced. On the other hand, if an adversary wants to craft large amounts of adversarial samples, performance is important. In our evaluation, we balanced these factors to craft adversarial samples against the DNN in less than a second. As far as our algorithm implementation was concerned, the most computationally expensive steps were the matrix manipulations required to construct adversarial saliency maps from the forward derivative matrix. The complexity is dependent of the number of input features. These matrix operations can be made more efficient, notably by making better use of GPU-accelerated computations.</p><p>Our efforts so far represent a first but meaningful step towards mitigating adversarial samples: the hardness and adversarial distance metrics lay out bases for defense mechanisms. Although designing such defenses is outside of the scope of this paper, we outline two classes of defenses: (1) adversarial sample detection and (2) improvements of DNN robustness.</p><p>Developing techniques for adversarial sample detection is a reactive solution. During our experimental process, we noticed that adversarial samples can for instance be detected by evaluating the regularity of samples. More specifically, in our application example, the sum of the squared difference between each pair of neighboring pixels is always higher for adversarial samples than for benign samples. However, there is no a priori reason to assume that this technique will reliably detect adversarial samples in different settings, so extending this approach is one avenue for future work. Another approach was proposed in <ref type="bibr" target="#b18">[19]</ref>, but it is unsuccessful as by stacking the denoising auto-encoder used for detection with the original DNN, the adversary can again produce adversarial samples.</p><p>The second class of solutions seeks to improve training to in return increase the robustness of DNNs. Interestingly, the problem of adversarial samples is closely linked to training. Work on generative adversarial networks showed that a two player game between two DNNs can lead to the generation of new samples from a training set <ref type="bibr" target="#b16">[17]</ref>. This can help augment training datasets. Furthermore, adding adversarial samples to the training set can act like a regularizer <ref type="bibr" target="#b17">[18]</ref>. We also observed in our experiments that training with adversarial samples makes crafting additional adversarial samples harder. Indeed, by adding 18,000 adversarial samples to the original MNIST training dataset, we trained a new instance of our DNN. We then run our algorithms again on this newly trained network and crafted a set of 9,000 adversarial samples. Preliminary analysis of these adversarial samples crafted showed that the success rate was reduced by 7.2% while the average distortion increased by 37.5%, suggesting that training with adversarial samples can make DNNs more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>The security of machine learning <ref type="bibr" target="#b1">[2]</ref> is an active research topic within the security and machine learning communities. A broad taxonomy of attacks and required adversarial capabilties are discussed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b2">[3]</ref> along with considerations for building defense mechanisms. Biggio et al. studied classifiers in adversarial settings and outlined a framework securing them <ref type="bibr" target="#b7">[8]</ref>. However, their work does not consider DNNs but rather other techniques used for binary classification like logistic regression or Support Vector Machines. Generally speaking, attacks against machine learning can be separated into two categories, depending on whether they are executed during training <ref type="bibr" target="#b8">[9]</ref> or at test time <ref type="bibr" target="#b9">[10]</ref>.</p><p>Prior work on adversarial sample crafting against DNNs derived a simple technique corresponding to the Architecture and Training Tools threat model, based on the backpropagation procedure used during network training <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. This approach creates adversarial samples by defining an optimization problem based on the DNN's cost function. In other words, instead of computing gradients to update DNN weights, one computes gradients to update the input, which is then misclassified as the target class by a DNN. The alternative approach proposed in this paper is to identify input regions that are most relevant to its classification by a DNN. This is accomplished by computing the saliency map of a given input, as described by Simonyan et al. in the case of DNNs handling images <ref type="bibr" target="#b32">[34]</ref>. We extended this concept to create adversarial saliency maps highlighting regions of the input that need to be perturbed in order to accomplish the adversarial goal.</p><p>Previous work by Yosinki et al. investigated how features are transferable between deep neural networks <ref type="bibr" target="#b36">[38]</ref>, while Szegedy et al. showed that adversarial samples can indeed be misclassified across models <ref type="bibr" target="#b34">[36]</ref>. They report that once an adversarial sample is generated for a given neural network architecture, it is also likely to be misclassified in neural networks designed differently, which explains why the attack is successful. However, the effectiveness of this kind of attack depends on (1) the quality and size of the surrogate dataset collected by the adversary, and (2) the adequateness of the adversarial network used to craft adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>Broadly speaking, this paper has explored adversarial behavior in deep learning systems. In addition to exploring the goals and capabilities of DNN adversaries, we introduced a new class of algorithms to craft adversarial samples based on computing forward derivatives. This technique allows an adversary with knowledge of the network architecture to construct adversarial saliency maps that identify features of the input that most significantly impact output classification. These algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample.</p><p>Solutions to defend DNNs against adversaries can be divided in two classes: detecting adversarial samples and improving the training phase. The detection of adversarial samples remains an open problem. Interestingly, the universal approximation theorem formulated by Hornik et al. states one hidden layer is sufficient to represent arbitrarily accurately a function <ref type="bibr" target="#b20">[21]</ref>. Thus, one can intuitively conceive that improving the training phase is key to resisting adversarial samples.</p><p>In future work, we plan to address the limitations of DNN trained in an unsupervised manner as well as cyclical recurrent neural networks (as opposed to acyclical networks considered throughout this paper). Also, as most models of our taxonomy have yet to be researched, this leaves room for further investigation of DL in various adversarial settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Adversarial sample generation -Distortion is added to input samples to force the DNN to output adversaryselected classification (min distortion = 0.26%, max distortion = 13.78%, and average distortion ε = 4.06%).</figDesc><graphic url="image-1.png" coords="1,322.31,219.79,244.52,225.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The output surface of our simplified Multi-Layer Perceptron for the input domain [0, 1] 2 . Blue corresponds to a 0 output while yellow corresponds to a 1 output.</figDesc><graphic url="image-2.png" coords="4,311.98,50.54,251.06,100.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 9 : 11 :</head><label>2911</label><figDesc>Crafting adversarial samples for LeNet-5 X is the benign image, Y * is the target network output, F is the function learned by the network during training, Υ is the maximum distortion, and θ is the change made to pixels. Input: X, Y * , F, Υ, θ 1: X * ← X 2: Γ = {1 . . . |X|} search domain is all pixels 3: max_iter = 784•Υ 2•100 4: s = arg max j F(X * ) j source class 5: t = arg max j Y * j target class 6: while s = t &amp; iter &lt; max_iter &amp; Γ = ∅ do 7: Compute forward derivative ∇F(X * ) 8: p 1 , p 2 = saliency_map(∇F(X * ), Γ, Y * ) Modify p 1 and p 2 in X * by θ 10: Remove p 1 from Γ if p 1 == 0 or p 1 == 1 Remove p 2 from Γ if p 2 == 0 or p 2 == 1 12: s = arg max j F(X * ) j 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Adversarial samples generated by feeding the crafting algorithm an empty input. Each sample produced corresponds to one target class from 0 to 9. Interestingly, for classes 0, 2, 3 and 5 one can clearly recognize the target digit.</figDesc><graphic url="image-6.png" coords="9,48.96,50.54,514.06,58.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Adversarial samples obtained by decreasing pixel intensities. Original samples from the MNIST dataset are found on the diagonal, whereas adversarial samples are all non-diagonal elements. Samples are organized by columns each corresponding to a class from 0 to 9.</figDesc><graphic url="image-7.png" coords="10,80.60,76.08,214.44,188.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Success rate per source-target class pair.</figDesc><graphic url="image-8.png" coords="11,311.98,50.54,251.05,194.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Average distortion ε of successful samples per sourcetarget class pair. The scale is a percentage of pixels.</figDesc><graphic url="image-9.png" coords="11,311.98,279.05,251.06,193.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Hardness matrix of source-target class pairs. Darker shades correspond to harder to achieve misclassifications.</figDesc><graphic url="image-10.png" coords="12,48.96,50.54,251.06,212.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Adversarial distance averaged per source-destination class pairs computed with 1000 samples.</figDesc><graphic url="image-11.png" coords="12,311.98,50.54,251.05,214.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Human perception of different distortions ε.</figDesc><graphic url="image-12.png" coords="13,86.04,73.85,190.69,119.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17 :</head><label>17</label><figDesc>Fig. 17: Human perception of different intensity variations θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 6: Example architecture of a feedforward deep neural network with notations used in the paper.</figDesc><table><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>…</cell><cell>1</cell><cell>1</cell><cell cols="2">Notations F: function learned by neural network during training</cell></row><row><cell></cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell><cell cols="2">X: input of neural network Y: output of neural network</cell></row><row><cell></cell><cell>2</cell><cell>2</cell><cell>…</cell><cell></cell><cell></cell><cell cols="2">M: input dimension (number of neurons on input layer) N: output dimension (number of neurons on output layer)</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>…</cell><cell>2</cell><cell>2</cell><cell cols="2">n: number of hidden layers in neural network f: activation function of a neuron</cell></row><row><cell>…</cell><cell>…</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H k</cell><cell>: output vector of layer k neurons</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Indices</cell></row><row><cell>M</cell><cell>m 1</cell><cell>m 2</cell><cell>…</cell><cell>m n</cell><cell>N</cell><cell cols="2">k: index for layers (between 0 and n+1) i: index for input X component (between 0 and N) j: index for output Y component (between 0 and M)</cell></row><row><cell>X</cell><cell cols="3">n hidden layers</cell><cell></cell><cell>Y</cell><cell cols="2">p: index for neurons (between 0 and</cell><cell>m k</cell><cell>for any layer k)</cell></row><row><cell cols="5">Algorithm 1 Crafting adversarial samples</cell><cell></cell><cell></cell></row><row><cell cols="2">X is the benign sample, Y</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* is the target network output, F is the function learned by the network during training, Υ is the maximum distortion, and θ is the change made to features. This algorithm is applied to a specific DNN in Algorithm 2.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A shallow neural network is a small neural network that operates (albeit at a smaller scale) identically to the DL networks considered throughout.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">This means that the algorithm does not require knowledge of the dataset used to train the DNN. Instead, it exploits knowledge of trained parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that we extracted original samples from the dataset for convenience. Any sample can be used as an input to the adversarial crafting algorithm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to warmly thank Dr. Damien Octeau and Aline Papernot for insightful discussions about this work. Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Validation setup details</head><p>To train and use the deep neural network, we use Theano <ref type="bibr" target="#b4">[5]</ref>, a Python package designed to simplify largescale scientific computing. Theano allows us to efficiently implement the network architecture, the training through backpropagation, and the forward derivative computation. We configure Theano to make computations with float32 precision, because they can then be accelerated using graphics processors. Indeed, all our experiments are facilitated using GPU acceleration on a machine equipped with a Xeon E5-2680 v3 processor and a Nvidia Tesla K5200 graphics processor.</p><p>Our deep neural network makes some simplifications, suggested in the Theano Documentation <ref type="bibr">[28]</ref>, to the original LeNet-5 architecture. Nevertheless, once trained on batches of 500 samples taken from the MNIST dataset <ref type="bibr" target="#b26">[27]</ref> with a learning parameter of η = 0.1 for 200 epochs, the learned network parameters exhibits a 98.93% accuracy rate on the MNIST training set and 99.41% accuracy rate on the MNIST test set, which are comparable to state-of-the-art accuracies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Amoroso</surname></persName>
		</author>
		<title level="m">Fundamentals of Computer Security Technology</title>
				<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The security of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="121" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can machine learning be secure?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM Symposium on Information, computer and communications security</title>
				<meeting>the 2006 ACM Symposium on Information, computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference (SciPy)</title>
				<meeting>the Python for scientific computing conference (SciPy)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack: Design issues and research challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page">1460002</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack. Knowledge and Data Engineering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
				<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Poisoning behavioral malware clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop</title>
				<meeting>the 2014 Workshop on Artificial Intelligent and Security Workshop</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cires ¸an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with task learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale malware classification using random projections and neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3422" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evading network anomaly detection systems: formal reasoning and practical techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fogla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM conference on Computer and communications security</title>
				<meeting>the 13th ACM conference on Computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
				<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>Computational and Biological Learning Society</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards deep neural network architectures robust to adversarial examples</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigazio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
				<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>Computational and Biological Learning Society</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM workshop on Security and artificial intelligence</title>
				<meeting>the 4th ACM workshop on Security and artificial intelligence</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How paypal beats the bad guys with machine learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knorr</surname></persName>
		</author>
		<ptr target="http://www.infoworld.com/article/2907877/machine-learning/how-paypal-reduces-fraud-with-machine-learning.html" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
				<meeting>the Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society</title>
				<meeting>the 2014 International Conference on Learning Representations. Computational and Biological Learning Society</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
