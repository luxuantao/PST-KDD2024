<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Deep Domain Adaptation Approach for Robust Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-06">February 6, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sining</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binbin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Unsupervised Deep Domain Adaptation Approach for Robust Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-06">February 6, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">325D26C67364214E92072385E00D3480</idno>
					<idno type="DOI">10.1016/j.neucom.2016.11.063</idno>
					<note type="submission">Received date: 15 June 2016 Revised date: 13 November 2016 Accepted date: 29 November 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>domain adaptation</term>
					<term>robust speech recognition</term>
					<term>deep neural network</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the robust speech recognition problem as an domain adaptation task. Specifically, we introduce an unsupervised deep domain adaptation (DDA) approach to acoustic modeling in order to eliminate the training-testing mismatch that is common in real-world use of speech recognition. Under a multi-task learning framework, the approach jointly learns two discriminative classifiers using one deep neural network (DNN). As the main task, a label predictor predicts phoneme labels and is used during training and at test time. As the second task, a domain classifier discriminates between the source and the target domains during training. The network is optimized by minimizing the loss of the label classifier and to maximize the loss of the domain classifier at the same time. The proposed approach is easy to implement by modifying a common feed-forward network. Moreover, this unsupervised approach only needs labeled training data from the source domain and some unlabeled raw data of the new domain. Speech recognition experiments on noise/channel distortion and domain shift confirm the effectiveness of the proposed approach. For instance, on the Aurora-4 corpus, compared with the acoustic model trained only using clean data, the DDA approach achieves relative 37.8% word error rate (WER) reduction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The increasing availability of multimedia big data, including various genres of speech, is fostering a new wave of multimedia analytics that aim to effectively access the content and pull meaning from the data. Automatic speech recognition (ASR), which transcribes speech into text, serves as a necessary preprocessing step for multimedia analytics. With the help of big data, supercomputing infrastructure and deep learning <ref type="bibr" target="#b0">[1]</ref>, the speech recognition accuracy has been dramatically lifted during the past years <ref type="bibr" target="#b1">[2]</ref>. Besides the Gaussian mixture model -hidden Markov model (GMM-HMM) architecture that dominates the acoustic modeling in speech recognition for many years, artificial neural networks have been historically used as an alternative model but with limited success <ref type="bibr" target="#b2">[3]</ref>. Only recently, neural network has re-emerged as an effective tool for acoustic modeling because of the power of big data and effective learning method <ref type="bibr" target="#b3">[4]</ref>. The DNN-HMM architecture has come to the cen-tral stage in speech recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>, replacing the GMM-HMM architecture. We have witnessed the success of various types of (deep) neural networks (DNNs) not only in speech recognition, but also in visual data processing, data mining and other areas <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Speech is a typical big data: not just in volume, but also noisy and heterogeneous. In practice, we desire a robust speech recognizer that is able to handle noisy data. For many machine learning tasks, including ASR, we usually assume that the training data and the testing data have the same probability distributions. However, real-world applications often fail to meet this hypothesis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. In speech recognition, both the GMM-HMM and DNN-HMM systems are Bayesian classifiers by nature. Theoretical investigation has shown that the trainingtesting mismatch notoriously leads to increase of errors in Bayesian classification <ref type="bibr" target="#b10">[11]</ref>. There are many reasons that lead to the mismatch such as environmental noises, channel distortions <ref type="bibr" target="#b11">[12]</ref> and room reverberations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. To improve the en-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>vironmental robustness of a speech recognizer, a common and efficient approach is multi-condition training <ref type="bibr" target="#b14">[15]</ref> that uses the contaminated noisy data, together with the clean data, in the acoustic modeling training. But it is impossible to cover all kinds of real-world conditions and the mismatch still exists. Therefore, environment robustness is still a big challenge remain unsolved. On the other hand, real-world speech data is heterogeneous. Speech in different domains, e.g., broadcast news, lectures, meeting recordings and conversations, has different characteristics. This causes another mismatch that apparently decrease the speech recognition performance <ref type="bibr" target="#b13">[14]</ref>.</p><p>In order to eliminate the training-testing mismatch, a large number of robust speech recognition methods have been proposed, which in general fall into two categories: feature-space approaches and model-space approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Most approaches need some prior knowledge about the mismatch. For example, noise characteristics have to be known beforehand or clean-noisy speech pairs 1 are needed <ref type="bibr" target="#b17">[18]</ref>. Model adaptation is a typical model-space approach that is quite useful in noise robustness. The acoustic model, e.g., GMM-HMMs, is adapted using the new data either in a supervised manner <ref type="bibr" target="#b18">[19]</ref> or an unsupervised manner <ref type="bibr" target="#b19">[20]</ref>. For feature-space approaches, it is common to combine information about speaker, environment and noise, such as using i-vector <ref type="bibr" target="#b20">[21]</ref>, to acoustic features.</p><p>In this paper, we regard the robust speech recognition problem as a domain adaptation (DA) task <ref type="bibr" target="#b21">[22]</ref>. Learning a discriminative classifier in the presence of the mismatch between training and testing distributions is known as domain adaptation. The essence of the domain adaptation and robust speech recognition is identical, that is, to eliminate the mismatch between the training data and the test data. We find that the speech features yield to different distributions if they come from different domains (such as clean and noisy speech conditions <ref type="bibr" target="#b15">[16]</ref>, and data sets with different genres). Specifically, if we train a DNN acoustic model using clean speech, we discovered that the feature distributions of clean and noisy speech yielded from this acoustic model are significantly different. Hence we would like to embed the domain information during the acoustic model training in order to obtain 1 Noisy speech may be generated manually by adding noises into clean speech.</p><p>a "domain-invariant feature extractor".</p><p>Our work is inspired by a recent DNN based unsupervised domain adaptation approach for image classification <ref type="bibr" target="#b22">[23]</ref>. This deep domain adaptation (DDA) approach combines domain adaptation and deep feature learning within a single training process. Specifically, under a multi-task learning framework, the approach jointly learns one feature extractor and two discriminative classifiers using one single DNN: the feature extractor is trained to extract domain-invariant and classification-discriminative featutes; the label predictor predicts class labels and is used both during training and testing; a domain predictor discriminates between the source and the target domains during training. In order to obtain domaininvariant and classification-discriminative features, the feature extractor sub-network is optimized by minimizing the loss of the label predictor and maximizing the loss of the domain predictor at the same time, which is achieved by a special objective function we defined later. The parameters of two predictor sub-networks are optimized in order to minimize their losses on the training set. Compared with other unsupervised adaptation approaches, the DDA approach is easy to implement by simply augmenting a common feed-forward network with few standard layers and a simple new gradient reversal layer. Moreover, this approach only needs the labeled training data from the source domain and some unlabeled raw data of the new domain. Experiments show that the DDA approach outperforms previous state-of-the-art image classification approaches on several popular datasets <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this study, we introduce the DDA approach to robust speech recognition. Applying DDA to speech recognition is not trivial. This is because speech recognition is a more challenging task as compared with image classification. We elaborate some of the major challenges as follows.</p><p>• The large number of labels: In the typical image classification task in <ref type="bibr" target="#b22">[23]</ref>, the number of classes are only dozens. In contrast, in speech recognition, the class labels are thousands of senones (i.e., phoneme states). The effectiveness of DDA on a large scale classification task like speech recognition desires an intensive study.</p><p>• Decoding: As compared with image classification, speech recognition is a rather complicated task with frame-level classification (clas-</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>sify each speech frame into senone labels) and decoding (Viterbi search from a large graph based on the classified frame labels). The accuracy gain in frame-level classification may not ensure consistent accuracy gain at the word level <ref type="bibr" target="#b23">[24]</ref>.</p><p>• Deeper networks: The neural networks in speech recognition usually have many hidden layers in order to learn highly nonlinear and discriminative features which are robust to irrelevant variabilities.</p><p>To bridge the gap, in this paper, we study how to integrate DDA into acoustic modeling and present a systematic analysis of the performance of DDA in robust speech recognition. Our study shows that the DDA approach can significantly boost the speech recognition performance in both noisy/channel distortion and domain-shift conditions.</p><p>The rest of this paper is structured as follows. Section 2 surveys the related work. Section 3 presents the framework of deep domain adaptation and studies how to use it in the speech recognition task. Experimental settings and results are discussed in Section 4, 5, 6 and finally conclusions are drawn in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>As we just mentioned, robust speech recognition methods can be classified into two categories: feature-space approaches and model-space approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Compared with model-space approaches, feature-space approaches do not need to modify or retrain the acoustic model. Instead, various operations can be performed in the acoustic features to improve the noise (or other distortions) robustness of the features. As for the model-space approaches, rather than focusing on the modification of features, the acoustic model parameters are adjusted to match the testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional methods</head><p>In the feature space, feature normalization is the most straightforward strategy to eliminate the training-testing mismatch. Popular strategies include cepstral mean subtraction (CMS) <ref type="bibr" target="#b24">[25]</ref>, cepstral mean variance normalization (CMVN) <ref type="bibr" target="#b25">[26]</ref> and histogram equalization (HEQ) <ref type="bibr" target="#b26">[27]</ref>. Obviously, speech enhancement methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> can be adopted to remove the noise before speech recognition. But the unavoidable distortions in the enhanced speech may cause another new mismatch problem.</p><p>Rather than updating the features, the acoustic model parameters can be compensated to match the testing conditions. A simple example of updating the models is to re-train them with the new data; or more popular, adding a variety of noise samples to clean training data, known as multistyle or multi-condition training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref>. However, due to the unpredictable nature of real-world noise, it is impossible to account for all noise conditions that may be encountered. Thus adaptive and predictive methods are proposed in the model-space. The adaptive methods update the model parameters when sufficient corrupted speech data are available. Popular methods include maximum a posteriori re-estimation (MAP) <ref type="bibr" target="#b29">[30]</ref> and maximum likelihood linear regression (MLLR) <ref type="bibr" target="#b30">[31]</ref>. In the predictive methods, a noise model is combined with the clean speech models to provide a corrupted speech acoustic model using some model of the acoustic environment. Parallel model combination (PMC) <ref type="bibr" target="#b31">[32]</ref> and vector taylor series (VTS) <ref type="bibr" target="#b32">[33]</ref> fall into this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DNN based methods</head><p>Compared with GMMs, DNNs have an outstanding non-linear learning ability, which makes DNN a more robust acoustic model. Hence the DNN-HMM architecture is inherently noise robust to some extent as compared with GMM-HMM <ref type="bibr" target="#b16">[17]</ref>. However, it is not enough to solve the mismatch problem merely relying on the non-linear learning ability. Recently, many methods in the feature and model spaces have been proposed to make DNN-HMM more robust to the mismatched test data. In order to account for the mismatch, many useful auxiliary features, reflecting environmental noise and speaker information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, are combined with acoustic features as the DNN input. Neural networks can be used as a speech enhancement tool. In <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, a denoising autoencoder (DAE) is adopted to reconstruct clean speech features from noisy ones. This kind of method needs stereo data, i.e., clean speech and corresponding noisy speech, to train the denoising DNN. DNN feature enhancement and DNN acoustic model can be trained jointly <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Multi-task training is another popular strategy to improve the robustness of the acoustic model <ref type="bibr" target="#b37">[38]</ref>. By adding one or more auxiliary output layers in</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>the DNN and optimizing several tasks (e.g., main task: prediction of senone labels, side task: denoising) at the same time, the network gains more robustness <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Domain Adaptation for Robust ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Model</head><p>We treat the training-testing mismatch problem as a domain adaptation task, bridging the target (testing) and the source (training) domains. The main purpose of deep domain adaptation (DDA) <ref type="bibr" target="#b22">[23]</ref> is to embed the domain information into the process of learning representation, so that the final classification decisions are made based on features that are both discriminative and invariant to the changes of domains. This means the representation learned by the DNN classifier has the same or very similar distributions in the source and the target domains.</p><p>Assume that the neural network model works with input samples x ∈ X and certain labels y ∈ Y where X and Y are input space and output space, respectively. Here in speech recognition, x and y are framewise acoustic features and senones (phoneme states), respectively. There are two distributions S(x, y) and T(x, y) on X ⊗ Y , which are referred to as the source distribution (for training) and the target distribution (for testing) and both the two distributions are assumed complicated and unknown. Due to domain shift, S and T are similar but different.</p><p>In the training-testing mismatch scenario, we train the model with S(x, y), but we test the model with the data yields to distribution T(x, y). However, we can access to many training samples {x 1 , x 2 , ..., x N } from source domain and target domain according to the marginal distributions S(x) and T(x). Denote with d i ([0, 1] or <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>) the (domain label ) for the i-th sample, which indicates whether x i comes from the source domain</p><formula xml:id="formula_2">(x i ∼ S(x) if d i = [1, 0]) or from the target domain (x i ∼ T(x) if d i = [0, 1]).</formula><p>The unsupervised deep domain adaptation architecture <ref type="bibr" target="#b22">[23]</ref> is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The architecture is simply based on a feed-forward neural network. But different from a common one, this network has two output layers, which are the main class label y ∈ Y and the domain label <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>}. Specifically, this model is decomposed into three parts to perform different mappings: a feature extractor G f , a label predictor G y and a domain predictor G d .</p><formula xml:id="formula_3">d ∈ {[0, 1],</formula><p>More formally, the mapping functions are:</p><formula xml:id="formula_4">f = G f (x; Θ f );<label>(1)</label></formula><formula xml:id="formula_5">y = G y (f; Θ y );<label>(2)</label></formula><formula xml:id="formula_6">d = G d (f; Θ d );<label>(3)</label></formula><p>where Θ f , Θ y , Θ d are the parameters of the network (in Figure <ref type="figure" target="#fig_0">1</ref>) and f is a D-dimension feature vector.</p><p>Our aim is to jointly train G f , G y and G d . Specifically, we want to seek Θ f to minimize the label prediction loss and to maximize the domain classification loss at the same time. The maximization of the domain classification loss is actually to make the two feature domain distributions as similar as possible. Meanwhile, In order to assure the domain classification, the Θ d has to make the mapping G d perform well in domain classification. This leads to the loss function of this network:</p><formula xml:id="formula_7">E(Θ f , Θy, Θ d ) = i=1,...N d i =[1,0] Ly(Gy(G f (xi; Θ f ); Θy), y i ) - λ i=1,...N L d (G d (G f (x i ; Θ f ); Θ d ), d i ) = i=1,...N di=[1,0] L i y (Θ f , Θ y ) -λ i=1,...N L i d (Θ f , Θ d )<label>(4)</label></formula><p>where L y (., .) and L d (., .) are loss functions for label and domain predictors respecitvely, while L i y (., .) and L i d (., .) denote the loss of the i-th training sample. Loss functions can be cross entropy or mean square error function depends on the tasks. λ is a positive hyper parameter used to trade off two losses in practice. Frankly, the similar loss functions are common used in many other machine learning task <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>According to the loss function derived from Section 3.1, we can optimize the DDA network using an approach similar to stochastic gradient decent (SGD) <ref type="bibr" target="#b41">[42]</ref>. The aim of the optimization is to seek the optimized parameters that: </p><formula xml:id="formula_8">( Θf , Θy ) = arg min Θ f ,Θy E(Θ f , Θ d , Θ y ),<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Θd = arg max</head><formula xml:id="formula_9">Θ d E(Θ f , Θ d , Θ y ). (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Although Θ d is optimized by maximizing Eq (4), it equals to minimize the second item of Eq (4). So Θ d will make sure the performance of domain predictor. Θ f is optimized by minimizing the first item and maximizing the second item (because of the minus symbol). This training strategy will keep the feature extracted from the neural network domaininvariant and classification-discriminative. Under the multi-task learning framework, the following equations are used to update the parameters:</p><formula xml:id="formula_11">Θ f ← Θ f -µ( ∂L i y ∂Θ f -λ ∂L i d Θ f )<label>(7)</label></formula><formula xml:id="formula_12">Θ d ← Θ d -µ ∂L i d ∂Θ d (8) Θ y ← Θ y -µ ∂L i y ∂Θ y (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where µ is step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Applying DDA to Speech Recognition</head><p>State-of-the-art ASR systems are Bayesian classifiers by nature. A typical speech recognition system can be formulated as a simple equation:</p><formula xml:id="formula_14">Ŵ = argmax W∈L P (X|W)P (W)<label>(10)</label></formula><p>where W = {w 1 , w 2 , . . . } is a possible word sequence in langauge L, X = {x 1 , x 2 , . . . } is the observation sequence with frame-level acoustic feature x, P (X|W) is the acoustic model and P (W) is the language model. Therefore speech recognition (or decoding) is to find out the optimal word sequence Ŵ that maximizes the joint acoustic and language probabilities. As for the language model, word level N -gram model <ref type="bibr" target="#b42">[43]</ref>, trained from a large set of textual data, is usually used. The acoustic model is often built at fine-grained phoneme (subword) level, trained from labelled speech data with transcripts. The distribution of speech data is complex and the speech production is apparently a dynamic process. Traditionally, hidden Markov models (HMMs) are used to model this dynamic process in a phoneme through state transitions, while Gaussian mixture models (GMMs) are used to depict the distribution of speech data at HMM state level (sub-phoneme or so-called senone). This is the so-called GMM-HMM architecture. In practice, context-dependent models, e.g., triphones, are used to model the important coarticulation phenomenon in speech production. Recently, neural networks have re-emerged as a powerful acoustic modeling tool with superior performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>, replacing GMMs to depict the distribution of speech data, namely the DNN-HMM architecture. Either GMM-HMM or DNN-GMM, if the distributions of the training data and the test data have some differences, the error of the Bayesian classifier will be increased <ref type="bibr" target="#b10">[11]</ref>. Hence in this study, we use the unsupervised deep domain adaptation (DDA) strategy to adjust the acoustic model during the training time. Our purpose is to let the DNN acoustic model learn similar distributions both in the training data and the test data, which may increase the robustness of the Bayesian classifier.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows how to use the DDA strategy in speech recognition. A speech recognition system is composed of an acoustic model training stage<ref type="foot" target="#foot_0">2</ref> and a testing stage. In the acoustic model training stage, the first step is to extract acoustic fea-tures (represented by input vector x in Figure <ref type="figure" target="#fig_1">2</ref>), such as MFCC or FBank, for the training speech samples. Then the acoustic feature sequences are used to train triphone GMM-HMM acoustic models (so-called senones). The GMM-HMM models are just used to perform forced alignment to the training samples, obtaining the labelled training samples (speech frame and its corresponding senone label). Within the pairwise frame-label data, a DNN acoustic model is thus learned that classifies the input frame-level acoustic vector into senone label. In this process, we can use the DDA approach to learn the senone label classifier and the domain classifier at the same time using the labelled training data and some of the unlabelled raw data from the testing domain. At the test stage, the domain predictor is discarded and we only use the senone predictor as the acoustic model.</p><p>Given the predicted senone label scores, a speech recognizer still needs a decoder to obtain the best word sequence. As we mentioned in the beginning of this section, decoding involves not only an acoustic model, but also a language model. The acoustic score and the language score are combined in the decoding process for the decision of the final word sequence. Here we use the weighted finitestate transducers (WFST) <ref type="bibr" target="#b23">[24]</ref> based static decoder to do the combination. In order to compose the decoding WFST, apart from the acoustic model and the language model, a lexicon and the context are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>also needed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. Using the compose operation in WFST, the different level representations are integrated in just one WFST graph, which maps the HMM states to words. For efficiency reasons, token passing <ref type="bibr" target="#b44">[45]</ref> and beam search algorithms are ofen applied in the decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments for Noise/Channel Robustness</head><p>We evaluate the noise robustness of DDA on Aurora-4 <ref type="bibr" target="#b14">[15]</ref>, a popular corpus for robust ASR research. Aurora-4 is designed to verify the effectiveness of robust ASR methods on a medium vocabulary continuous speech recognition task. There are two different training conditions: (1) clean training condition, which includes 7138 utterances recorded with the primary microphone without any added noise or distortions; and (2) multi-condition training condition, including the same 7138 utterances, but with one half of the data was recorded by the primary microphone and the other half recorded using the second microphone; all are contaminated with six types of added noises at 10-20 dB SNR. In order to investigate different noise/channel distortion conditions, the Aurora-4 test set is composed of four subsets.</p><p>• Subset A (Clean): 330 clean utterances without any noises or distortions, recorded with the primary microphone;</p><p>• Subset B (Noise): 330 × 6 utterances, by corrupting Subset A with six different noises;</p><p>• Subset C: (Channel distortion): 330 utterances, same as Subset A, but recorded with the second microphone, without any added noises.</p><p>• Subset D (Noise+Channel distortion): 330 × 6 utterances, by corrupting Subset C with six different noises</p><p>All the speech files are sampled at 16KHz, quantified by 16 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Clean condition training with multi-condition testing</head><p>This experiment is designed to evaluate the robustness of the DDA approach in mismatched training-testing condition: acoustic model is trained using clean speech while tested in multiple conditions with contaminated speech. Specifically, we use the clean-condition training set of Aurora-4, which includes 7138 utterances, to train a triphone GMM-HMM acoustic model. The acoustic feature is 39-dim MFCC. Then the GMM-HMM acoustic model is used to align the training data to obtain the triphone state (senones) labels.</p><p>After that, two different DNN-HMM acoustic models are trained: the conventional DNN-HMM model trained with a standard feed-forward network and the new DNN-HMM model trained using the DDA approach in Figure <ref type="figure" target="#fig_0">1</ref> Because the data from the target domain does not have senone labels, we randomly generate senone labels for the target domain data in order to train the model in a uniform framework. Specifically, we use a binary flag to control if the errors of the current frame is used to optimize the feature extractor and the senone labels predictor or not. If the current frame comes from the target domain, the senone predictor errors are thus discarded. As for the domain predictor, we also have two domain labels to predict. Although there are various kinds of noises in our training data, we do not distinguish them because we do not want to use too much priori knowledge of the data. Hence for simplicity, there are just two class labels to predict (clean and noise).</p><p>For the two DNN-HMM systems, the input layer is a context window of 11 frames of 40-dim FBANK with delta and acceleration coefficients (40×3×11). The G f part of the network has 6 hidden layers with 1024 units in each layer. We also compare our approach with a state-of-the-art approach -DNN-PP <ref type="bibr" target="#b34">[35]</ref>. Two DNNs are used in this approach <ref type="bibr" target="#b34">[35]</ref>: speech enhancement DNN and acoustic model DNN. The first DNN, as a pre-processor for denoising, trained with clean-noisy speech pairs. All the training data, including clean and noisy samples, go through the first DNN and then used for DNN acoustic model (the second DNN) training. Apart from these experiments, we also experiment with the semi-supervised method for compar- ison. For the target domain data, we do not have senone labels. Hence we first decode the unlabeled target data using the Clean-DNN-HMM model and get the senone labels. Please note that the resultant senone labels do have inevitable errors. The adapted model, namely Semi-Ada-DNN-HMM, is then obtained by fine-tuning the the Clean-DNN-HMM acoustic model using these labels. The Semi-Ada-DNN-HMM model is used to test the target domain test data.</p><formula xml:id="formula_15">A C C E P T E D M A N U S C R I P T</formula><p>Table <ref type="table" target="#tab_0">1</ref> shows the experimental results. From the results, we notice that the Clean-DNN-HMM model, which is trained using clean data, performs badly under noisy and channel mismatch conditions. The word error rate sharply increases from 3.36% to 50.73% when the system encounters both noise and channel distortions. Meanwhile, we clearly observe that the DDA-DNN-HMM model consistently reduces the word error rates for all testing subsets. Especially for the most challenging condition, i.e., subset D (with both noise and channel distortion), the WER is significantly dropped from 50.73% to 34.55%. In average, DDA-DNN-HMM achieves relative 37.8% WER reduction (from 36.22% to 22.53%). Our approach is even better than the Semi-Ada-DNN-HMM model. This is because of the inevitablely wrong senone labels used for model fine-tuning in the semi-supervised approach. The average WER of DDA-DNN-HMM is even close to DNN-PP <ref type="bibr" target="#b34">[35]</ref>, a method that needs pairwise clean-noisy data for front-end speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of Hyper-parameters</head><p>We also investigate the impacts of hyperparameters λ, the position of feature representation layer f and the amount of adaptation data. Their impacts are depicted in Figure <ref type="figure" target="#fig_4">3</ref>. Figure <ref type="figure" target="#fig_4">3 (a)</ref> shows how λ affects the average WER. When λ = 0, the DDA-DNN-HMM model becomes the Clean-DNN-HMM model, in which the domain predictor is not working. We can see that WER goes down with the increase of λ and the lowest WER is achieved when λ = 0.45. On the contrary, when we set λ a value below zero, WER increases. This is because the domain difference is enlarged when λ is set to a negative value, as seen in Eq. ( <ref type="formula" target="#formula_9">6</ref>). Another factor which may affect the DDA-DNN-HMM acoustic model is the position where we put the feature layer f. If we regard the G f and G y as an whole network and change the position of feature representation layer from top (near to softmax layer of G y ) to down (near to the input of G f ), we find that WER increases as shown in Figure <ref type="figure" target="#fig_4">3</ref> (b). Figure <ref type="figure" target="#fig_4">3</ref> (c) shows the relationship between WER and the amount of adaptation data. We find that the performance improves with the increase of adaptation data. But beyond 4000 adaptation utterances, the performance gain becomes very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-condition training with surprise noise</head><p>testing As we pointed out in Section 2, multi-condition training is an effective approach to improve the robustness of an ASR system. This is achieved by training the acoustic model using contaminated speech. Hence the distributions of the training data and test data become identical or similar. However, in real-world, multi-condition training cannot cover all types of contamination (noise or channel distortion). We carry out an experiment to check if the DDA approach still works when the the multicondition trained ASR system encounters some surprise types of noise. In the experiment, test data is derived by adding three kinds of new noise to the clean test data with 5-10 dB SNR <ref type="foot" target="#foot_1">3</ref> . The multicondition DNN-HMM, denoted as MultiCon-DNN-HMM, is trained only using the multi-condition training data from Aurora-4. The DDA-DNN-HMM is trained using the multi-condition training and 3000 noisy utterances corrupted by the three new noises. The network is the same with that in Section 4.1. Results are summarized in Table <ref type="table" target="#tab_1">2</ref>. We notice that multi-condition training is quite effective and the WER of MultiCon-DNN-HMM is significantly decreased as compared with the Clean-DNN-HMM in Table <ref type="table" target="#tab_1">2</ref>. But with the DDA approach, the WER is further reduced from 8.22% to 7.45% and relative WER reduction of 9.36% is thus achieved.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments for Domain Shift</head><p>As we discussed in Section 1, real-world speech is heterogeneous with different genres. We test the proposed DDA approach to see if it shows robustness when the speech recognizer is used in another domain.</p><p>In this experiment, we regard the WSJ <ref type="bibr" target="#b45">[46]</ref> and Librispeech <ref type="bibr" target="#b46">[47]</ref> corpus as data from different "domains". The WSJ0 and WSJ1 corpus 4 consist primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text. WSJ0 includes a 5000-word text while WSJ1 includes a 20000-word text. Each 4 The WSJ corpus contains WSJ0 and WSJ1.</p><p>utterance was recorded in two channels: a highquality "primary" microphone (a head-mounted, noise-cancelling Sennheiser HMD410), and an additional microphone (desk-mounted Crown or other). The total duration of WSJ0 and WSJ1 are about 80 hours. LibriSpeech is a 1000-hours corpus derived from audiobooks that are part of the LibriVox Project. The WSJ and LibriSpeech corpus can be used to train large vocabulary continuous speech recognition (LVCSR) acoustic models.</p><p>We first train a GMM-HMM acoustic model according to the configuration in <ref type="bibr" target="#b46">[47]</ref>, resulting in 3414 senones. Then, we train a DNN-HMM acoustic model using the 80-hour WSJ data as a baseline system. After that, we train a DDA-DNN-HMM acoustic model using 80-hour WSJ data (with senone labels) and 40-hour adaptation data from Librispeech (without senone labels) out of 500hour "train-other-500" subset. The DNN has the same topology with the one used in Section 4. We use the 5.4-hour Librispeech "test-other" set for testing. Table <ref type="table" target="#tab_2">3</ref> shows the results on this test set. We can see that about 6.9% relative WER reduction is achieved when the DDA approach is used. This confirms that the proposed approach shows robustness to domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis</head><p>As we mentioned in Section 3.1, our purpose is to learn domain-invariant feature representations which have the same or similar distributions in the source and the target domains. In our experiments,  we regard the training data as the target domain and the test data as the target domain. The learned feature representations, denoted as f in Figure <ref type="figure" target="#fig_0">1</ref>, can be visualized for analysis. The dimension of this representation is 1024 in our model and we randomly choose two dimensions to visualize. To this end, we feed some clean speech frames and corresponding noisy speech frames to Clean-DNN-HMM and DDA-DNN-HMM models, respectively, discussed in Section 4.1 and the two feature dimensions are plotted in Figure <ref type="figure" target="#fig_6">4</ref>. From the top figure in Figure <ref type="figure" target="#fig_6">4</ref>, it is obvious that the representations of clean speech (denoted as red points) and noisy speech (denoted as green points) obtained by Clean-DNN-HMM acoustic model have very different distributions, which shows the mismatch between the training and test data. In contrast, this difference in distributions clearly becomes smaller for DDA-DNN-HMM, in which the deep domain adaptation approach effectively narrows the training-testing mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we has addressed the trainingtesting mismatch problem in speech recognition us-ing an unsupervised deep domain adaptation approach. Through a multi-task learning framework, a deep neural network feature extractor is learned by minimizing the loss of the phoneme classifier (main task) and to maximize the loss of the domain classifier (second task) at the same time. Specifically, during the acoustic model training, the domain classifier tries to eliminate the differences of data distribution between the source and the target domains. This approach significantly improves the performance of DNN acoustic model using some unlabeled data from the new domain. When evaluated in the "clean condition training and multicondition testing" scenario on Aurora-4 corpus, the proposed approach decreases the word error rate from 36.22% to 22.53%, with 37.8% relative error reduction. In the domain shift experiment, the approach achieves 6.9% relative word error rate reduction. Analysis shows that the performance gain comes from the elimination of the mismatches between the distributions of the training and testing data. In the future work, we plan to implement the domain adaptation approach in convolutional neural network (CNN) <ref type="bibr" target="#b47">[48]</ref> and recurrent neural networks (RNN) <ref type="bibr" target="#b48">[49]</ref> that have shown superior performances in speech recognition. We also</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>want to investigate the performances if treating different types of noises as different domains in the DDA framework. We notice that a recent multitasking training (MTL) approach has similar idea with our proposed DDA approach. In <ref type="bibr" target="#b49">[50]</ref>, an MTL approach is proposed to simultaneously predict the class label and the clean speech from the noisy speech input. We plan to experimentally compare the DDA approach with this MTL approach in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Unsupervised deep domain adaptation architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The DDA approach used for robust ASR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. For clarity, they are named as Clean-DNN-HMM and DDA-DNN-HMM, respectively. The Clean-DNN-HMM model is trained using all the 7138 clean-condition training utterances, as a baseline model. The training data of DDA-DNN-HMM consists of two parts: 7138 clean-condition utterances with senone labels and 3000 multi-condition utterances without senone labels. The clean-condition utterances are used to train the whole network (G f , G y , G d ) while the multi-condition utterances are used to train the feature extractor and the domain classifier (G f , G d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Impact of adaptation data size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relationship between WER and (a) hyper-parameter λ, (b) position of feature representation layer and (c) the amount of adaptation data. For comparison, the blue dotted line represents the WER of Clean-DNN-HMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of learned feature representations of Clean-DNN-HMM and DDA-DNN-HMM. The top figure is obtained by feeding the clean and corresponding noisy speech to the Clean-DNN-HMM acoustic model described in Section 4.1. The bottom figure is obtained by feeding the same clean and noisy speech to the DDA-DNN-HMM acoustic model. We only visualize two dimensions for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results for clean condition training with multi-condition test on Aurora-4 in terms of WER (Word Error Rate). The hyper-parameter λ = 0.45 for DDA-DNN-HMM.</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>Avg.</cell></row><row><cell>Clean-DNN-HMM</cell><cell cols="2">3.36 29.74</cell><cell cols="3">21.02 50.73 36.22</cell></row><row><cell>DDA-DNN-HMM</cell><cell cols="2">3.24 14.52</cell><cell cols="3">17.82 34.55 22.53</cell></row><row><cell cols="3">Semi-Ada-DNN-HMM 4.13 17.55</cell><cell cols="3">15.67 37.73 25.11</cell></row><row><cell>DNN-PP [35]</cell><cell>5.1</cell><cell>12.0</cell><cell>10.5</cell><cell>29.0</cell><cell>18.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results for multi-condition training with surprise noise testing on Aurora-4.</figDesc><table><row><cell>Model</cell><cell>WER (%)</cell></row><row><cell cols="2">MultiCon-DNN-HMM 8.22</cell></row><row><cell>DDA-DNN-HMM</cell><cell>7.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experimental results for domain shift. The DDA-DNN-HMM acoustic model is trained using 80h WSJ labelled data and 30h LibriSpeech unlabelled data.</figDesc><table><row><cell>Model</cell><cell>WER (%)</cell></row><row><cell>Baseline</cell><cell>31.19</cell></row><row><cell cols="2">DDA-DNN-HMM 29.40</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>A language model is also needed, but its training is out of the scope of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>These three types of noise are from another noise dataset and they are totally different with the noises in Aurora-4.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Yaroslav Ganin for the constructive discussions when performing this study.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep learning: Methods and applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Li Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of hybrid ann/hmm models for automatic speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Trentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="126" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arch: Adaptive recurrent-convolutional hybrid networks for long-term action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="87" to="102" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Finite-time recurrent neural networks for solving nonlinear optimization problems and their application</title>
		<author>
			<persName><forename type="first">P</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<pubPlace>Neurocomputing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Robust finite-time h??? control for a class of uncertain switched neural networks of neutral-type with distributed time varying delays</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saravanan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Nedjah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M G</forename><surname>França</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Macedo Mourelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Weightless neural systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="806" to="814" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Techniques for noise robustness in automatic speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An overview of noise-robust automatic speech recognition, Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="745" to="777" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reverberation robust acoustic modeling using i-vectors with time delay neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTER-SPEECH. ISCA</title>
		<meeting>INTER-SPEECH. ISCA</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A summary of the reverb challenge: state-of-the-art and remaining challenges in reverberant speech processing research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Noise-robust speech recognition using deep neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="7398" to="7402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An investigation into using parallel data for far-field speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adaptation and discriminative training of acoustic models, Techniques for Noise Robustness in Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deléglise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="283" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust automatic speech recognition using acoustic model adaptation prior to missing feature reconstruction</title>
		<author>
			<persName><forename type="first">U</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="535" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stafylakis, I-vectorbased speaker adaptation of deep neural networks for french broadcast audio transcription</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6334" to="6338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The use of cepstral means in conversational speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Westphal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>EUROSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature space normalization in adverse acoustic conditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Molau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hilger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">656</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">P T tion for noise robust large vocabulary speech recognition, Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hilger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney ; C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="845" to="854" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Quantile based histogram equaliza-A</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction, Acoustics, Speech and Signal Processing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating rasta-plp into speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994. 1994. 1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">421</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains, Speech and audio processing</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ieee transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Maximum likelihood linear transformations for hmm-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer speech &amp; language</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Parallel model combination for speech recognition in noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A vector taylor series approach for environment-independent speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for noise reduction in robust asr</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Robust speech recognition with speech enhanced deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint training of front-end and back-end deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4375" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Two-stage noise aware training using asymmetric deep denoising autoencoder</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task jointlearning of deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>recent Developments on Deep Big Vision</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal deep autoencoder for human pose recovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5659" to="5670" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view ensemble manifold regularization for 3d object recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The kaldi speech recognition toolkit, in: IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Signal Processing Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Token passing: a simple conceptual model for connected speech recognition systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thornton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based csr corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lib-Rispeech</forename></persName>
		</author>
		<imprint>
			<publisher>AN ASR CORPUS BASED ON PUBLIC DOMAIN AUDIO BOOKS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
	<note>2012 IEEE International Conference</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<title level="m">Neural network adaptive beamforming for robust multichannel speech recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Proc. Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
