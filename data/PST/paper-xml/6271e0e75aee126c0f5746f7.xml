<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieval-Enhanced Machine Learning</title>
				<funder ref="#_ehppvQT">
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
							<email>zamani@cs.umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
							<email>diazf@acm.org</email>
						</author>
						<author>
							<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
							<email>dehghani@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<email>metzler@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Retrieval-Enhanced Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3477495.3531722</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems ? Information retrieval</term>
					<term>? Computing methodologies ? Machine learning</term>
					<term>Retrieval Augmentation</term>
					<term>Memory Augmentation</term>
					<term>Knowledge Grounding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although information access systems have long supported people in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The vast majority of existing machine learning (ML) systems are designed to be self-contained, with both knowledge and reasoning encoded in model parameters. Consequently, increasing the capacity of machine learning models by increasing their parameter size generally leads to higher accuracy <ref type="bibr" target="#b16">[17]</ref>. For example, the number of parameters used in state-of-the-art language models has increased from 94 million in ELMo <ref type="bibr" target="#b44">[45]</ref> to 1.6 trillion in Switch Transformers <ref type="bibr" target="#b12">[13]</ref>, an over 16? increase in just three years <ref type="bibr">(2018 -2021)</ref>. Despite these successes, improving performance by increasing the number of model parameters can incur significant cost and limit access to a handful of organizations that have the resources to train them <ref type="bibr" target="#b3">[4]</ref>. As such, focusing model development on the number of parameters is neither scalable nor sustainable in the long run.</p><p>Motivated by recent work demonstrating both that high capacity models memorize training data <ref type="bibr" target="#b5">[6]</ref> and that using retrieval-style methods can offload memorization to storage <ref type="bibr" target="#b4">[5]</ref>, we propose the augmenting ML models with access to stored information through information retrieval (IR) techniques. Whereas IR has proven an effective tool to support people accessing large text corpora, we believe that IR can be extended to support machines accessing not just large text corpora but more abstractly-represented knowledge stores. By designing machine learning architectures that have explicit access to an information retrieval system, we can decouple reasoning from memory, reducing the required model parameters and leveraging the efficiency, scalability, and effectiveness of IR techniques. We refer to this class of approaches as retrieval-enhanced machine learning (REML). In this paper, we describe how core principles of indexing, representation, retrieval, and ranking can be used to develop REML models.</p><p>Using retrieval to improve model accuracy is not without precedent. Predating modern machine learning methods, the IR community developed some of the earliest known retrieval-enhanced machine learning models. For example, pseudo-relevance feedback <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> leverages a retrieval system to analyze results of an 'initial' search query before producing a final ranking. This purely algorithmic use of a retrieval system in order to improve ranking model performance foreshadows its usefulness in modern applications. More recently, natural language processing models that incorporate retrieval capabilities have been shown to improve model performance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. Although leveraging rather basic retrieval models, these approaches present an opportunity for ML systems to be further improved with more sophisticated IR methods.</p><p>We introduce a generic framework that enables ML models to be augmented with IR capabilities that support querying a corpus for useful information, utilizing retrieved results, providing feedback to the retrieval model, and, if necessary, storing information for future access. This framework is flexible enough to both represent several existing ML models and scaffold future models.</p><p>This paper is organized in order to motivate, describe, and ground REML as a research program. We begin in Section 2 by describing the motivation for REML, specifically demonstrating why IR techniques provide a unique opportunity for ML. In Section 3, we discuss the challenges in developing each component of the proposed framework and suggest three categories of optimization approaches for REML models: (1) independent optimization of prediction and retrieval models, (2) their conditional optimization, and (3) their joint end-to-end optimization. Using this framework, in Section 4, we review several existing ML models in order to draw connections to REML. And, although these related models suggest the potential benefit of REML, substantial open research questions limit the applicability and effectiveness of contemporary IR methods. In Section 5, we conclude with a broad research program in REML, touching on the opportunity for the different subareas of IR research to contribute to the advancement of ML model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>Despite the success of modern high capacity models, focusing on the number of parameters as a primary mechanism to improve performance can be brittle, unsustainable, and opaque <ref type="bibr" target="#b3">[4]</ref>. We argue that these concerns can be addressed by developing ML models that, instead of encoding knowledge in parameters, can access large collections of information items using efficient, effective, and robust retrieval technologies. Some of the major applications of REML is presented below: Generalization. Recent work has shown that many ML models can significantly benefit from simple retrieval augmentation approaches. For instance, KNN-LM <ref type="bibr" target="#b31">[32]</ref> linearly interpolates large language model predictions with the nearest neighbors of the given context input. This approach does not even require further training or finetuning. The authors showed substantial improvements in terms of language model perplexity in both in-distribution and out-ofdistribution test sets, demonstrating the generalizability of this approach. KNN-LM together with several other examples reviewed in Section 4 suggest that enhancing ML models using retrieval models will have a large impact on the generalizability of the models. Retrieval enhancement is expected to have large impact on domain adaptation, zero-shot, and few-shot learning tasks. Scalability. ML models compress information from training data to support accurate prediction at inference time. Although increasing model capacity by adding parameters often translates into an improvement in predictive power, recent studies demonstrate that large deep learning models often memorize training instances and concepts associated with them in their model parameters <ref type="bibr" target="#b5">[6]</ref>. As an alternative to such implicit memorization, retrieval systems can explicitly store information either directly from the training set or from concepts derived during the learning process. Because retrieval architectures are often designed to scale, a retrieval system can provide efficient access to this information, substantially reducing the need for high capacity models and increasing throughput. Collection Updates and the Temporal Aspect. Current ML models make predictions solely based on the data observed during training. Although effective in stationary domains, this approach can be brittle in nonstationary domains, such as news, where new information constantly emerges. And, while periodic retraining is possible in some slowly-changing domains, for quickly-changing domains, this solution is impractical. An information access system can decouple reasoning from knowledge, allowing it to be maintained and updated independent of model parameters at a cadence aligned with the corpus. Interpretability and Explainability. Because the knowledge in training data is encoded in learned model parameters, explanations of model predictions often appeal to abstract and difficultto-interpret distributed representations. By grounding inference on retrieved information, predictions can more easily be traced specific data, often stored in a human-readable format such as text. On-Device Machine Learning. State-of-the-art ML models require significant computational power and memory availability, which are not available on devices such as smartphones. Retrievalenhanced ML models can potentially decouple memorization from generalization and store a large collection (memory) of information items on a remote server. Thus, a small, efficient ML model can be hosted on-device. By minimizing the interactions between the retrieval component and the ML model, this can potentially revolutionize the applications of on-device machine learning. If privacy is an issue, the information items stored on the remote server can be encrypted and methods, such as the recently developed distancepreserving encryption schemes for nearest neighbor search <ref type="bibr" target="#b15">[16]</ref>, can be adopted for privacy-preserving retrieval.</p><p>Collectively, these properties of IR techniques suggest the development of REML, which we pursue in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RETRIEVAL-ENHANCED MACHINE LEARNING</head><p>This paper focuses on predictive ML models. Let X be the input (feature) space for the task and Y be the output (prediction) space. Given an input ? ? X, a ML model produces a prediction in the output space ? ? Y. Supervised learning models are often trained by minimizing an empirical prediction loss (error) over instances in a training set ? = {(?, ?) ? X ? Y}.</p><p>Retrieval-enhanced machine learning (REML) refers to models composed of two coupled components: one model that makes predictions by communicating with ? models each mediating access to a repository of information or knowledge. A REML model is defined as ? ? (?;</p><formula xml:id="formula_0">? ? 1 , ? ? 2 , ? ? ? , ? ? ? ). The model ? ? parameterized by</formula><p>? is called the prediction model and ? ? ? denotes the ? th information access model parameterized by ? ? . Thus, to produce ?, the prediction model can interface with ? information access models. Each ? ? ? includes a collection or repository ? ? that is available-through an information access model-to the prediction model. This repository could be composed of natural language documents-as with text retrieval-or some other indexed representation. As such, ? ? s reflect a large set of parameters available to the model that can be leveraged ad hoc, as with many non-parametric and lazy learning techniques. The goal of retrieval-enhanced supervised learning models is to minimize the empirical risk,</p><formula xml:id="formula_1">1 |? | ?? (?,?) ?? L ? ? (?; ? ? 1 , ? ? 2 , ? ? ? , ? ? ? ), ?<label>(1)</label></formula><p>where L is a loss function for each training instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We define the following necessary requirements (Reqs) for REML:   lize the response returned by the information access models for making predictions. Considering these three requirements, we can envision the first category of REML models. A high-level overview of models in this category is presented in Figure <ref type="figure" target="#fig_1">1</ref>(a). Most existing retrievalenhanced ML models, such as REALM <ref type="bibr" target="#b20">[21]</ref>, belong to this category.</p><p>REML may also benefit from two additional optional properties: Opt 1 Storing: the prediction model may store some information items in a memory for future access during both training and inference. Such information items will be accessible to the model through querying (Req 1). Opt 2 Feedback: the prediction model may be able to provide feedback to the information access models. This enables the information access models to improve based on the feedback. Figure <ref type="figure" target="#fig_1">1</ref>(b) depicts the second category of REML models that take advantage of Opt 1 by storing information in a memory and accessing the information later. On the other hand, Figure <ref type="figure" target="#fig_1">1(c)</ref> demonstrates a high-level overview of the third category of REML models that can provide feedback (Opt 2) to the information access systems. The last category (Figure <ref type="figure" target="#fig_1">1(d)</ref>) implements both of these optional properties and supports querying, utilizing retrieval responses, storing information, and providing feedback to the information access systems.</p><p>Based on these requirements and optional properties, Figure <ref type="figure">2</ref> envisions a generic framework for REML. The framework consists of two major parts: the prediction model ? ? and the information access models ? ? ? s. For each input ?, the model ? ? may decide to run multiple retrieval processes by either submitting multiple queries, accessing multiple data repositories and/or memories, providing feedback to the information access component, or a combination of the above. The number of retrieval processes can be zero for some inputs, and thus REML generalizes typical predictive modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Access in REML</head><p>In its most generic form, each information access system in the proposed REML framework consists of five components: (1) Query Generation, (2) Retrieval Model, (3) Response Processing, (4) Feedback Handler, and (5) Storage Handler. In the following subsections, we discuss potential implementations for each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Query Generation.</head><p>In current information access systems, queries mostly take the form of unstructured text (e.g., keyword queries or natural language questions), structured query language (e.g., SQL), or multi-media items (e.g., images). Such query languages and formats can be also adopted by retrieval-enhanced ML models. The Query Generation component is responsible for generating one of these query formats. Note that depending on the application and due to efficiency or effectiveness requirements, one may simply cast the query generation problem to query selection from a set of pre-defined queries. In either case, the Query Generation (or Selection) component should be able to translate the information need of the prediction model ? ? to a query language or format that can be efficiently processed by the information access model ? ? ? . Since retrieval models accessible by ? ? may accept different query languages, the Query Generation component may be unique to each retrieval model.</p><p>Existing information access systems are designed for people and, therefore, existing query formats (mentioned above) are understandable by people. In the context of REML, we can relax the requirement of an interpretable query language. Besides the common query languages and formats, the prediction models can produce any latent representation (e.g., a high-dimensional dense vector) as a query. For instance, any hidden layer representation produced by the prediction model ? ? may be used as a query for retrieval. That being said, queries may also be generated from the input ? itself without any involvement of the prediction model parameters.</p><p>Under REML, prediction models do not have restrictions on the number of queries that can be submitted for each input ?. As a result, a model may generate multiple, sequential queries produced for each input ?, resulting in a query session analogous to human search sessions. While current search engines base sessions on temporally-adjacent user queries, REML prediction models can, when querying, explicitly indicate a unique session ID associated with the input ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Retrieval</head><p>Model. The retrieval model component aims at retrieving information items from the given collection, repository, or memory in response to each query produced by the Query Generation component. Existing retrieval models are mostly designed based on the probability ranking principle (PRP) <ref type="bibr" target="#b48">[49]</ref>, in which documents are ranked based on their probability of relevance to the query. In the IR literature, relevance can be defined in five levels <ref type="bibr" target="#b51">[52]</ref>: (1) systematic or algorithmic relevance, (2) topical relevance, (3) cognitive relevance or pertinence, (4) situational relevance or utility, and (5) motivational or affective relevance. However, these definitions assume that the retrieved documents are consumed by humans. This assumption no longer holds for REML models, thus the notion of relevance needs to be revisited for REML.</p><p>When designing retrieval models for REML, relevance can be thought of as the utility that the prediction model obtains by consuming the results produced by the retrieval model; this is similar to task-based perspectives on (human) information retrieval <ref type="bibr" target="#b29">[30]</ref>. For simplicity and without loss of generality, assume for each input ?, the prediction model ? ? only submits a single query ? to a retrieval model that returns a result list ? ? = {(? 1 , ? (? 1 )), (? 2 , ? (? 2 )), ? ? ? , (? ? , ? (? ? ))}, where each ? ? is a document<ref type="foot" target="#foot_0">1</ref> in the collection and ? (? ? ) encodes a list of features and properties associated with document ? ? . For instance, ? (? ? ) may contain the document score produced by the retrieval model in addition to a number of features used by the retrieval model to compute the score. With a slight abuse of notation, let ? (?; ? ? ) denote the prediction function that submits the query ? to a retrieval model and uses its response (i.e., ? ? ) to make a prediction. Then, the utility gain can be defined as:</p><formula xml:id="formula_2">UtilityGain(?, ? ? ; ? ? , ?) = ? (? ? (?; ? ? ), ?) -? (? ? (?; ?), ?) (2)</formula><p>where ? (?, ?) represents some desired utility function. This definition assumes that data points (?, ?) are i.i.d. samples. Utility gain depends on how the prediction model ? ? consumes ? ? for producing ?. Utility gain can take on both positive and negative values. A negative gain means that the retrieval results ? ? have negative impact on predicting the ground truth label. This definition can be extended to multiple queries per ?.</p><p>The implementation of retrieval models for REML depends on the nature of documents in the collection. For instance, one can use the vector space model and employ the inner product as the similarity function between query and document vectors. Section 3.3 provides more information on the optimization of retrieval models in REML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Response Processing.</head><p>The way the prediction models consume the retrieved items has a substantial impact on their end-toend performance. The Response Processing component takes the results returned by the retrieval models for each query ? (i.e., ? ? s) and prepares it for consumption by the prediction model.</p><p>This component can be implemented by returning the content of the retrieved documents, synthesizing a summary of their content, producing one or more semantic representations of their content, combining all the information presented in ? ? in some way, and so on. There are many design choices here and the best choice will largely depend on the nature of the machine learning model and the task it is being applied to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Feedback Handler.</head><p>When training retrieval models, it is often desirable to get feedback from the machine learning model. Such feedback can then be used as a signal for optimizing the retrieval model. We can imagine various forms of feedback in this context. For example, the model can compute the utility gain of documents returned by the retrieval model using Equation (2). As another example, the feedback may be computed based on the gradients of the prediction loss with respect to the retrieved information. Section 3.3 discusses how the model's feedback can be used for optimizing retrieval models in REML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Storage Handler.</head><p>If the prediction model has the ability to store information in the repository (or memory), the Storage Handler can expand the collection by storing the information item into the memory. However, for efficient storage and access of a large number of items, careful consideration of memory management techniques, hardware requirements, and storage data structures beyond existing technologies (e.g., inverted indexes) is required. Besides information storage, this component is also responsible for storage management. Thus, it should implement caching, compression, access controls, and time-to-live requirements as necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">REML Optimization</head><p>We envision three optimization approaches for REML: (1) independent optimization of prediction and information access models, (2) conditional optimization of these models such that the quality of one impacts the optimization of the other, and (3) joint end-to-end optimization of both models. Without loss of generality, here we assume that there only exists one information access model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Independent Optimization of Prediction and Information</head><p>Access Models. In independent optimization, the training process of the prediction model ? ? is independent of the retrieval performance. For example, we can assume that the retrieval model is optimal. Formally, we can optimize the prediction model of REML as:</p><formula xml:id="formula_3">? * = arg min ? 1 |? | ?? (?,?) ?? L (? ? (?; ? opt ), ?)<label>(3)</label></formula><p>where ? opt denotes an optimal retrieval model and can be modeled using ground truth relevance information, if available. Similar to <ref type="bibr" target="#b71">[72]</ref>, we can also model imperfect retrieval models by introducing noise to an optimal ranking behavior. The retrieval model can be trained using typical learning-to-rank (LTR) formulation, independent of ? ? . For the same of space, we refer the reader to Liu <ref type="bibr" target="#b41">[42]</ref> for more information on LTR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Conditional</head><p>Optimization of Prediction and Information Access Models. In conditional optimization, the prediction model parameters get updated conditioned on the retrieval model's performance and vice versa. This process can be done iteratively until a stopping criterion is met (e.g., convergence or early stopping based on performance on a held-out validation set). Therefore, the prediction model can be optimized as:</p><formula xml:id="formula_4">? (? ) = arg min ? 1 |? | ?? (?,?) ?? L (? ? (?; ? ? (? ) ), ?)<label>(4)</label></formula><formula xml:id="formula_5">? (? +1) = arg min ? 1 |? | ?? (?,?) ?? L (? ? (? ) (?; ? ? ), ?)<label>(5)</label></formula><p>where ? (? ) and ? (? ) denote the parameters of the prediction model and the information access model at the ? th iteration, respectively. These equations assume that both models are being optimized. In case of using unsupervised retrieval models, the second optimization process would be skipped (i.e., ? ? +1 = ? ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Joint</head><p>End-to-End Optimization. In end-to-end optimization of REML, both ML and information access models are trained jointly by optimizing a single objective function. Formally, it is defined as:</p><formula xml:id="formula_6">? * , ? * = arg min ?,? 1 |? | ?? (?,?) ?? L (? ? (?; ? ? ), ?)<label>(6)</label></formula><p>For optimizing this objective via gradient descent-based optimizers, the whole REML process (both models and their interactions) is required to be differentiable. End-to-end optimization is expected to perform better than the last two optimization approaches, but given the complexity of retrieval from large collections, this requirement may be difficult to satisfy in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extending REML to Multiple ML Models</head><p>Previous sections consider only a single prediction model that interacts with multiple retrieval processes (see Figure <ref type="figure">2</ref>). This section extends the REML framework to multiple prediction models. Similar to current search engines that provide service to many users, retrieval models can be also employed by multiple ML models.</p><p>Assume there are ? prediction models</p><formula xml:id="formula_7">? ? 1 , ? ? 2 , ? ? ? , ? ? ? that use ? information access models denoted by ? ? 1 , ? ? 2 , ? ? ? , ? ? ? .</formula><p>Each ? ? ? should provide service to multiple prediction models. This introduces the following challenges: Shared Query Language: All prediction models may need to share the same query language for interacting with retrieval systems. Shared Response Formats: The responses produced by each retrieval system will be used by all prediction models. Therefore, the prediction models should be able to utilize the response format used by each retrieval model. Shared Storage: The storage used by each retrieval model is shared between all prediction models. Storage is a limited resource, thus a policy may be required to regulate storage usage for each prediction model. Moreover, the data stored by each prediction model may not be interpretable by other models or may not be shared due to privacy restrictions. The Storage Handling component should develop memory management and access restriction policies and functionalities for each storage request.</p><p>Personalization: <ref type="foot" target="#foot_1">2</ref> The prediction models have special needs and they utilize the retrieval responses differently. Therefore, in response to a query ? submitted by two prediction models ? ? ? and ? ? ? , the retrieval models may want to respond differently. In this case, retrieval models would need to implement models and techniques for personalizing the search results. Comparable Feedback Across Prediction Models: Comparable feedback across prediction models enables us to easily aggregate the obtained feedback. Otherwise, the feedback can be used for each individual prediction model as a form of personalization. Optimizing Retrieval Models: In case of dealing with trainable retrieval models, the optimization solutions introduced in Section 3.3 need further adjustments. Let L ? denote the loss function associated with the ? th prediction model. Thus, the joint end-to-end optimization of models can be achieved as follows:</p><formula xml:id="formula_8">arg min ?,? 1 ? ? ?? ?=1 1 |? ? | ?? (?,?) ?? ? ? ? L ? (? ? ? (?; ? ? ), ?)<label>(7)</label></formula><p>where ? ? denotes the training data for the ? th prediction task. This formulation assumes that the loss values are comparable across prediction models. The hyper-parameter ? ? s control the weight of each loss function. The conditional optimization formulation can be adjusted, similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Information Access Evaluation in REML</head><p>The prediction model should be evaluated based on its performance on the downstream task, and appropriate evaluation methodologies and metrics should be chosen considering the downstream task. This evaluation is the same for any predictive model designed for that task. Therefore, we skip the evaluation of prediction models and discuss approaches for evaluating the information access models. Evaluating information access in REML is particularly important for diagnosing the retrieval process and designing retrieval systems that provide service to multiple prediction models (see Section 3.4). The retrieval component in REML can be evaluated either extrinsically or intrinsically: Extrinsic Evaluation: The information access quality can be quantified by measuring its impact on the prediction model for the downstream task. This is perhaps the most important factor in evaluating information access in REML. Note that in case of having multiple prediction models, extrinsic evaluation is defined for each prediction model independently. However, aggregating the downstream performances for different prediction models is challenging, because prediction models may be evaluated based on various metrics and methodologies and they may not aggregate easily. Extrinsic evaluation can be done both through offline and online evaluation. Intrinsic Evaluation: In intrinsic evaluation, the retrieval model is evaluated independent of the prediction models. To do so, one may define relevance based on the desired documents expected to be retrieved for a prediction model. This definition may be obtained from experts or by analyzing observations from prediction models' behavior. Then presumably an annotation process, e.g., through pooling, may be employed for creating data collections for intrinsic evaluation of the information access model. Metrics used in intrinsic evaluation are expected to have high correlations with the downstream performance of the prediction models. We highlight that most metrics used in the IR literature have been developed based on user behaviors with search engines. For instance, many of them assume that users assess documents sequentially. However, such assumptions may not hold for many ML models. Thus, new evaluation metrics may need to be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CASE STUDIES</head><p>Since REML is a general framework, we can discuss related approaches as special cases of REML. This exercise helps us understand how and when REML might work and suggests opportunities for extending existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Grounding</head><p>Fully data-driven ML models, despite demonstrating success across a wide number of tasks, still lack grounding in the real world. Access to external knowledge, via knowledge grounding, may help with this issue <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b75">76]</ref>. Knowledge grounding models make predictions based on the results returned by a retrieval model.</p><p>In the context of language modeling, one class of methods uses retrieval results as evidence to support reasoning. For example, the knowledge retriever module in REALM <ref type="bibr" target="#b20">[21]</ref> accesses information from an encoded Wikipedia corpus during pre-training. In text generation, RetGen <ref type="bibr" target="#b74">[75]</ref> combines a grounded text generator with a document retriever. Grounding the generation helps with the issue of hallucinated facts, and the retrieval component makes the grounding effective and efficient. Lewis et al. <ref type="bibr" target="#b38">[39]</ref> highlighted the importance of retrieval in knowledge-intensive NLP tasks and introduced retrieval-augmented generation (RAG) by augmenting a generator with the output of a non-parametric retriever that uses maximum inner product search.</p><p>Entities as Experts (EaE) <ref type="bibr" target="#b14">[15]</ref> introduces an entity memory that can be accessed by the model and the retrieved representations of entities are combined with the input representation for entity linking, mention detection, and masked language modeling tasks. Similarly, Fact as Experts (FaE) <ref type="bibr" target="#b60">[61]</ref> incorporates a fact memory for language modeling. Such a mechanism gives access to factual information, that may expand or change over time, while there is no need for additional training or fine-tuning.</p><p>In open-domain QA, a common approach is to retrieve documents or passages from Wikipedia or even the Web and then extract answers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>. Lee et al. <ref type="bibr" target="#b36">[37]</ref> used an encoded Wikipedia corpus to train a retrieval model and then fine-tune the prediction model for a QA objective. Khattab et al. <ref type="bibr" target="#b32">[33]</ref> used a retrieval component for multi-hop reasoning, where the retrieved facts from each hop are summarized into a short context and becomes a part of the query for the subsequent hops. Similarly, Das et al. <ref type="bibr" target="#b9">[10]</ref> performed iterative retrieval for expanding and rewriting multi-hop questions. This is also the case for task-oriented dialogues. For instance, LaMDA <ref type="bibr" target="#b57">[58]</ref> shows the benefit of granting dialogue systems access to external knowledge for reducing unsourced statement hallucination <ref type="bibr" target="#b52">[53]</ref>.</p><p>The approaches presented in this subsection mostly use simple retrieval models, e.g., TF-IDF or inner product of learned representations, for finding factual information from external knowledge bases. Therefore, one can look at knowledge grounding as an implementation of REML, mostly based on Category 1: Retrieval-only (Figure <ref type="figure" target="#fig_1">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Memory-Augmented Machine Learning</head><p>Using a memory component where the model can read from and/or write into is one of the most common ways of implementing REML in neural networks. The main motivation is to use an explicit storage buffer to make it easier for the network to rapidly incorporate new information and not to forget in the future.</p><p>A model may use an internal memory where it compresses and accumulates information to access them in later stages of the process. This has been the base of several neural architecture classes. For instance, Long Short-Term Memory networks (LSTMs) <ref type="bibr" target="#b24">[25]</ref> or Gated Recurrent Networks <ref type="bibr" target="#b6">[7]</ref> that use a latent state as a memory to collect information from previous time steps. Attention-based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b59">60]</ref> also treat different parts of the input as memories and use soft access as the retrieval mechanism to manage the interaction between them. However, memory-augmented neural networks refers to cases of using an external memory <ref type="bibr" target="#b50">[51]</ref>. Among main works in this area, memory networks <ref type="bibr" target="#b54">[55]</ref> explicitly store information in a form that is element-wise addressable. Neural Turing machines <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> are well-known examples of ML models that can read from and write into an external memory matrix in order to represent and manipulate complex data structures.</p><p>The common target property of memory-augmented neural networks is incorporating an external memory that is trained endto-end with the objective and data from downstream tasks. This most resonates with the fourth category of REML: Retrieval with memory and feedback (Figure <ref type="figure" target="#fig_1">1(d)</ref>). However, the memory size in existing models is relatively small and extending the memory size is an exciting and challenging research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval-Enhanced Input Representation</head><p>A number of retrieval-enhanced models use the retrieved items to update the representations of the model's input. This is different from knowledge grounding in the sense that the information items do not necessary include the knowledge required for accomplishing the task. Instead, the retrieved information contains patterns that can help the model to learn more expressive representations.</p><p>Pseudo relevance feedback (PRF) is an example of such models. It uses the top retrieved documents for updating the query representation through query expansion. It has shown successful results in a wide range of retrieval tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b72">73]</ref>, demonstrating the quality of the produced query representations for retrieval. Recently, Hashemi et al. <ref type="bibr" target="#b21">[22]</ref> proposed Guided Transformer, an extension to the Transformer network that includes cross attention for contextualizing inputs with retrieved information from multiple information sources to learn more accurate representations of the model's input. In their subsequent work <ref type="bibr" target="#b22">[23]</ref>, the authors proposed an approach for learning multiple representations for query intents by utilizing the retrieval results and taking advantage of the Guided Transformer network for representation adjustment. More recently, Borgeaud et al. <ref type="bibr" target="#b4">[5]</ref> proposed RETRO for language modeling and showed that by using networks like Guided Transformer one can enable access to a trillion-scale database for a relatively small model. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref>. For example, Xu et al. <ref type="bibr" target="#b68">[69]</ref> studied the task of image inpainting whose goal is to restore missing regions of an image. They introduced a "texture memory" that augments a neural network with access to patches extracted from unmasked regions of the input image. For the task of 3D scene reconstruction, Siddiqui et al. <ref type="bibr" target="#b53">[54]</ref> used retrieval for creating multiple approximate reconstructions and then fusing them with an attention-based blending module to generate the output. For object detection, Kuo et al. <ref type="bibr" target="#b34">[35]</ref> used retrieval from a large-scale dataset of 3D models to understand the underlying 3D structure of objects seen in a 2D image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related approaches have been also used in computer vision</head><p>Similar to knowledge grounding, retrieval-enhanced representation learning can take advantage of information items that are similar to the input by learning from patterns observed in the retrieved results. Thus, the first (retrieval-only) and the third (retrieval with feedback) REML categories are often used for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization through Memorization</head><p>Combining retrieval-based and generative approaches has been explored in a number of applications. In this case, the retrieval component can contribute by producing accurate responses when memorization is sufficient.</p><p>Motivated by the goal of memorizing rare patterns explicitly, Khandelwal et al. <ref type="bibr" target="#b31">[32]</ref> introduced KNN-LM, where a retrieval mechanism is used to find the nearest neighbor tokens given the prefix as query. KNN-LM linearly interpolates the predicted distribution for the next token using distance information from the retrieval mechanism. BERT-KNN <ref type="bibr" target="#b28">[29]</ref> employs a similar nearest neighbor algorithm to augment a BERT model to learn better representations for rare facts. This idea has also been extended to machine translation <ref type="bibr" target="#b30">[31]</ref>. It is shown that retrieval augmentation improves domain adaptation by using a domain-specific datastore for retrieval. Tay et al. <ref type="bibr" target="#b56">[57]</ref> proposed training a large model that memorizes the mapping of document content to document ids, which can be used to retrieve relevant document ids given a query at inference time. This model could be an alternative to KNN based models we discussed above to serve a REML system as a differential index.</p><p>In dialogue systems, given a dialogue history as a query, a retrieval unit can be used to return the top ranked candidate response as the next dialogue utterance <ref type="bibr" target="#b49">[50]</ref>. Such retrieval-based approaches can also be combined with response generation models and form a hybrid solution for dialogue systems <ref type="bibr" target="#b69">[70]</ref>.</p><p>Another approach to improve generalization through memorization is through updating retrieval results. In some cases, editing an existing candidate output is easier than generating it from scratch, especially in complex structured output generation tasks, like code generation. Hashimoto et al. <ref type="bibr" target="#b23">[24]</ref> proposed to retrieve a training example given the input and edit it to the desired output. The retriever and the editing modules are trained jointly. Pasupat et al. <ref type="bibr" target="#b43">[44]</ref> proposed using exemplar retrieval for semantic parsing. In their setup, given a query, the parser retrieves a set of related exemplars, augments the query using the retrieved information, and then incorporates a seq2seq model <ref type="bibr" target="#b55">[56]</ref> to produce an output parse.</p><p>The aforementioned methods try to use a retrieval component to handle memorization cases. It is found useful, especially for cases where sufficient training data is not available. Many existing models are based on a retrieval-only implementation of REML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficient Access to Longer Context</head><p>Due to memory constraints as well as efficiency and effectiveness reservations, consuming and representing large inputs, e.g., long text documents or videos, are challenging. REML offers a solution to address this issue by giving access to the context of any size via a retrieval mechanism. Here we mention a few examples of studies that exploit this idea.</p><p>Wu et al. <ref type="bibr" target="#b62">[63]</ref> proposed using a long-term feature bank for detailed video understanding. The long-term feature bank stores a rich, time-indexed representation of a long video. Then the video understanding model consults with the bank through a retrieval module to get features that encode information about past and future scenes, objects, and actions. Similarly, MemViT <ref type="bibr" target="#b63">[64]</ref>, proposes to process videos in an online fashion and store information in memory at each iteration. The model can retrieve prior context from the memory to enable long-term modeling for the recognition task. Similar approaches have also been used for video object segmentation <ref type="bibr" target="#b42">[43]</ref> and video summarization <ref type="bibr" target="#b37">[38]</ref>.</p><p>For processing long documents, researchers often split the documents into passages. For instance, Dai and Callan <ref type="bibr" target="#b8">[9]</ref> only used the first passage of each document for document retrieval. Xiong et al. <ref type="bibr" target="#b65">[66]</ref> used the passage with the maximum similarity score with the query. The end-to-end intra-document cascading model <ref type="bibr" target="#b25">[26]</ref> can be seen as a REML model with feedback. It first selects (retrieves) a number of passages from the document and then consumes the selected passages for scoring the document.</p><p>The methods presented in this subsection are perhaps the simplest implementations of REML: the retrieval collection is not large, and some of them do not use feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Retrieval-Enhanced Optimization</head><p>All the methods mentioned above use a retrieval component at the inference time for making accurate predictions. Some approaches use retrieval components solely for the purpose of optimization, e.g., for producing training data and/or computing loss functions. Thus, the retrieval model will not be used during inference.</p><p>A natural application of retrieval-enhanced optimization is for retrieval tasks. Dehghani et al. <ref type="bibr" target="#b11">[12]</ref> introduced a weak supervision approach for IR by producing large-scale training data through BM25 and training ML models for document ranking. Zamani and Croft <ref type="bibr" target="#b70">[71]</ref> used the top retrieved documents to produce a relevance model distribution for training queries and learn relevance-based word embedding. Producing hard negative instances for training learning-to-rank models is another application of REML. For instance, ANCE <ref type="bibr" target="#b65">[66]</ref> and its extensions <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46]</ref> are dense retrieval models that iteratively use the model parameters to retrieve documents for producing 'hard' negative samples for training the model.</p><p>Wu et al. <ref type="bibr" target="#b64">[65]</ref> used a retrieval unit to enable unsupervised training of machine translations, i.e., using two monolingual corpora in the source and target languages with no alignment. As an alternative to back translation, they proposed retrieving a sentence from the target corpus using the source sentence and applying some changes using an editing mechanism to the retrieved target to generate source-target pairs and train the MT model. Triantafillou et al. <ref type="bibr" target="#b58">[59]</ref> proposed an approach for few-shot learning through retrieval. This approach retrieves items for each input and uses them for making predictions. Via this approach, a model can adapt to a new domain without additional training or new data.</p><p>An interesting use case of REML is the pre-training task of CLIP <ref type="bibr" target="#b47">[48]</ref> and VideoCLIP <ref type="bibr" target="#b66">[67]</ref> which are practically optimizing for text-image and text-video retrieval, respectively. They are in fact capturing cross-modal relevance that led to learning representations that are effective in various setups, like zero-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A RESEARCH AGENDA</head><p>While Section 4 provides evidence of the efficacy and broad applicability of REML, there remain significant open research challenges in fully realizing the general REML vision, some of which are already mentioned in previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Querying</head><p>In developing a prediction model that supports retrieval, understanding how to query becomes a core research question. First, this involves knowing when to query. In some situations, a prediction model may not benefit from a retrieval operation (even if it benefits on average). Although current retrieval-enhanced systems issue the equivalent of queries for every instance, when querying incurs some cost, be it in the form of latency or financial expense, developing models that "know when they don't know" would allow the prediction algorithm to explicitly trade off cost and benefit. A prediction model that has access to multiple information access services can make this decision for individual corpora, perhaps select the appropriate source for the instance. Second, at a more granular level, how retrieval might benefit a model may vary by instance ?. For example, retrieval may support uncertainty in one part of the ? for one instance and uncertainty in another part of ? for another instance. This self-interrogation can be explicitly designed or implicitly learned. Nevertheless, even learnable behavior requires an architecture and parameters to adapt. Finally, many retrieval situations can benefit from the searcher conveying non-semantic meta-retrieval information such as uncertainty in (aspects of) the query or context of the retrieval itself. People often convey similar information to human intermediaries <ref type="bibr" target="#b0">[1]</ref> and we suspect that more expressive querying can also emerge in REML.</p><p>In developing an information access model to support a prediction model, similar questions arise. First, developing or learning a query language requires expressiveness that captures the breadth of model needs. At the same time, it should allow for communication of meta-retrieval or structured properties of the retrieved set. Moreover, these properties need to be explored within the effectiveness and efficiency constraints. Second, although a query may be effective and efficient in general, it may be ambiguous or imprecise for a particular retrieval scenario. This is especially likely in situations where multiple models may develop inconsistent uses of the query language (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Storing</head><p>The ability of the prediction model to store items presents unique problems not encountered in traditional retrieval or ML research. Although architectures like memory networks <ref type="bibr" target="#b61">[62]</ref> provide modestly sized storage, we anticipate models storing or serializing on a larger scale with more permanence. In situations with multiple models (Section 3.4), we anticipate the corpus operating as a means to share derived knowledge (to avoid re-computation during inference) or prediction model parameters (to support learning).</p><p>In developing a prediction model that supports storage, understanding how to store becomes a core research question. Just as with querying, a model needs to reason about when to store, what to store from its parameters or reasoning, and how to represent that information. Each of these questions is relevant both to sharing derived knowledge as well as model parameters. Like queries, stored items may include auxiliary information such as the model's confidence in the derived data or parameter values, the prediction task, and other information that may be valuable for an information access system to make retrieval decisions. More so than with queries, a model might need to be more judicious in storage operations, since injecting irrelevant or erroneous content into the corpus can significantly degrade its usefulness.</p><p>In developing an information access model to support storage, classic problems related to indexing arise. First, as with queries, the language, schema, or representation of an item requires careful construction to optimize for effectiveness and efficiency. Second, in accepting a storage request from a prediction model, the information access system needs to model the value of the content. Redundant items can either add noise or improve coverage, depending on the task. Or, an item may require processing to make indexing and retrieval more effective. These decisions can be based on the content of the item or meta-data about the item, such as the confidence of the model or, in the case of multiple models, confidence in the prediction model itself. Third, if an item should be stored, there is the question of how to store it. This includes questions of item compression and representation, both of which need to occur incrementally but improve with batch, corpus-wide computation. Finally, in the case of limited capacity in the retrieval index, storage operations may necessitate purging less effective content. This requires that the information access model reason about how collection management decisions impact prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Searching</head><p>Ranking functions, a fundamental property of traditional information access systems, influence design decisions about how to store content compactly, how to search that content quickly, and how to return results effectively. In moving toward REML, several fundamental research questions need to be addressed in order to satisfy these properties for machines. First, items in REML indexes are likely to be differently structured than existing text documents (see Section 5.2). Although representations like dense, fixed dimensional vectors are amenable to efficient storage and retrieval, structures that include uncertainty and other attributes may require embedding as a representation amenable to fast retrieval (e.g., vectors) or different indexing schemes altogether. Second, the representations of items in the index themselves should be selected for effectiveness in supporting prediction models, as well as the space and runtime efficiency. In some cases, this means accurate and fast score computation. When a retrieval involves more elaborate post-processing before returning results, this may mean decomposing items before indexing (as is often done when retrieving passages, as opposed to documents). Third, in situations where there are multiple prediction models, the information access system can use the identity of the model in order to 'personalize' results for that model. Similarly, we can interpret the feedback from prediction models based on where it comes from; some models may not provide actionable feedback early in learning, others may be quite reliable, while others yet might be adversarial. Third, these representations and their associated ranking functions themselves should be tunable given feedback from prediction models (see Section 5.5). Adjustments to representations and model weights should be sensitive to confidence in the feedback signal in situations where feedback includes a confidence estimate or if the information access model can estimate the reliability of the feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Information Presentation &amp; Consumption</head><p>Representing the retrieval results in traditional information access involves returning a ranked list of items. Although items include scores, these are often only used to sort items and are rarely presented to the user. In the context of REML, we can consider more elaborate representations of retrieval results because they are being consumed by machines. This introduces a number of exciting research directions. First, system designers will need to understand the appropriate information to communicate to prediction models, be it an item ranking, a scored set, a set where each item is associated with a score distribution, a graph of inter-item relationships, or some other object derived from the retrieval. Each of these choices needs to satisfy improving the prediction model's effectiveness, within any cost constraints (e.g., bandwidth, compute). Moreover, in situations with multiple prediction models, the consistency, interpretability, and maintainability of this representation language become extremely important. Second, from an efficiency perspective, just as computing a top ? ranking can suggest fast document scoring, information about the representation can introduce opportunities for more efficient computations of objects like graphs and score distributions. Third, a prediction model with access to multiple information access models needs to reason over multiple sets of results. Information encoded in the results-explicitly or notcan allow the prediction model to consider the reliability of results before incorporating them into inference. Finally, from a machine learning perspective, how to incorporate results into inference will become an important area of work. Current approaches based on neighbors provide a simple approach, although more sophisticated techniques are likely to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Feedback</head><p>Modern information access systems use implicit user feedback in order to optimize model parameters. Although we can imagine a prediction model providing loss information in its feedback similar to how users might provide slate-level feedback, machines may be able to convey more granular and expressive feedback to the information access model. As such, the first area of research centers on forms of feedback, including scalar values, vectors of values, and more expressive data with goal of helping the information access model improve. While single scalar feedback values seem simplest, even modern search engines exploit implicit item-level feedback. We can imagine more targeted and attributed feedback provided by the prediction model. This structured feedback can include attribution to different components of the retrieval structure (Section 5.4). Of course, this requires the prediction model being able to identify the relationship between prediction error and different parts of the retrieval result; in the case of multiple information access services, attribution to individual corpus results. The second area of research focuses on how an information access model might adjust model parameters given rich feedback from the prediction model. Current ranking models, with appropriate treatment of different biases, can interpret user feedback as attributed to individual items in the ranking. A machine may be able to provide feedback that has fewer biases and better calibration than human feedback. This includes exploring a new space of feedback beyond scalar item-level values. This also calls for novel approaches for optimizing information access models based on the provided feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluation</head><p>The objective of REML is to support machines. As such, standard methods of evaluating modeling performance (e.g., Equation <ref type="formula" target="#formula_1">1</ref>) can be adopted to assess prediction model performance. Nevertheless, REML introduces several research directions around model evaluation. First, because of the large, flexible storage capacity, REML can memorize training data or cache previous predictions, resulting in performance metrics (e.g., accuracy) conflating a model's ability to reason (i.e., the prediction model) and its ability to remember (i.e., the information access model). Methods of selecting evaluation instances or ablation experiments can isolate the contribution of each component. Second, in situations with multiple prediction models, we need methods to assess performance changes for a group of models with a shared information access service. Although these per-model losses can be aggregated into a simple average, this may obscure model-or task-specific under-performance. That said, in some situations, storage operations might result in sharing information, boosting collective performance, and necessitating an evaluation method that decouples reasoning from memorization. Finally, efficiency metrics that capture the cost of query and response operations (e.g., latency, financial) will need to be developed.</p><p>In some cases, we are interested in evaluating the information access model in isolation to make a claim about generalizability of a specific retrieval model to new prediction models, just as we traditionally consider evaluation queries as a sample from the full set of queries we would like to apply a system to. Although we can evaluate information access models using the existing information access evaluation methods (e.g., Cranfield-style offline evaluation, click feedback), we anticipate the opportunity-and sometimes need-to develop entirely new evaluation schemes. First, although a prediction model can be evaluated by its loss function, an information access model can be evaluated by its adoption. Indeed, if a retrieval component is not used, then perhaps it can be removed altogether. To see why retrieval systems may be more or less valuable over time, consider the situation where a prediction model can store items such as partial inference or complete inference; in this case, the storage can act like a cache, with queries likely to grow with time, depending on the data. Or, if there are multiple information access services, the usefulness of some may increase or decrease over time. Nonstationarity can also arise when instances have serial dependencies, such as when a retrieval system is repeatedly queried during a dialog or multi-hop task. Second, estimating an information access model's performance on out of sample domains or tasks requires careful selection of training and evaluation tasks. Third, in developing offline or batch evaluation methods, although we can avoid some issues, labeling items for relevance and designing metrics reflective of model use becomes difficult, since existing ranking metrics are unlikely to approximate how a machine would consume results (see Section 5.4). Finally, REML presents a tremendous opportunity to study these questions in silico. This means that experimentation and analysis, although more complicated, will be much faster than systems serving people, without safety concerns, since experiments can be run isolated from people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Although the large number of parameters in models such as deep neural networks has, in part, resulted in impressive improvements in performance across a wide range of tasks, evidence suggests that these successes may be partially due to the increased capacity to store information in model parameters <ref type="bibr" target="#b73">[74]</ref>. We claim that, if model capacity is being used to store information, then we should decouple reasoning from memory and expand the scope of information retrieval to also support ML models. Starting from this claim, we have presented a general framework, its relation to existing methods, and its ability to substantially advance how we think about information retrieval and how we do machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 :</head><label>4</label><figDesc>Retrieval with memory &amp; feedback</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Retrieval-enhanced machine learning models should implement three necessary requirements (querying, retrieval, and response utilization) and may implement two optional properties (storing information and providing feedback to the information access model). This results in four categories of REML models presented above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a)) or Category 3: Retrieval with feedback (Figure 1(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Response Utilization: the prediction model ? ? should uti-</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Storage</cell><cell></cell><cell>store</cell><cell>Collection /</cell></row><row><cell></cell><cell>Handling</cell><cell></cell><cell></cell><cell>Memory /</cell></row><row><cell>output</cell><cell></cell><cell></cell><cell></cell><cell>Index</cell></row><row><cell></cell><cell>Feedback</cell><cell>feedback</cell><cell></cell></row><row><cell></cell><cell>Handler</cell><cell></cell><cell></cell></row><row><cell>Prediction</cell><cell>Query</cell><cell>query</cell><cell>Retrieval</cell></row><row><cell>Model</cell><cell>Generation</cell><cell></cell><cell>Model</cell></row><row><cell></cell><cell>Response</cell><cell>response</cell><cell></cell></row><row><cell></cell><cell>Processing</cell><cell></cell><cell></cell></row><row><cell>input</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Information Access</cell><cell>? 0 .. N</cell></row><row><cell cols="5">Figure 2: A generic framework for REML.</cell></row><row><cell cols="5">Req 1 Querying: the prediction model ? ? should be able to submit</cell></row><row><cell cols="5">input-dependent queries to the information access models,</cell></row><row><cell>i.e., ? ? ? s.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Req 2 Retrieval: each information access model ? ? ? should be</cell></row><row><cell cols="5">able to efficiently process the prediction model's queries</cell></row><row><cell cols="5">and retrieve relevant information items from a memory or</cell></row><row><cell>collection ? ? .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Req 3</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper, we refer to retrievable items, e.g., unstructured text, image, or even latent vectors, as documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Personalization is often used for humans. We stick to the same terminology to be consistent with the IR literature.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by the <rs type="grantName">Google Visiting Scholar program</rs> and in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors. We would like to thank <rs type="person">Marc Najork</rs> for providing feedback on the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ehppvQT">
					<orgName type="grant-name">Google Visiting Scholar program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2021 Conference on Human Information Interaction and Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local Feedback in Full-Text Retrieval Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Attar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Fraenkel</surname></persName>
		</author>
		<idno type="DOI">10.1145/322017.322021</idno>
		<ptr target="https://doi.org/10.1145/322017.322021" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="397" to="417" />
			<date type="published" when="1977-07">1977. jul 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT &apos;21)</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04426[cs.CL]</idno>
		<title level="m">Improving language models by retrieving from trillions of tokens</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07805</idno>
		<ptr target="https://arxiv.org/abs/2012.07805" />
		<title level="m">Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Language Models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>USENIX Security Symposium</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Probabilistic Models of Document Retrieval Without Relevance Information</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Documentation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeper Text Understanding for IR with Contextual Neural Language Modeling</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331303</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331303" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Kavarthapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5816</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-5816" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to transform, combine, and reason in open-domain question answering</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="681" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Ranking Models with Weak Supervision</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080832</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080832" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<title level="m">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple Bernoulli Relevance Models for Image and Video Annotation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, D.C., USA; USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1002" to="1009" />
		</imprint>
	</monogr>
	<note>CVPR&apos;04)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entities as Experts: Sparse Memory Access with Entity Supervision</title>
		<author>
			<persName><surname>Thibault F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.400</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.400" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4937" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximate Distance-Comparison-Preserving Symmetric Encryption</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Fuchsbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riddhi</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Hauke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam O'</forename><surname>Neill</surname></persName>
		</author>
		<ptr target="https://ia.cr/2021/1666" />
	</analytic>
	<monogr>
		<title level="j">Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="1666">2021. 2021/1666</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwi?ska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-Modal Retrieval Augmentation for Multi-Modal Classification</title>
		<author>
			<persName><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.11</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-emnlp.11" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="111" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval Augmented Language Model Pre-Training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/guu20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided Transformer: Leveraging Multiple External Sources for Representation Learning in Conversational Search</title>
		<author>
			<persName><forename type="first">Helia</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401061</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SI-GIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SI-GIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1131" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning Multiple Intent Representations for Search Queries</title>
		<author>
			<persName><forename type="first">Helia</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482445</idno>
		<ptr target="https://doi.org/10.1145/3459637.3482445" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="669" to="679" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Retrieve-and-Edit Framework for Predicting Structured Outputs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/cd17" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>d3ce3b64f227987cd92cd701cc58-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462889</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462889" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1349" to="1358" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="DOI">10.1145/860435.860459</idno>
		<ptr target="https://doi.org/10.1145/860435.860459" />
		<title level="m">Automatic Image Annotation and Retrieval Using Cross-Media Relevance Models (SIGIR &apos;03)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.307</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.307" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics</title>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3424" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">NSF Workshop on Taskbased Information Search Systems</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Capra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nearest Neighbor Machine Translation</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=7wCBOfJ8hJM" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalization through Memorization: Nearest Neighbor Language Models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HklBjCEKvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Internet-Augmented Dialogue Generation</title>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07566</idno>
		<ptr target="https://arxiv.org/abs/2107.07566" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mask2cad: 3d shape prediction by learning to segment and retrieve</title>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="260" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relevance Based Language Models</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383972</idno>
		<ptr target="https://doi.org/10.1145/383952.383972" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, Louisiana, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;01)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00300</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A memory network approach for story-based temporal summarization of 360 videos</title>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyoung</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1410" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
	<note>b493230205f780e1bc26945df7481e5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Databaseassisted object retrieval for real-time 3d reconstruction</title>
		<author>
			<persName><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer graphics forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="435" to="446" />
			<date type="published" when="2015">2015</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">More Robust Dense Retrieval with Contrastive Dual Learning</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3471158.3472245</idno>
		<ptr target="https://doi.org/10.1145/3471158.3472245" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="287" to="296" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to Rank for</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000016</idno>
		<ptr target="https://doi.org/10.1561/1500000016" />
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009-03">2009. mar 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName><forename type="first">Seoung</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Controllable Semantic Parsing via Retrieval Augmentation</title>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.607" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Tau</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="7683" to="7698" />
		</imprint>
	</monogr>
	<note>Virtual Event / Punta Cana</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning Robust Dense Retrieval Models from Incomplete Relevance Labels</title>
		<author>
			<persName><forename type="first">Prafull</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Killingback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463106</idno>
		<ptr target="https://doi.org/10.1145/3404835.3463106" />
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1728" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Passage Retrieval for Outside-Knowledge Visual Question Answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462987</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462987" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1753" to="1757" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The Probability Ranking Principle in IR</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="281" to="286" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recipes for Building an Open-Domain Chatbot</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.24</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.eacl-main.24" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="300" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Relevance reconsidered</title>
		<author>
			<persName><forename type="first">Tefko</forename><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Conceptions of Library and Information Science</title>
		<meeting>the Second Conference on Conceptions of Library and Information Science<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07567</idno>
		<title level="m">Retrieval augmentation reduces hallucination in conversation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yawar</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00024[cs.CV]</idno>
		<title level="m">RetrievalFuse: Neural 3D Scene Reconstruction with a Database</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06991</idno>
		<title level="m">Transformer memory as a differentiable search index</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239[cs.CL]</idno>
		<title level="m">LaMDA: Language Models for Dialog Applications</title>
		<editor>
			<persName><forename type="first">Alejandra</forename><surname>Molina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erin</forename><surname>Hoffman-John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Josh</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravi</forename><surname>Rajakumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alena</forename><surname>Butryna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Viktoriya</forename><surname>Kuzmina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Fenton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Bernstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Blaise</forename><surname>Aguera-Arcas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marian</forename><surname>Croak</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Few-Shot Learning through an Information Retrieval Lens</title>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<title level="s">NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2252" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.288</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.naacl-main.288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3678" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Memory Networks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Haoqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
	<note>Kaiming He, Philipp Krahenbuhl, and Ross Girshick</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08383</idno>
		<title level="m">MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1120</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1120" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1173" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding</title>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.544</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.emnlp-main.544" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6787" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Query Expansion Using Local and Global Document Analysis</title>
		<author>
			<persName><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/243199.243202</idno>
		<ptr target="https://doi.org/10.1145/243199.243202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Zurich, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;96)</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Texture memory-augmented deep patch-based image inpainting</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="9112" to="9124" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A Hybrid Retrieval-Generation Neural Conversation Model</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357881</idno>
		<ptr target="https://doi.org/10.1145/3357384.3357881" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management<address><addrLine>Beijing, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
	<note>CIKM &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Relevance-Based Word Embedding</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080831</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080831" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
	<note>SI-GIR &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On the Theory of Weak Supervision for Information Retrieval</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3234944.3234968</idno>
		<ptr target="https://doi.org/10.1145/3234944.3234968" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2018 ACM SIGIR International Conference on Theory of Information Retrieval<address><addrLine>Tianjin, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
	<note type="report_type">IC-TIR &apos;18</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Model-Based Feedback in the Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/502585.502654</idno>
		<ptr target="https://doi.org/10.1145/502585.502654" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Information and Knowledge Management</title>
		<meeting>the Tenth International Conference on Information and Knowledge Management<address><addrLine>Atlanta, Georgia, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
	<note>CIKM &apos;01)</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy8gdB9xx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Joint Retrieval and Generation Training for Grounded Text Generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on opendomain question answering</title>
		<author>
			<persName><forename type="first">Fengbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00774</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
