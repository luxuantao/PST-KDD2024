<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRIMAL: Power Inference using Machine Learning</title>
				<funder ref="#_sNerdmR">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel ISRA Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University ? NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoxing</forename><surname>Ren</surname></persName>
							<email>haoxingr@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Yanqing</forename><surname>Zhang</surname></persName>
							<email>yanqingz@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Keller</surname></persName>
							<email>benk@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
							<email>bkhailany@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
							<email>zhiruz@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University ? NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">DAC &apos;19</orgName>
								<address>
									<addrLine>June 2-6, Las Vegas</addrLine>
									<postCode>2019</postCode>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PRIMAL: Power Inference using Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3316781.3317884</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Power estimation, machine learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces PRIMAL, a novel learning-based framework that enables fast and accurate power estimation for ASIC designs. PRIMAL trains machine learning (ML) models with design verication testbenches for characterizing the power of reusable circuit building blocks. The trained models can then be used to generate detailed power proles of the same blocks under dierent workloads. We evaluate the performance of several established ML models on this task, including ridge regression, gradient tree boosting, multi-layer perceptron, and convolutional neural network (CNN). For average power estimation, ML-based techniques can achieve an average error of less than 1% across a diverse set of realistic benchmarks, outperforming a commercial RTL power estimation tool in both accuracy and speed (15x faster). For cycleby-cycle power estimation, PRIMAL is on average 50x faster than a commercial gate-level power analysis tool, with an average error less than 5%. In particular, our CNN-based method achieves a 35x speed-up and an error of 5.2% for cycle-by-cycle power estimation of a RISC-V processor core. Furthermore, our case study on a NoC router shows that PRIMAL can achieve a small estimation error of 4.5% using cycle-approximate traces from SystemC simulation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern VLSI design requires extensive optimization and exploration in a large design space to meet the ever-stringent requirements with respect to performance, area, and power. Existing ASIC CAD tools can provide reasonably accurate area and performance estimates at register transfer level (RTL) or even behavioral level with the aid of high-level synthesis (HLS) tools. However, in order  to achieve power closure, designers must obtain detailed power proles for a diverse range of workloads from dierent application use cases or even from dierent levels of design hierarchy. Currently, the common practice is to feed gate-level netlist and simulation results to power analysis tools such as Synopsys PrimeTime PX (PTPX) to generate cycle-level power traces. Figure <ref type="figure" target="#fig_1">1a</ref> depicts a typical ASIC power analysis ow, which oers accurate estimates but runs at a very low speed (in the order of 10-100s of cycles per second). Given the high complexity of present-day ASIC designs, it can take hours or days to perform gate-level power analysis for one intellectual property (IP) core under desired workloads.</p><p>An alternative is to analyze power above gate level. There exists a rich body of research on power analysis at RTL or a higher abstraction level <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. These eorts typically make use of measured constants or simple curve tting techniques such as linear regression to characterize the power of a given circuit, improving the speed of power analysis at the expense of estimation accuracy. For accurate power characterization, many low-level details of the circuit need to be modeled, including standard cell parameters, sizing of the gates, and clock gating status of the registers. Gate-level power analysis uses them to estimate the switching capacitance and activity factor of each circuit node. However, these low-level details are unavailable at (or above) RTL by design. It is also very dicult for simple analytical models or linear regression models to capture the complex nonlinear relationship between the register toggles and the total switching capacitance.</p><p>To enable fast and accurate high-level power estimation, we propose PRIMAL 1 , a learning-based power inference framework that enables fast and accurate power characterization of reusable IP cores at RTL or behavioral level. PRIMAL leverages gate-level power analysis to train machine learning (ML) models on a subset of verication testbenches. These trained models can then be used to infer power proles of the same IP core under dierent user-specied workloads. Figure <ref type="figure" target="#fig_1">1b</ref> illustrates the inference ow of PRIMAL, which only requires inputs from RTL or SystemC simulation to rapidly generate accurate power estimates (&gt;1k cycles per second). By greatly reducing the required number of gate-level simulation cycles, PRIMAL allows designers to perform power-directed design space exploration in a much more productive manner. The major technical contributions of this work are vefold: ? We present PRIMAL, a novel ML-based methodology for rapid power estimation with RTL or timed SystemC simulation traces. The trained ML models can provide accurate, cycleby-cycle power inference for user workloads even when they dier signicantly from those used for training. than PTPX for cycle-accurate power estimation with a small error. Notably, our CNN-based approach is 35x faster than PTPX with a 5.2% error for estimating power of a RISC-V core. PRIMAL also achieves a 15x speedup over a commercial RTL power analysis tool for average power estimation. ? Using a NoC router design as a case study, we demonstrate that PRIMAL can be extended to enable accurate power estimation for timed SystemC design. The remainder of this paper is organized as follows: Section 2 surveys related work, and Section 3 presents the overall design methodology and intended use cases of PRIMAL. Section 4 introduces our feature construction methods. Experimental results are reported in Section 5. Section 6 gives concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Power estimation is an extensively studied research topic. Existing works focus on power estimation from behavioral-level and RTL. Behavioral-level power estimation provides optimization guidance early in the design ow. In an earlier work, Chen et al. <ref type="bibr" target="#b6">[7]</ref> combine proling and simple analytical models to estimate FPGA power consumption. Later eorts use the HLS tool to perform scheduling and back-annotation, and rely on RTL power analysis <ref type="bibr" target="#b2">[3]</ref>, gate-level power analysis <ref type="bibr" target="#b20">[21]</ref>, or a per-control-step ML power model <ref type="bibr" target="#b13">[14]</ref> for power estimation.</p><p>Compared to behavioral-level analysis, more implementation details are available at RTL. Earlier works in RTL power estimation use simple regression models, such as linear regression and regression trees, to characterize small circuit blocks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. The regression models are trained with gate-level power analysis results. Average power and cycle-by-cycle power of the whole design can be obtained by summing up the outputs from multiple models. PrEsto <ref type="bibr" target="#b22">[23]</ref> uses linear models to characterize larger modules, where heavy feature engineering and feature selection are applied to reduce the complexity of power models. A more recent work by Yang et al. <ref type="bibr" target="#b23">[24]</ref> uses a single linear model to characterize the whole design. A feature selection technique based on singular value decomposition (SVD) is applied to reduce model complexity so that the regression model can be eciently mapped onto an FPGA. Both PrEsto and <ref type="bibr" target="#b23">[24]</ref> can provide cycle-by-cycle power estimates.   In general, existing RTL power estimation techniques either model small circuit sub-modules or try to use simple regression models to characterize the whole design. Sub-module-level modeling cannot accurately reect the power consumption of intermediate logic, and simple regression models such as linear models are not a good t for large, complex designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRIMAL Methodology</head><p>Unlike previous works, PRIMAL uses state-of-the-art ML models for fast and accurate high-level power estimation. Our methodology can readily be applied to RTL and SystemC power estimation. Figure <ref type="figure" target="#fig_3">2</ref> shows the two phases of the PRIMAL work ow. The characterization phase (Figure <ref type="figure" target="#fig_3">2a</ref>) requires an RTL/SystemC model of the module, the gate-level netlist, and a set of unit-level testbenches for training. RTL or SystemC simulation traces and the power traces generated by gate-level power analysis are used to train the ML models. The characterization process only needs to be performed once per IP block. The trained power models can then be used to estimate power for dierent workloads as illustrated in Figure <ref type="figure" target="#fig_3">2b</ref>.</p><p>It is important to note that the training testbenches may be very dierent from the actual user workloads. For example, designers can use functional verication testbenches to train the power models, which then generalize to realistic workloads. By using state-of-theart ML models, PRIMAL can accommodate diverse workloads and model large, complex circuit blocks. The ML models are trained for cycle-by-cycle power estimation to provide detailed power prole and enable more eective design optimization.</p><p>In this work we explore a set of established ML models for power estimation. The classical ridge linear regression model is used as a baseline. We also experiment with gradient tree boosting, a promising non-linear regression technique <ref type="bibr" target="#b16">[17]</ref>. For linear models and gradient tree boosting models, we apply principal component analysis (PCA) <ref type="bibr" target="#b11">[12]</ref> to the input data to reduce model complexity and avoid overtting. We also study the ecacy of deep learning models, which are capable of approximating more complex non-linear functions. Specically, we experiment with multi-layer perceptron (MLP) and CNN models for power estimation. MLP contains only fully-connected network layers and is more compute-ecient than CNN. However, the parameter count of MLP grows quickly with respect to the feature size of the design, resulting in overtting and training convergence issues. CNNs have shown impressive performance in image classication tasks. Since the power of a certain logic cone is only correlated with a small set of registers, the convolutional windows of CNNs are able to gather useful information if the input images are constructed properly. In addition, thanks to the structure of convolutional layers, CNN is a more scalable choice than MLP for large designs since the parameter count of a CNN does not increase signicantly as the input image size grows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Construction</head><p>This section describes the feature construction procedure using the circuit in Figure <ref type="figure">3a</ref> as an example. Figure <ref type="figure">3b</ref> shows the register waveform, where each "edge" in the gure corresponds to a clock rising edge. We use register switching activities in the simulation traces as input features, because register switching activities are representative of the circuit's state transitions. In addition, there is a one-to-one correspondence between registers in RTL and gate-level netlist 2 . Because we use cycle-accurate power traces from gate-level simulation as ground truth, the ML models are essentially learning the complex relationship between the switching power for all gatelevel cells and register switching activities. For clarity we focus on RTL power estimation in this section, but our feature construction methods can also be naturally applied or extended to SystemC power estimation.</p><p>Feature Encoding for Cycle-by-Cycle Power Estimation -For cycle-by-cycle power estimation, we use RTL register and I/O signal switching activities as input features without any manual feature selection. Switching activities of both internal registers and I/O signals are required to capture complete circuit state transitions. These features can be easily collected from RTL simulation. Because we are targeting cycle-by-cycle power estimation, each cycle in the simulation trace is constructed as an independent sample.</p><p>A good feature encoding should dierentiate between switching and non-switching events. A concise encoding, which we refer to as switching encoding, is to represent each register switching event as a 1, and non-switching event as a 0. For an RTL module with n registers, each cycle in the RTL simulation trace is represented as a 1 ? n vector. Figure <ref type="figure">4a</ref> shows the corresponding encoding for the waveform in Figure <ref type="figure">3b</ref>. Each vector in Figure <ref type="figure">4a</ref> represents one cycle (one clock rising edge to be precise) in the waveform. We use this one-dimensional (1D) switching encoding for all but the CNN models. The same feature encoding is used in <ref type="bibr" target="#b23">[24]</ref>.</p><p>In order to leverage well-studied two-dimensional (2D) CNN models, we create a three-channel 2D image representation for every cycle in the register trace. For an RTL module with n registers, 2 In cases where retiming with register optimization is used, mapping between RTL and gate-level registers can be retrieved from the logic synthesis tool. we use a d p ne ? d p ne ? 3 image to encode one cycle in the RTL simulation trace. We use one-hot encoding in the channel dimension to represent the switching activities of each register: non-switching is represented as [1, 0, 0], switching from zero to one is represented as [0, 1, 0], and switching from one to zero is [0, 0, 1]. We refer to this encoding as default 2D encoding. Figure <ref type="figure">4b</ref> shows how we encode edge 1 of the waveform in Figure <ref type="figure">3b</ref>. If the total number of pixels in the image is greater than n, we add padding pixels to the image, shown as d's in Figure <ref type="figure">4b</ref>. These padding pixels do not represent any register in the module, and they have zero values in all three channels in our implementation. Every other pixel corresponds to one register in the module. For this default 2D encoding, the registers are mapped by their sequence in the training traces. For example, since in Figure <ref type="figure">3b</ref> the order of registers is A, B, C, D, and E, in each channel the top-left pixel in Figure <ref type="figure">4b</ref> corresponds to A, the top-right pixel is mapped to C, and the center pixel refers to E. We observe that this default mapping from registers to pixels is not completely random. The tool ow we are using would actually cluster most of the registers within a submodule together. As a result, in our experiments, this default 2D encoding actually preserves a considerable amount of circuit structural information.</p><p>Mapping Registers and Signals to Pixels -In the default 2D encoding described above, the mapping between registers and pixel locations are determined by the way the registers are arranged in the trace le. The amount of structural information that is preserved is dependent on the tool ow. As a result, this mapping method cannot guarantee meaningful local structures in the constructed images. Registers that are mapped to adjacent pixels may not be correlated or physically connected. CNNs are most eective when there are spatial relationships in their 2D inputs. Therefore, the register-to-pixel mapping should reect the connectivity or physical placement of the registers. Since the gate-level netlist of the design is available during the characterization phase, it is possible to use the outputs of logic synthesis tools to map RTL registers to netlist nodes. Because we only use register and I/O switching activities, we ignore all combinational components and only extract register connection graphs when processing the gate-level netlist. The graph for the example circuit in Figure <ref type="figure">3a</ref> is shown in Figure <ref type="figure">3c</ref>. Each node  in the graph corresponds to one register in the design, and two nodes are connected if their corresponding registers are connected by some combinational data path.</p><p>We propose two graph-based methods for generating register-topixel mappings, which introduce local structures into the images according to the structural similarities between nodes. Notice that the proposed graph-based mapping methods only change the register mapping in the width and height dimensions of the image: we still use the channel-wise one-hot encoding for every register. Each register's contribution to each pixel is proportional to the overlapping area of the register's occupied region and the pixel. In other words, with the graph-based encoding methods the pixel values are non-negative real numbers rather than binary numbers.</p><p>The rst method is based on graph partitioning, in which the graph is recursively divided into two partitions of similar sizes, and the partitions are mapped to corresponding regions in the image (see Figure <ref type="figure" target="#fig_5">5a</ref>). The area allocated for each partition is computed according to the number of nodes in the partition. The second method is based on node embedding. Node embedding techniques map each node in the graph to a point in a vector space, where similar nodes are mapped close to each other in the vector space. Our ow for embedding-based register mapping is shown in Figure <ref type="figure" target="#fig_5">5b</ref>. We use node2vec <ref type="bibr" target="#b9">[10]</ref> for node embedding, then apply PCA <ref type="bibr" target="#b11">[12]</ref> and t-SNE <ref type="bibr" target="#b15">[16]</ref> to project the vector representations to 2D space. The resulting 2D vector representations are scaled according to the image size and indicate the mapping locations of the registers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have implemented our proposed framework in Python 3.6, leveraging networkx <ref type="bibr" target="#b10">[11]</ref>, metis <ref type="bibr" target="#b12">[13]</ref>, and a node2vec package <ref type="bibr" target="#b9">[10]</ref>. MLP and CNN models are implemented using Keras <ref type="bibr" target="#b1">[2]</ref>. Other ML models are realized in scikit-learn <ref type="bibr" target="#b18">[19]</ref> and XGBoost <ref type="bibr" target="#b7">[8]</ref>. We conduct our experiments on a server with an Intel Xeon E5-2630 v4 CPU and a 128GB RAM. We run neural network training and inference on a NVIDIA 1080Ti GPU. The SystemC models of our designs are synthesized with Mentor Catapult HLS. We use Synopsys Design Compiler for RTL and logic synthesis, targeting a 16nm FinFET standard cell library. The RTL register traces and gate-level power traces are obtained from Synopsys VCS and PTPX, respectively. Gate-level power analysis is performed on another server with an Intel Xeon CPU and 64GB RAM using a maximum of 30 threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmarks</head><p>Table <ref type="table" target="#tab_2">1</ref> lists the benchmarks used to evaluate PRIMAL. Our benchmarks include a number of xed-and oating-point arithmetic units from <ref type="bibr" target="#b17">[18]</ref>. We also test our approach against two complex designs -a NoC router used in a CNN accelerator and a RISC-V processor core. The NoC router block is written in Sys-temC and synthesized to RTL by an HLS tool. The RISC-V core is an RV64IMAC implementation of the open-source Rocket Chip Generator <ref type="bibr" target="#b4">[5]</ref> similar to the SmallCore instance. We use dierent portions of random stimulus traces as training and test sets for the arithmetic units. For the NoC router and the RISC-V core, we select functional verication testbenches for training and use realistic workloads for test. For the NoC router, we test on actual traces of mesh network trac from a CNN accelerator SoC. In the RISC-V experiment, dhrystone, median, multiply, qsort, towers, and vvadd form the set of test workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RTL Power Estimation Results</head><p>Figure <ref type="figure">6</ref> summarizes the results for RTL power estimation. Here we use RTL register traces as the raw input and apply the feature construction techniques described in Section 4. Two percent of the training data is used as a validation set for hyper-parameter tuning of the ML models. They are also used for early stopping when training the deep neural networks.</p><p>All models except CNNs use the 1D switching encoding, while CNNs use the 2D image encoding methods introduced in Section 4. We also experimented with 1D one-hot encoding, where we encode the switching activity of each register using three binary numbers as described in Section 4. Since the results with such encoding is similar with 1D switching encoding, we omit the results due to space limitations. For ridge regression and gradient tree boosting, we apply PCA to reduce the size of input features to 256, except for qadd_pipe which has only 160 features with 1D feature encoding. We use three-layer MLP models for the arithmetic unit and fourlayer MLP models for the NoC router and the RISC-V core. We use an open-source implementation <ref type="bibr" target="#b0">[1]</ref> of ShueNet V2 <ref type="bibr" target="#b14">[15]</ref> for CNN-based power estimation because of its parameter-ecient architecture and fast inference speed. The v0.5 conguration in <ref type="bibr" target="#b14">[15]</ref> is used for the arithmetic units, while the v1.5 conguration is used for the NoC router and RISC-V core. The CNN models are trained from scratch. CNN-default, CNN-partition, and CNN-embedding in Figure <ref type="figure">6</ref> refer to the default 2D encoding, graph-partition-based register mapping, and node-embedding-based register mapping methods introduced in Section 4, respectively.</p><p>Cycle-by-Cycle Power Estimation Results -We use normalized root-mean-squared-error (NRMSE) as our evaluation metric. Suppose the ground-truth power trace is represented as a ndimensional vector y, and the estimated power trace is a vector ? : Performance of dierent machine learning models on test sets -The ML models used by PRIMAL achieve high accuracy for both cycle-by-cycle and average power estimation, while oering signicant speedup against both Synopsys PTPX and the commercial RTL power analysis tool (Comm). PRIMAL is also signicantly more accurate than Comm in average power estimation.</p><p>of the same shape. Then</p><formula xml:id="formula_0">N RMSE = 1 y s ? n i=1 (y i ?i ) 2 n</formula><p>As shown in Figure <ref type="figure">6a</ref>, all ML models can achieve an average estimation error of less than 5% across our benchmarks. The training time for each ML model is summarized in Table <ref type="table" target="#tab_3">2</ref>. For small designs, XGBoost oers competitive accuracy with much less training eort. CNN models show signicant advantage over other ML models for larger designs like the RISC-V core. Notably, our CNN model with default 2D encoding achieves an impressive 5.2% error on the test set, while MLP, XGBoost and Linear model achieves around 8%, 11% and 13% error, respectively. Noticeably, the estimation error for the RISC-V core is considerably higher than other designs. Compared with other benchmarks, the RISC-V core contains ~5x to ~95x more gates but has only ve pipeline stages. Each pipeline stage is signicantly more complex and harder to model. In addition, it is harder to create a comprehensive training set for the models to approximate a larger design. Figure <ref type="figure" target="#fig_8">7</ref> compares the estimation of CNN-default and PCA+ Linear with the ground truth power trace. The CNN estimation ts the ground truth curve more closely. These results demonstrate the superior capability of deep neural networks in approximating complex non-linear functions. We observe that the graph-based register mapping methods do not provide much benet over default 2D encoding because of the rich structural information in the default encoding. In fact, the advanced encodings result in a lower accuracy on some benchmarks. One possible reason is that with the graph-based encoding methods, information of multiple registers is clustered onto a small number of pixels, making it more dicult for the CNN models to distinguish between high-power and low-power samples. Essentially, with the 2D encoding methods, we are trying to gure out the latent space of toggling registers and show it in  Average Power Estimation Results -The average power consumption for a workload can be easily obtained from a cycleaccurate power trace. We compare the ML-based techniques with a commercial RTL power analysis tool (Comm). According to Figure <ref type="figure">6b</ref>, all of the ML techniques achieve less than 1% average error, while the commercial tool has an average error of 20%. NoCRouter has higher error for average power estimation, because the training set and the test set have very dierent average power. Interestingly, while the CNN models achieve similar or higher accuracy compared with other ML models for cycle-by-cycle power estimation, their accuracy for average power estimation is slightly worse because the CNN models tend to consistently overestimate or underestimate power by a very small margin. This behavior may be caused by a mismatch in average power between the training and test sets: the CNN models learn the average power of the training set better, causing a small yet consistent shift in their estimations for the test set. The ML models require a signicant amount of time to be trained for complex designs as shown in Table <ref type="table" target="#tab_3">2</ref>. Therefore, the commercial RTL power analysis tool is still favorable for power estimation of non-reusable modules.</p><p>Speedup-Figure <ref type="figure">6c</ref> presents the speedup of the commercial RTL power analysis tool and the PRIMAL techniques against Synopsys PTPX. Notice that for PRIMAL, the reported speedup is for model inference only, which is the typical use case. While the commercial tool is only ~3x faster than PTPX on average, all ML models achieve much higher estimation speed. Even the most compute-intensive CNN models provide ~50x average speedup against PTPX. Linear model, XGBoost and MLP has an additional 8x, 5x and 10x speedup compared with CNNs, respectively. Note that the linear and gradient tree boosting models are executed on CPU, while MLP and CNN inference is performed on a single GPU. As a result, if more ecient implementations of the ML models and more compute resources are available, higher speedup can be expected with a modest hardware cost. For small designs, linear model and gradient tree boosting are almost always more favorable choices, since the neural network models do not provide signicant accuracy improvement but require much more compute and training eort. For complex designs such as the RISC-V core, CNN provides the best accuracy with ~35x speedup, while other models are faster but less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NoC Router SystemC Power Estimation</head><p>We use the NoC router design as a case study to demonstrate the applicability of PRIMAL to SystemC power estimation. We continue to employ functional verication testbenches as our training set, and test on 3.5k cycles of chip-level convolution testbenches. Cycle-by-cycle power estimation may not be applicable to SystemC designs, because traces that consist of the values of SystemC variables over time are transaction-accurate rather than cycle-accurate. For our specic NoC router design, a SystemC variable trace can differ from the RTL trace by up to seven cycles. Therefore, aside from cycle-by-cycle power estimation, we also train the ML models to estimate the average power inside xed-size time windows. In such cases, we perform an element-wise sum of the encoded features inside the time window and use the results for ML model training and inference. While the SystemC and RTL trace lengths are the same for our training and test sets, this may not be generally true: the eect of SystemC trace inaccuracy is sometimes accumulative, resulting in signicant dierences in the lengths of SystemC and RTL simulation traces. Our feature preprocessing technique does not address this issue, and we leave the question of how to handle trace length mismatch for future work.</p><p>Because SystemC power estimation is more dicult with the nonconstant signal shift, we ne-tune a VGG16 model <ref type="bibr" target="#b21">[22]</ref> pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> for window-by-window estimation. Compared with the ShueNet V2 models, VGG16 has more parameters and larger receptive elds in its convolutional layers, enabling it to model even more complex functions. The estimation accuracy of the VGG16 model with dierent window sizes is shown in Figure <ref type="figure" target="#fig_9">8</ref>. The default 2D encoding is used for this experiment. While the CNN model performs reasonably well for small window sizes, there is a clear error decrease when the window size is larger than seven as the eect of trace inaccuracy is mitigated. When the window size is larger than 8, the CNN model is able to achieve less than 4.5% error, which is satisfactory in most cases. Nevertheless, the error of SystemC power estimation remains higher than that of RTL power estimation because of the trace inaccuracy and the information loss in the feature construction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented PRIMAL, a learning-based framework that enables fast and accurate power estimation for ASIC designs. Using state-of-the-art ML models, PRIMAL can be applied to complex hardware such as a RISC-V core, and the trained power models can generalize to workloads that are dissimilar to the training testbenches. The ML-based techniques achieve less than 5% and 1% average error for cycle-by-cycle and average power estimation, respectively. Compared with Synopsys PTPX, PRIMAL provides at least 50x speedup across our selection of benchmarks. We also demonstrate that PRIMAL can be readily extended to SystemC power estimation through a case study on NoC router.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conventional ASIC power estimation ow vs. PRI-MAL -(a) With existing tools, designers must rely on slow gatelevel power analysis for accurate power proles. (b) PRIMAL trains ML-based power models for reusable IPs. Using the trained models, detailed power traces are obtained by running ML model inference on RTL or timed SystemC simulation traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two phases of the PRIMAL work ow -Power models are trained once per module, then used across dierent workloads and in dierent designs that instantiate the module.In general, existing RTL power estimation techniques either model small circuit sub-modules or try to use simple regression models to characterize the whole design. Sub-module-level modeling cannot accurately reect the power consumption of intermediate logic, and simple regression models such as linear models are not a good t for large, complex designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Simple circuit example with waveform of register outputs and register connection graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Graph-based register mapping schemes -(a) Register mapping based on graph partitioning. The register connection graph is recursively partitioned into two parts. Each partition also divides the map into two non-overlapping parts. (b)Register mapping based on node embedding. Node embedding maps each graph node as a point in high-dimensional space, then dimensionality reduction techniques project the high-dimensional representations onto the 2D space. In the generated mapping each register occupies a unit square whose area is equivalent to one pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure6: Performance of dierent machine learning models on test sets -The ML models used by PRIMAL achieve high accuracy for both cycle-by-cycle and average power estimation, while oering signicant speedup against both Synopsys PTPX and the commercial RTL power analysis tool (Comm). PRIMAL is also signicantly more accurate than Comm in average power estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Ground truth vs. CNN-default and PCA+Linear for RISC-V -Showing 300 cycles from the dhrystone benchmark. the two-dimensional space. This problem itself is an interesting research direction, and we leave it for future work.Average Power Estimation Results -The average power consumption for a workload can be easily obtained from a cycleaccurate power trace. We compare the ML-based techniques with a commercial RTL power analysis tool (Comm). According to Figure6b, all of the ML techniques achieve less than 1% average error, while the commercial tool has an average error of 20%. NoCRouter has higher error for average power estimation, because the training set and the test set have very dierent average power. Interestingly, while the CNN models achieve similar or higher accuracy compared with other ML models for cycle-by-cycle power estimation, their accuracy for average power estimation is slightly worse because the CNN models tend to consistently overestimate or underestimate power by a very small margin. This behavior may be caused by a mismatch in average power between the training and test sets: the CNN models learn the average power of the training set better, causing a small yet consistent shift in their estimations for the test set. The ML models require a signicant amount of time to be trained for complex designs as shown in Table2. Therefore, the commercial RTL power analysis tool is still favorable for power estimation of non-reusable modules.Speedup-Figure6cpresents the speedup of the commercial RTL power analysis tool and the PRIMAL techniques against Synopsys</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: SystemC power estimation accuracy of NoCRouter vs. window size, using a VGG16 CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? We investigate several established ML models for power estimation, and report trade-os between accuracy, training eort, and inference speed. Our study suggests that nonlinear models, especially convolutional neural nets (CNNs), can eectively learn power-related design characteristics for large circuits. ? We explore feature engineering techniques to construct image representations from register traces. The constructed features are used by CNNs for training and inference. ? We demonstrate that PRIMAL is at least 50x faster on average</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Benchmark information -We evaluate PRIMAL with a diverse set of benchmark designs. For NoC router and RISC-V core, the test sets are realistic workloads which are potentially dierent from the corresponding training set.</figDesc><table><row><cell>Design</cell><cell>Description</cell><cell>Register + I/O signal count</cell><cell>Gate count</cell><cell>PTPX throughput (cycles/s)</cell><cell>Training set (# cycles)</cell><cell>Test set (# cycles)</cell></row><row><cell>qadd_pipe</cell><cell>32-bit xed point adder</cell><cell>160</cell><cell>838</cell><cell>1250</cell><cell>Random stimulus (480k)</cell><cell>Random stimulus (120k)</cell></row><row><cell>qmult_pipe{1, 2, 3}</cell><cell>32-bit xed point multiplier with 1, 2, or 3 pipeline stages</cell><cell cols="4">{384, 405, 438} {1721, 1718, 1749} {144.9, 135.1, 156.3} Random stimulus (480k)</cell><cell>Random stimulus (120k)</cell></row><row><cell>oat_adder</cell><cell>32-bit oating point adder</cell><cell>381</cell><cell>1239</cell><cell>714.3</cell><cell>Random stimulus (480k)</cell><cell>Random stimulus (120k)</cell></row><row><cell>oat_mult</cell><cell>32-bit oating point multiplier</cell><cell>372</cell><cell>2274</cell><cell>454.5</cell><cell>Random stimulus (480k)</cell><cell>Random stimulus (120k)</cell></row><row><cell>NoCRouter</cell><cell>Network-on-chip router for a CNN accelerator</cell><cell>5651</cell><cell>15076</cell><cell>44.7</cell><cell cols="2">Unit-level testbenches (910k) Convolution tests (244k)</cell></row><row><cell>RISC-V Core</cell><cell>RISC-V Rocket Core (SmallCore)</cell><cell>24531</cell><cell>80206</cell><cell>45</cell><cell>RISC-V ISA tests (2.2M)</cell><cell>RISC-V benchmarks (1.7M)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training time of dierent ML models</figDesc><table><row><cell>Design</cell><cell>PCA</cell><cell cols="2">Ridge Regression XGBoost</cell><cell>MLP</cell><cell>CNN</cell></row><row><cell cols="2">arithmetic units ~10 min</cell><cell>~1 min</cell><cell>~15 min</cell><cell>~25 min</cell><cell>~3 h</cell></row><row><cell>NoCRouter</cell><cell>~7 h</cell><cell>~15 min</cell><cell>~1 h</cell><cell>~1.5 h</cell><cell>~10 h</cell></row><row><cell>RISC-V Core</cell><cell>~20 h</cell><cell>~30 min</cell><cell>~1.5 h</cell><cell>~7 h *</cell><cell>~20 h *</cell></row><row><cell cols="3">* Use random 50% training data per training epoch.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>PRIMAL stands for power inference using machine learning.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported in part by <rs type="funder">NSF</rs> Award #<rs type="grantNumber">1512937</rs> and the <rs type="funder">Intel ISRA Program</rs>. We thank the anonymous reviewers for providing suggestions and helpful feedback to our work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sNerdmR">
					<idno type="grant-number">1512937</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/opconty/keras-shuenetV2" />
		<title level="m">Keras Implementation of ShueNet V2</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://keras.io/" />
		<title level="m">Keras: The Python Deep Learning library</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Power Estimation Methodology for A High-Level Synthesis Framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Quality of Electronic Design</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Power Estimation Techniques for FPGAs. Transactions on VLSI Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Najm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Rocket Chip Generator</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<idno>UCB/EECS-2016- 17</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Sciences, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regression-Based RTL Power Modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bogliolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Design Automation of Electronic Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-Level Power Estimation and Low-Power Design Space Exploration for FPGAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia and South Pacic Design Automation Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring Network Structure, Dynamics, and Function using NetworkX</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hagberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jollie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Statistical Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Scientic Computing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic Power and Performance Back-Annotation for Fast and Accurate Functional Hardware Simulation. Design, Automation &amp; Test in</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europe</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shuenet v2: Practical Guidelines for Ecient CNN Architecture Design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11164</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Boosting Algorithms as Gradient Descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><surname>Opencores</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="https://opencores.org/project/verilog_xed_point_math_library/manual" />
		<title level="m">Fixed Point Math Library for Verilog :: Manual</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-Learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Ravi</surname></persName>
		</author>
		<title level="m">Ecient RTL Power Estimation for Large Designs. International Conference on VLSI Design</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aladdin: A Pre-RTL, Power-Performance Accelerator Simulator Enabling Large Design Space Exploration of Customized Architectures</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PrEsto: An FPGA-Accelerated Power Estimation Methodology for Complex Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Early Stage Real-Time SoC Power Estimation using RTL Instrumentation. Asia and South Pacic Design Automation Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
