<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Red-Blue Pebbling Revisited: Near Optimal Parallel Matrix-Matrix Multiplication</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-29">29 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Grzegorz</forename><surname>Kwasniewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marko</forename><surname>Kabić</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Swiss National Supercomputing Centre (CSCS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joost</forename><surname>Vandevondele</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Swiss National Supercomputing Centre (CSCS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ra</forename><surname>Aele Solcà</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Swiss National Supercomputing Centre (CSCS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Torsten</forename><surname>Hoe Er</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Red-Blue Pebbling Revisited: Near Optimal Parallel Matrix-Matrix Multiplication</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-29">29 Aug 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1908.09606v2[cs.CC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose COSMA: a parallel matrix-matrix multiplication algorithm that is near communication-optimal for all combinations of matrix dimensions, processor counts, and memory sizes. e key idea behind COSMA is to derive an optimal (up to a factor of 0.03% for 10MB of fast memory) sequential schedule and then parallelize it, preserving I/O optimality. To achieve this, we use the red-blue pebble game to precisely model MMM dependencies and derive a constructive and tight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D algorithms, which x processor decomposition upfront and then map it to the matrix dimensions, it reduces communication volume by up to √ 3 times. COSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all scenarios up to 12.8x (2.2x on average), achieving up to 88% of Piz Daint's peak performance. Our work does not require any hand tuning and is maintained as an open source implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Matrix-matrix multiplication (MMM) is one of the most fundamental building blocks in scienti c computing, used in linear algebra algorithms <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b42">42]</ref>, (Cholesky and LU decomposition <ref type="bibr" target="#b42">[42]</ref>, eigenvalue factorization <ref type="bibr" target="#b13">[13]</ref>, triangular solvers <ref type="bibr" target="#b15">[15]</ref>), machine learning <ref type="bibr" target="#b6">[6]</ref>, graph processing <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b52">52]</ref>, computational chemistry <ref type="bibr" target="#b21">[21]</ref>, and others. us, accelerating MMM routines is of great signi cance for many domains. In this work, we focus on minimizing the amount of transferred data in MMM, both across the memory hierarchy (vertical I/O) and between processors (horizontal I/O, aka "communication") 1 . e path to I/O optimality of MMM algorithms is at least 50 years old.</p><p>e rst parallel MMM algorithm is by Cannon <ref type="bibr" target="#b10">[10]</ref>, which works for square matrices and square processor decompositions. Subsequent works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref> generalized the MMM algorithm to rectangular matrices, di erent processor decompositions, and communication pa erns. PUMMA <ref type="bibr" target="#b17">[17]</ref> package generalized previous works to transposed matrices and di erent data layouts. SUMMA algorithm <ref type="bibr" target="#b56">[56]</ref> further extended it by optimizing the communication, introducing pipelining and communication-computation overlap.</p><p>is is now a state-of-the-art so-called 2D algorithm (it decomposes processors in a 2D grid) used e.g., in ScaLAPACK library <ref type="bibr" target="#b14">[14]</ref>.</p><p>Agarwal et al. <ref type="bibr" target="#b0">[1]</ref> showed that in a presence of extra memory, one can do be er and introduces a 3D processor decomposition. e 2.5D algorithm by Solomonik and Demmel <ref type="bibr" target="#b53">[53]</ref> e ectively interpolates between those two results, depending on the available memory. However, <ref type="bibr">Demmel et al.</ref> showed that algorithms 1 We also focus only on "classical" MMM algorithms which perform n 3 multiplications and additions. We do not analyze Strassen-like routines <ref type="bibr" target="#b54">[54]</ref>, as in practice they are o en slower <ref type="bibr" target="#b19">[19]</ref>. optimized for square matrices o en perform poorly when matrix dimensions vary signi cantly <ref type="bibr" target="#b22">[22]</ref>. Such matrices are common in many relevant areas, for example in machine learning <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b61">61]</ref> or computational chemistry <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b49">49]</ref>. ey introduced CARMA <ref type="bibr" target="#b22">[22]</ref>, a recursive algorithm that achieves asymptotic lower bounds for all con gurations of dimensions and memory sizes. is evolution for chosen steps is depicted symbolically in Figure <ref type="figure" target="#fig_13">2</ref>.</p><p>Unfortunately, we observe several limitations with state-of-the art algorithms. ScaLAPACK <ref type="bibr" target="#b14">[14]</ref> (an implementation of SUMMA) supports only the 2D decomposition, which is communicationine cient in the presence of extra memory. Also, it requires a user to ne-tune parameters such as block sizes or a processor grid size. CARMA supports only scenarios when the number of processors is a power of two <ref type="bibr" target="#b22">[22]</ref>, a serious limitation, as the number of processors is usually determined by the available hardware resources. Cyclops Tensor Framework (CTF) <ref type="bibr" target="#b50">[50]</ref> (an implementation of the 2.5D decomposition) can utilize any number of processors, but its decompositions may be far from optimal ( § 9). We also emphasize that asymptotic complexity is an insu cient measure of practical performance. We later ( § 6.2) identify that CARMA performs up to √ 3 more communication. Our observations are summarized in Table <ref type="table">1</ref>.</p><p>eir practical implications are shown in Figure <ref type="figure" target="#fig_10">1</ref>, where we see that all existing algorithms perform poorly for some con gurations.</p><p>In this work, we present COSMA (Communication Optimal Spartition-based Matrix multiplication Algorithm): an algorithm 2D <ref type="bibr" target="#b56">[56]</ref> 2.5D <ref type="bibr" target="#b53">[53]</ref> recursive <ref type="bibr" target="#b22">[22]</ref> COSMA (this work)</p><p>Input: User-specified grid Available memory Available memory, matrix dimensions Available memory, matrix dimensions</p><p>Step 1 Split m and n Split m, n, k Split recursively the largest dimension Find the optimal sequential schedule</p><p>Step 2 Map matrices to processor grid Map matrices to processor grid Map matrices to recursion tree Map sequential domain to matrices ŏ Requires manual tuning ŏ Asymptotically more comm.</p><p>Optimal for m = n ŏ Ine icient for m n or n m ŏ Ine icient for some p</p><p>Asymptotically optimal for all m, n, k, p ŏ Up to √ 3 times higher comm. cost ŏ p must be a power of 2 Optimal for all m, n, k Optimal for all p Best time-to-solution Table <ref type="table">1</ref>: Intuitive comparison between the COSMA algorithm and the state-of-the-art 2D, 2.5D, and recursive decompositions. C = AB, A ∈ R m×k , B ∈ R k ×n that takes a new approach to multiplying matrices and alleviates the issues above. COSMA is I/O optimal for all combinations of parameters (up to the factor of √ S/( √ S + 1−1), where S is the size of the fast memory 2 ). e driving idea is to develop a general method of deriving I/O optimal schedules by explicitly modeling data reuse in the red-blue pebble game. We then parallelize the sequential schedule, minimizing the I/O between processors, and derive an optimal domain decomposition. is is in contrast with the other discussed algorithms, which x the processor grid upfront and then map it to a sequential schedule for each processor. We outline the algorithm in § 3. To prove its optimality, we rst provide a new constructive proof of a sequential I/O lower bound ( § 5.2.7), then we derive the communication cost of parallelizing the sequential schedule ( § 6.2), and nally we construct an I/O optimal parallel schedule ( § 6.3). e detailed communication analysis of COSMA, 2D, 2.5D, and recursive decompositions is presented in Table <ref type="table" target="#tab_6">3</ref>. Our algorithm reduces the data movement volume by a factor of up to √ 3 ≈ 1.73x compared to the asymptotically optimal recursive decomposition and up to max{m, n, k} times compared to the 2D algorithms, like Cannon's <ref type="bibr" target="#b39">[39]</ref> or SUMMA <ref type="bibr" target="#b56">[56]</ref>.</p><p>Our implementation enables transparent integration with the ScaLAPACK data format <ref type="bibr" target="#b16">[16]</ref> and delivers near-optimal computation throughput. We later ( § 7) show that the schedule naturally expresses communication-computation overlap, enabling even higher speedups using Remote Direct Memory Access (RDMA). Finally, our I/O-optimal approach is generalizable to other linear algebra kernels. We provide the following contributions:</p><p>• We propose COSMA: a distributed MMM algorithm that is nearlyoptimal (up to the factor of √ S/( √ S + 1 − 1)) for any combination of input parameters ( § 3).</p><p>• Based on the red-blue pebble game abstraction <ref type="bibr" target="#b34">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lemma 4), which may be used to generate optimal schedules ( § 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We rst describe our machine model ( § 2.1) and computation model ( § 2.2). We then de ne our optimization goal: the I/O cost ( § 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Model</head><p>We model a parallel machine with p processors, each with local memory of size S words. A processor can send and receive from any other processor up to S words at a time. To perform any computation, all operands must reside in processor' local memory. If shared memory is present, then it is assumed that it has in nite capacity. A cost of transferring a word from the shared to the local memory is equal to the cost of transfer between two local memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation Model</head><p>We now brie y specify a model of general computation; we use this model to derive the theoretical I/O cost in both the sequential and parallel se ing. An execution of an algorithm is modeled with the computational directed acyclic graph (CDAG) G = (V , E) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>A vertex ∈ V represents one elementary operation in the given computation. An edge (u, ) ∈ E indicates that an operation depends on the result of u. A set of all immediate predecessors (or successors) of a vertex are its parents (or children). Two selected subsets I, O ⊂ V are inputs and outputs, that is, sets of vertices that have no parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type="bibr" target="#b34">[34]</ref> models an execution of an algorithm in a two-level memory structure with a small-and-fast as well as large-and-slow memory.</p><p>A red (or a blue) pebble placed on a vertex of a CDAG denotes that the result of the corresponding elementary computation is inside the fast (or slow) memory. In the initial (or terminal) con guration, only inputs (or outputs) of the CDAG have blue pebbles. ere can be at most S red pebbles used at any given time. A complete CDAG calculation is a sequence of moves that lead from the initial to the terminal con guration. One is allowed to: place a red pebble on any vertex with a blue pebble (load), place a blue pebble on any vertex with a red pebble (store), place a red pebble on a vertex whose parents all have red pebbles (compute), remove any pebble, red or blue, from any vertex (free memory). An I/O optimal complete CDAG calculation corresponds to a sequence of moves (called pebbling of a graph) which minimizes loads and stores. In the MMM context, it is an order in which all n 3 multiplications are performed. In scenario (a), a straightforward 3D decomposition divides every dimension in p 1/3 = 2. In scenario (b), COSMA starts by finding a near optimal sequential schedule and then parallelizes it minimizing crossing data reuse V R, i ( § 5). The total communication volume is reduced by 17% compared to the former strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization Goals</head><p>roughout this paper we focus on the input/output (I/O) cost of an algorithm. e I/O cost Q is the total number of words transferred during the execution of a schedule. On a sequential or shared memory machine equipped with small-and-fast and slow-and-big memories, these transfers are load and store operations from and to the slow memory (also called the vertical I/O). For a distributed machine with a limited memory per node, the transfers are communication operations between the nodes (also called the horizontal I/O). A schedule is I/O optimal if it minimizes the I/O cost among all schedules of a given CDAG. We also model a latency cost L, which is a maximum number of messages sent by any processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">State-of-the-Art MMM Algorithms</head><p>Here we brie y describe strategies of the existing MMM algorithms.</p><p>roughout the whole paper, we consider matrix multiplication</p><formula xml:id="formula_0">C = AB, where A ∈ R m×k , B ∈ R k ×n , C ∈ R m×n ,</formula><p>where m, n, and k are matrix dimensions. Furthermore, we assume that the size of each matrix element is one word, and that S &lt; min{mn, mk, nk}, that is, none of the matrices ts into single processor's fast memory.</p><p>We compare our algorithm with the 2D, 2.5D, and recursive decompositions (we select parameters for 2.5D to also cover 3D). We assume a square processor grid [ √ p, √ p, 1] for the 2D variant, analogously to Cannon's algorithm <ref type="bibr" target="#b10">[10]</ref>, and a cubic grid [ p/c, p/c, c] for the 2.5D variant <ref type="bibr" target="#b53">[53]</ref>, where c is the amount of the "extra" memory c = pS/(mk + nk). For the recursive decomposition, we assume that in each recursion level we split the largest dimension m, n, or k in half, until the domain per processor ts into memory. e detailed complexity analysis of these decompositions is in Table <ref type="table" target="#tab_6">3</ref>. We note that ScaLAPACK or CTF can handle non-square decompositions, however they create di erent problems, as discussed in § 1. Moreover, in § 9 we compare their performance with COSMA and measure signi cant speedup in all scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COSMA: HIGH-LEVEL DESCRIPTION</head><p>COSMA decomposes processors by parallelizing the near optimal sequential schedule under constraints: (1) equal work distribution and (2) equal memory size per processor. Such a local sequential schedule is independent of matrix dimensions. us, intuitively, instead of dividing a global domain among p processors (the topdown approach), we start from deriving a near I/O optimal sequential schedule. We then parallelize it, minimizing the I/O and latency costs Q, L (the bo om-up approach); Figure <ref type="figure" target="#fig_1">3</ref> presents more details. COSMA is sketched in Algorithm 1. In Line 1 we derive a sequential schedule, which consists of series of a ×a outer products. (Figure <ref type="figure" target="#fig_11">4 b</ref>). In Line 2, each processor is assigned to compute b of these products, forming a local domain D (Figure <ref type="figure" target="#fig_11">4 c</ref>), that is each D contains a × a × b vertices (multiplications to be performed -the derivation of a and b is presented in § 6.3). In Line 3, we nd a processor grid G that evenly distributes this domain by the matrix dimensions m, n, and k. If the dimensions are not divisible by a or b, this function also evaluates new values of a opt and b opt by ing the best matching decomposition, possibly not utilizing some processors ( § 7.1, Figure <ref type="figure" target="#fig_11">4 d-f</ref>). e maximal number of idle processors is a tunable parameter δ . In Line 5, we determine the initial decomposition of matrices A, B, and C to the submatrices A l , B l , C l that are local for each processor. COSMA may handle any initial data layout, however, an optimal block-recursive one ( § 7.6) may be achieved in a preprocessing phase. In Line 6, we compute the size of the communication step, that is, how many of b opt outer products assigned to each processor are computed in a single round, minimizing the latency ( § 6.3). In Line 7 we compute the number of sequential steps (Lines 8-11) in which every processor:</p><p>(1) distributes and updates its local data A l and B l among the grid G (Line 9), and (2) multiplies A l and B l (Line 10). Finally, the partial results C l are reduced over G (Line 12).</p><p>I/O Complexity of COSMA Lines 2-7 require no communication (assuming that the parameters m, n, k, p, S are already distributed). e loop in Lines 8-11 executes 2ab/(S − a 2 ) times. In Line 9, each processor receives |A l | + |B l | elements. Sending the partial results in Line 12 adds a 2 communicated elements. In § 6.3 we derive the optimal values for a and b, which yield a total of min S + C ← Reduce(C l , G) reduce the partial results 13: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ARBITRARY CDAGS: LOWER BOUNDS</head><p>We now present a mathematical machinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type="bibr" target="#b34">[34]</ref>, which provides a method to nd an I/O lower bound for a given CDAG. at lemma, however, does not give a tight bound, as it overestimates a reuse set size (cf. Lemma 3). Our key result here, Lemma 4, allows us to derive a constructive proof of a tighter I/O lower bound for a sequential execution of the MMM CDAG ( § 5).</p><p>e driving idea of both Hong and Kung's and our approach is to show that some properties of an optimal pebbling of a CDAG (a problem which is PSPACE-complete <ref type="bibr" target="#b40">[40]</ref>) can be translated to the properties of a speci c partition of the CDAG (a collection of subsets V i of the CDAG; these subsets form subcomputations, see § 2.2).</p><p>One can use the properties of this partition to bound the number of I/O operations of the corresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type="bibr" target="#b34">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, called Xpartition, that is the base of our analysis. We describe symbols used in our analysis in Table <ref type="table">2</ref>.</p><formula xml:id="formula_1">MMM m, n, k Matrix dimensions A, B Input matrices A ∈ R m×k and B ∈ R k ×n C = AB Output matrix C ∈ R m×n p</formula><p>The number of processors graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head><p>A directed acyclic graph G = (V , E) P r ed( ) A set of immediate predecessors of a vertex :</p><formula xml:id="formula_2">P r ed ( ) = {u : (u, ) ∈ E } Succ( ) A set of immediate successors of a vertex : Succ( ) = {u : ( , u) ∈ E } I/O complexity</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>The number of red pebbles (size of the fast memory)</p><formula xml:id="formula_3">V i An i-th subcomputation of an S -partition Dom(V i ), Min(V i ) Dominator and minimum sets of subcomputation V i V R,i</formula><p>The reuse set: a set of vertices containing red pebbles (just before V i starts) and used by V i H (S )</p><p>The smallest cardinality of a valid S -partition R(S )</p><p>The maximum size of the reuse set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q</head><p>The I/O cost of a schedule (a number of I/O operations)</p><formula xml:id="formula_4">ρ i</formula><p>The computational intensity of V i ρ = max i {ρ i }</p><p>The maximum computational intensity Schedules S = {V 1 , . . . , V h } The sequential schedule (an ordered set of V i ) P = {S 1 , . . . , Sp } The parallel schedule (a set of sequential schedules S j )</p><formula xml:id="formula_5">D j = V i ∈S j V i</formula><p>The local domain (a set of vertices in S j a, b</p><p>Sizes of a local domain:</p><formula xml:id="formula_6">| D j | = a 2 b</formula><p>Table <ref type="table">2</ref>: The most important symbols used in the paper.</p><p>X -Partitions Before we de ne X -partitions, we rst need to de ne two sets, the dominator set and the minimum set. Given a subset V i ∈ V , de ne a dominator set Dom(V i ) as a set of vertices in V , such that every path from any input of a CDAG to any vertex in V i must contain at least one vertex in Dom(V i ). De ne also the minimum set Min(V i ) as the set of all vertices in V i that do not have <ref type="bibr" target="#b3">(3)</ref> have no cyclic dependencies between them, and (4) their dominator and minimum sets are at most of size</p><formula xml:id="formula_7">any children in V i . Now, given a CDAG G = (V , E), let V 1 , V 2 , . . . V h ∈ V be a series of subcomputations that (1) are pairwise disjoint (∀ i, j,i j V i ∩ V j = ∅), (2) cover the whole CDAG ( i V i = V ),</formula><formula xml:id="formula_8">X (∀ i (|Dom(V i )| ≤ X ∧ |Min(V i )| ≤ X )).</formula><p>ese subcomputations V i correspond to some execution order (a schedule) of the CDAG, such that at step i, only vertices in V i are pebbled. We call this series an X -partition or a schedule of the CDAG and denote this schedule with S(X ) = {V 1 , . . . , V h }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Existing General I/O Lower Bound</head><p>Here we need to brie y bring back the original lemma by Hong and Kung, together with an intuition of its proof, as we use a similar method for our Lemma 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intuition</head><p>e key notion in the existing bound is to use X = 2S-partitions for a given fast memory size S. For any subcomputation V i , if |Dom(V i )| = 2S, then at most S of them could contain a red pebble before V i begins. us, at least S additional pebbles need to be loaded from the memory. e similar argument goes for Min(V i ). erefore, knowing the lower bound on the number of sets V i in a valid 2S-partition, together with the observation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type="bibr" target="#b34">[34]</ref>). e minimal number Q of I/O operations for any valid execution of a CDAG of any I/O computation is bounded by</p><formula xml:id="formula_9">Q ≥ S • (H (2S) − 1)<label>(1)</label></formula><p>P . Assume that we know the optimal complete calculation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type="bibr" target="#b34">[34]</ref>. Divide the complete calculation into h consecutive subcomputations V 1 , V 2 , ..., V h , such that during the execution of V i , i &lt; h, there are exactly S I/O operations, and in V h there are at most S operations. Now, for each V i , we de ne two subsets of V , V R,i and V B,i . V R,i contains vertices that have red pebbles placed on them just before V i begins. V B,i contains vertices that have blue pebbles placed on them just before V i begins, and have red pebbles placed on them during V i . Using these de nitions, we have:</p><formula xml:id="formula_10">V R,i ∪ V B,i = Dom(V i ), |V R,i | ≤ S, |V B,i | ≤ S, and |V R,i ∪ V B,i | ≤ |V R,i | + |V B,i | ≤ 2S.</formula><p>We de ne similar subsets W B,i and W R,i for the minimum set Min(V i ). W B,i contains all vertices in V i that have a blue pebble placed on them during V i , and W R,i contains all vertices in V i that have a red pebble at the end of V i . By the de nition of V i , |W B,i | ≤ S, by the constraint on the red pebbles, we have |W R,i | ≤ S, and by te de nition of the minimum set,Min(V i ) ⊂ W R,i ∪ W B,i . Finally, by the de nition of S-partition, V 1 , V 2 , ..., V h form a valid 2S-partition of the CDAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalized I/O Lower Bounds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Data Reuse.</head><p>A more careful look at sets V R,i , V B,i ,W R,i , and W B,i allows us to re ne the bound on the number of I/O operations on a CDAG. By de nition, V B,i is a set of vertices on which we place a red pebble using the load rule; We call V B,i a load set of V i . Furthermore, W B,i contains all the vertices on which we place a blue pebble during the pebbling of V i ; We call W B,i a store set of V i . However, we impose more strict V R,i and W R,i de nitions: V R,i contains vertices that have red pebbles placed on them just before V i begins and -for each such vertex ∈ V R,i -at least one child of is pebbled during the pebbling of V i using the compute rule of the red-blue pebble game. We call V R,i a reuse set of V i . Similarly, W R,i contains vertices that have red pebbles placed on them a er V i ends and were pebbled during V i and -for each such vertex ∈ W R,iat least one child of is pebbled during the pebbling of V i+1 using the compute rule of the red-blue pebble game. We call W R,i a cache set</p><formula xml:id="formula_11">of V i . erefore, if Q i is the number of I/O operations during the subcomputation V i , then Q i ≥ |V B,i | + |W B,i |.</formula><p>We rst observe that, given the optimal complete calculation, one can divide this calculation into subcomputations such that each subcomputation V i performs an arbitrary number of Y I/O operations. We still have |V R,i | ≤ S, |W R,i | ≤ S, 0 ≤ |W B,i | (by the de nition of the red-blue pebble game rules). Moreover, observe that, because we perform exactly Y I/O operations in each subcomputation, and all the vertices in V B,i by de nition have to be loaded,</p><formula xml:id="formula_12">|V B,i | ≤ Y . A similar argument gives 0 ≤ |W B,i | ≤ Y . Denote an upper bound on |V R,i | and |W B,i | as R(S) (∀ i max{|V R,i |, |W B,i |} ≤ R(S) ≤ S). Further, denote a lower bound on |V B,i | and |W B,i | as T (S) (∀ i 0 ≤ T (S) ≤ min{|V B,i |, |W B,i |}).</formula><p>We can use R(S) and T (S) to tighten the bound on Q. We call R(S) a maximum reuse and T (S) a minimum I/O of a CDAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Reuse-Based Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type="bibr" target="#b34">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>2. An optimal complete calculation of a CDAG G = (V , E), which performs q I/O operations, is associated with an X -partition of</p><formula xml:id="formula_13">G such that q ≥ (X − R(S) − T (S)) • (h − 1)</formula><p>for any value of X ≥ S, where h is the number of subcomputations in the X -partition, R(S) is the maximum reuse set size, and T (S) is the minimum I/O in the given X -partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>. We use analogous reasoning as in the original lemma. We associate the optimal pebbling with h consecutive subcomputations V 1 , . . . V h with the di erence that each subcomputation V i performs Y = X − R(S) + T (S) I/O operations. Within those Y operations, we consider separately q i,s store and q i,l load operations. For each V i we have q i,s + q i,l = Y , q i,s ≥ T (S), and</p><formula xml:id="formula_14">q i,l ≤ Y − T (S) = X − R(S). ∀ i : |V B,i | ≤ q l,i ≤ Y − T (S) ∀ i : |V R,i | ≤ q s,i ≤ R(S) ≤ S Since V R,i ∪ V B,i = Dom(V i ): |Dom(V i )| ≤ |V R,i | + |V B,i | |Dom(V i )| ≤ R(S) + Y − T (R) = X</formula><p>By an analogous construction for store operations, we show that |Min(V i )| ≤ X . To show that S(X ) = {V 1 . . . V h } meets the remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type="bibr" target="#b34">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; (X − R(S)</p><formula xml:id="formula_15">+ T (S)) • (h − 1) I/O operations has an associated S(X ), such that |S(X )| = h (if q = (X −R(S)+T (S))•(h −1), then |S(X )| = h −1).</formula><p>From the previous lemma, we obtain a tighter I/O lower bound. L 3. Denote H (X ) as the minimum number of subcomputations in any valid X -partition of a CDAG G = (V , E), for any X ≥ S.</p><p>e minimal number Q of I/O operations for any valid execution of a CDAG G = (V , E) is bounded by</p><formula xml:id="formula_16">Q ≥ (X − R(S) + T (S)) • (H (X ) − 1)<label>(2)</label></formula><p>where R(S) is the maximum reuse set size and T (S) is the minimum store set size. Moreover, we have</p><formula xml:id="formula_17">H (X ) ≥ |V | |V max |<label>(3)</label></formula><p>where</p><formula xml:id="formula_18">V max = arg max V i ∈S(X ) |V i | is the largest subset of vertices in the CDAG schedule S(X ) = {V 1 , . . . , V h }. P . By de nition, H (X ) = min S(X ) |S(X )| ≤ h, so Q ≥ (X − R(S) + T (S)) • (H (X ) − 1) immediately follows from Lemma 2.</formula><p>To prove Eq. ( <ref type="formula" target="#formula_17">3</ref>), observe that V max by de nition is the largest subset in the optimal X -partition. As the subsets are disjoint, any other subset covers fewer remaining vertices to be pebbled than V max . Because there are no cyclic dependencies between subsets, we can order them topologically as V 1 , V 2 , ...V H (X ) . To ensure that the indices are correct, we also de ne V 0 ≡ ∅. Now, de ne W i to be the set of vertices not included in any subset from 1 to i, that is</p><formula xml:id="formula_19">W i = V − i j=1 V j . Clearly, W 0 = V and W H (X ) = ∅. en, we have ∀ i |V i | ≤ |V max | |W i | = |W i−1 | − |V i | ≥ |W i−1 | − |V max | ≥ |V | − i |V max | |W H (X ) | = 0 ≥ |V | − H (X ) • |V max | that is, a er H (X ) steps, we have H (X )|V max | ≥ |V |.</formula><p>From this lemma, we derive the following lemma that we use to prove a tight I/O lower bound for MMM ( eorem 1):</p><formula xml:id="formula_20">L 4.</formula><p>De ne the number of computations performed by V i for one loaded element as the computational intensity </p><formula xml:id="formula_21">ρ i = |V i | X −|V R, i |+|W B,i | of the subcomputation V i . Denote ρ = max i (ρ i ) ≤ |V max | X −R(S )</formula><formula xml:id="formula_22">Q = H (X ) i=1 Q i ≥ H (X ) i=1 |V i | ρ = |V | ρ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TIGHT I/O LOWER BOUNDS FOR MMM</head><p>In this section, we present our main theoretical contribution: a constructive proof of a tight I/O lower bound for classical matrix-matrix multiplication. In § 6, we extend it to the parallel setup ( eorem 2). is result is tight (up to diminishing factor</p><formula xml:id="formula_23">√ S/( √ S + 1 − 1)</formula><p>), and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type="bibr" target="#b34">[34]</ref> derived an asymptotic bound Ω n 3 / √ S for the sequential case. Irony et al. <ref type="bibr" target="#b33">[33]</ref> extended the lower bound result to a parallel machine with p processes, each having a fast private memory of size S, proving the e proof of eorem 1 requires Lemmas 5 and 6, which in turn, require several de nitions. Intuition: Restricting the analysis to greedy schedules provides explicit information of a state of memory (sets V r , V R,r , W B,r ), and to a corresponding CDAG pebbling. Additional constraints ( § 5.2.7) guarantee feasibility of a derived schedule (and therefore, lower bound tightness). For every t 3 th partial update of element (t 1 , t 2 ) in matrix C, and an associated point = (c, (t 1 , t 2 , t 3 )) ∈ C we de ne ϕ c ( ) = (t 1 , t 2 ) to be a projection of this point to matrix C, ϕ a ( ) = (a, (t 1 , t 3 )) ∈ A is its projection to matrix A, and ϕ b ( ) = (b, (t 3 , t 2 )) ∈ B is its projection to matrix B. Note that while ϕ a ( ), ϕ b ( ) ∈ V , projection ϕ c ( ) V has not any associated point in V . Instead, vertices associated with all k partial updates of an element of C have the same projection ϕ c ( ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">De nitions</head><formula xml:id="formula_24">∀ =(c,(p 1 ,p 2 ,p 3 )),w =(c,(q 1 ,q 2 ,q 3 ))∈ C : (p 1 = q 1 ) ∧ (p 2 = q 2 ) ⇐⇒ ϕ c (p) = ϕ c (q) (4) As a consequence, ϕ c ((c, (t 1 , t 2 , t 3 ))) = ϕ c ((c, (t 1 , t 2 , t 3 − 1))). A t 3 th update of (t 1 , t 2 ) element in matrix C of a classical MMM is formulated as C(t 1 , t 2 , t 3 ) = C(t 1 , t 2 , t 3 − 1) + A(t 1 , t 3 ) • B(t 3 , t 2 ).</formula><p>erefore for each = (c, (t 1 , t 2 , t 3 )) ∈ C, t 3 &gt; 1, we have following edges in the CDAG: (ϕ a ( ), ), (ϕ b ( ), ), (c, (t 1 , t 2 , t 3 − 1)), ) ∈ E. 5.1.2 α, β, γ, Γ. For a given subcomputation V r ⊆ C, we denote its projection to matrix A as α r = ϕ a (V r ) = { : = ϕ a (c), c ∈ V r }, its projection to matrix B as β r = ϕ b (V r ), and its projection to matrix C as γ r = ϕ c (V r ). We further de ne Γ r ⊂ C as a set of all vertices in C that have a child in V r . e sets α, β, Γ therefore correspond to the inputs of V r that belong to matrices A, B, and previous partial results of C, respectively.</p><p>ese inputs form a minimal dominator set of V r :</p><formula xml:id="formula_25">Dom(V r ) = α r ∪ β r ∪ Γ r<label>(5)</label></formula><p>Because Min(V r ) ⊂ C, and each vertex ∈ C has at most one child w with ϕ c ( ) = ϕ c (w) (Equation <ref type="formula">4</ref>), the projection ϕ c (Min(V r )) is also equal to γ r :</p><formula xml:id="formula_26">ϕ c (V r ) = ϕ c (Γ r ) = ϕ c (Min(V r )) = γ r<label>(6)</label></formula><p>5.1.3 Red(). De ne Red(r ) as the set of all vertices that have red pebbles just before subcomputation V r starts, with Red(1) = ∅. We further have Red(P), P ⊂ V is the set of all vertices in some subset P that have red pebbles and Red(ϕ c (P)) is a set of unique pairs of rst two coordinates of vertices in P that have red pebbles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Greedy schedule.</head><p>We call a schedule S = {V 1 , . . . , V h } greedy if during every subcomputation V r every vertex u that will hold a red pebble either has a child in V r or belongs to V r :</p><formula xml:id="formula_27">∀ r : Red(r ) ⊂ α r −1 ∪ β r −1 ∪ V r −1 (7)</formula><p>5.2 I/O Optimality of Greedy Schedules </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>. We start by creating an X -partition for an MMM CDAG (the values of Y and R(S) are parameters that we determine in the course of the proof). e proof is divided into the following 6 steps (Sections 5.2.1 to 5.2.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Red Pebbles During and A er Subcomputation. Observe that each vertex in</head><formula xml:id="formula_28">c = (t 1 , t 2 , t 3 ) ∈ C, t 1 = 1 . . . m, t 2 = 1 . . . n, t 3 = 1 . . . k − 1 has only one child c = (t 1 , t 2 , t 3 + 1).</formula><p>erefore, we can assume that in an optimal schedule there are no two vertices (t 1 , t 2 , t 3 ), (t 1 , t 2 , t 3 + f ) ∈ C, f ∈ N + that simultaneously hold a red vertex, as when the vertex (t 1 , t 2 , t 3 + 1) is pebbled, a red pebble can be immediately removed from (t 1 , t 2 , t 3 ):</p><formula xml:id="formula_29">|Red(V r )| = |ϕ c (Red(V r ))|<label>(8)</label></formula><p>On the other hand, for every vertex , if all its predecessors Pred( ) have red pebbles, then vertex may be immediately computed, freeing a red pebble from its predecessor w ∈ C, due to the fact, that is the only child of w:</p><formula xml:id="formula_30">∀ ∈V ∀ r : Pred( ) ⊂ Dom(V r ) ∪ V r =⇒ ∃ t ≤r ∈ V t<label>(9)</label></formula><p>Furthermore, a er subcomputation V r , all vertices in V r that have red pebbles are in its minimum set:</p><formula xml:id="formula_31">Red(r + 1) ∩ V r = Red(r + 1) ∩ Min(V r )<label>(10)</label></formula><p>Combining this result with the de nition of a greedy schedule (Equation <ref type="formula">7</ref>), we have</p><formula xml:id="formula_32">Red(r + 1) ⊆ α r ∪ β r ∪ Min(V r )<label>(11)</label></formula><p>5.2.2 Surface and volume of subcomputations. By the de nition of X -partition, the computation is divided into</p><formula xml:id="formula_33">H (X ) subcomputa- tions V r ⊂ C, r ∈ {1, . . . H (X )}, such that Dom(V r ), Min(V r ) ≤ X .</formula><p>Inserting Equations 5, 6, and 8, we have:</p><formula xml:id="formula_34">|Dom(V r )| = |α r | + |β r | + |γ r | ≤ X (12) |Min(V r )| = |γ r | ≤ X</formula><p>On the other hand, the Loomis-Whitney inequality <ref type="bibr" target="#b41">[41]</ref> bounds the volume of V r :</p><formula xml:id="formula_35">V r ≤ |α r ||β r ||γ r |<label>(13)</label></formula><p>Consider sets of all di erent indices accessed by projections α r , β r , γ r : </p><formula xml:id="formula_36">T 1 = {t 1,1 , . . . , t 1,a }, |T 1 | = a T 2 = {t 2,1 , . . . , t 2,b }, |T 2 | = b T 3 = {t 3,1 , . . . , t 3,c }, |T 3 | = c α r ⊆ {(t 1 , t 3 ) : t 1 ∈ T 1 , t 3 ∈ T 3 } (14) β r ⊆ {(t 3 , t 2 ) : t 3 ∈ T 3 , t 2 ∈ T 2 } (15) γ r ⊆ {(t 1 , t 2 ) : t 1 ∈ T 1 , t 2 ∈ T 2 } (16) V r ⊆ {(t 1 , t 2 , t 3 ) : t 1 ∈ T 1 , t 2 ∈ T 2 , t 3 ∈ T 3 }<label>(17</label></formula><formula xml:id="formula_37">|α r | = ac, |β r | = bc, |γ r | = ab, |V r | = abc,<label>(18)</label></formula><p>achieving the upper bound (Equation <ref type="formula" target="#formula_35">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Reuse set V R,r and store set W B,r . Consider two subsequent computations, V r and V r +1 . A er V r , α r , β r , and V r may have red pebbles (Equation <ref type="formula">7</ref>). On the other hand, for the domi-</p><formula xml:id="formula_38">nator set of V r +1 we have |Dom(V r +1 )| = |α r +1 | + |β r +1 | + |γ r +1 |.</formula><p>en, the reuse set V R,i+1 is an intersection of those sets. Since α r ∩ β r = α r ∩ γ r = β r ∩ γ r = ∅, we have (confront Equation <ref type="formula" target="#formula_32">11</ref>):</p><formula xml:id="formula_39">V R,r +1 ⊆ (α r ∩ α r +1 ) ∪ (β r ∩ β r +1 ) ∪ (Min(V r ) ∩ Γ r +1 ) |V R,r +1 | ≤ |α r ∩ α r +1 | + |β r ∩ β r +1 | + |γ r ∩ γ r +1 |<label>(19)</label></formula><p>Note that vertices in α r and β r are inputs of the computation: therefore, by the de nition of the red-blue pebble game, they start in the slow memory (they already have blue pebbles). Min(V r ), on the other hand, may have only red pebbles placed on them. Furthermore, by the de nition of the S-partition, these vertices have children that have not been pebbled yet.</p><p>ey either have to be reused forming the reuse set V R,r +1 , or stored back, forming W B,r and requiring the placement of the blue pebbles. Because Min(V r ) ∈ C and C ∩ A = C ∩ B = ∅, we have: if at most one of the overlapping projections is non-empty (and therefore, there is no overlapping computation).</p><formula xml:id="formula_40">W B,r ⊆ Min(V r ) \ Γ r +1 |W B,r | ≤ |γ r \ γ r +1 |<label>(20</label></formula><p>5.2.5 Maximizing computational intensity. Computational intensity ρ r of a subcomputation V r is an upper bound on ratio between its size |V r | and the number of I/O operations required. e number of I/O operations is minimized when ρ is maximized (Lemma 4):</p><formula xml:id="formula_41">maximize ρ r = |V r | X − R(S) + T (S) ≥ |V r | Dom(V r ) − |V R,r | + |W B,r | subject to: |Dom(V r )| ≤ X |V R,r | ≤ S</formula><p>To maximize the computational intensity, for a xed number of I/O operations, the subcomputation size |V r | is maximized. Based on Observation 5.2.4, it is maximized only if at most one of the overlapping projections α r ∩α r +1 , β r ∩ β r +1 , γ r ∩γ r +1 is not empty. Inserting Equations 13, 12, 19, and 20, we have the following three equations for the computational intensity, depending on the nonempty projection:</p><formula xml:id="formula_42">α r ∩ α r +1 ∅ : ρ r = |α r ||β r ||γ r | |α r | + |β r | + |γ r | − |α r ∩ α r +1 | + |γ r |<label>(21)</label></formula><p>β r ∩ β r +1 ∅ :</p><formula xml:id="formula_43">ρ r = |α r ||β r ||γ r | |α r | + |β r | + |γ r | − |β r ∩ β r +1 | + |γ r |<label>(22)</label></formula><p>γ r ∩ γ r +1 ∅ :</p><formula xml:id="formula_44">ρ r = |α r ||β r ||γ r | |α r | + |β r | + |γ r | − |γ r ∩ γ r +1 | + |γ r \ γ r +1 | (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>ρ r is maximized when γ r = γ r +1 , γ r ∩ γ r +1 ∅, γ r \ γ r +1 = ∅ (Equation <ref type="formula" target="#formula_44">23</ref>). en, inserting Equations 18, we have:</p><formula xml:id="formula_46">maximize ρ r = abc ac + cb subject to: ab + ac + cb ≤ X ab ≤ S a, b, c ∈ N + ,</formula><p>where X is a free variable. Simple optimization technique using Lagrange multipliers yields the result:</p><formula xml:id="formula_47">a = b = √ S , c = 1, (<label>24</label></formula><formula xml:id="formula_48">)</formula><formula xml:id="formula_49">|α r | = |β r | = √ S , |γ r | = √ S 2 , |V r | = √ S 2 , X = √ S 2 + 2 √ S ρ r = √ S 2<label>(25)</label></formula><p>From now on, to keep the calculations simpler, we use assume that √ S ∈ N+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">MMM I/O complexity of greedy schedules.</head><p>By the computational intensity corollary (cf. page 4 in the main paper):</p><formula xml:id="formula_50">Q ≥ |V | ρ = 2mnk √ S</formula><p>is is the I/O cost of pu ing a red pebble at least once on every vertex in C. Note however, that we did not put any blue pebbles on the outputs yet (all vertices in C had only red pebbles placed on them during the execution). By the de nition of the red-blue pebble game, we need to place blue pebbles on mn output vertices, corresponding to the output matrix C, resulting in additional mn I/O operations, yielding nal bound</p><formula xml:id="formula_51">Q ≥ 2mnk √ S +<label>mn</label></formula><p>5.2.7 A ainability of the Lower Bound. Restricting the analysis to greedy schedules provides explicit information of a state of memory (sets V r , V R,r , W B,r ), and therefore, to a corresponding CDAG pebbling. In Section 5.2.5, it is proven that an optimal greedy schedule is composed of mnk R(S ) outer product calculations, while loading R(S) elements of each of matrices A and B. While the lower bound is achieved for R(S) = S, such a schedule is infeasible, as at least some additional red pebbles, except the ones placed on the reuse set V R,r , have to be placed on 2 R(S) vertices of A and B.</p><p>A direct way to obtain a feasible greedy schedule is to set X = S, ensuring that the dominator set can t into the memory. en each subcomputation is an outer-product of column-vector of matrix A and row-vector of B, both holding more than a lower bound, which quickly approach 1 for large S.</p><p>Listing 1 provides a pseudocode of this algorithm, which is a wellknown rank-1 update formulation of MMM. However, we can do be er. Let's consider a generalized case of such subcomputation V r . Assume, that in each step: Each vertex in α r has b children in V r (each of which has also a parent in β r ). Similarly, each vertex in β r has a children in V r , each of which has also a parent in α r . We rst note, that ab &lt; S (otherwise, we cannot do any computation while keeping all ab partial results in fast memory). Any red vertex placed on α r should not be removed from it until all b children are pebbled, requiring red-pebbling of corresponding b vertices from β r . But, in turn, any red pebble placed on a vertex in β r should not be removed until all a children are red pebbled.</p><p>erefore, either all a vertices in α r , or all b vertices in β r have to be hold red pebbles at the same time, while at least one additional red pebble is needed on β r (or α r ). W.l.o.g., assume we keep red pebbles on all vertices of α r . We then have:</p><formula xml:id="formula_52">maximize ρ r = ab a + b subject to: ab + a + 1 ≤ S a, b ∈ N + ,<label>(26)</label></formula><p>e solution to this problem is</p><formula xml:id="formula_53">a opt =        (S − 1) 3 − S + 1 S − 2        &lt; √ S<label>(27)</label></formula><formula xml:id="formula_54">b opt =        − 2 S + (S − 1) 3 − S 2 − 1 (S − 1) 3 − S + 1        &lt; √ S (<label>28</label></formula><formula xml:id="formula_55">)</formula><formula xml:id="formula_56">1 for i 1 = 1 : m a opt 2 for j 1 = 1 : n b opt 3 for r = 1 : k 4 for i 2 = i 1 • T : min((i 1 + 1) • aopt , m) 5 for j 2 = j 1 • T : min((j 1 + 1) • bopt , n) 6 C(i 2 , j 2 ) = C(i 2 , j 2 ) + A(i 2 , r ) • B(r, j 2 )</formula><p>Listing 1: Pseudocode of near optimal sequential MMM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Greedy vs Non-greedy Schedules</head><p>In § 5.2.6, it is shown that the I/O lower bound for any greedy schedule is Q ≥ 2mnk √ S + mn. Furthermore, Listing 1 provide a schedule that a ains this lower bound (up to a a opt b opt /S factor). To prove that this bound applies to any schedule, we need to show, that any non-greedy cannot perform be er (perform less I/O operations) than the greedy schedule lower bound. L 6. Any non-greedy schedule computing classical matrix multiplication performs at least 2mnk √ S + mn I/O operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>. Lemma 3 applies to any schedule and for any value of X . Clearly, for any general schedule we cannot directly model V R,i , V B,i , W R,i , and W B,i , and therefore T (S) and R(S). However, it is always true that 0 ≤ T (S) and R(S) ≤ S. Also, the dominator set formed in Equation 5 applies for any subcomputation, as well as a bound on |V r | from Inequality 13. We can then rewrite the computational intensity maximization problem:</p><formula xml:id="formula_57">maximize ρ r = |V r | X − R(S) + T (S) ≤ |α r ||β r ||γ r | |α r | + |β r | + |γ r | − S subject to: S &lt; |α r | + |β r | + |γ r | = X (29) is is maximized for |α r | = |β r | = |γ r | = X /3, yielding ρ r = (X /3) 3/2</formula><p>X − S Because mnk/ρ r is a valid lower bound for any X &gt; S (Lemma 4), we want to nd such value X opt for which ρ r is minimal, yielding the highest (tightest) lower bound on Q:</p><formula xml:id="formula_58">minimize ρ r = (X /3) 3/2 X − S subject to: X ≥ S<label>(30)</label></formula><p>which, in turn, is minimized for X = 3S. is again shows, that the upper bound on maximum computational intensity for any schedule is √ S/2, which matches the bound for greedy schedules (Equation <ref type="formula" target="#formula_49">25</ref>).</p><p>We note that Smith and van de Gein <ref type="bibr" target="#b48">[48]</ref> in their paper also bounded the number of computations (interpreted geometrically as a subset in a 3D space) by its surface and obtained an analogous result for this surface (here, a dominator and minimum set sizes). However, using computational intensity lemma, our bound is tighter by 2S (+mn, counting storing the nal result). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OPTIMAL PARALLEL MMM</head><p>We now derive the schedule of COSMA from the results from § 5.2.7.</p><p>e key notion is the data reuse, that determines not only the sequential execution, as discussed in § 4.2 , but also the parallel scheduling. Speci cally, if the data reuse set spans across multiple local domains, then this set has to be communicated between these domains, increasing the I/O cost (Figure <ref type="figure" target="#fig_1">3</ref>). We rst introduce a formalism required to parallelize the sequential schedule ( § 6.1). In § 6.2, we generalize parallelization strategies used by the 2D, 2.5D, and recursive decompositions, deriving their communication cost and showing that none of them is optimal in the whole range of parameters. We nally derive the optimal decomposition (Find-OptimalDomain function in Algorithm 1) by expressing it as an optimization problem ( § 6.3), and analyzing its I/O and latency cost.</p><p>e remaining steps in Algorithm 1: FitRanks, GetDataDecomp, as well as DistrData and Reduce are discussed in § 7.1, § 7.6, and § 7.2, respectively. For a distributed machine, we assume that all matrices t into collective memories of all processors: pS ≥ mn + mk + nk. For a shared memory se ing, we assume that all inputs start in a common slow memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sequential and Parallel Schedules</head><p>We now describe how a parallel schedule is formed from a sequential one. e sequential schedule S partitions the CDAG G = (V , E) into H (S) subcomputations V i . e parallel schedule P divides S among p processors: P = {D 1 , . . . D p }, p j=1 D j = S. e set D j of all V k assigned to processor j forms a local domain of j (Fig. <ref type="figure" target="#fig_11">4c</ref>). If two local domains D k and D l are dependent, that is, ∃u, ∃ : u ∈ D k ∧ ∈ D l ∧ (u, ) ∈ E, then u has to be communicated from processor k to l. e total number of vertices communicated between all processors is the I/O cost Q of schedule P. We say that the parallel schedule P opt is communication-optimal if Q(P opt ) is minimal among all possible parallel schedules.</p><p>e vertices of MMM CDAG may be arranged in an [m × n × k] 3D grid called an iteration space <ref type="bibr" target="#b59">[59]</ref>. e orthonormal vectors i, j, k correspond to the loops in Lines 1-3 in Listing 1 (Figure <ref type="figure" target="#fig_1">3a</ref>). We call a schedule P parallelized in dimension d if we "cut" the CDAG along dimension COSMA, on the other hand, may use any of the possible parallelizations, depending on the problem parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parallelization Strategies for MMM</head><p>e sequential schedule S ( § 5) consists of mnk/S elementary outer product calculations, arranged in √ S × √ S × k "blocks" (Figure <ref type="figure" target="#fig_11">4</ref>).</p><p>e number p 1 = mn/S of dependency-free subcomputations V i (i.e., having no parents except for input vertices) in S determines the maximum degree of parallelism of P opt for which no reuse set V R,i crosses two local domains D j , D k . e optimal schedule is parallelized in dimensions ij. ere is no communication between the domains (except for inputs and outputs), and all I/O operations are performed inside each D j following the sequential schedule. Each processor is assigned to p 1 /p local domains D j of size e former option creates dependencies between the local domains, which results in additional communication (Figure <ref type="figure" target="#fig_11">4e</ref>). e la er does not utilize the whole available memory, making the sequential schedule not I/O optimal and decreasing the computational intensity ρ (Figure <ref type="figure" target="#fig_11">4d</ref>). We now analyze three possible parallelization strategies (Figure <ref type="figure" target="#fig_11">4</ref>) which generalize 2D, 2.5D, and recursive decomposition strategies; see Table <ref type="table" target="#tab_6">3</ref> for details.</p><p>Schedule P i j e schedule is parallelized in dimensions i and j.</p><p>e processor grid is G i j = m a , n a , 1 , where a = mn p . Because all dependencies are parallel to dimension k, there are no dependencies between D j except for the inputs and the outputs. Because a &lt; √ S, the corresponding sequential schedule has a reduced computational intensity ρ i j &lt; √ S/2. Schedule P i jk e schedule is parallelized in all dimensions. e processor grid is</p><formula xml:id="formula_59">G i jk = m √ S , n √ S</formula><p>, k pS . e computational in- Schedules of the State-of-the-Art Decompositions If m = n, the P i j scheme is reduced to the classical 2D decomposition (e.g., Cannon's algorithm <ref type="bibr" target="#b10">[10]</ref>), and P i jk is reduced to the 2.5D decomposition <ref type="bibr" target="#b53">[53]</ref>. CARMA <ref type="bibr" target="#b22">[22]</ref> asymptotically reaches the P cubic scheme, guaranteeing that the longest dimension of a local cuboidal domain is at most two times larger than the smallest one. We present a detailed complexity analysis comparison for all algorithms in Table <ref type="table" target="#tab_6">3</ref>.</p><formula xml:id="formula_60">tensity ρ i jk = √ S/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">I/O Optimal Parallel Schedule</head><p>Observe that none of those schedules is optimal in the whole range of parameters. As discussed in § 5, in sequential scheduling, intermediate results of C are not stored to the memory: they are consumed (reused) immediately by the next sequential step. Only the nal result of C in the local domain is sent. erefore, the optimal parallel schedule P opt minimizes the communication, that is, sum of the inputs' sizes plus the output size, under the sequential I/O constraint on subcomputations </p><formula xml:id="formula_61">∀ V i ∈ D j ∈ P opt |Dom(V i )| ≤ S ∧ |Min(V i )| ≤ S. e local domain D j is a grid of size [a × a × b],</formula><formula xml:id="formula_62">Q ≥ a 2 b ρ = min 2mnk p √ S + S, 3 mnk p 2 3 (<label>33</label></formula><formula xml:id="formula_63">)</formula><p>is can be intuitively interpreted geometrically as follows: if we imagine the optimal local domain "growing" with the decreasing number of processors, then it stays cubic as long as it is still "small enough" (its side is smaller than  e number of steps (latency) is equal to the total communication volume of D divided by the volume per step L = Q/I st ep . To reduce the latency, one either has to decrease Q or increase I st ep , under the memory constraint that I st ep +a 2 ≤ S (otherwise we cannot t both the inputs and the outputs in the memory). Express I st ep = a • h, where h is the number of sequential subcomputations V i we merge in one communication. We can express the I/O-latency trade-o : min(Q, L) subject to:    greater than the latency cost, therefore our schedule by default minimizes Q and uses extra memory (if any) to reduce L.</p><formula xml:id="formula_64">√ S). A</formula><formula xml:id="formula_65">Q = 2ab + a 2 , L = b h a 2 + 2ah ≤ S (I/O constraint)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION</head><p>We now present implementation optimizations that further increase the performance of COSMA on top of the speedup due to our near I/O optimal schedule. e algorithm is designed to facilitate the overlap of communication and computation § 7.3. For this, to leverage the RDMA mechanisms of current high-speed network interfaces, we use the MPI one-sided interface § 7.4. In addition, our implementation also o ers alternative e cient two-sided communication back end that uses MPI collectives. We also use a blocked data layout § 7.6, a grid-ing technique § 7.1, and an optimized binary broadcast tree using static information about the communication pa ern ( § 7.2) together with the bu er swapping ( § 7.5). For the local matrix operations, we use BLAS routines for highest performance. Our code is publicly available at h ps://github.com/ethcscs/COSMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Processor Grid Optimization</head><p>roughout the paper, we assume all operations required to assess the decomposition (divisions, roots) result in natural numbers. We note that in practice it is rarely the case, as the parameters usually emerge from external constraints, like a speci cation of a performed calculation or hardware resources ( § 8). If matrix dimensions are not divisible by the local domain sizes a, b (Equation <ref type="formula">32</ref>), then a straightforward option is to use the oor function, not utilizing the "boundary" processors whose local domains do not t entirely in the iteration space, which result in more computation per processor. e other option is to nd factors of p and then construct the processor grid by matching the largest factors with largest matrix dimensions. However, if the factors of p do not match m, n, and k, this may result in a suboptimal decomposition. Our algorithm allows to not utilize some processors (increasing the computation volume per processor) to optimize the grid, which reduces the communication volume. Figure <ref type="figure" target="#fig_5">5</ref> illustrates the comparison between these options. We balance this communication-computation trade-o by "stretching" the local domain size derived in § 6.3 to t the global domain by adjusting its width, height, and length. e range of this tuning (how many processors we drop to reduce communication) depends on the hardware speci cation of the machine (peak op/s, memory and network bandwidth). For our experiments on Piz Daint we chose the maximal number of unutilized cores to be 3%, accounting for up to 2.4 times speedup for the square matrices using 2,198 cores ( § 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Enhanced Communication Pattern</head><p>As shown in Algorithm 1, COSMA by default executes in t = 2ab S −a 2 rounds. In each round, each processor receives s = ab/t = (S −a 2 )/2 elements of A and B. us, the input matrices are broadcast among the i and j dimensions of the processor grid. A er the last round, the partial results of C are reduced among the k dimension. e communication pa ern is therefore similar to ScaLAPACK or CTF.</p><p>To accelerate the collective communication, we implement our own binary broadcast tree, taking advantage of the known data layout, processor grid, and communication pa ern. Knowing the initial data layout § 7.6 and the processor grid § 7.1, we cra the binary reduction tree in all three dimensions i, j, and k such that the distance in the grid between communicating processors is minimized. Our implementation outperforms the standard MPI broadcast from the Cray-MPICH 3.1 library by approximately 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Communication-Computation Overlap</head><p>e sequential rounds of the algorithm t i = 1, . . . , t, naturally express communication-computation overlap. Using double bu ering, at each round t i we issue an asynchronous communication (using either MPI Get or MPI Isend / MPI Irecv § 7.4) of the data required at round t i+1 , while locally processing the data received in a previous round. We note that, by the construction of the local domains D j § 6.3, the extra memory required for double bu ering is rarely an issue. If we are constrained by the available memory, then the space required to hold the partial results of C, which is a 2 , is much larger than the size of the receive bu ers s = (S − a 2 )/2. If not, then there is extra memory available for the bu ering.</p><p>Number of rounds: e minimum number of rounds, and therefore latency, is t = 2ab S −a 2 ( § 6.3) . However, to exploit more overlap, we can increase the number of rounds t 2 &gt; t. In this way, in one round we communicate less data s 2 = ab/t 2 &lt; s, allowing the rst round of computation to start earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">One-Sided vs Two-Sided Communication</head><p>To reduce the latency <ref type="bibr" target="#b27">[27]</ref> we implemented communication using MPI RMA <ref type="bibr" target="#b32">[32]</ref>. is interface utilizes the underlying features of Remote Direct Memory Access (RDMA) mechanism, bypassing the OS on the sender side and providing zero-copy communication: data sent is not bu ered in a temporary address, instead, it is wri en directly to its location.</p><p>All communication windows are pre-allocated using MPI Win allocate with the size of maximum message in the broadcast tree 2 s−1 D ( § 7.2). Communication in each step is performed using the MPI Get and MPI Accumulate routine.</p><p>For compatibility reasons, as well as for the performance comparison, we also implemented a communication back-end using MPI two-sided (the message passing abstraction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Communication Bu er Optimization</head><p>e binary broadcast tree pa ern is a generalization of the recursive structure of CARMA. However, CARMA in each recursive step dynamically allocates new bu ers of the increasing size to match the message sizes 2 s−1 D, causing an additional runtime overhead.</p><p>To alleviate this problem, we pre-allocate initial, send, and receive bu ers for each of matrices A, B, and C of the maximum size of the message ab/t, where t = 2ab S −a 2 is the number of steps in COSMA (Algorithm 1). en, in each level s of the communication tree, we move the pointer in the receive bu er by 2 s−1 D elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Blocked Data Layout</head><p>COSMA's schedule induces the optimal initial data layout, since for each D j it determines its dominator set Dom(D j ), that is, elements accessed by processor j. Denote A l, j and B l, j subsets of elements of matrices A and B that initially reside in the local memory of processor j. e optimal data layout therefore requires that A l, j , B l, j ⊂ Dom(D j ). However, the schedule does not specify exactly which elements of Dom(D j ) should be in A l, j and B l, j . As a consequence of the communication pa ern § 7.2, each element of A l, j and B l, j is communicated to m , n processors, respectively.</p><p>To prevent data reshu ing, we therefore split each of Dom(D j ) into m and n smaller blocks, enforcing that consecutive blocks are assigned to processors that communicate rst. is is unlike the distributed CARMA implementation <ref type="bibr" target="#b22">[22]</ref>, which uses the cyclic distribution among processors in the recursion base case and requires local data reshu ing a er each communication round. Another advantage of our blocked data layout is a full compatibility with the block-cyclic one, which is used in other linear-algebra libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION</head><p>We evaluate COSMA's communication volume and performance against other state-of-the-art implementations with various combinations of matrix dimensions and memory requirements. ese scenarios include both synthetic square matrices, in which all algorithms achieve their peak performance, as well as " at" (two large dimensions) and real-world "tall-and-skinny" (one large dimension) cases with uneven number of processors.</p><p>Comparison Targets As a comparison, we use the widely used ScaLAPACK library as provided by Intel MKL (version: 18.0.2.199) <ref type="foot" target="#foot_1">3</ref> , as well as Cyclops Tensor Framework<ref type="foot" target="#foot_2">4</ref> , and the original CARMA implementation <ref type="foot" target="#foot_3">5</ref> . We manually tune ScaLAPACK parameters to achieve its maximum performance. Our experiments showed that on Piz Daint it achieves the highest performance when run with 4 MPI ranks per compute node, 9 cores per rank. erefore, for each matrix sizes/node count con guration, we recompute the optimal rank decomposition for ScaLAPACK. Remaining implementations use default decomposition strategy and perform best utilizing 36 ranks per node, 1 core per rank.</p><p>Infrastructure and Implementation Details All implementations were compiled using the GCC 6.2.0 compiler. We use Cray-MPICH 3.1 implementation of MPI. e parallelism within a rank of ScaLAPACK<ref type="foot" target="#foot_4">6</ref> is handled internally by the MKL BLAS (with GNU OpenMP threading) version 2017.4.196. To pro le MPI communication volume, we use the mpiP pro ler version 3.4.1 <ref type="bibr" target="#b57">[57]</ref>.</p><p>Experimental Setup and Architectures We run our experiments on the CPU partition of the CSCS Piz Daint, which has 1,813 XC40 nodes with dual-socket Intel Xeon E5-2695 v4 processors (2 • 18 cores, 3.30 GHz, 45 MiB L3 shared cache, 64 GiB DDR3 RAM), interconnected by the Cray Aries network with a dragon y network topology. We set p to a number of available cores <ref type="foot" target="#foot_5">7</ref> and S to the main memory size per core ( § 2.1). To additionally capture cache size per core, the model can be extended to a three-level memory hierarchy. However, cache-size tiling is already handled internally by the MKL.</p><p>Matrix Dimensions and Number of Cores We use square</p><formula xml:id="formula_66">(m = n = k), "largeK" (m = n k), "largeM" (m n = k),</formula><p>and " at" (m = n k) matrices. e matrix dimensions and number of cores are (1) powers of two m = 2 r 1 , n = 2 r 2 , m = 2 r 3 , (2) determined by the real-life simulations or hardware architecture (available nodes on a computer), (3) chosen adversarially, e.g, n 3 + 1. Tall matrix dimensions are taken from an application benchmark, namely the calculation of the random phase approximation (RPA) energy of water molecules <ref type="bibr" target="#b21">[21]</ref>. ere, to simulate w molecules, the sizes of the matrices are m = n = 136w and k = 228w 2 . In the strong scaling scenario, we use w = 128 as in the original paper, yielding m = n = 17,408, k = 3,735,552. For performance runs, we scale up to 512 nodes <ref type="bibr">(18,432 cores)</ref>.</p><p>Selection of Benchmarks We perform both strong scaling and memory scaling experiments. e memory scaling scenario xes the input size per core ( To provide more information about the impact of communication optimizations on the total runtime, for each of the matrix shapes we also separately measure time spent by COSMA on di erent parts of the code. for each matrix shape we present two extreme cases of strong scaling -with smallest number of processors (most computeintense) and with the largest (most communication-intense). To additionally increase information provided, we perform these measurements with and without communication-computation overlap.</p><p>Programming Models We use either the RMA or the Message Passing models. CTF also uses both models, whereas CARMA and ScaLAPACK use MPI two-sided (Message Passing).</p><p>Experimentation Methodology For each combination of parameters, we perform 5 runs, each with di erent node allocation. As all the algorithms use BLAS routines for local matrix computations, for each run we execute the kernels three times and take the minimum to compensate for the BLAS setup overhead. We report median and 95% con dence intervals of the runtimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RESULTS</head><p>We now present the experimental results comparing COSMA with the existing algorithms. For both strong and memory scaling, we measure total communication volume and runtime on both square and tall matrices. Our experiments show that COSMA always communicates least data and is the fastest in all scenarios.</p><p>Summary and Overall Speedups As discussed in § 8, we evaluate three benchmarks -strong scaling, "limited memory" (no redundant copies of the input are possible), and "extra memory" (p 1/3 extra copies of the input can t into combined memory of all cores). Each of them we test for square, "largeK", "largeM", and , " at" matrices, giving twelve cases in total. In Table <ref type="table" target="#tab_7">4</ref>, we present arithmetic mean of total communication volume per MPI rank across all core counts. We also report the summary of minimum, geometric mean, and maximum speedups vs the second best-performing algorithm.</p><p>Communication Volume As analyzed in § 5 and § 6, COSMA reaches I/O lower bound (up to the factor of √ S/( √ S + 1−1)). Moreover, optimizations presented in § 7 secure further improvements compared to other state-of-the-art algorithms. In all cases, COSMA performs least communication. Total communication volume for square and "largeK" scenarios is shown in Figures <ref type="figure" target="#fig_10">6 and 10</ref>. Square Matrices Figure <ref type="figure">8</ref> presents the % of achieved peak hardware performance for square matrices in all three scenarios. As COSMA is based on the near optimal schedule, it achieves the highest performance in all cases. Moreover, its performance pa ern is the most stable: when the number of cores is not a power of two, the performance does not vary much compared to all remaining three implementations. We note that matrix dimensions in the strong scaling scenarios (m = n = k = 2 14 ) are very small for distributed se ing. Yet even in this case COSMA maintains relatively high performance for large numbers of cores: using 4k cores it achieves 35% of peak performance, compared to ¡5% of CTF and ScaLAPACK, showing excellent strong scaling characteristics.</p><p>Tall and Skinny Matrices Figure <ref type="figure" target="#fig_10">10</ref> presents the results for "largeK" matrices -due to space constraints, the symmetric "largeM" case is For strong scaling, the minimum number of cores is 2048 (otherwise, the matrices of size m = n =17,408, k =3,735,552 do not t into memory). Again, COSMA shows the most stable performance with a varying number of cores.</p><p>"Flat" Matrices Matrix dimensions for strong scaling are set to m = n = 2 17 =131,072 and k = 2 9 =512. Our weak scaling scenario models the rank-k update kernel, with xed k =256, and m = n scaling accordingly for the "limited" and "extra" memory cases. Such kernels take most of the execution time in, e.g., matrix factorization algorithms, where updating Schur complements is performed as a rank-k gemm operation <ref type="bibr" target="#b31">[31]</ref>.</p><p>Unfavorable Number of Processors Due to the processor grid optimization ( § 7.1), the performance is stable and does not su er from unfavorable combinations of parameters. E.g., the runtime of COSMA for square matrices m = n = k =16,384 on p 1 =9,216= 2 10 • 3 2 cores is 142 ms. Adding an extra core (p 2 =9,217= 13 • 709), does not change COSMA's runtime, as the optimal decomposition does not utilize it. On the other hand, CTF for p 1 runs in 600 ms, while for p 2 the runtime increases to 1613 ms due to a non-optimal processor decomposition.</p><p>Communication-Computation Breakdown Figure <ref type="figure" target="#fig_0">12</ref> presents the total runtime breakdown of COSMA into communication and computation routines. Combined with the comparison of communication volumes (Figures <ref type="figure">6 and 7</ref>, Table <ref type="table" target="#tab_7">4</ref>) we see the importance of I/O optimization for distributed se ing even for traditionally compute-bound MMM. E.g., for square or " at" matrix and 16k cores, COSMA communicates more than two times less than the second-best (CARMA). Assuming constant time-per-MB, COSMA would be 40% slower if it communicated that much, being slower than CARMA by 30%. For "largeK", the situation is even more extreme, with COSMA su ering 2.3 times slowdown if communicating 10 times more -as much as the second-best algorithm, CTF.</p><p>Detailed Statistical Analysis Figure <ref type="figure" target="#fig_10">13</ref> provides a distribution of the achieved peak performance across all numbers of cores for all six scenarios. It can be seen that, for example, in the strong scaling scenario and square matrices, COSMA is comparable to the other implementations (especially CARMA). However, for talland-skinny matrices with limited memory available, COSMA lowest achieved performance is higher than the best performance of CTF and ScaLAPACK.  </p><formula xml:id="formula_67">• • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   </p><formula xml:id="formula_68">• • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref> 256 </p><formula xml:id="formula_69">n = m = k = pS 3 • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   </p><formula xml:id="formula_70">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work) ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   # of cores MB communicated per core <ref type="bibr" target="#b50">50</ref> 22</p><formula xml:id="formula_71">(b) Limited memory,m = n = 979p 1 3 , k =1.184p 2 3 • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   </p><formula xml:id="formula_72">• • • • • • • • • • • • • • • • • • • • •• • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>    </p><formula xml:id="formula_73">• • • • • • • • • • • • • • • • • • • • • • • • • • • CARMA [ ] CTF [ ] COSMA (</formula><formula xml:id="formula_74">n = m = k = pS 3 • • • • • • • • • • • • • • • • •• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>     (c) Extra memory, m = n = k = (p 2/3 S )/3  </p><formula xml:id="formula_75">• • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   </p><formula xml:id="formula_76">• • • • • • • • • • • • • • • • • • • • •• • • • •• CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>  </p><formula xml:id="formula_77">= n = 979p 1 3 , k =1.184p 2 3 • • • • • • • • • • • • • • • • • • • • • • • • • •• • CARMA [21]</formula><p>CTF <ref type="bibr" target="#b49">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>  Figure <ref type="figure" target="#fig_10">10</ref>: Achieved % of peak performance by COSMA, CTF, ScaLAPACK and CARMA for "largeK" matrices. We show median and 95% confidence intervals. (a) Strong scaling, n = m =17,408, k = 3,735,552 q q q q q q q q 1,024 </p><formula xml:id="formula_78">(b) Limited memory, m = n = 979p 1 3 , k =1.184p<label>2 3</label></formula><p>q q q q q q q q 512 2,048 q CARMA <ref type="bibr" target="#b21">[21]</ref> COSMA (this work) CTF <ref type="bibr" target="#b46">[46]</ref> ScaLAPACK <ref type="bibr" target="#b51">[51]</ref> (c) Extra memory, m = n = 979p   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">RELATED WORK</head><p>Works on data movement minimization may be divided into two categories: applicable across memory hierarchy (vertical, also called I/O minimization), or between parallel processors (horizontal, also called communication minimization). Even though they are "two sides of the same coin", in literature they are o en treated as separate topics. In our work we combine them: analyze trade-o s between communication optimal (distributed memory) and I/O optimal schedule (shared memory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">General I/O Lower Bounds</head><p>Hong and Kung <ref type="bibr" target="#b34">[34]</ref> analyzed the I/O complexity for general CDAGs in their the red-blue pebble game, on which we base our work. As a special case, they derived an asymptotic bound Ω n 3 / √ S for MMM. Elango et al. <ref type="bibr" target="#b23">[23]</ref> extended this work to the red-blue-white game and Liu and Terman <ref type="bibr" target="#b40">[40]</ref> proved that it is also P-SPACE complete. Irony et al. <ref type="bibr" target="#b33">[33]</ref> extended the MMM lower bound result to a parallel machine with p processors, each having a fast private memory of size S, proving the n 3 2</p><formula xml:id="formula_79">√ 2p √ S</formula><p>− S lower bound on the communication volume per processor. Chan <ref type="bibr" target="#b12">[12]</ref> studied di erent variants of pebble games in the context of memory space and parallel time. Aggarwal and Vi er <ref type="bibr" target="#b1">[2]</ref> introduced a two-memory machine that models a blocked access and latency in an external storage. Arge et al. <ref type="bibr" target="#b3">[3]</ref> extended this model to a parallel machine. Solomonik et al. <ref type="bibr" target="#b51">[51]</ref> combined the communication, synchronization, and computation in their general cost model and applied it to several linear algebra algorithms. Smith and van de Geijn <ref type="bibr" target="#b48">[48]</ref> derived a sequential lower bound 2mnk/ √ S − 2S for MMM. ey showed that the leading factor 2mnk/ √ S is tight. We improve this result by 1) improving an additive factor of 2S, but more importantly 2) generalizing the bound to a parallel machine. Our work uses a simpli ed model, not taking into account the memory block size, as in the external memory model, nor the cost of computation. We motivate it by assuming that the block size is signi cantly smaller than the input size, the data is layout contiguously in the memory, and that the computation is evenly distributed among processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Shared Memory Optimizations</head><p>I/O optimization for linear algebra includes such techniques as loop tiling and skewing <ref type="bibr" target="#b59">[59]</ref>, interchanging and reversal <ref type="bibr" target="#b58">[58]</ref>. For programs with multiple loop nests, Kennedy and McKinley <ref type="bibr" target="#b35">[35]</ref> showed various techniques for loop fusion and proved that in general this problem is NP-hard. Later, Darte <ref type="bibr" target="#b20">[20]</ref> identi ed cases when this problem has polynomial complexity. Toledo <ref type="bibr" target="#b55">[55]</ref> in his survey on Out-Of-Core (OOC) algorithms analyzed various I/O minimizing techniques for dense and sparse matrices. Mohanty <ref type="bibr" target="#b43">[43]</ref> in his thesis optimized several OOC algorithms. Irony et al. <ref type="bibr" target="#b33">[33]</ref>   MMM on a parallel machine. Ballard et al. <ref type="bibr" target="#b5">[5]</ref> proved analogous results for Strassen's algorithm. is analysis was extended by Sco et al. <ref type="bibr" target="#b46">[46]</ref> to a general class of Strassen-like algorithms.</p><p>Although we consider only dense matrices, there is an extensive literature on sparse matrix I/O optimizations. Bender et al. <ref type="bibr" target="#b7">[7]</ref> extended Aggarwal's external memory model <ref type="bibr" target="#b1">[2]</ref> and showed I/O complexity of the sparse matrix-vector (SpMV) multiplication. Greiner <ref type="bibr" target="#b29">[29]</ref> extended those results and provided I/O complexities of other sparse computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Distributed Memory Optimizations</head><p>Distributed algorithms for dense matrix multiplication date back to the work of Cannon <ref type="bibr" target="#b10">[10]</ref>, which has been analyzed and extended many times <ref type="bibr" target="#b30">[30]</ref>  <ref type="bibr" target="#b39">[39]</ref>. In the presence of extra memory, Aggarwal et al. <ref type="bibr" target="#b0">[1]</ref> included parallelization in the third dimension. Solomonik and Demmel <ref type="bibr" target="#b53">[53]</ref> extended this scheme with their 2.5D decomposition to arbitrary range of the available memory, e ectively interpolating between Cannon's 2D and Aggarwal's 3D scheme. A recursive, memory-oblivious MMM algorithm was introduced by Blumofe et al. <ref type="bibr" target="#b9">[9]</ref> and extended to rectangular matrices by Frigo et al. <ref type="bibr" target="#b26">[26]</ref>. Demmel el al. <ref type="bibr" target="#b22">[22]</ref> introduced CARMA algorithm which achieves the asymptotic complexity for all matrix and memory sizes. We compare COSMA with these algorithms, showing that we achieve be er results both in terms of communication complexity and the actual runtime performance. Lazzaro et al. <ref type="bibr" target="#b38">[38]</ref> used the 2.5D technique for sparse matrices, both for square and rectangular grids. Koanantakool et al. <ref type="bibr" target="#b37">[37]</ref> observed that for sparse-dense MMM, 1.5D decomposition performs less communication than 2D and 2.5D schemes, as it distributes only the sparse matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSIONS</head><p>In this work we present a new method (Lemma 3) for assessing tight I/O lower bounds of algorithms using their CDAG representation and the red-blue pebble game abstraction. As a use case, we prove a tight bound for MMM, both for a sequential ( eorem 1) and parallel ( eorem 2) execution. Furthermore, our proofs are constructive: our COSMA algorithm is near I/O optimal (up to the factor of</p><formula xml:id="formula_80">√ S √ S +1−1</formula><p>, which is less than 0.04% from the lower bound for 10MB of fast memory) for any combination of matrix dimensions, number of processors and memory sizes. is is in contrast with the current state-of-the-art algorithms, which are communication-ine cient in some scenarios.</p><p>To further increase the performance, we introduce a series of optimizations, both on an algorithmic level (processor grid optimization ( § 7.1) and blocked data layout ( § 7.6)) and hardwarerelated (enhanced communication pa ern ( § 7.2), communicationcomputation overlap ( § 7.3), one-sided ( § 7.4) communication). e experiments con rm the superiority of COSMA over the other analyzed algorithms -our algorithm signi cantly reduces communication in all tested scenarios, supporting our theoretical analysis. Most importantly, our work is of practical importance, being maintained as an open-source implementation and achieving a time-tosolution speedup of up to 12.8x times compared to highly optimized state-of-the-art libraries.</p><p>e important feature of our method is that it does not require any manual parameter tuning and is generalizable to other machine models (e.g., multiple levels of memory) and linear algebra kernels (e.g., LU or Cholesky decompositions), both for dense and sparse matrices. We believe that the "bo om-up" approach will lead to developing more e cient distributed algorithms in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Percentage of peak flop/s across the experiments ranging from 109 to 18,432 cores achieved by COSMA and the state-of-the-art libraries (Sec. 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Domain decomposition using p = 8 processors. In scenario (a), a straightforward 3D decomposition divides every dimension in p 1/3 = 2. In scenario (b), COSMA starts by finding a near optimal sequential schedule and then parallelizes it minimizing crossing data reuse V R, i ( § 5). The total communication volume is reduced by 17% compared to the former strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>+T (S ) to be the maximal computational intensity. en, the number of I/O operations Q is bounded by Q ≥ |V |/ρ. P . Note that the term H (X ) − 1 in Equation 2 emerges from a fact that the last subcomputation may execute less than Y −R(S) + T (S) I/O operations, since |V H (X ) | ≤ |V max |. However, because ρ is de ned as maximal computational intensity, then performing |V H (S ) | computations requires at least Q H (S ) ≥ |V H (S ) |/ρ. e total number of I/O operations therefore is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the communication volume per process. Recently, Smith and van de Gein [48] proved a tight sequential lower bound (up to an additive term) of 2mnk/ √ S − 2S. Our proof improves the additive term and extends it to a parallel schedule. T 1 (S M M I/O ). Any pebbling of MMM CDAG which multiplies matrices of sizes m × k and k × n by performing mnk multiplications requires a minimum number of 2mnk √ S + mn I/O operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 1 . 1</head><label>11</label><figDesc>Vertices, Projections, and Edges in the MMM CDAG. e set of vertices of MMM CDAG G = (V , E) consists of three subsets V = A ∪ B ∪ C, which correspond to elements in matrices A, B, and mnk partial sums of C. Each vertex is de ned uniquely by a pair (M,T ), where M ∈ {a, b, c} determines to which subset A, B, C vertex belongs to, and T ∈ N d is a vector of coordinates, d = 2 for M = a ∨ b and d = 3 for M = c. E.g., = (a, (1, 5)) ∈ A is a vertex associated with element (1, 5) in matrix A, and = (c, (3, 6, 8)) ∈ C is associated with 8th partial sum of element (3, 6) of matrix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>L 5 .</head><label>5</label><figDesc>Any greedy schedule that multiplies matrices of sizes m × k and k × n using mnk multiplications requires a minimum number of 2mnk √ S + mn I/O operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>)</head><label></label><figDesc>For xed sizes of the projections |α r |, |β r |, |γ r |, then the volume |V r | is maximized when le and right side of Inequalities 14 to 16 are equal. Using 5 and 9 we have that 17 is an equality too, and:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>) 5 . 2 . 4 ( 1 )</head><label>5241</label><figDesc>Overlapping computations. Consider two subcomputations V r and V r +1 . Denote shared parts of their projections as α s = α r ∩ α r +1 , β s = β r ∩ β r +1 , and γ s = γ r ∩ γ r +1 . en, there are two possibilities: V r and V r +1 are not cubic, resulting in their volume smaller than the upper bound|V r +1 | &lt; |α r +1 ||β r +1 ||γ r +1 | (Equation13), (2) V r and V r +1 are cubic. If all overlapping projections are not empty, then they generate an overlapping computation, that is, there exist vertices , such that ϕ ik ( ) ∈ α s , ϕ k j ( ) ∈ β s , ϕ i j ( ) ∈ γ s . Because we consider greedy schedules, those vertices cannot belong to computation V r +1 (Equation9). erefore, again|V r +1 | &lt; |α r +1 ||β r +1 ||γ r +1 |.Now consider sets of all di erent indices accessed by those rectangular projections (Section 5.2.2, Inequalities 14 to 16). Fixing two non-empty projections we de ne all three sets T 1 ,T 2 ,T 3 , which in turn, generate the third (non-empty) projection, resulting again in overlapping computations which reduce the size of |V r +1 |. erefore, for cubic subcomputations, their volume is maximized |V r +1 | = |α r +1 ||β r +1 ||γ r +1 |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>√ S + 1 − 1</head><label>11</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )( 4 )</head><label>14</label><figDesc>a elements of A (forming α r ) are loaded, (2) b elements of B (forming β r ) are loaded, (3) ab partial results of C are kept in the fast memory (forming Γ r ) ab values of C are updated (forming V r ), (5) no store operations are performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proof of eorem 1 :</head><label>1</label><figDesc>Lemma 5 establishes that the I/O lower bound for any greedy schedule is Q = 2mnk/ √ S + mn. Lemma 6 establishes that no other schedule can perform less I/O operations. Corollary: e greedy schedule associated with an X = S-partition performs at most √ S √ S +1−1 more I/O operations than a lower bound. e optimal greedy schedule is associated with an X = a opt b opt + a opt + b opt -partition and performs √ S (a opt +b opt ) a opt b opt I/O operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) An MMM CDAG as a 3D grid (iteration space). Each vertex in it (except for the vertices in the bo om layer) has three parents -blue (matrix A), red (matrix B), and yellow (partial result of matrix C) and one yellow child (except for vertices in the top layer). (b) A union of inputs of all vertices in V i form the dominator set Dom(V i ) (two blue, two red and four dark yellow). Using approximation √ S + 1 − 1 ≈ √ S , we have |Dom(V i,opt )| = S . (c) A local domain D consists of b subcomputations V i , each of a dominator size |Dom(V i )| = a 2 + 2a. (d-f) Di erent parallelization schemes of near optimal sequential MMM for p = 24 &gt; p 1 = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>d. More formally, each local domain D j , j = 1 . . . p is a grid of size either [m/p, n, k], [m, n/p, k], or [m, n, k/p]. e schedule may also be parallelized in two dimensions (d 1 d 2 ) or three dimensions (d 1 d 2 d 3 ) with a local domain size [m/p m , n/p n , k/p k ] for some p m , p n , p k , such that p m p n p k = p. We call G = [p m , p n , p k ] the processor grid of a schedule. E.g., Cannon's algorithm is parallelized in dimensions ij , with the processor grid [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 √</head><label>2</label><figDesc>Sk + S I/O operations ( eorem 1), giving a total of Q = 2mnk/(p √ S) + mn/p I/O operations per processor. When p &gt; p 1 , the size of local domains |D j | is smaller than √ S × √ S × k. en, the schedule has to either be parallelized in dimension k, or has to reduce the size of the domain in ij plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>a direct consequence of Lemma 3 and the computational intensity (Lemma 4). e load balance constraint enforces a size of each local domain |D j | = mnk/p. e I/O cost is then bounded by |D j |/ρ. Schedule P opt maximizes ρ by the formulation of the optimization problem (Equation 31). I/O-Latency Trade-o As showed in this section, the local domain D of the near optimal schedule P is a grid of size [a ×a ×b], where a, b are given by Equation (32). e corresponding sequential schedule S is a sequence of b outer products of vectors of length a. Denote the size of the communicated inputs in each step by I st ep = 2a. is corresponds to b steps of communication (the latency cost is L = b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>a 2</head><label>2</label><figDesc>b = mnk p (load balance constraint) Solving this problem, we have Q = 2mnk pa + a 2 and L = 2mnk pa(S −a 2 ) , where a ≤ √ S. Increasing a we reduce the I/O cost Q and increase the latency cost L. For minimal value of Q ( eorem 2), L = 2ab S −a 2 , where a = min{ √ S, (mnk/p) 1/3 } and b = max{ mnk pS , (mnk/p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>(a) 1 × 5 Figure 5 :</head><label>155</label><figDesc>Figure 5: Processor decomposition for square matrices and 65 processors. (a) To utilize all resources, the local domain is drastically stretched. (b) Dropping one processor results in a symmetric grid which increases the computation per processor by 1.5%, but reduces the communication by 36%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>pSI</head><label></label><figDesc>, I = mn + mk + nk), as opposed to the work per core ( mnk p const). We evaluate two cases: (1) "limited memory" ( pS I = const), and (2) "extra memory" ( p 2/3 S I = const).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Strong scaling, n = m = k = 16,384</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>3 Figure 6 :</head><label>36</label><figDesc>Figure 6: Total communication volume per core carried out by COSMA, CTF, ScaLAPACK and CARMA for square matrices, as measured by the mpiP profiler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>2 9 9 Figure 7 :</head><label>297</label><figDesc>Figure 7: Total communication volume per core carried out by COSMA, CTF, ScaLAPACK and CARMA for "largeK" matrices, as measured by the mpiP profiler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Strong scaling, n = m = k = 16,384</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>3 Figure 8 :</head><label>38</label><figDesc>Figure8: Achieved % of peak performance by COSMA, CTF, ScaLAPACK and CARMA for square matrices, strong and weak scaling. We show median and 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Total runtime of COSMA, CTF, ScaLAPACK and CARMA for square matrices, strong and weak scaling. We show median and 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>Strong scaling, n = m =17,408, k = 3,735,552</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Limited memory, m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>2 9 , k =1.184p 4 9 Figure 11 :</head><label>2911</label><figDesc>Figure 11: Total runtime of COSMA, CTF, ScaLAPACK and CARMA for "largeK" matrices, strong and weak scaling. We show median and 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Time distribution of COSMA communication and computation kernels for strong scaling executed on the smallest and the largest core counts for each of the matrix shapes. Le bar: no communication-computation overlap. Right bar: overlap enabled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Distribution of achieved % of peak performance of the algorithms across all number of cores for tall-and-skinny matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 is optimal. e parallelization in k dimension creates dependencies between local domains, requiring communication and increasing the I/O cost.Schedule P cubic e schedule is parallelized in all dimensions.</figDesc><table><row><cell>e grid is m a c , n a c , k a c , where a c = min mnk p cause a c &lt; √ S, the corresponding computational intensity ρ cubic 1/3 , S 3 . Be-</cell></row></table><note>&lt; √ S/2 is not optimal. e parallelization in k dimension creates dependencies between local domains, increasing communication.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>containing b outer products of vectors of length a. e optimization problem of nding P opt using the computational intensity (Lemma 4) is formulated as follows:</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>a = min</cell><cell>√ S,</cell><cell>mnk p</cell><cell>1/3</cell><cell>, b = max</cell><cell>mnk pS</cell><cell>,</cell><cell>mnk p</cell><cell>1/3</cell><cell>(32)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">e I/O complexity of this schedule is:</cell><cell></cell></row><row><cell></cell><cell cols="2">maximize ρ =</cell><cell>a 2 b ab + ab + a 2</cell><cell>(31)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">subject to:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">a 2 ≤ S (the I/O constraint)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a 2 b =</cell><cell>mnk p</cell><cell cols="2">(the load balance constraint)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>pS ≥ mn + mk + nk (matrices must t into memory) e I/O constraint a 2 ≤ S is binding (changes to equality) for p ≤ mnk S 3/2 . erefore, the solution to this problem is:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1/3 }. Based on our experiments, we observe that the I/O cost is vastly</figDesc><table><row><cell></cell><cell></cell><cell cols="3">"Tall" matrices, "extra" memory available: m = n =</cell><cell>√ p, k = p 3/2 /4, S = 2nk/p 2/3 , p = 2 3n+1</cell></row><row><cell>I/O cost latency cost L</cell><cell>p 3/2 /2 p 3/2 log 2 (</cell><cell>√ p)/4</cell><cell>p 4/3 /2 + p 1/3 1</cell><cell cols="2">3p/4 1</cell><cell>p 3 − 2 1/3 /2 4/3 ≈ 0.69p 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The comparison of complexities of 2D, 2.5D, recursive, and COSMA algorithms. The 3D decomposition is a special case of 2.5D, and can be obtained by instantiating c = p 1/3 in the 2.5D case. In addition to the general analysis, we show two special cases. If the matrices are square and there is no extra memory available, 2D, 2.5D and COSMA achieves tight communication lower bound 2n 2 / √ p, whereas CARMA performs √ 3 times more communication. If one dimension is much larger than the others and there is extra memory available, 2D, 2.5D and CARMA decompositions perform O(p 1/2 ), O(p 1/3 ), and 8% more communication than COSMA, respectively. For simplicity, we assume that parameters are chosen such that all divisions have integer results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Average communication volume per MPI rank and measured speedup of COSMA vs the second-best algorithm across all core counts for each of the scenarios.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">total comm. volume per rank [MB]</cell><cell>speedup</cell></row><row><cell cols="3">shape benchmark</cell><cell cols="4">ScaLAPACK CTF CARMA COSMA min mean max</cell></row><row><cell cols="2">A C B</cell><cell>strong scaling limited memory extra memory</cell><cell>203 816 303</cell><cell>222 986 350</cell><cell>195 799 291</cell><cell>107 1.07 1.94 4.81 424 1.23 1.71 2.99 151 1.14 2.03 4.73</cell></row><row><cell cols="2">A C B</cell><cell>strong scaling limited memory extra memory</cell><cell cols="2">2636 2278 368 541 133 152</cell><cell>659 128 48</cell><cell>545 1.24 2.00 6.55 88 1.30 2.61 8.26 35 1.31 2.55 6.70</cell></row><row><cell>A</cell><cell>C B</cell><cell>strong scaling limited memory extra memory</cell><cell cols="2">3507 2024 989 672 122 77</cell><cell>541 399 77</cell><cell>410 1.31 2.22 3.22 194 1.42 1.7 2.27 29 1.35 1.76 2.8</cell></row><row><cell cols="2">A C B</cell><cell>strong scaling limited memory extra memory</cell><cell>134 47 15</cell><cell>68 101 15</cell><cell>10 26 10</cell><cell>7 1.21 4.02 12.81 8 1.31 2.07 3.41 3 1.5 2.29 3.59</cell></row><row><cell></cell><cell></cell><cell>overall</cell><cell></cell><cell></cell><cell></cell><cell>1.07 2.17 12.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Strong scaling, n = m =17,408, k = 3,735,552</figDesc><table><row><cell>MB communicated per core</cell><cell>256 362 512 724 1,024 1,448 2,048</cell><cell>128</cell><cell>256</cell><cell>512 # of cores</cell><cell>1,024</cell><cell>50</cell><cell>2,048 22</cell></row><row><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>proved the I/O lower bound of classicalFigure 13: Distribution of achieved % of peak performance of the algorithms across all number of cores for "flat" and square matrices.</figDesc><table><row><cell></cell><cell>'flat' extra memory</cell><cell>'flat', limited memory</cell><cell>'flat', strong scaling</cell><cell cols="2">square extra memory</cell><cell>square, limited memory</cell><cell>square, strong scaling</cell></row><row><cell></cell><cell>100</cell><cell>from left to right:</cell><cell>COSMA (this work)</cell><cell>ScaLAPACK [14]</cell><cell cols="2">CTF [ ] 49 50</cell><cell>CARMA [21] 22</cell></row><row><cell>% peak performance</cell><cell>25 50 75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>'largeK', extra memory</cell><cell>'largeK', limited memory</cell><cell>'largeK', strong scaling</cell><cell cols="2">'largeM', extra memory</cell><cell>'largeM', limited memory</cell><cell>'largeM', strong scaling</cell></row><row><cell></cell><cell>100</cell><cell>from left to right:</cell><cell>COSMA (this work)</cell><cell>ScaLAPACK [14]</cell><cell cols="2">CTF [ ] 49 50</cell><cell>CARMA [21] 22</cell></row><row><cell>% peak performance</cell><cell>25 50 75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">2 3/2 √ p log 2 (p) √ p log 2 (p)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">the latest version available on Piz Daint when benchmarks were performed (August 2018). No improvements of P[S,D,C,Z]GEMM have been reported in the MKL release notes since then.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">h ps://github.com/cyclops-community/ctf, commit ID 244561c onMay 15, 2018   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">h ps://github.com/lipshitz/CAPS, commit ID 7589212 onJuly 19, 2013   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">only ScaLAPACK uses multiple cores per ranks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">for ScaLAPACK, actual number of MPI ranks is p/9</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yishai Oltchik and Niels Gleinig for invaluable help with the theoretical part of the paper, and Simon Pintarelli for advice and support with the implementation. We also thank CSCS for the compute hours needed to conduct all experiments. is project has received funding from the European Research Council (ERC) under the European Union's Horizon2020 programme (grant agreement DAPP, No.678880), and additional funding from the Platform for Advanced Scienti c Computing (PASC).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decomposition</head><p>2D <ref type="bibr" target="#b56">[56]</ref> 2.5D <ref type="bibr" target="#b53">[53]</ref> recursive <ref type="bibr" target="#b22">[22]</ref> COSMA (this paper) Parallel schedule P P i j for m = n P i jk for m = n</p><p>"General case":  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A three-dimensional approach to parallel matrix multiplication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vi Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Output Complexity of Sorting and Related Problems. CACM (Sept</title>
		<author>
			<persName><surname>Input</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Lars</forename><surname>Arge</surname></persName>
		</author>
		<title level="m">SPAA</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Parallel triangle counting and enumeration using matrix algebra</title>
		<author>
			<persName><forename type="first">Ariful</forename><surname>Azad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In IPDPSW</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph expansion and communication costs of fast matrix multiplication</title>
		<author>
			<persName><forename type="first">Grey</forename><surname>Ballard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<publisher>JACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</title>
		<idno>CoRR abs/1802.09941</idno>
	</analytic>
	<monogr>
		<title level="m">Tal Ben-Nun and Torsten Hoe er</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal sparse matrix dense vector multiplication in the I/O-model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCS</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SlimSell: A Vectorizable Graph Representation for Breadth-First Search</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of dag-consistent distributed sharedmemory algorithms</title>
		<author>
			<persName><surname>Robert D Blumofe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPAA</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Cellular Computer to Implement the Kalman Filter Algorithm</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cannon</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Ph.D. Dissertation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Register allocation via coloring</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Chaitin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Languages</title>
		<imprint>
			<date type="published" when="1981">1981. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Just a Pebble Game</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In CCC</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Eigenvalues of Matrices: Revised Edition</title>
		<author>
			<persName><forename type="first">Franàoise</forename><surname>Chatelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers</title>
		<author>
			<persName><forename type="first">Jaeyoung</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FRONTIERS</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design and Implementation of the ScaLAPACK LU, QR, and Cholesky Factorization Routines</title>
		<author>
			<persName><forename type="first">Jaeyoung</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Program</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ScaLAPACK: a portable linear algebra library for distributed memory computers -design issues and performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Comp. Phys. Comm.</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PUMMA: Parallel universal matrix multiplication algorithms on distributed memory concurrent computers</title>
		<author>
			<persName><forename type="first">Jaeyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="543" to="570" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Omas H Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cli</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><surname>Ord Stein</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
				<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using recursion to boost ATLAS&apos;s performance</title>
		<author>
			<persName><forename type="first">Paolo D'</forename><surname>Alberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Nicolau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the complexity of loop fusion</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Darte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enabling simulation at the h rung of DFT: Large scale RPA calculations with excellent time to solution</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Del</surname></persName>
			<affiliation>
				<orgName type="collaboration">Comp. Phys. Comm.</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ben</forename></persName>
			<affiliation>
				<orgName type="collaboration">Comp. Phys. Comm.</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communication-Optimal Parallel Recursive Rectangular Matrix Multiplication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Data access complexity: e red/blue pebble game revisited</title>
		<author>
			<persName><forename type="first">V</forename><surname>Elango</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Solving problems on concurrent processors</title>
		<author>
			<persName><forename type="first">Geo</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix algorithms on a hypercube I: Matrix multiplication</title>
		<author>
			<persName><forename type="first">Geo</forename><surname>Rey C Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">W</forename><surname>O O</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">Jg</forename><surname>Hey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cache-oblivious algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enabling Highly-Scalable Remote Memory Access Programming with MPI-3 One Sided</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gerstenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>In SC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pebbling Problem is Complete in Polynomial Space</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
				<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sparse matrix computations and their I/O complexity</title>
		<author>
			<persName><forename type="first">Gero</forename><surname>Greiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Ph.D. Dissertation. Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalability of Parallel Algorithms for Matrix Multiplication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Harnessing GPU tensor cores for fast FP16 arithmetic to speed up mixedprecision iterative re nement solvers</title>
		<author>
			<persName><forename type="first">Azzam</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hoe Er</surname></persName>
		</author>
		<title level="m">Remote Memory Access Programming in MPI-3. TOPC</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Communication Lower Bounds for Distributed-memory Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Dror</forename><surname>Irony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPDC</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">I/O complexity: e red-blue pebble game</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Tsung</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
				<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Maximizing loop parallelism and improving data locality via loop fusion and distribution</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>LCPC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mathematical foundations of the GraphBLAS</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05790</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Communication-avoiding parallel sparsedense matrix-matrix multiplication</title>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Increasing the e ciency of sparse matrix-matrix multiplication with a 2.5 D algorithm and one-sided MPI</title>
		<author>
			<persName><surname>Al O Lazzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PASC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized Cannon&apos;s Algorithm for Parallel Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Red-Blue and Standard Pebble Games : Complexity and Applications in the Sequential and Parallel Models</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An inequality related to the isoperimetric inequality</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassler</forename><surname>Whitney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949. 1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Matrix analysis and applied linear algebra</title>
		<author>
			<persName><forename type="first">Meyer</forename><surname>Carl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">2010. I/O E cient Algorithms for Matrix Computations</title>
		<author>
			<persName><forename type="first">Sraban</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohanty</forename></persName>
		</author>
		<idno>CoRR abs/1006.1307</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Computational Chemistry Using the PC</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matrix multiplication I/O-complexity by path routing</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Sco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPAA</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Complete Register Allocation Problems</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
				<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pushing the Bounds for Matrix-Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Geijn</surname></persName>
		</author>
		<idno>CoRR abs/1702.02017</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">E cient Implementation of antum Materials Simulations on Distributed CPU-GPU Systems</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Ra Aele Solcà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azzam</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanimire</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omas</forename><forename type="middle">C</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><surname>Schulthess</surname></persName>
		</author>
		<idno type="DOI">10.1145/2807591.2807654</idno>
		<ptr target="ps://doi.org/10.1145/2807591.2807654" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;15)</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;15)<address><addrLine>New York, NY, USA, Article 10</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cyclops tensor framework: Reducing communication and eliminating load imbalance in massively parallel contractions</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Solomonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Trade-O s Between Synchronization, Communication, and Computation in Parallel Linear Algebra omputations</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Solomonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOPC</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Scaling Betweenness Centrality using Communication-E cient Sparse Matrix Multiplication</title>
		<author>
			<persName><forename type="first">E</forename><surname>Solomonik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>In SC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Solomonik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>In EuroPar</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gaussian Elimination is Not Optimal</title>
		<author>
			<persName><surname>Volker Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<date type="published" when="1969">1969. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A survey of out-of-core algorithms in numerical linear algebra</title>
		<author>
			<persName><forename type="first">Sivan</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">External Memory Algorithms and Visualization</title>
				<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SUMMA: Scalable universal matrix multiplication algorithm</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerrell</forename><surname>Wa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="255" to="274" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Statistical scalability analysis of communication operations in distributed applications</title>
		<author>
			<persName><forename type="first">Er</forename><surname>Je Rey S Ve</surname></persName>
		</author>
		<author>
			<persName><surname>Michael O Mccracken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="123" to="132" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Data Locality Optimizing Algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">More iteration space tiling</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing&apos;89: Proceedings of the 1989 ACM/IEEE conference on Supercomputing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Optimizing Tall-and-Skinny Matrix-Matrix Multiplication on GPUs</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Ph.D. Dissertation. UC Riverside</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent</title>
		<author>
			<persName><forename type="first">Qinqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>La Erty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
