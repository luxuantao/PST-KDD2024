<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<email>kilianw@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania Levine Hall</orgName>
								<address>
									<addrLine>3330 Walnut Street</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
							<email>blitzer@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania Levine Hall</orgName>
								<address>
									<addrLine>3330 Walnut Street</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
							<email>lsaul@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania Levine Hall</orgName>
								<address>
									<addrLine>3330 Walnut Street</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C92B76BE753428E619CF142DBDC437A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classification by semidefinite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification-for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modification or extension for problems in multiway (as opposed to binary) classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The k-nearest neighbors (kNN) rule <ref type="bibr" target="#b2">[3]</ref> is one of the oldest and simplest methods for pattern classification. Nevertheless, it often yields competitive results, and in certain domains, when cleverly combined with prior knowledge, it has significantly advanced the state-ofthe-art <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. The kNN rule classifies each unlabeled example by the majority label among its k-nearest neighbors in the training set. Its performance thus depends crucially on the distance metric used to identify nearest neighbors.</p><p>In the absence of prior knowledge, most kNN classifiers use simple Euclidean distances to measure the dissimilarities between examples represented as vector inputs. Euclidean distance metrics, however, do not capitalize on any statistical regularities in the data that might be estimated from a large training set of labeled examples.</p><p>Ideally, the distance metric for kNN classification should be adapted to the particular problem being solved. It can hardly be optimal, for example, to use the same distance metric for face recognition as for gender identification, even if in both tasks, distances are computed between the same fixed-size images. In fact, as shown by many researchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, kNN classification can be significantly improved by learning a distance metric from labeled examples. Even a simple (global) linear transformation of input features has been shown to yield much better kNN classifiers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. Our work builds in a novel direction on the success of these previous approaches.</p><p>In this paper, we show how to learn a Mahanalobis distance metric for kNN classification. The metric is optimized with the goal that k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Our goal for metric learning differs in a crucial way from those of previous approaches that minimize the pairwise distances between all similarly labeled examples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. This latter objective is far more difficult to achieve and does not leverage the full power of kNN classification, whose accuracy does not require that all similarly labeled inputs be tightly clustered.</p><p>Our approach is largely inspired by recent work on neighborhood component analysis <ref type="bibr" target="#b6">[7]</ref> and metric learning by energy-based models <ref type="bibr" target="#b1">[2]</ref>. Though based on the same goals, however, our methods are quite different. In particular, we are able to cast our optimization as an instance of semidefinite programming. Thus the optimization we propose is convex, and its global minimum can be efficiently computed.</p><p>Our approach has several parallels to learning in support vector machines (SVMs)-most notably, the goal of margin maximization and a convex objective function based on the hinge loss. In light of these parallels, we describe our approach as large margin nearest neighbor (LMNN) classification. Our framework can be viewed as the logical counterpart to SVMs in which kNN classification replaces linear classification.</p><p>Our framework contrasts with classification by SVMs, however, in one intriguing respect: it requires no modification for problems in multiway (as opposed to binary) classification. Extensions of SVMs to multiclass problems typically involve combining the results of many binary classifiers, or they require additional machinery that is elegant but nontrivial <ref type="bibr" target="#b3">[4]</ref>. In both cases the training time scales at least linearly in the number of classes. By contrast, our learning problem has no explicit dependence on the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Let {( x i , y i )} n i=1 denote a training set of n labeled examples with inputs x i ∈ R d and discrete (but not necessarily binary) class labels y i . We use the binary matrix y ij ∈ {0, 1} to indicate whether or not the labels y i and y j match. Our goal is to learn a linear transformation L : R d → R d , which we will use to compute squared distances as:</p><formula xml:id="formula_0">D( x i , x j ) = L( x i -x j ) 2 .</formula><p>(1)</p><p>Specifically, we want to learn the linear transformation that optimizes kNN classification when distances are measured in this way. We begin by developing some useful terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target neighbors</head><p>In addition to the class label y i , for each input x i we also specify k "target" neighborsthat is, k other inputs with the same label y i that we wish to have minimal distance to x i , as computed by eq. ( <ref type="formula">1</ref>). In the absence of prior knowledge, the target neighbors can simply be identified as the k nearest neighbors, determined by Euclidean distance, that share the same label y i . (This was done for all the experiments in this paper.) We use η ij ∈ {0, 1} to indicate whether input x j is a target neighbor of input x i . Like the binary matrix y ij , the matrix η ij is fixed and does not change during learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost function</head><p>Our cost function over the distance metrics parameterized by eq. ( <ref type="formula">1</ref>) has two competing terms. The first term penalizes large distances between each input and its target neighbors, while the second term penalizes small distances between each input and all other inputs that do not share the same label. Specifically, the cost function is given by:</p><formula xml:id="formula_1">ε(L) = ij η ij L( x i -x j ) 2 + c ijl η ij (1-y il ) 1 + L( x i -x j ) 2 -L( x i -x l ) 2 + ,<label>(2)</label></formula><p>where in the second term [z] + = max(z, 0) denotes the standard hinge loss and c &gt; 0 is some positive constant (typically set by cross validation). Note that the first term only penalizes large distances between inputs and target neighbors, not between all similarly labeled examples. The second term in the cost function incorporates the idea of a margin. In particular, for each input x i , the hinge loss is incurred by differently labeled inputs whose distances do not exceed, by one absolute unit of distance, the distance from input x i to any of its target neighbors. The cost function thereby favors distance metrics in which differently labeled inputs maintain a large margin of distance and do not threaten to "invade" each other's neighborhoods. The learning dynamics induced by this cost function are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> for an input with k = 3 target neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large margin</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallels with SVMs</head><p>The competing terms in eq. ( <ref type="formula" target="#formula_1">2</ref>) are analogous to those in the cost function for SVMs <ref type="bibr" target="#b10">[11]</ref>. In both cost functions, one term penalizes the norm of the "parameter" vector (i.e., the weight vector of the maximum margin hyperplane, or the linear transformation in the distance metric), while the other incurs the hinge loss for examples that violate the condition of unit margin. Finally, just as the hinge loss in SVMs is only triggered by examples near the decision boundary, the hinge loss in eq. ( <ref type="formula" target="#formula_1">2</ref>) is only triggered by differently labeled examples that invade each other's neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convex optimization</head><p>We can reformulate the optimization of eq. ( <ref type="formula" target="#formula_1">2</ref>) as an instance of semidefinite programming <ref type="bibr" target="#b15">[16]</ref>. A semidefinite program (SDP) is a linear program with the additional constraint that a matrix whose elements are linear in the unknown variables is required to be positive semidefinite. SDPs are convex; thus, with this reformulation, the global minimum of eq. ( <ref type="formula" target="#formula_1">2</ref>) can be efficiently computed. To obtain the equivalent SDP, we rewrite eq. ( <ref type="formula">1</ref>) as:</p><formula xml:id="formula_2">D( x i , x j ) = ( x i -x j ) M( x i -x j ),<label>(3)</label></formula><p>where the matrix M = L L, parameterizes the Mahalanobis distance metric induced by the linear transformation L. Rewriting eq. ( <ref type="formula" target="#formula_1">2</ref>) as an SDP in terms of M is straightforward, since the first term is already linear in M = L L and the hinge loss can be "mimicked" by introducing slack variables ξ ij for all pairs of differently labeled inputs (i.e., for all i, j such that y ij = 0). The resulting SDP is given by:</p><formula xml:id="formula_3">Minimize ij η ij ( x i -x j ) M( x i -x j ) + c ij η ij (1 -y il )ξ ijl subject to: (1) ( x i -x l ) M( x i -x l ) -( x i -x j ) M( x i -x j ) ≥ 1 -ξ ijl (2) ξ ijl ≥ 0 (3) M 0.</formula><p>The last constraint M 0 indicates that the matrix M is required to be positive semidefinite. While this SDP can be solved by standard online packages, general-purpose solvers tend to scale poorly in the number of constraints. Thus, for our work, we implemented our own special-purpose solver, exploiting the fact that most of the slack variables {ξ ij } never attain positive values <ref type="foot" target="#foot_0">1</ref> . The slack variables {ξ ij } are sparse because most labeled inputs are well separated; thus, their resulting pairwise distances do not incur the hinge loss, and we obtain very few active constraints. Our solver was based on a combination of sub-gradient descent in both the matrices L and M, the latter used mainly to verify that we had reached the global minimum. We projected updates in M back onto the positive semidefinite cone after each step. Alternating projection algorithms provably converge <ref type="bibr" target="#b15">[16]</ref>, and in this case our implementation worked much faster than generic solvers<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We evaluated the algorithm in the previous section on seven data sets of varying size and difficulty. We first compare kNN classification error rates using Mahalanobis versus Euclidean distances. To break ties among different classes, we repeatedly reduced the neighborhood size, ultimately classifying (if necessary) by just the k = 1 nearest neighbor. Fig. <ref type="figure" target="#fig_2">2</ref> summarizes the main results. Except on the smallest data set (where over-training appears to be an issue), the Mahalanobis distance metrics learned by semidefinite programming led to significant improvements in kNN classification, both in training and testing. The training error rates reported in Fig. <ref type="figure" target="#fig_2">2</ref> are leave-one-out estimates.</p><p>We also computed test error rates using a variant of kNN classification, inspired by previous work on energy-based models <ref type="bibr" target="#b1">[2]</ref>. Energy-based classification of a test example x t was done by finding the label that minimizes the cost function in eq. ( <ref type="formula" target="#formula_1">2</ref>). In particular, for a hypothetical label y t , we accumulated the squared distances to the k nearest neighbors of x t that share the same label in the training set (corresponding to the first term in the cost function); we also accumulated the hinge loss over all pairs of differently labeled examples that result from labeling x t by y t (corresponding to the second term in the cost function). Finally, the test example was classified by the hypothetical label that minimized the combination of these two terms:</p><formula xml:id="formula_4">y t = argmin yt j η tj L( x t -x j ) 2 +c j,i=t∨l=t η ij (1-y il ) 1 + L( x i -x j ) 2 -L( x i -x l ) 2 +</formula><p>As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, energy-based classification with this assignment rule generally led to even further reductions in test error rates.</p><p>Finally, we compared our results to those of multiclass SVMs <ref type="bibr" target="#b3">[4]</ref>. On each data set (except MNIST), we trained multiclass SVMs using linear and RBF kernels; Fig. <ref type="figure" target="#fig_2">2</ref> reports the results of the better classifier. On MNIST, we used a non-homogeneous polynomial kernel of degree four, which gave us our best results. (See also <ref type="bibr" target="#b8">[9]</ref>.)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small data sets with few classes</head><p>The wine, iris, and balance data sets are small data sets, with less than 500 training examples and just three classes, taken from the UCI Machine Learning Repository<ref type="foot" target="#foot_2">3</ref> . On data sets of this size, a distance metric can be learned in a matter of seconds. The results in Fig. <ref type="figure" target="#fig_2">2</ref> were averaged over 100 experiments with different random 70/30 splits of each data set. Our results on these data sets are roughly comparable (i.e., better in some cases, worse in others) to those of neighborhood component analysis (NCA) and relevant component analysis (RCA), as reported in previous work <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face recognition</head><p>The AT&amp;T face recognition data set <ref type="foot" target="#foot_3">4</ref> contains 400 grayscale images of 40 individuals in 10 different poses. We downsampled the images from to 38 × 31 pixels and used PCA to obtain 30-dimensional eigenfaces <ref type="bibr" target="#b14">[15]</ref>. Training and test sets were created by randomly sampling 7 images of each person for training and 3 images for testing. The task involved 40-way classification-essentially, recognizing a face from an unseen pose. Fig. <ref type="figure" target="#fig_2">2</ref> shows the improvements due to LMNN classification. Fig. <ref type="figure" target="#fig_4">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spoken letter recognition</head><p>The Isolet data set from UCI Machine Learning Repository has 6238 examples and 26 classes corresponding to letters of the alphabet. We reduced the input dimensionality (originally at 617) by projecting the data onto its leading 172 principal components-enough to account for 95% of its total variance. On this data set, Dietterich and Bakiri report test error rates of 4.2% using nonlinear backpropagation networks with 26 output units (one per class) and 3.3% using nonlinear backpropagation networks with a 30-bit error correcting code <ref type="bibr" target="#b4">[5]</ref>. LMNN with energy-based classification obtains a test error rate of 3.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text categorization</head><p>The 20-newsgroups data set consists of posted articles from 20 newsgroups, with roughly 1000 articles per newsgroup. We used the 18828-version of the data set <ref type="foot" target="#foot_4">5</ref> which has crosspostings removed and some headers stripped out. We tokenized the newsgroups using the rainbow package <ref type="bibr" target="#b9">[10]</ref>. Each article was initially represented by the weighted word-counts of the 20,000 most common words. We then reduced the dimensionality by projecting the data onto its leading 200 principal components. The results in Fig. <ref type="figure" target="#fig_2">2</ref> were obtained by averaging over 10 runs with 70/30 splits for training and test data. Our best result for LMMN on this data set at 13.0% test error rate improved significantly on kNN classification using Euclidean distances. LMNN also performed comparably to our best multiclass SVM <ref type="bibr" target="#b3">[4]</ref>, which obtained a 12.4% test error rate using a linear kernel and 20000 dimensional inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handwritten digit recognition</head><p>The MNIST data set of handwritten digits<ref type="foot" target="#foot_5">6</ref> has been extensively benchmarked <ref type="bibr" target="#b8">[9]</ref>. We deskewed the original 28×28 grayscale images, then reduced their dimensionality by retaining only the first 164 principal components (enough to capture 95% of the data's overall variance). Energy-based LMNN classification yielded a test error rate at 1.3%, cutting the baseline kNN error rate by over one-third. Other comparable benchmarks <ref type="bibr" target="#b8">[9]</ref> (not exploiting additional prior knowledge) include multilayer neural nets at 1.6% and SVMs at 1.2%. Fig. <ref type="figure" target="#fig_6">4</ref> shows some digits whose nearest neighbor changed as a result of learning, from a mismatch using Euclidean distance to a match using Mahanalobis distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Many researchers have attempted to learn distance metrics from labeled examples. We briefly review some recent methods, pointing out similarities and differences with our work.  Xing et al <ref type="bibr" target="#b16">[17]</ref> used semidefinite programming to learn a Mahalanobis distance metric for clustering. Their algorithm aims to minimize the sum of squared distances between similarly labeled inputs, while maintaining a lower bound on the sum of distances between differently labeled inputs. Our work has a similar basis in semidefinite programming, but differs in its focus on local neighborhoods for kNN classification.</p><p>Shalev-Shwartz et al <ref type="bibr" target="#b11">[12]</ref> proposed an online learning algorithm for learning a Mahalanobis distance metric. The metric is trained with the goal that all similarly labeled inputs have small pairwise distances (bounded from above), while all differently labeled inputs have large pairwise distances (bounded from below). A margin is defined by the difference of these thresholds and induced by a hinge loss function. Our work has a similar basis in its appeal to margins and hinge loss functions, but again differs in its focus on local neighborhoods for kNN classification. In particular, we do not seek to minimize the distance between all similarly labeled inputs, only those that are specified as neighbors.</p><p>Goldberger et al <ref type="bibr" target="#b6">[7]</ref> proposed neighborhood component analysis (NCA), a distance metric learning algorithm especially designed to improve kNN classification. The algorithm minimizes the probability of error under stochastic neighborhood assignments using gradient descent. Our work shares essentially the same goals as NCA, but differs in its construction of a convex objective function.</p><p>Chopra et al <ref type="bibr" target="#b1">[2]</ref> recently proposed a framework for similarity metric learning in which the metrics are parameterized by pairs of identical convolutional neural nets. Their cost function penalizes large distances between similarly labeled inputs and small distances between differently labeled inputs, with penalties that incorporate the idea of a margin. Our work is based on a similar cost function, but our metric is parameterized by a linear transformation instead of a convolutional neural net. In this way, we obtain an instance of semidefinite programming.</p><p>Relevant component analysis (RCA) constructs a Mahalanobis distance metric from a weighted sum of in-class covariance matrices <ref type="bibr" target="#b12">[13]</ref>. It is similar to PCA and linear discriminant analysis (but different from our approach) in its reliance on second-order statistics.</p><p>Hastie and Tibshirani [?] and Domeniconi et al <ref type="bibr" target="#b5">[6]</ref> consider schemes for locally adaptive distance metrics that vary throughout the input space. The latter work appeals to the goal of margin maximization but otherwise differs substantially from our approach. In particular, Domeniconi et al <ref type="bibr" target="#b5">[6]</ref> suggest to use the decision boundaries of SVMs to induce a locally adaptive distance metric for kNN classification. By contrast, our approach (though similarly named) does not involve the training of SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we have shown how to learn Mahalanobis distance metrics for kNN classification by semidefinite programming. Our framework makes no assumptions about the structure or distribution of the data and scales naturally to large number of classes. Ongoing</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Schematic illustration of one input's neighborhood x i before training (left) versus after training (right). The distance metric is optimized so that: (i) its k = 3 target neighbors lie within a smaller radius after training; (ii) differently labeled inputs lie outside this smaller radius, with a margin of at least one unit distance. Arrows indicate the gradients on distances arising from the optimization of the cost function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training and test error rates for kNN classification using Euclidean versus Mahalanobis distances. The latter yields lower test error rates on all but the smallest data set (presumably due to over-training). Energy-based classification (see text) generally leads to further improvement. The results approach those of state-of-the-art multiclass SVMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>illustrates the improvements more graphically by showing how the k = 3 nearest neighbors change as a result of learning a Mahalanobis metric. (Though the algorithm operated on low dimensional eigenfaces, for clarity the figure shows the rescaled images.) Among 3 nearest neighbors before but not after training: Test Image: Among 3 nearest neighbors after but not before training:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Images from the AT&amp;T face recognition data base. Top row: an image correctly recognized by kNN classification (k = 3) with Mahalanobis distances, but not with Euclidean distances. Middle row: correct match among the k = 3 nearest neighbors according to Mahalanobis distance, but not Euclidean distance. Bottom row: incorrect match among the k = 3 nearest neighbors according to Euclidean distance, but not Mahalanobis distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top row: Examples of MNIST images whose nearest neighbor changes during training. Middle row: nearest neighbor after training, using the Mahalanobis distance metric. Bottom row: nearest neighbor before training, using the Euclidean distance metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table1compares the different data sets. Principal components analysis (PCA) was used to reduce the dimensionality of image, speech, and text data, both to speed up training and avoid overfitting. Except for Isolet and MNIST, all of the experimental results are averaged over several runs of randomly generated 70/30 splits of the data. Isolet and MNIST have pre-defined training/test splits. For the other data sets, we randomly generated 70/30 splits for each run. Both the number of target neighbors (k) and the weighting parameter (c) in eq. (2) were set by cross validation. (For the purpose of cross-validation, the training sets were further partitioned into training and validation sets.) We begin by reporting overall trends, then discussing the individual data sets in more detail.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Properties of data sets and experimental parameters for LMNN classification.</figDesc><table><row><cell></cell><cell>Iris</cell><cell cols="2">Wine Faces</cell><cell cols="2">Bal</cell><cell cols="2">Isolet</cell><cell>News</cell><cell>MNIST</cell></row><row><cell>examples (train)</cell><cell>106</cell><cell>126</cell><cell>280</cell><cell cols="2">445</cell><cell></cell><cell>6238</cell><cell>16000</cell><cell>60000</cell></row><row><cell>examples (test)</cell><cell>44</cell><cell>52</cell><cell>120</cell><cell>90</cell><cell></cell><cell></cell><cell>1559</cell><cell>2828</cell><cell>10000</cell></row><row><cell>classes</cell><cell>3</cell><cell>3</cell><cell>40</cell><cell>3</cell><cell></cell><cell></cell><cell>26</cell><cell>20</cell><cell>10</cell></row><row><cell>input dimensions</cell><cell>4</cell><cell>13</cell><cell>1178</cell><cell>4</cell><cell></cell><cell></cell><cell>617</cell><cell>30000</cell><cell>784</cell></row><row><cell>features after PCA</cell><cell>4</cell><cell>13</cell><cell>30</cell><cell>4</cell><cell></cell><cell></cell><cell>172</cell><cell>200</cell><cell>164</cell></row><row><cell>constraints</cell><cell cols="8">5278 7266 78828 76440 37 Mil 164 Mil</cell><cell>3.3 Bil</cell></row><row><cell>active constraints</cell><cell>113</cell><cell>1396</cell><cell>7665</cell><cell cols="2">3099</cell><cell cols="2">45747</cell><cell>732359 243596</cell></row><row><cell>CPU time (per run)</cell><cell>2s</cell><cell>8s</cell><cell>7s</cell><cell cols="2">13s</cell><cell></cell><cell>11m</cell><cell>1.5h</cell><cell>4h</cell></row><row><cell>runs</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell cols="2">100</cell><cell></cell><cell>1</cell><cell>10</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MNIST</cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>NEWS</cell><cell></cell><cell></cell><cell cols="2">17.6 13.4 13.0 12.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ISOLET</cell><cell cols="2">4.7 3.7 3.3</cell><cell>8.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>BAL</cell><cell></cell><cell cols="2">9.7 8.4 7.8</cell><cell>14.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FACES</cell><cell>2.6 2.7</cell><cell cols="2">5.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>WINE</cell><cell>2.6 2.7</cell><cell></cell><cell></cell><cell>19.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IRIS</cell><cell cols="2">4.3 4.7 5.8 4.4</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A great speedup can be achieved by solving an SDP that only monitors a fraction of the margin conditions, then using the resulting solution as a starting point for the actual SDP of interest.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A matlab implementation is currently available at http://www.seas.upenn.edu/∼kilianw/lmnn.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Available at http://www.ics.uci.edu/∼mlearn/MLRepository.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Available at http://www.uk.research.att.com/facedatabase.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Available at http://people.csail.mit.edu/jrennie/20Newsgroups/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Available at http://yann.lecun.com/exdb/mnist/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>work is focused in three directions. First, we are working to apply LMNN classification to problems with hundreds or thousands of classes, where its advantages are most apparent. Second, we are investigating the kernel trick to perform LMNN classification in nonlinear feature spaces. As LMMN already yields highly nonlinear decision boundaries in the original input space, however, it is not obvious that "kernelizing" the algorithm will lead to significant further improvement. Finally, we are extending our framework to learn locally adaptive distance metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> that vary across the input space. Such metrics should lead to even more flexible and powerful large margin classifiers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similiarty metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions in Information Theory, IT-13</title>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Solving multiclass learning problems via error-correcting output codes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large margin nearest neighbor classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="899" to="909" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminant adaptive nearest neighbor classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="607" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 International Conference on Artificial Neural Networks (ICANN-95)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Fogelman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</editor>
		<meeting>the 1995 International Conference on Artificial Neural Networks (ICANN-95)<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/mccallum/bow" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online and batch learning of pseudo-metrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adjustment learning and relevant component analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh European Conference on Computer Vision (ECCV-02)</title>
		<meeting>the Seventh European Conference on Computer Vision (ECCV-02)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="776" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient pattern recognition using a new transformation distance</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semidefinite programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="95" />
			<date type="published" when="1996-03">March 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance metric learning, with application to clustering with side-information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
