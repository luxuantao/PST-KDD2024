<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effect of Character and Word Features in Bidirectional LSTM-CRF for NER</title>
				<funder ref="#_ezk7Yes">
					<orgName type="full">Republic of Korea</orgName>
				</funder>
				<funder>
					<orgName type="full">Government-wide R&amp;D Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chirawan</forename><surname>Ronran</surname></persName>
							<email>chirawan@kisti.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">University of Science and Technology (UST) Research Data Sharing Center Korea Institute of Science organization and Technology Information (KISTI)</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">nd Seungwoo Lee University of Science and Technology (UST) Research Data Sharing Center Korea Institute of Science organization and Technology Information (KISTI)</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effect of Character and Word Features in Bidirectional LSTM-CRF for NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/BigComp48618.2020.00132</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NER</term>
					<term>Bidirectional LSTM</term>
					<term>CRF</term>
					<term>Word Feature</term>
					<term>Character Feature</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) is a challenging task in natural language processing (NLP). Nowadays, the NLP research community pays attention to the NER system, adopting a well-known deep neural network (DNN) techniques to extract the named entity in a text. So, we studied the effect of the existing Glove and Fasttext word embedding to improve NER performance. We also examined the impact of a combination of additional word and character input features to CNN, Bidirectional LSTM with and without the CRF models. In the proposed work, we did not preprocess the data and did not use any lexicon for further enhancement. Our experiment mainly focuses on the effectiveness of the word and character features for NER. The F1 measure was used for comparing the effectiveness of our additional input features with Chui's word and character feature. To our knowledge, the best result is obtained using Glove 840B as word embedding along with word pattern, and character pattern input for CNN and two-layers of Bidirectional LSTM with CRF. The experiment achieves 91.10% on the CoNLL-2003 and outperforms Chiu state of the art (SOTA) results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Named entity recognition (NER) aims at identifying names in text and classifying them among several categories of interest such as a person (PER), organization (ORG), location (LOC), and geopolitical entities (GPE). NER has gained constant research attention as the initial step in a wide range of NLP tasks, e.g. event extraction, relation extraction, question answering, and co-reference resolution <ref type="bibr">[1]</ref>.</p><p>Earlier, NER models were based on handcrafted rules, ontologies, lexicon, and orthographic. Subsequently, new approaches like feature-engineering and statistical models like machine learning were introduced <ref type="bibr" target="#b4">[2]</ref>. Later, neural networks became the standard base for the existing state of the art model, starting with <ref type="bibr">Collobert et al., 2011 [3]</ref>. <ref type="bibr">Huang et al., 2015 [4]</ref> presented a word LSTM with conditional random field (CRF) to improve the NER model. <ref type="bibr">Kim et al., 2016 [5]</ref> proposed a highway network over CNN for character sequences of the word and using LSTM with softmax to predict the final result.</p><p>A combination of word embedding with word and character features is proven to be reliable for increasing the accuracy of NER system by <ref type="bibr">Chiu et al., 2015 [6]</ref>. They construct these features by preprocessing the corpus and using Senna and DBpedia lexicons. Later, they pass it through a Convolutional Neural Network (CNN) and Bidirectional LSTMs. This paper, we combined two techniques proposed in Chiu et al., 2015 (CNN + Bidirectional LSTM) and Huang et al., 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features <ref type="bibr" target="#b8">[6,</ref><ref type="bibr">4]</ref>. We explore existing word embeddings that is distinct from the previous studies including (1) Glove 300 embedding of 42B, 840B word vectors trained on Common Crawl <ref type="bibr" target="#b9">[7]</ref> and (2) Fasttext 300 dimension of two million word vectors trained on Common Craw and one million word trained on Wikipedia 2017, UMBC web base corpus and statmt.org news dataset <ref type="bibr" target="#b10">[8]</ref>. The embeddings are fed into Bidirectional LSTM for predicting the name entities. A best embedding is selected from the above-mentioned word embeddings based on the model's performance and then combined with word and character features in the next state.</p><p>To investigate the impact of a word and character features on the NER task, we follow the processes mentioned below. In our work, we do not preprocess the data and do not use any lexicon for improving the accuracy of the model. We focus on the word and character feature with CNN, Bidirectional LSTM with and without CRF Model.</p><p>The F1-Score is used for evaluating the performance of our technique on the standard CoNLL-2003 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH</head><p>The existing algorithms for the NER task can be classified into three approaches <ref type="bibr" target="#b11">[9]</ref> : rule-based, machine-deep learning, and hybrid. The rule-based algorithm applies a set of rules in order to extract patterns, i.e., rule base for Malay NER <ref type="bibr" target="#b12">[10]</ref>.</p><p>With the emergence of the machine and deep learning, various models were proposed for the task. The machine learning approach involves the usage of structured and unstructured techniques, such as CRF that was implemented for DrugNER <ref type="bibr" target="#b13">[11]</ref>. While in deep learning, various kind of neural network work along with the vector representations constitute to create the model for NER including: ELMO and BERT, set state-of-the-art performance on many tasks, including NER <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b16">14]</ref>. Third, a hybrid NER implements both the rule-based machine and deep learning, for instance, Bio-Ner <ref type="bibr" target="#b17">[15]</ref> that was experimented with a rule-based and classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. WORD AND CHARACTER FEATURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Most research, including our related work, use the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), obtained from the Reuters RCV1 corpus. It is tagged with four types of named entities: location, organization, person, and miscellaneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distribution Representations</head><p>The CoNLL-2003 train set was processed to generate three kinds of input representations. (1) Word embeddings, we conduct the experiment using the public embedding i.e. Glove <ref type="bibr" target="#b9">[7]</ref> and Fasttext <ref type="bibr" target="#b10">[8]</ref> with a different dimension, corpus, and vocabulary size (Table <ref type="table">2</ref>). As mentioned previously adding additional information as an input along with word embedding enhances the accuracy. Therefore, we compose two additional information, i.e., word and character features in the next process .</p><p>(2) Word feature representations, <ref type="bibr">Collobert et al., 2011 [3]</ref> used word feature representation to add capitalization information. In our work, to compose word feature we not only included capitalization feature but also incorporated pattern extractor. To extract the standard pattern format of words (e.g., DateTime, URL, and telephone number), we extend features into Collobert word feature, modify the word pattern feature of Heang et al., 2015 <ref type="bibr">[4]</ref>, and adjust masks of numbers of <ref type="bibr">Ratinov et al., 2009 [16]</ref>. Consequently, we experiment with four types of word features:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Models</head><p>In this section, we utilized CNN model to transform the character features into an embedding. Additionally, we used two model i.e. one/two layers of Bidirectional LSTM, and Bidirectional LSTM CRF to predict the named entity by using Chui's hyper-parameters.</p><p>To encode the groups of character features, we used the CNN layer, as demonstrated in  These features are utilized to recognize the named entity in the sequence of words. For this task, we build models with two different approaches for input processing. In the first approach, we use a Bidirectional LSTM layer to process the concatenation of word embedding, word feature, and character feature. In the second approach, we utilize the two-layers of Bidirectional LSTM. The first layer processes the combination of word embedding and character features. While the second layer processes the combination of the previous Bidirectional LSTM output and word feature, as illustrated in Fig <ref type="figure" target="#fig_3">2</ref>. These Bidirectional LSTMs add some dependency between adjacent words within a single input by traversing in forward and backward direction. Each direction generates hidden states, which is concatenated to get the result vector before being passed on the softmax layer to get the prediction result. Additionally, we also perform the experiment by adding the CRF layer as an extension for our Bidirectional LSTM model to predict the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL STRUCTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Word Embeddings</head><p>The F1 score in Table <ref type="table">2</ref> evaluates the impact of public embedding on Bidirectional LSTM. The best embedding is Glove 840B, which found the highest number of vocabulary in this embedding. Furthermore, when we compare the F1 score of Glove for all the dimensions of vocabulary size 400K, and found that word embedding with 300 dimensions gives more significant improvement than embedding with 50 dimensions.</p><p>We assume the dimension of word embedding and the number of vocabulary found in the dataset are the main reasons for the difference in the impact on NER. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Word Features</head><p>Table <ref type="table">3</ref> displays the experiment results with word features passed through the train embedding layer with random initial embedding. A significant result is found using the "Digit Mask" word. Compared with the baseline results, our features have no significant impact when the representation passes through Bidirectional LSTM. However, approximately one point is improved when the representation fed into Bidirectional LSTM and CRF. We believe one of the reasons for this different impact is word patterns. Notably, the number formats (i.e., 2_digits, 3_digits and 4_digits) were able to improve the F1 score performance of word feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Character Features</head><p>Table <ref type="table">4</ref> shows that adding the detail of punctuation also has promising effect for NER. Furthermore, the F1 score of character features shows slightly better results than word features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effective of Model Structures</head><p>Bidirectional LSTM shows improvement in the results, when concatenating word embedding with either word or character representation. However, the F1 score was reduced when word and character features are combined (Fig. <ref type="figure" target="#fig_4">3</ref>). The F1 score is improved by adding CRF layer (Fig. <ref type="figure" target="#fig_5">4</ref>). Our best performance of this model achieves the F1 score at 90.97%, outperforming the baseline model that achieved F1 of 90.69%. However, Bidirectional LSTM often has difficulty in predicting the output from the very long input sequences. To overcome this, we use the two layers of Bidirectional LSTM for reducing the input sequences. In the first layer, we combine the word embedding and character features passed through Bidirectional LSTM. The second layer, the first layer output is concatenated with word features fed into another Bidirectional LSTM and using CFR to predict the final result. By using the two-layers Bidirectional LSTM CRF model technique was able to achieve our best performance at 91.10% when combining word pattern with character pattern (Fig. <ref type="figure" target="#fig_6">5</ref>)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have analyzed the set of character and word features. We also studied the impact of various features and their combination on the neural network model. Our model incorporates two layers Bidirectional LSTM with CRF without preprocessing and lexicon that achieves the 91.10% F1 score on the CoNLL-2003 dataset. The word and character feature boost over this system is a statistically significant improvement on the benchmarks. We plan to extend the model to enhancing NER in the biomedical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Explore the CoNLL-2003 train set to analyze the features of the context. 2. Compose a new word feature for representing capitalized information, and extract standard format of words (e.g., URL, Date, Time, and telephone number). After the pattern processing, the analyzed word patterns are processed by the Bidirectional LSTM layer 3. To categorize character features, we discard to use Chui's characters embedding 1 and use a different model to represent character features. We use groups of characters and pass them through the CNN layer. These character feature representations are further processed by the Bidirectional LSTM layer. 4. Concatenate 2 and 3 with our best performed public word embedding (Glove840B) and send it to one or two layers of Bidirectional LSTM with and without the CRF layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 1 .</head><label>1</label><figDesc>It is different from Chiu et al., 2015 method, in that it combines both character feature and character embedding before passing though the CNN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CNN layer encodes the categorized character features before using max-pooling layer to get the word feature representation.</figDesc><graphic url="image-2.png" coords="3,95.29,281.59,166.05,98.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two-layer of Bidirectional LSTM with a word embedding, character feature, and word feature</figDesc><graphic url="image-3.png" coords="3,72.82,513.91,205.56,92.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The F1 score of character features compared with the F1 score of combination between word features and character features passed through Bidirectional LSTM model.</figDesc><graphic url="image-5.png" coords="4,82.55,401.66,192.39,58.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The F1 score of character features compared with the F1 score of combination between word features and character features passed through the Bidirectional LSTM CRF model. In this task, The Chiu representation and our word pattern is our best combination feature in this model.</figDesc><graphic url="image-6.png" coords="4,80.56,493.09,194.49,61.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The F1 score between one and two layers of Bidirectional LSTM CRF with a combination of word and character features, achieves the F1 score at 91.10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>1. Collobert: consists of Caps, mixedCaps, upperInitial,</cell></row><row><cell>lowercase, and Noinfo.</cell></row><row><cell>2. Collobert Extension contains Collobert word features,</cell></row><row><cell>Digit, and DateTime.</cell></row><row><cell>3. Digit Mask Modification: comprises of initialUpper,</cell></row><row><cell>allUpper, allLower, mainly_digit, contains_digit, digit,</cell></row><row><cell>1_digit, 2_digits, and 4_digits.</cell></row><row><cell>4. Word Pattern Modification: implemented by algorithm</cell></row><row><cell>appeared in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I .</head><label>I</label><figDesc>Chui, et al., 2015  [6]  extracted the character feature by transforming a character into an embedding. Whereas, in our experiment, we classify the characters into groups. The characters in the same group are converted into similar character representation. GNU Standard Character Set 2 is used to classify characters into two possible groups of character features, i.e., standard character and Ascii. Besides, we examined the CoNLL-2003 train set for punctuation. The majority was found in Outside 3 , so we hypothesized some punctuation impacts on the NER and append two experiments named punctuation expansion and character pattern. So, we use four types of word features for our experiment compared with Chiu character representation:</figDesc><table><row><cell>semicolon, and colon, (3) open brackets, braces,</cell></row><row><cell>guillemets, and parentheses, (4) close brackets, braces,</cell></row><row><cell>guillemets, and parentheses, (5) single quote, double quote,</cell></row><row><cell>ellipsis, and apostrophe.</cell></row><row><cell>4. Character Pattern: transformed each word into "word</cell></row><row><cell>pattern modification" (in section B) before using Chui's</cell></row><row><cell>techniques to generate the representation of character</cell></row><row><cell>features 4 with punctuations, numbers, a capital letter, and a</cell></row><row><cell>lower letter.</cell></row><row><cell>1. Standard character: categorized into lower case, upper</cell></row><row><cell>case, digit, and punctuation</cell></row><row><cell>2. Ascii: segmented character into different ranges including</cell></row><row><cell>(1) 32 through 64 -punctuation, symbols, numbers, and</cell></row><row><cell>space, (2) 65 through 90 -uppercase, (3) 97 through 122 -</cell></row><row><cell>lowercase, (4) between 91 through 96 and 123 through 126</cell></row><row><cell>-additional graphemes, such as [, \, { and |</cell></row><row><cell>3. Punctuation Expansion: separated the punctuation into</cell></row><row><cell>groups, concerning the meaning and position of characters.</cell></row><row><cell>The group comprises of (1) sentence endings, (2) comma,</cell></row></table><note><p><p>WORD PATTERN MODIFICATION ALGORITHM Algorithm 1: procedure WordPatternFeature(word) 2: word ? re.sub(r'[A-Z]', 'A', word) 3: word ? re.sub(r'[a-z]', 'a', word) 4: word ? re.sub(r'AA+', 'AA', word) 5: word ? re.sub(r'aa+', 'aa', word) 6: word ? re.sub(r'[0-9]', 'D', word) 8: word ? re.sub(r'99999+', 'D5', word) 9: word ? re.sub(r'9999', 'D4', word) 10: word ? re.sub(r'999', 'D3', word) 11: word ? re.sub(r'99', 'D2', word)</p>(3) Character feature representations,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II .</head><label>II</label><figDesc>THE F1 SCORE OF WORD EMBEDDINGS</figDesc><table><row><cell>Word</cell><cell>Corpus</cell><cell>Vocabulary</cell><cell>Dim</cell><cell>F1</cell></row><row><cell>Embedding</cell><cell></cell><cell>Size</cell><cell></cell><cell>Score</cell></row><row><cell>Glove</cell><cell>Common Crawl</cell><cell>840B</cell><cell>300D</cell><cell>89.16</cell></row><row><cell></cell><cell></cell><cell>42B</cell><cell>300D</cell><cell>86.50</cell></row><row><cell></cell><cell>Wikipedia, Web text</cell><cell>400 K</cell><cell>50D</cell><cell>83.67</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100D</cell><cell>85.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell>200D</cell><cell>85.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell>300D</cell><cell>86.23</cell></row><row><cell>Fasttext</cell><cell>Common Crawl</cell><cell>2M</cell><cell>300D</cell><cell>86.76</cell></row><row><cell></cell><cell>Wikipedia, UMBC</cell><cell>1M</cell><cell>300D</cell><cell>88.57</cell></row><row><cell></cell><cell>web base, Statmt.org</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>news</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III .</head><label>III</label><figDesc>THE F1 SCORE OF WORD FEATURES</figDesc><table><row><cell>Model</cell><cell>Collobert</cell><cell>Collobert</cell><cell>Digit Mask</cell><cell>Word</cell></row><row><cell></cell><cell></cell><cell>Extension</cell><cell></cell><cell>Pattern</cell></row><row><cell>BiLSTM</cell><cell>89.66</cell><cell>89.65</cell><cell>89.70</cell><cell>89.69</cell></row><row><cell>BiLSTM CRF</cell><cell>90.55</cell><cell>90.69</cell><cell>90.70</cell><cell>90.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell>.</cell><cell cols="4">THE F1 SCORE OF CHARACTER FEATURES</cell></row><row><cell>Model</cell><cell>Chui</cell><cell></cell><cell>Standard</cell><cell>Punctuation</cell><cell>Ascii</cell><cell>Char</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Char</cell><cell>Expansion</cell><cell cols="2">Pattern</cell></row><row><cell>BiLSTM</cell><cell cols="2">89.99</cell><cell>90.00</cell><cell cols="2">89.98 90.02</cell><cell>90.10</cell></row><row><cell>BiLSTM CRF</cell><cell cols="2">90.78</cell><cell>90.70</cell><cell cols="2">90.85 90.77</cell><cell>90.75</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>This research was supported by <rs type="funder">Government-wide R&amp;D Fund</rs> project for infectious disease research (GFID), <rs type="funder">Republic of Korea</rs> (grant number: <rs type="grantNumber">HG18C0093</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ezk7Yes">
					<idno type="grant-number">HG18C0093</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">The Chui&apos;s characters including: !&quot;$%&amp;&apos;()*+,-./0123456789:;=?@ABCDEFGHIJKLMNO PQRSTUVW XYZ</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Number of punctuation was found in CoNLL-2003 Location</title>
		<meeting><address><addrLine>Org</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">37701</biblScope>
		</imprint>
	</monogr>
	<note>Peson:16, Malicious:29, and Outside</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">The Characters Pattern including: !&quot;$%&amp;&apos;()*+,-./0123456789:;=?@DAa REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF Models for Sequence Tagging</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Characteraware neural language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Transactions of the Association for Computational Linguistics</title>
		<meeting>Transactions of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">15321543</biblScope>
		</imprint>
	</monogr>
	<note>in proceeding of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching Word Vectors with Subword Information, Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Named Entity Recognition Approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Affendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mamat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Network Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="344" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Malay Named Entity Recognition Based on Rule-Based Approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anthony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring word embedding for drug name recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Suarez-Paniagua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Workshop on Health Text Mining and Information Analysis (LOUHI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in proceeding of NAACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in proceeding of NAACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bio-NER: Biomedical Named Entity Recognition using Rule-Based and Statistical Learners</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banbhrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
