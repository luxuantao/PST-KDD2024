<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Deep Learning-based Fine-grained Object Classification and Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<postCode>613000</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117583</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117583</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<postCode>613000</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117583</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on Deep Learning-based Fine-grained Object Classification and Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE48B3467D56DB3F23051455A1ACEF41</idno>
					<idno type="DOI">10.1007/s11633-017-1053-3</idno>
					<note type="submission">received July 1, 2016; accepted September 30, 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>fine-grained image classification</term>
					<term>semantic segmentation</term>
					<term>convolutional neural network (CNN)</term>
					<term>recurrent neural network (RNN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deep learning technology has shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. In particular, recent advances of deep learning techniques bring encouraging performance to fine-grained image classification which aims to distinguish subordinate-level categories, such as bird species or dog breeds. This task is extremely challenging due to high intra-class and low inter-class variance. In this paper, we review four types of deep learning based fine-grained image classification approaches, including the general convolutional neural networks (CNNs), part detection based, ensemble of networks based and visual attention based fine-grained image classification approaches. Besides, the deep learning based semantic segmentation approaches are also covered in this paper. The region proposal based and fully convolutional networks based approaches for semantic segmentation are introduced respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has recently achieved superior performance on many tasks such as image classification, object detection and neural language processing. The core of the deep learning technology is that the layers of the features are not designed by human engineers and instead learned from data using a general-purpose learning procedure. There are a huge number of variants of the deep learning architecture. Most of them are branched from some original parent architectures. In this survey, we mainly focus on the convolutional neural network (CNN) and recurrent neural network (RNN) based approaches.</p><p>CNN is a type of feed-forward artificial neural network consisting of one or more convolutional layers which are then followed by one or more fully connected layers as in a standard MultiLayer perceptron (MLP). The convolutional layer is the core building block of a CNN. The layer s parameters comprise a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. CNN has wide applications in image classification, object detection and image retrieval systems. Fully convolutional network (FCN) is a special convolutional neural network which replaces all the fully connected layers in CNNs with convolutional layers. FCN can be trained end-to-end, pixels-to-pixels, which is very suitable for the task of semantic segmentation.</p><p>RNN is a kind of neural network where connections between units form a directed cycle, thus the activations can flow round in a loop. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition <ref type="bibr" target="#b0">[1]</ref> or speech recognition <ref type="bibr" target="#b1">[2]</ref> . One of the most popular RNNs is the long-short term memory (LSTM) <ref type="bibr" target="#b2">[3]</ref> which can remember a value for an arbitrary length of time. An LSTM unit contains multiple gates that determine when the input is significant enough to be remembered, when it should continue to remember or forget the value, and when it should output the value. Other RNN models include GNU <ref type="bibr" target="#b3">[4]</ref> , MGU <ref type="bibr" target="#b4">[5]</ref> .</p><p>Most deep learning networks can be trained end-to-end efficiently using backpropagation. It is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in order to minimize the loss function.</p><p>Different from backpropagation, reinforcement learning is another kind of technology that lets the networks learn what to do -how to map situations to actions -so as to maximize a numerical reward signal. The networks are not told which actions to take, as in most forms of deep learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.</p><p>In this survey, we introduce the deep learning based approaches using the backpropagation or reinforcement learning. More concretely, the deep learning based fine-grained object classification will be firstly elaborated and then the deep learning based image semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep fine-grained image classification</head><p>With the advancement of deep learning, fine-grained image classification received considerable attention. Many deep learning based approaches have been proposed in recent years. Fine-grained object classification aims to distinguish objects from different subordinate-level categories within a general category, e.g., different species of birds, dogs or different classes of cars. However, fine-grained classification is a very challenging task, because objects from similar subordinate categories may have marginal visual differences that are even difficult for humans to recognize. In addition, objects within the same subordinate category may present large appearance variations due to changes of scales or viewpoints, complex backgrounds and occlusions. Fig. <ref type="figure" target="#fig_0">1</ref> demonstrates three different species of gulls with high intraclass variance and small inter-class variance. Existing deep learning based fine-grained image classification approaches can be classified into the following four groups according to the use of additional information or human inference: 1) those approaches that directly use the general deep neural networks (mostly the CNNs) to classify the fine-grained images, 2) those using the deep neural networks as the feature extractor to better localize different parts of the fine-grained object and do alignment, 3) those using multiple deep neural networks to better differentiate highly visually-similar fine-grained images, and 4) those using the visual attention mechanism to find the most discriminative regions of the fine-grained images.</p><p>In this section, we will first introduce several convolutional neural networks which are mostly used for finegrained image classification. Then, part detection and alignment based approaches and ensemble of networks based approaches will be elaborated respectively. The last part of this section will review the attention based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General CNN for fine-grained image classification</head><p>CNN has a long history in computer vision. It was firstly introduced by LeCun et al. <ref type="bibr" target="#b5">[6]</ref> and has consistently been competitive with other methods for recognition tasks. Recently, with the advent of large-scale category-level training data, e.g., ImageNet <ref type="bibr" target="#b6">[7]</ref> , CNN exhibits superior performance in large-scale visual recognition. The impressive performance of CNN <ref type="bibr" target="#b7">[8]</ref> also motivates researchers to adapt CNNs pre-trained on ImageNet to other domains and datasets, such as the fine-grained image datasets. Besides, CNN usually is able to yield more discriminative representation of the image, which is essential for fine-grained image classification. Most of the current state-of-the-art CNNs can be adopted for fine-grained image classification.</p><p>AlexNet <ref type="bibr" target="#b5">[6]</ref> is a deep convolutional neural network which is a winner of the ILSVRC-2012 competition with top-5 test error rate of 15.3 %, compared to 26.2 % achieved by the second-best entry. It contains eight learnable layers. The first five are convolutional and the remaining three are fullyconnected. Fig. <ref type="figure">2</ref> illustrates the architecture of AlexNet.</p><p>Fig. <ref type="figure">2</ref> Framework of AlexNet. This figure is from the original paper <ref type="bibr" target="#b7">[8]</ref> .</p><p>The VGG net <ref type="bibr" target="#b8">[9]</ref> increases the depth of the neural networks, which not only achieves the state-of-the-art accuracy on ILSVRC classification and localization tasks, but also is applicable to other image recognition datasets. The VGG-16 has 13 convolutional layers with 3 fully connected layers, while the VGG-19 has 3 more convolutional layers than a VGG-16 model. They use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). All hidden layers are equipped with the rectification non-linearity.</p><p>The GoogLeNet <ref type="bibr" target="#b9">[10]</ref> sets the new state-of-the-art for classification and detection in the ImageNet large-scale visual recognition challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This is achieved by a carefully crafted design called "inception module" that allows for increasing the depth and width of the network while keeping the computational budget constant. GoogLeNet is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). By stacking the inception modules, it uses 12 times fewer parameters than the winning architecture of Krizhevsky et al. <ref type="bibr" target="#b7">[8]</ref> The inception module is depicted as Fig. <ref type="figure" target="#fig_1">3</ref>. The 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. Some other general deep convolutional feature extractors for image classification include CNN features off-theshelf <ref type="bibr" target="#b10">[11]</ref> , ONE <ref type="bibr" target="#b11">[12]</ref> and InterActive <ref type="bibr" target="#b12">[13]</ref> . When using these CNNs for fine-grained image classification, the last fullyconnected layer will be set as the class number of the finegrained images such as 200 for the CUB-Bird-2011 dataset. The classification results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part detection and alignment based approaches</head><p>Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Localizing the parts in an object is therefore important for establishing correspondence between object instances and discounting object pose variations and camera view position changes. Many traditional approaches follow the pipeline illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. The parts of the fine-grained object are first localized, such as head and torso for bird species classification, then the part alignment is done and the last is the classification using the feature extracted on the aligned parts. POOF <ref type="bibr" target="#b13">[14]</ref> learns a set of intermediate features using data mining techniques. Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. To find accurate part such as face and eyes of dogs, Liu et al. <ref type="bibr" target="#b14">[15]</ref> build exemplar-based geometric and appearance models of dog breeds and their face parts. Yang et al. <ref type="bibr" target="#b15">[16]</ref> propose a template model to discover the common geometric patterns of object parts and the co-occurrence statistics of the patterns. Features are extracted within the aligned cooccurred patterns for finegrained recognition. Similarly, Gavves et al. <ref type="bibr" target="#b16">[17]</ref> and Chai et al. <ref type="bibr" target="#b17">[18]</ref> segment images and align the image segments in an unsupervised fashion. The alignments are then used to transfer part annotations from training images to test images and extract features for classification. In this subsection, we will introduce the part detection methods based on deep learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Part-based R-CNN</head><p>With deep convolutional features, the part detector which is widely used in many approaches improves its performance. Therefore, the Part-based R-CNN <ref type="bibr" target="#b18">[19]</ref> learns the part detectors by leveraging deep convolutional features computed on bottom-up region proposals. It extends R-CNN <ref type="bibr" target="#b19">[20]</ref> to detect objects and localize their parts under a geometric prior. The whole process is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>. Starting from several region proposals using selective search <ref type="bibr" target="#b20">[21]</ref> , both object and part detectors are trained based on the deep convolutional features. During testing, all proposals are scored by all detectors, and non-parametric geometric constraints are applied to rescore the proposals and choose the best object and part detections. The final step is to extract features on the localized semantic parts for fine-grained recognition for a pose-normalized representation and then train a classifier for the final categorization. In order to make the deep CNN derived features more discriminative for the target task of fine-grained bird classification, ImageNet pre-trained CNN is first fine-tuned for the 200-way bird classification task from ground truth bounding box crops of the original CUB images. In particular, the original 1 000-way fc8 classification layer in CaffeNet, which is almost identical as the AlexNet, is replaced with a new 200-way fc8 layer. Both the full objects bounding box annotations and a fixed set of semantic parts annotations are used to train multiple detectors. All objects and each of their parts are initially treated as independent object categories: A one-versus-all linear SVM is trained on the convolutional feature descriptors extracted over region proposals. Because the individual part detectors are less than perfect, the window with highest individual part detector scores is not always correct, especially when there are occlusions. A geometric constraint over the layout of the parts relative to the object location is considered to filter out incorrect detections.</p><p>In testing, the bottom-up region proposals are scored by all detectors, and the non-parametric geometric constraints are imposed to rescore the windows and choose the best object and part detections. At the final step, features for the predicted whole object or part region are extracted and concatenated using the network fine-tuned for that particular whole object or part. Then, a one-versus-all linear SVM classifier is trained using the final feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Part localization using multi-proposal consensus</head><p>Different from the part-based R-CNN, which uses the geometric constraint to better locate the parts, multi-proposal consensus <ref type="bibr" target="#b21">[22]</ref> predicts the keypoint and region (head, torso, body) using a single neural network based on the AlexNet <ref type="bibr" target="#b7">[8]</ref> architecture.</p><p>The multi-proposal consensus modifies the AlexNet to simultaneously predict all keypoint locations and their visibilities for any given image patch. The final fc8 layer is replaced with two separate output layers for keypoint localization and visibility, respectively. The network is trained on edge box crops <ref type="bibr" target="#b22">[23]</ref> extracted from each image and is initialized with a pre-trained AlexNet trained on the ImageNet <ref type="bibr" target="#b6">[7]</ref> dataset. After getting the keypoint predictions and their visibilities, the ones with low visibility confidences will be removed. The remaining predictions will have a peaky distribution around the ground truth. Therefore, medoid is used as a robust estimator for this peak. Fig. <ref type="figure">6</ref> demonstrates the process of finding the right eye keypoint. The best location of the right eye is determined by performing confidence thresholding and finding the medoid. Black edge boxes without associated dots make predictions with confidences below the set threshold, and green denotes an outlier with a high confidence score.</p><p>Fig. <ref type="figure">6</ref> Part localization using multi-proposal consensus. This figure is from the original paper <ref type="bibr" target="#b21">[22]</ref> .</p><p>Using the keypoints, three regions are identified from each bird: head, torso, and whole body. The head is defined as the tightest box surrounding the beak, crown, forehead, eyes, nape, and throat. Similarly, the torso is the box around the back, breast, wings, tail, throat, belly, and legs. The whole body bounding box is the object bounding box provided in the annotations. To perform classification, fc6 features of AlexNet are extracted from these localized regions. These CNN features are then concatenated into a feature vector of length 4 096 × 3, and used for 200-way linear one-vs-all SVM classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Pose normalized nets</head><p>The pose normalized net <ref type="bibr" target="#b23">[24]</ref> first computes an estimate of the object s pose which is used to compute local image features. These local features in turn are used for classification. The features are computed by applying deep convolutional networks to image patches that are located and normalized by the pose. The pose normalized net integrates lower-level feature layers (conv5, fc6) with pose-normalized extraction routines and higher-level feature layers (fc8) with unaligned image features as shown in Fig. <ref type="figure">7</ref>.</p><p>In training, the pose normalized net uses the DPM <ref type="bibr" target="#b24">[25]</ref> to predict 2D locations and the visibility of 13 semantic part keypoints or directly uses the pre-provided object bounding box and part annotations to learn the pose prototypes. Then, different parts of the object will be warped and fed Fig. <ref type="figure">7</ref> Pose normalized nets. This figure is from the original paper [24].</p><p>into different deep neural networks (AlexNet) to extract the features. Finally, the classifier is trained with the concatenated convolutional features extracted from each prototype region and the entire image.</p><p>In testing, given a test image, groups of detected keypoints or the oracle parts annotations are used to compute multiple warped image regions that are aligned with prototypical models. Multiple warped image regions are then aligned with prototypical models. Then, the pose-warped image regions are each fed into a feature extractor, which is a deep convolutional neural network <ref type="bibr" target="#b7">[8]</ref> . It is proved that a model that integrates lower-level feature layers with posenormalized extraction routines and higher-level feature layers with unaligned image features works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Part-stack CNN</head><p>Based on manually-labeled strong part annotations, the part-stacked CNN (PS-CNN) model <ref type="bibr" target="#b25">[26]</ref> consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that encodes object-level and part-level cues simultaneously, as shown in Fig. <ref type="figure" target="#fig_5">8</ref>.</p><p>An FCN is achieved by replacing the parameter-rich fully connected layers in standard CNN architectures by convolutional layers with 1 × 1 kernels. Given an input RGB image, the output of an FCN is a feature map in the reduced dimension compared to the input. The computation of each unit in the feature map only corresponds to pixels inside a region with fixed size in the input image, which is called its receptive field. FCN is preferred in PS-CNN due to the following three reasons: 1) Feature maps generated by FCN can be directly utilized as the part localization results in the classification network. 2) Results of multiple object parts can be obtained simultaneously using an FCN. 3) FCN is very efficient in both learning and inference.</p><p>Using M keypoints annotated at the center of each ob-ject part, the localization network, which is a fully convolutional network <ref type="bibr" target="#b26">[27]</ref> , is trained to generate dense output feature maps for locating object parts. A Gaussian kernel is used to remove isolated noise in the feature maps. The final output of the localization network is M locations in the conv5 feature map, each of which is computed as the location with the maximum response for one object part. Then, the part locations are fed into the classification network, in which a two-level architecture is adopted to analyze images at both object-level (bounding boxes) and part-level (part landmarks). At the part level, the computation of multiple parts is first conducted via a shared feature extraction route, and then separated through a part crop layer. The input for the part crop layer is a set of feature maps, e.g., the output of the conv5 layer, and the predicted part locations from the previous localization network, which also reside in conv5 feature maps. For each part, the part crop layer extracts a local neighborhood region centered at the detected part location. At the object level, bounding-box supervision is used to extract object-level CNN features, i.e., pool5 features. Three fully connected layers achieve the final classification results based on a concatenated feature map containing information from all parts and the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Deep LAC</head><p>The deep LAC <ref type="bibr" target="#b27">[28]</ref> incorporates part localization, alignment, and classification in one deep neural network. Its framework is demonstrated in Fig. <ref type="figure">9</ref>  Fig. <ref type="figure">9</ref> Framework of deep LAC. This figure is from the original paper <ref type="bibr" target="#b27">[28]</ref> .</p><p>The part localization sub-network consists of 5 convolutional layers and 3 fully connected ones. It outputs the commonly used coordinates for the top-left and bottomright bounding-box corners, given an input natural image for fine-grained recognition. In the training phase, deep LAC regresses bounding boxes of part regions. Ground truth bounding boxes are generated with part annotations.</p><p>The alignment sub-network receives part locations (i.e., bounding box) from the localization sub-network, performs template alignment <ref type="bibr" target="#b28">[29]</ref> and feeds a pose-aligned part image to classification. The alignment sub-network offsets translation, scaling, and rotation for pose-aligned part region generation, which is important for accurate classification. Apart from pose aligning, this sub-network plays a crucial role in bridging the backward-propagation (BP) stage of the whole LAC model, which helps utilize the classification and alignment results to refine localization.</p><p>In the deep LAC framework, the VLF in the alignment sub-network is the most essential part which optimally connects the localization and classification modules. It not only connects all sub-networks, but also functions as information valve to compromise classification and alignment errors. If the alignment is good enough in the forward propagation stage, VLF guarantees corresponding accurate classification. Otherwise, errors propagated from classification finely tune the previous modules. These effects make the whole network reach a stable state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ensemble of networks based approaches</head><p>Dividing the fine-grained dataset into multiple visually similar subsets or directly using multiple neural networks to improve the performance of classification is another widely used method in many deep learning based fine-grained image classification systems. We will introduce these methods in this subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Subset feature learning networks</head><p>As shown in Fig. <ref type="figure" target="#fig_6">10</ref>, the subset feature learning networks <ref type="bibr" target="#b29">[30]</ref> consist of two main parts: a domain-generic convolution neural network and several specific convolutional neural networks. The domain-generic convolution neural network is first pre-trained on a large-scale dataset of the same domain as the target dataset and then finetuned on the target dataset. Using the fc6 feature with linear discriminant analysis (LDA) to reduce its dimensionality, visually similar species are clustered into K subsets to train multiple specific CNNs in the second part. A separate CNN is learned for each of the K pre-clustered subsets. The aim is to learn features for each subset which can more easily differentiate visually similar species. The fc6 feature of each individual CNNs is used as the learned subset feature for each subset. Each specific convolutional neural networks is suitable for a subset of the images. Therefore, how to choose the best specific convolutional neural networks for an image is the core problem of the subset feature learning networks.</p><p>A subset selector CNN (SCNN) is utilized to select the most relevant CNNs to make prediction for a given image. Using the output from the pre-clustering as the class labels, SCNN is trained by changing the softmax layer fc8 to make it have K outputs. The softmax layer predicts the probability of the test image belonging to a specific subset, and then max voting is applied to this prediction to choose the most likely subset. As with the previously trained CNNs, the weights of SCNN are trained via backpropagation and stochastic gradient descent (SGD) using the AlexNet <ref type="bibr" target="#b7">[8]</ref> as the starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Mixture of deep CNN</head><p>Similar to subset feature learning networks, the MixDCNN <ref type="bibr" target="#b30">[31]</ref> system will also learn K specific CNNs. However, it does not require pre-dividing images into K subsets of similar images. The image will be fed into all the K CNNs and the outputs from each CNN are combined to form a single classification decision. In contrast to subset feature learning networks, MixDCNN adopts the occupation probabilities equation to perform joint end-to-end training of the K CNNs simultaneously.</p><p>The occupation probability is defined as</p><formula xml:id="formula_0">α k = e C k K c=1 e Cc (1)</formula><p>where C k is the best classification result for the k-th CNN. The occupation probability gives a higher weight to components that are confident about their prediction. The overall structure of this network is shown in Fig. <ref type="figure" target="#fig_7">11</ref>.</p><p>The occupation probability of each subset is based on the classification confidence from each component, which makes it possible to jointly train the K DCNNs (components) without having to estimate a separate label vector y or train a separate gating network as in subset feature learning networks. Classification is performed by multiply-ing the output of the final layer from each component by the occupation probability and then summing over the K components. This mixes the network outputs together and the probability for each class is then produced by applying the softmax function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">CNN tree</head><p>Motivated by the observation that a class is usually confused by a few number of other classes, which are called the confusion set, in multi-class classification, more discriminative features could be learned by a specific CNN to distinguish the classes only in this set. Based on this observation, CNN tree <ref type="bibr" target="#b31">[32]</ref> is used to progressively learn fine-grained features for different confusion sets.</p><p>Given a node of the tree, a CNN model is first trained on its class set. Next, the confusion set of each class is estimated by using the trained model. Then, these confusion sets are packed into several confusion supersets and each of them is assigned to a new child node for further learning. This procedure repeats until it reaches the maximal depth. The tree structure is shown in Fig. <ref type="figure" target="#fig_8">12</ref>. The CNN tree progressively learns fine-grained features to distinguish a subset of classes, by learning features only among these classes. Such features are expected to be more discriminative, compared to features learned for all the classes. Besides, test images that are misclassified by the root CNN model might be the correctly classified by its descendent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Multiple granularity CNN</head><p>A core observation is that a subordinate-level label carries an implied hierarchy of labels, each corresponding to a level in the domain ontology. For instance, melanerpes formicivorus, also known as acorn woodpecker, can also be called melanerpes at genus level, or picidae at family level. These labels are free for extracting their corresponding discriminative patches and features. These free labels can be used to train a series of CNN-based classifiers, each specialized at one grain level. The internal representations of these networks have different regions of interest, allowing the construction of multi-grained descriptors that encode informative and discriminative features covering all the grain levels.</p><p>Based on this idea, the multiple granularity CNN <ref type="bibr" target="#b32">[33]</ref> contains a parallel set of deep convolutional neural networks as shown in Fig. <ref type="figure" target="#fig_9">13</ref>, each optimized to classify at a given granularity. In other words, the multiple granularity CNN is composed of a set of single-grained descriptors. Saliency in their hidden layers guides the selection of regions of interest (ROI) from a common pool of bottom-up proposed image patches. ROI selection is therefore by definition granularity-dependent, in the sense that selected patches are results of the associated classifier of a given granularity. Meanwhile, ROI selections are also cross-granularity dependent: The ROIs of a more detailed granularity are typically sampled from those at the coarser granularities. Finally, per-granularity ROIs are fed into the second stage of the framework to extract per-granularity descriptors, which are then merged to give classification results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Bilinear deep network models</head><p>The bilinear models <ref type="bibr" target="#b33">[34]</ref> are a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture, as shown in Fig. <ref type="figure" target="#fig_2">14</ref>, can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization.</p><p>A bilinear model for image classification consists of a quadruple B = (fA, fB, P, C). Here fA and fB are feature functions, P is a pooling function and C is a classification function. A feature function is a mapping f : L×I → R c×D that takes as input an image I and a location L and outputs a feature of size c × D. The locations generally can include position and scale. The feature outputs are combined at each location using the matrix outer product, i.e., the bilinear feature combination of fA and fB at a location l is given by bilinear (l, I, fA, fB) = fA(l, I) fB(l, I). Both fA and fB must have the feature dimension c to be compatible. To obtain an image descriptor, the pooling function P aggregates the bilinear features across all locations in the image. One choice of pooling is to simply sum all the bilinear features, i.e., φ(I) = l∈L bilinear (l, I, fA, fB). An alternative is max-pooling. Both ignore the locations of the features and are hence orderless. If fA and fB extract features of size C × M and C × N respectively, then φ(I) is of size M ×N . The bilinear vector obtained by reshaping φ(I) to size MN × 1 is a general-purpose image descriptor that can be used with a classification function C. Intuitively, the bilinear form allows the outputs of the feature extractors fA and fB to be conditioned on each other by considering all their pairwise interactions similar to a quadratic kernel expansion.</p><p>Fig. <ref type="figure" target="#fig_2">14</ref> Framework of bilinear CNN model. This figure is from the original paper [34].</p><p>A natural candidate for the feature function f is a CNN consisting of a hierarchy of convolutional and pooling layers. In this paper, the authors use two different CNNs pretrained on the ImageNet dataset <ref type="bibr" target="#b6">[7]</ref> truncated at a convolutional layer including non-linearities as feature extractors. By pre-training, bilinear deep network model will benefit from additional training data in the cases of domain specific data scarcity. This has been shown to be beneficial for a number of recognition tasks ranging from object detection, texture recognition, to fine-grained classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> . Another advantage of using only the convolutional layers is that the resulting CNN can process images of an arbitrary size in a single forward-propagation step and produce outputs indexed by the location in the image and the feature channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visual attention based approaches</head><p>One of the most curious facets of the human visual system is the presence of attention. Rather than compressing Fig. <ref type="figure" target="#fig_3">15</ref> Framework of two-level attention. This figure is from the original paper [38].</p><p>an entire image into a static representation, the attention system allows for salient features to dynamically come to the forefront as needed. This is especially important when there are many clutters in an image. A visual attention mechanism is also used in many fine-grained image classification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Two-level attention</head><p>The two-level attention model <ref type="bibr" target="#b37">[38]</ref> , illustrated in Fig. <ref type="figure" target="#fig_3">15</ref> integrates three types of attention: the bottom-up attention that proposes candidate patches, the object-level topdown attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. These attention types are combined to train domain-specific deep nets, and then used to find foreground object or object parts to extract discriminative features. The model is easy to generalize, since it does not require the object bounding box or part annotation.</p><p>Then, a DomainNet is trained with the patches selected by the FilterNet. Essentially, spectral clustering is performed on the similarity matrix S to partition the filters in a middle layer into k groups. Each cluster acts as a part detector. The patches selected by the part detector are then wrapped back to the input size of DomainNet to generate activations. The activations of different parts and the original image are concatenated and used to train an SVM as the part-based classifier. Finally, the prediction results of the object-level attention and the part-level attention are merged to utilize the advantage of the two level attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Attention for fine-grained categorization</head><p>Inspired from the way how humans perform visual sequence recognition, such as reading by continually moving the fovea to the next relevant object or character, recognizing the individual object, and adding the recognized object to the internal representation of the sequence, the attention for fine-grained categorization (AFGC) system is proposed <ref type="bibr" target="#b38">[39]</ref> . It is a deep recurrent neural network that at each step, it processes a multi-resolution crop of the input image, called a glimpse. The network uses information from the glimpse to update its internal representation of the input, and outputs the next glimpse location and possibly the next object in the sequence.</p><p>Fig. <ref type="figure" target="#fig_10">16</ref> shows the framework of the attention module in AFGC. It uses an RNN and a powerful visual network (GoogLeNet) to perform fine-grained classification. The system as a whole takes as input an image of any size and outputs N -way classification scores using a softmax classifier, which is a task similar to find digits and digit addition <ref type="bibr" target="#b39">[40]</ref> . The model is a recurrent neural network, with N steps that correlate with N "glimpses" into the input image. At step n, the model receives row and column coordinates ln, which describe a point in the input image. The network extracts a multi-resolution patch from the input image at those coordinates, passes the pixels through fullyconnected layers which combine with activations from the previous glimpse step, and either outputs coordinates ln for the next glimpse or a final classification ys. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">FCN attention</head><p>FCN attention <ref type="bibr" target="#b40">[41]</ref> is a reinforcement learning-based fully convolutional attention localization network to adaptively select multiple task-driven visual attention regions. Compared to previous reinforcement learning-based models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> , the proposed approach is noticeably more computationally efficient during both training and testing because of its fully-convolutional architecture, and it is capable of simultaneous focusing its glimpse on multiple visual attention regions. The part-localization component uses a fullyconvolutional neural network to locate part locations. Given an input image, the basis convolutional feature maps are extracted using the VGG 16 model <ref type="bibr" target="#b8">[9]</ref> pre-trained on Im-ageNet dataset <ref type="bibr" target="#b6">[7]</ref> and fine-tuned for the target fine-grained dataset. The attention localization network localizes multiple parts by generating a score map for each part using the basis convolutional feature map. Each score map is generated using two stacked convolutional layers and one spatial softmax layer. The first convolutional layer uses sixty-four 3 × 3 kernels, and the second one uses one 3 × 3 kernel to output a single-channel confidence map. The spatial softmax layer is applied to the confidence map to convert the confidence score into probability. The attention region with highest probability is selected as the part location. The same process is applied for a fixed number of time steps for multiple part locations. Each time step generates the location for a particular part.</p><p>The classification component contains one deep CNN classifier for each part as well as the whole image. Different parts might have different sizes, and a local image region is cropped around each part location according to its size. An image classifier for each local image region is trained as well as the whole image separately. The final classification result is the average of all the classification results from the individual classifiers. In order to discriminate the subtle visual differences, each local image region is resized to high resolution. A deep convolutional neural network is trained for each part for classification separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Diversified visual attention</head><p>A diversified visual attention network (DVAN) <ref type="bibr" target="#b42">[43]</ref> is proposed to pursue the diversity of attention and is able to gather discriminative information to the maximal extent. The architecture of the proposed DVAN model is described in Fig. <ref type="figure" target="#fig_14">18</ref>, which includes four components: attention canvas generation, CNN feature learning, diversified visual attention and classification. DVAN first localizes several regions of the input image at different scales and takes them as the "canvas" for following visual attention. A convolutional neural network (i.e., VGG-16) is then adopted to learn convolutional features from each canvas of attention. To localize important parts or components of the object within each canvas, a diversified visual attention component is introduced to predict the attention maps, so that important locations within each canvas are highlighted and information gain across multiple attention canvases is maximized. Different from traditional attention models focusing on a single discriminative location, DVAN jointly identifies diverse locations with the help of a well designed diversity promoting loss function. According to the generated attention maps, the convolutional features will be dynamically pooled and accumulated into the diversified attention model. Meanwhile, the attention model will predict the object class at each time step. All the predictions will be averaged to obtain the final classification results.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance comparison and analysis</head><p>We list the classification accuracy of CUB200-2011 dataset <ref type="bibr" target="#b43">[44]</ref> using the above mentioned deep learning approaches in Table <ref type="table" target="#tab_0">1</ref>. The classification accuracy is defined as the average of class classification accuracy. CUB-200-2011 dataset consists of 11 778 images from 200 bird categories. It provides rich annotations, including image-level labels, object bounding boxes, attribute annotations and part landmarks. There are 5 994 images for training and 5 794 images for testing. Note that some approaches are omitted since they did not report the result on this dataset.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, it can be seen that the approaches are grouped into three groups. The approaches in first group are based on the part detection and alignment and the approaches in second group ensemble multiple neural networks to boost the classification performance. While the visual attention based models in the third group simulate the observation process of human beings and usually do not need the bounding box information or part annotation. These approaches are based on different neural networks such as AlexNet <ref type="bibr" target="#b7">[8]</ref> , VGGNet <ref type="bibr" target="#b8">[9]</ref> or GoogLeNet <ref type="bibr" target="#b9">[10]</ref> . Some approaches may use the bounding box and part annotations in training and testing, while some only use the category label to train the networks.</p><p>In general, part localization-based fine-grained recognition algorithms can localize important regions using a set of predefined parts. However, detailed part annotations are usually difficult to obtain. Recent fine-grained object classification methods are capable of learning discriminative region localizers only from category labels with reinforcement learning. However, they cannot accurately find multiple distinctive regions without utilizing any explicit part information. Comparing to the labor and time consuming part annotation for fine-grained object classification, the attribute labeling is more amenable. Therefore, attribute label information can be used as a weak supervision to the part localization to further improve the classification accuracy.</p><p>Meanwhile, the recurrent visual attention models are effective in localizing the parts and learn their discriminative representations in an end-to-end way. Many recurrent visual attention models are proposed in recent years. Existing visual attention models can be classified as soft or hard attention. Soft attention models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> predict the attention regions in a deterministic way. As a consequence, it is differentiable and can be trained using back-propagation. Hard attention models <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b46">47]</ref> predict the attention points of an image, which are stochastic. They are usually trained by reinforcement learning <ref type="bibr" target="#b47">[48]</ref> or maximizing an approximate variational lower bound. In general, soft attention models are more efficient than hard attention models, since hard attention models require sampling for training while soft attention models can be trained end-to-end. However, the visual attention models also suffer from several drawbacks in practice. Firstly, by far the soft attention models only result in small performance improvement. More powerful visual attention model is expected to improve the classification accuracy. Secondly, the hard attention methods using reinforcement learning techniques usually are not as efficient as the soft attention methods. Methods which can improve the efficiency of the hard attention model should be explored further. Part-based R-CNN <ref type="bibr" target="#b18">[19]</ref> AlexNet BBox + Parts BBox 76.4</p><p>Part-based R-CNN <ref type="bibr" target="#b18">[19]</ref> AlexNet BBox + Parts -73.9</p><p>Multi-proposal consensus <ref type="bibr" target="#b21">[22]</ref> AlexNet BBox BBox 80.3</p><p>PoseNorm <ref type="bibr" target="#b23">[24]</ref> Alexnet BBox + Parts -75.7</p><p>PS-CNN <ref type="bibr" target="#b25">[26]</ref> AlexNet BBox + Parts BBox 76.2</p><p>Deep LAC <ref type="bibr" target="#b27">[28]</ref> AlexNet BBox BBox 80.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble of networks based approaches</head><p>Subset FL <ref type="bibr" target="#b29">[30]</ref> AlexNet --77.5</p><p>MixDCNN <ref type="bibr" target="#b30">[31]</ref> AlexNet BBox BBox 74.1</p><p>Multiple granularity CNN <ref type="bibr" target="#b32">[33]</ref> VGGNet BBox -83.0</p><p>Multiple granularity CNN <ref type="bibr" target="#b32">[33]</ref> VGGNet --81.7</p><p>Bilinear CNN <ref type="bibr" target="#b33">[34]</ref> VGGNet BBox BBox 77.2</p><p>Bilinear CNN <ref type="bibr" target="#b33">[34]</ref> VGGNet --72.5</p><p>Visual attention based approaches Two-level attention <ref type="bibr" target="#b37">[38]</ref> AlexNet --69.7</p><p>FCN attention <ref type="bibr" target="#b40">[41]</ref> GoogLeNet BBox -84.3</p><p>FCN attention <ref type="bibr" target="#b40">[41]</ref> GoogLeNet --82.0 DVAN <ref type="bibr" target="#b42">[43]</ref> VGGNet --79.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep image semantic segmentation</head><p>Deep learning based image semantic segmentation aims to predict a category label for every image pixel, which is an important yet challenging task for image understanding. Recent approaches have applied convolutional neural network (CNNs) <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref> to this pixel-level labeling task and achieved remarkable success. A number of these CNN-based methods for segmentation are region-proposalbased methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b51">52]</ref> , which first generate region proposals and then assign category labels to each. Very recently, FCN <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> has become a popular choice for semantic segmentation, because of its effective feature generation and end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region proposal based approaches</head><p>In R-CNN <ref type="bibr" target="#b19">[20]</ref> , semantic image segmentation is performed based on the object detection results. The detection system is demonstrated in Fig. <ref type="figure" target="#fig_16">20</ref>. Taking an input image, selective search <ref type="bibr" target="#b20">[21]</ref> is used to extract around 2 000 bottom-up region proposals. A convolutional neural network takes the affine warped regions as the input to generate a fixed-size CNN feature, regardless of the region s shape. Then, several class-specific linear SVMs are used to classify different regions. Finally, the category-specific mask of the surviving candidate is predicted using the features from the CNN. Similar to R-CNN, simultaneous detection and segmentation (SDS) <ref type="bibr" target="#b51">[52]</ref> starts the semantic segmentation with category-independent bottom-up object proposals. The framework of SDS is shown in Fig. <ref type="figure" target="#fig_17">21</ref>. Multisale combinatorial grouping (MCG) <ref type="bibr" target="#b53">[54]</ref> is chosen in the paper to generate 2 000 region candidates per image. Two CNNs (bBox CNN and region CNN) are trained to extract the features from the bounding box of the region and the cropped, warped region with the background of the region masked out (with the mean image). Compared to using the same CNN for both inputs (image windows and region masks), using separate networks where each network is fine tuned for its respective role dramatically improves the performance. This figure is from the original paper [52].</p><p>A region classifier is then trained using the CNN features to assign a score for each category to each candidate. To generate the final semantic segmentation, SDS first learns to predict a coarse, top-down figure-ground mask for each region. The final segmentation is generated by projecting the coarse mask to superpixels by assigning to each superpixel the average value of the coarse mask in the superpixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FCN based approaches</head><p>DAG <ref type="bibr" target="#b50">[51]</ref> is a fully convolutional network (FCN) trained end-to-end, pixels-to-pixels on semantic segmentation. Fig. <ref type="figure" target="#fig_18">22</ref> demonstrates framework of DAG net. It transforms the fully connected layers into convolutional layers and enables a classification net to output a heatmap. A spatial loss is used to train the FCN end-to-end efficiently. This approach does not make use of pre-processing and post-processing complications, including superpixels <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52]</ref> , proposals <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55]</ref> , or post-hoc refinement by random fields or local classifiers <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55]</ref> .</p><p>The deconvolution networks as shown in Fig. <ref type="figure" target="#fig_19">23</ref> are composed of two parts: convolution and deconvolution networks. The convolution network corresponds to the feature extractor that transforms the input image to multidimensional feature representation, whereas the deconvolution network is a shape generator that produces object segmentation from the features extracted from the convolution network. The final output of the network is a probability map with the same size as the input image, indicating the probability of each pixel that belongs to one of the predefined classes.</p><p>VGG 16-layer net <ref type="bibr" target="#b8">[9]</ref> is employed for the convolutional part with its last classification layer removed. The deconvolution network has 13 convolutional layers altogether, where rectification and pooling operations are sometimes performed between convolutions, and 2 fully connected layers are augmented at the end to impose class-specific projection. The deconvolution network is a mirrored version of the convolution network, and has multiple series of unpooling, deconvolution, and rectification layers. In contrary to the convolution network that reduces the size of activa-tions through feed-forwarding, deconvolution network enlarges the activations through the combination of unpooling and deconvolution operations. Finally, the dense pixel-wise class prediction map is constructed through multiple series of unpooling, deconvolution and rectification operations.</p><p>DeepLab <ref type="bibr" target="#b56">[57]</ref> employed the convolution with upsampled filters or "atrous convolution" as a powerful tool for image segmentation. Atrous convolution explicitly controls the resolution at which feature responses are computed within deep convolutional neural networks. It also effectively enlarges the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. The atrous spatial pyramid pooling (ASPP) is used to segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-ofviews, thus capturing objects as well as image context at multiple scales. Another contribution of DeepLab is that it improves the localization of object boundaries by combining methods from DCNNs and a fully connected conditional random field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. The framework of DeepLab model is illustrated as Fig. <ref type="figure" target="#fig_20">24</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The paper surveys some recent progress in deep learning based fine-grained image classification and semantic segmentation. Several general convolutional neural networks are first introduced including the AlexNet, VGG net and GoogLeNet. They can be directly adapted to find-grained image classification. Since the subtle differences of visually similar fine-grained objects usually exist in some common parts, many approaches resort to deep learning technology to boost the performance of part localization, while some approaches integrate the part localization into the deep learning framework and can be trained end-to-end. Some fine-grained classification approaches also combine multiple neural networks to gain more classification capability for fine-grained images. By integrating the attention mechanism, some visual attention based approaches can automatically localize the most discriminative regions of the fine-grained images without using any bounding box or part  annotation. The approaches for possible growth of the finegrained image classification include: 1) Using deeper neural networks to boost the performance. Residual Networks are one of the recently proposed deep neural networks with a depth of up to 152 layers which is 8 times deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 % error on the ImageNet test set. Using these deeper networks as the feature extractor or base networks will certainly improve the accuracy. 2) Using reinforcement learning technique <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b57">58]</ref> to learn task-specific policies will also benefit the fine-grained object classification, because they can learn the part localization and discriminative representation in an end-to-end way, and they do not require manually labeled object. For the semantic segmentation, region proposal based approaches and FCN based approaches are introduced respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Two species of gulls from CUB 200 dataset illustrate the difficulty of fine-grained object classification: large intra-class variance and small inter-class variance. The pose, background and viewpoint of the gull within the same species vary largely, and different specie of gulls display high visual similarity. The discriminative differences only exist in some subtle regions, e.g., the beak or wings.</figDesc><graphic coords="2,57.02,406.95,226.80,140.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Inception module of GoogLeNet. This figure is from the original paper [10].</figDesc><graphic coords="3,53.54,258.99,234.48,120.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 General framework for part detection and alignment based approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Part-based R-CNN for fine-grained image classification. This figure is from the original paper [19].</figDesc><graphic coords="3,311.42,549.39,236.88,141.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. A valve linkage function (VLF) is proposed for back-propagation chaining, and to form the deep localization, alignment and classification (LAC) system. The VLF can adaptively compromise the errors of classification and alignment when training the LAC model. It in turn helps update localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Part-stack CNN. This figure is from the original paper [26].</figDesc><graphic coords="5,103.22,511.39,396.96,211.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Framework of subset feature learning networks. This figure is from the original paper [30].</figDesc><graphic coords="6,316.34,501.75,226.80,129.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 Framework of MixDCNN. This figure is from the original paper [31].</figDesc><graphic coords="7,314.42,162.39,231.12,78.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12</head><label>12</label><figDesc>Fig.12Framework of CNN tree. This figure is from the original paper[32].</figDesc><graphic coords="7,110.42,539.62,382.41,180.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13</head><label>13</label><figDesc>Fig. 13 Framework of Multiple Granularity NN. This figure is from the original paper [33].</figDesc><graphic coords="8,54.02,453.79,232.56,141.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16</head><label>16</label><figDesc>Fig.16 Framework of attention for fine-grained categorization. This figure is from the original paper[39].</figDesc><graphic coords="9,312.38,480.55,234.96,118.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17 illustrates the architecture of the fully convolutional attention localization network. It can localize multiple object parts using the attention mechanism. Different parts can have different pre-defined sizes. The network contains two components: part localization component and classification component.The part-localization component uses a fullyconvolutional neural network to locate part locations. Given an input image, the basis convolutional feature maps are extracted using the VGG 16 model<ref type="bibr" target="#b8">[9]</ref> pre-trained on Im-ageNet dataset<ref type="bibr" target="#b6">[7]</ref> and fine-tuned for the target fine-grained dataset. The attention localization network localizes multiple parts by generating a score map for each part using the basis convolutional feature map. Each score map is generated using two stacked convolutional layers and one spatial softmax layer. The first convolutional layer uses sixty-four 3 × 3 kernels, and the second one uses one 3 × 3 kernel to output a single-channel confidence map. The spatial softmax layer is applied to the confidence map to convert the confidence score into probability. The attention region with highest probability is selected as the part location. The same process is applied for a fixed number of time steps for multiple part locations. Each time step generates the location for a particular part.The classification component contains one deep CNN classifier for each part as well as the whole image. Different parts might have different sizes, and a local image region is cropped around each part location according to its size. An image classifier for each local image region is trained as well as the whole image separately. The final classification result is the average of all the classification results from the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17Framework of FCN attention. This figure is from the original paper [41].</figDesc><graphic coords="10,103.22,475.75,396.96,219.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Fig. 17Framework of FCN attention. This figure is from the original paper [41].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18</head><label>18</label><figDesc>Fig. 18 Framework of diversified visual attention networks. This figure is from the original paper [43].</figDesc><graphic coords="11,103.22,94.99,396.96,127.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 DVAN</head><label>19</label><figDesc>Fig. 19 DVAN attention component. This figure is from the original paper[43].</figDesc><graphic coords="11,71.06,331.91,199.44,133.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 20</head><label>20</label><figDesc>Fig. 20 Framework of region CNN. This figure is from the original paper [20].</figDesc><graphic coords="12,47.54,668.67,245.76,66.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 21</head><label>21</label><figDesc>Fig. 21 Framework of simultaneous detection and segmentation.</figDesc><graphic coords="12,311.90,508.03,236.16,69.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 22</head><label>22</label><figDesc>Fig. 22 Framework of DAG nets. This figure is from the original paper [51].</figDesc><graphic coords="13,53.54,140.79,233.76,122.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 23</head><label>23</label><figDesc>Fig. 23 Framework of deconvolution networks. This figure is from the original paper [56].</figDesc><graphic coords="13,103.22,617.39,396.96,100.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 24</head><label>24</label><figDesc>Fig. 24 Framework of DeepLab. This figure is from the original paper [57].</figDesc><graphic coords="14,53.54,95.35,234.24,105.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Performance comparison with different approaches</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell>Train annotation</cell><cell>Test annotation</cell><cell>Accuracy</cell></row><row><cell></cell><cell cols="2">Part detection and alignment based approaches</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China (Nos. 61373121 and 61328205), Program for Sichuan Provincial Science Fund for Distinguished Young Scholars (No. 13QNJJ0149), the Fundamental Research Funds for the Central Universities, and China Scholarship Council (No. 201507000032). Recommended by Associate Editor Nazim Mir-Nasiri c Institute of Automation, Chinese Academy of Sciences and Springer-Verlag Berlin Heidelberg 2017 Bo Zhao received the B. Sc. degree in networking engineering from Southwest Jiaotong University in 2010. He is a Ph. D. degree candidate at School of Information Science and Technology, Southwest Jiaotong University, China. Currently, he is at the Department of Electrical and Computer Engineering, National University of Singapore, Singapore as a visiting scholar.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Links to codes</head><p>His research interests include machine learning, computer vision and multimedia. E-mail: eleyans@nus.edu.sg</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the International Speech Communication Association, ISCA</title>
		<meeting>the 15th Annual Conference of the International Speech Communication Association, ISCA<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Minimal gated unit for recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Im-ageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
			<pubPlace>Lake Tahoe, USA</pubPlace>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image classification and retrieval are ONE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive: Inter-layer activeness propagation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">POOF: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision</title>
		<meeting>the 12th European Conference on Computer Vision<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7572</biblScope>
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised template learning for fine-grained object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25, NIPS</title>
		<meeting><address><addrLine>Lake Tahoe, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3122" to="3130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1713" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BiCoS: A Bi-level co-segmentation method for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Part localization using multi-proposal consensus for fine-grained categorization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06332</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision<address><addrLine>Zurich</addrLine></address></meeting>
		<imprint>
			<publisher>Switzerland</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient large-scale structured learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1806" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Part-stacked CNN for fine-grained visual categorization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08086</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multidigit recognition using a space displacement neural network</title>
		<author>
			<persName><forename type="first">O</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 4, NIPS</title>
		<meeting><address><addrLine>San Mateo, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep LAC: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mutualinformation-based registration of medical images: A survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B A</forename><surname>Maintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="986" to="1004" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subset feature learning for fine-grained category classification</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="46" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fine-grained classification via mixture of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision<address><addrLine>Lake Placid, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning finegrained features via a CNN tree for large-scale classification</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04534</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7054</idno>
		<title level="m">Attention for fine-grained categorization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
			<date type="published" when="2014">2014</date>
			<pubPlace>Montréal, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08572</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset, Computation &amp; Neural Systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>CNS-TR, California Institute of Technology, USA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
			<date type="published" when="2015">2015</date>
			<pubPlace>Montréal, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Boxsup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference Computer Vision</title>
		<meeting>the 13th European Conference Computer Vision<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feature selection and feature learning for high-dimensional batch reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
