<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Object Segmentation by Redrawing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mickaël</forename><surname>Chen</surname></persName>
							<email>mickael.chen@lip6.fr</email>
						</author>
						<author>
							<persName><forename type="first">Thierry</forename><surname>Artières</surname></persName>
							<email>thierry.artieres@centrale-marseille.fr</email>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
							<email>denoyer@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">Université de Toulon</orgName>
								<orgName type="institution" key="instit3">CNRS, LIS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ecole Centrale Marseille</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Facebook Artificial Intelligence Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Object Segmentation by Redrawing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D88687579A3EDC6942CAAD4BED19D68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object segmentation is a crucial problem that is usually solved by using supervised learning approaches over very large datasets composed of both images and corresponding object masks. Since the masks have to be provided at pixel level, building such a dataset for any new domain can be very costly. We present ReDO, a new model able to extract objects from images without any annotation in an unsupervised way. It relies on the idea that it should be possible to change the textures or colors of the objects without changing the overall distribution of the dataset. Following this assumption, our approach is based on an adversarial architecture where the generator is guided by an input sample: given an image, it extracts the object mask, then redraws a new object at the same location. The generator is controlled by a discriminator that ensures that the distribution of generated images is aligned to the original one. We experiment with this method on different datasets and demonstrate the good quality of extracted masks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image segmentation aims at splitting a given image into a set of non-overlapping regions corresponding to the main components in the image. It has been studied for a long time in an unsupervised setting using prior knowledge on the nature of the region one wants to detect using e.g. normalized cuts and graph-based methods. Recently the rise of deep neural networks and their spectacular performances on many difficult computer vision tasks have led to revisit the image segmentation problem using deep networks in a fully supervised setting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref>, a problem referred as semantic image segmentation.</p><p>Although such modern methods allowed learning successful semantic segmentation systems, their training requires large-scale labeled datasets with usually a need for pixel-level annotations. This feature limits the use of such techniques for many image segmentation tasks for which no such large scale supervision is available. To overcome this drawback, we follow here a very recent trend that aims at revisiting the unsupervised image segmentation problem with new tools and new ideas from the recent history and success of deep learning <ref type="bibr" target="#b54">[55]</ref> and from the recent results of supervised semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>Building on the idea of scene composition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b55">56]</ref> and on the adversarial learning principle <ref type="bibr" target="#b16">[17]</ref>, we propose to address the unsupervised segmentation problem in a new way. We start by postulating an underlying generative process for images that relies on an assumption of independence between regions of an image we want to detect. This means that replacing one object in the image with another one, e.g. a generated one, should yield a realistic image. We use such a generative model as a backbone for designing an object segmentation model we call ReDO (ReDrawing of Objects), which outputs are then used to modify the input image by redrawing detected objects. Following ideas from adversarial learning, the supervision of the whole system is provided by a discriminator that is trained to distinguish between real images and fake images generated accordingly to the generative process. Despite being a simplified model for images, we find this generative process effective for learning a segmentation model.</p><p>The paper is organized as follows. We present related work in Section 2, then we describe our method in Section 3. We first define the underlying generative model that we consider in Section 3.2 and detail how we translate this hypothesis into a neural network architecture to learn a segmentation module in Section 3.3. Then we give implementation details in Section 4. Finally, we present experimental results on three datasets in Section 5 that explore the feasibility of unsupervised segmentation within our framework and compare its performance against a baseline supervised with few labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image segmentation is a very active topic in deep learning that boasts impressive results when using large-scale labeled datasets. Those approaches can effectively parse high-resolution images depicting complex and diverse real-world scenes into informative semantics or instance maps. State-of-the-art methods use clever architectural choices or pipelines tailored to the challenges of the task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>However, most of those models use pixel-level supervision, which can be unavailable in some settings, or costly to acquire in any case. Some works tackle this problem by using fewer labeled images or weaker overall supervision. One common strategy is to use image-level annotations to train a classifier from which class saliency maps can be obtained. Those saliency maps can then be exploited with other means to produce segmentation maps. For instance, WILDCAT <ref type="bibr" target="#b12">[13]</ref> uses a Conditional Random Field (CRF) for spatial prediction in order to post-process class saliency maps for semantic segmentation. PRM <ref type="bibr" target="#b58">[59]</ref>, instead, finds pixels that provoke peaks in saliency maps and uses these as a reference to choose the best regions out of a large set of proposals previously obtained using MCG <ref type="bibr" target="#b1">[2]</ref>, an unsupervised region proposal algorithm. Both pipelines use a combination of a deep classifier and a method that take advantage of spatial and visual handcrafted image priors.</p><p>Co-segmentation, introduced by Rother et al. 2006 <ref type="bibr" target="#b45">[46]</ref>, addresses the related problem of segmenting objects that are shared by multiple images by looking for similar data patterns in all those images. Like the aforementioned models, in addition to prior image knowledge, deep saliency maps are often used to localize those objects <ref type="bibr" target="#b22">[23]</ref>. Unsupervised co-segmentation <ref type="bibr" target="#b21">[22]</ref>, i.e. the task of covering objects of a specific category without additional data annotations, is a setup that resembles ours. However, unsupervised co-segmentation systems are built on the idea of exploiting features similarity and can't easily be extended to a class-agnostic system. As we aim to ultimately be able to segment very different objects, our approach instead relies on independence between the contents of different regions of an image which is a more general concept.</p><p>Fully unsupervised approaches have traditionally been more focused on designing handcrafted features or energy functions to define the desired property of objectness. Impressive results have been obtained when making full use of depth maps in addition to usual RGB images <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref> but it is much harder to specify good energy functions for purely RGB images. W-NET <ref type="bibr" target="#b54">[55]</ref> extracts latent representations via a deep auto-encoder that can then be used by a more classic CRF algorithm. Kanezaki 2018 <ref type="bibr" target="#b27">[28]</ref> further incorporate deep priors and train a neural network to directly minimize their chosen intra-region pixel distance. A different approach is proposed by Ji et al. 2019 <ref type="bibr" target="#b25">[26]</ref> whose method finds clusters of pixels using a learned distance invariant to some known properties. Unlike ours, none of these approaches are learned entirely from data.</p><p>Our work instead follows a more recent trend by inferring scene decomposition directly from data. Stemming from DRAW <ref type="bibr" target="#b18">[19]</ref>, many of those approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> use an attention network to read a region of an image and a Variational Auto-encoder (VAE) to partially reconstruct the image in an iterative process in order to flesh out a meaningful decomposition. LR-GAN <ref type="bibr" target="#b55">[56]</ref> is able to generate simple scenes recursively, building object after object, and Sbai et al. 2018 <ref type="bibr" target="#b47">[48]</ref> decompose an image into single-colored strokes for vector graphics. While iterative processes have the advantage of being able to handle an arbitrary number of objects, they are also more unstable and difficult to train. Most of those can either only be used in generation <ref type="bibr" target="#b55">[56]</ref>, or only handle very simple objects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. As a proof of concept, we decided to first ignore this additional difficulty by only handling a set number of objects but our model can naturally be extended with an iterative composition process. This choice is common among works that, like ours, focus on other aspects of image compositionality. Van Steenkiste et al. 2018 <ref type="bibr" target="#b51">[52]</ref> advocates for a generative framework that accounts for relationship between objects. While they do produce masks as part of their generative process, they cannot segment a given image. Closer to our setup, the very recent IODINE <ref type="bibr" target="#b17">[18]</ref> propose a VAE adapted for multi-objects representations. Their learned representations include a scene decomposition, but they need a costly iterative refinement process whose performance have only been demonstrated on simulated datasets and not real images. Like ours, some prior work have tried to find segmentation mask by recomposing new images. SEIGAN <ref type="bibr" target="#b41">[42]</ref> and Cut &amp; Paste <ref type="bibr" target="#b44">[45]</ref> learns to separate object and background by moving the region corresponding to the object to another background and making sure the image is still realistic. These methods however, need to have access to background images without objects, which might not be easy to obtain.</p><p>Our work also ties to recent research in disentangled representation learning. Multiple techniques have been used to separate information in factored latent representations. One line of work focuses on understanding and exploiting the innate disentangling properties of Variational Auto-Encoders. It was first observed by β-VAE <ref type="bibr" target="#b20">[21]</ref> that VAEs can be constrained to produce disentangled representations by imposing a stronger penalty on the Kullback-Leibler divergence term on the VAE loss. FactorVAE <ref type="bibr" target="#b29">[30]</ref> and β-TCVAE <ref type="bibr" target="#b6">[7]</ref> extract a total correlation term from the KL term of the VAE objective and specifically re-weight it instead of the whole KL term. In a similar fashion, HFVAE <ref type="bibr" target="#b14">[15]</ref> introduces a hierarchical decomposition of the KL term to impose a structure on the latent space. A similar property can be observed with GAN-based models, as shown by InfoGAN <ref type="bibr" target="#b8">[9]</ref> which forces a generator to map a code to interpretable features by maximizing the mutual information between the code and the output. Using adversarial training is also a good way to split and control information in latent embeddings. Fader Networks <ref type="bibr" target="#b31">[32]</ref> uses adversarial training to remove specific class information from a vector. This technique is also used in adversarial domain adaptation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51]</ref> to align embeddings from different domains. Similar methods can be used to build factorial representations instead of simply removing information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>. Like our work, they use adversarial learning to match an implicitly predefined generative model but for purposes unrelated to segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>A segmentation process F splits a given image I ∈ R W ×H×C into a set of non-overlapping regions. F can be described as a function that assigns to each pixel coordinate of I one of n regions. The problem is then to find a correct partition F for any given image I. Lacking supervision, a common strategy is to define properties one wants the regions to have, and then to find a partition that produces regions with such properties. This can be done by defining an energy function and then finding an optimal split. The challenge is then to accurately describe and model the statistical properties of meaningful regions as a function one can optimize.</p><p>We address this problem differently. Instead of trying to define the right properties of regions at the level of each image, we make assumptions about the underlying generative process of images in which the different regions are explicitly modeled. Then, by using an adversarial approach, we learn the parameters of the different components of our model so that the overall distribution of the generated images matches the distribution of the dataset. We detail the generative process in the section 3.2, while the way we learn F is detailed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Process</head><p>We consider that images are produced by a generative process that operates in three steps: first, it defines the different regions in the image i.e the organization of the scene (composition step). Then, given this segmentation, the process generates the pixels for each of the regions independently (drawing step). At last, the resulting regions are assembled into the final image (assembling step).</p><p>Let us consider a scene composed of n -1 objects and one background we refer to as object n. Let us denote M k ∈ {0, 1} W ×H the mask corresponding to object k which associates one binary value to each pixels in the final image so that M k</p><p>x,y = 1 iff the pixel of coordinate (x, y) belongs to object k. Note that, since one pixel can only belong to one object, the masks have to satisfy n k=1 M k</p><p>x,y = 1 and the background mask M n can therefore easily be retrieved computed from the object masks as</p><formula xml:id="formula_0">M n = 1 - n-1 k=1 M k .</formula><p>The pixel values of each object k are denoted V k ∈ R W ×H×C . Given that the image we generate is of size W × H × C, each object is associated with an image of the same size but only the pixels selected by the mask will be used to compose the output image. The final composition of the objects into an image is computed as follows:</p><formula xml:id="formula_1">I ← n k=1 M k V k .</formula><p>To recap, the underlying generative process described previously can be summarized as follow: i) first, the masks M k are chosen together based on a mask prior p(M). ii) Then, for each object independently, the pixel values are chosen based on a distribution p(V k |M k , k). iii) Finally, the objects are assembled into a complete image. This process makes an assumption of independence between the colors and textures of the different objects composing a scene. While this is a naive assumption, as colorimetric values such as exposition, brightness, or even the real colors of two objects, are often related, this simple model still serves as a good prior for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">From Generative Process to Object Segmentation</head><p>Now, instead of considering a purely generative process where the masks are generated following a prior p(M), we consider the inductive process where the masks are extracted directly from any input image I through the function F which is the object segmentation function described previously. The role of F is thus to output a set of masks given any input I. The new generative process acts as follows: i) it takes a random image in the dataset and computes the masks using F(I) → M 1 , . . . , M n , and ii) it generates new pixel values for the regions in the image according to a distribution p(V k |M k , k).</p><p>iii) It aggregates the objects as before.</p><p>In order for output images to match the distribution of the training dataset, all the components (i.e F and p(V k |M k , k)) are learned adversarially following the GAN approach. Let us define D : R W ×H×C → R a discriminator function able to classify images as fake or real. Let us denote G F (I, z 1 , . . . , z n ) our generator function able to compose a new image given an input image I, an object segmentation function F, and a set of vectors z 1 , . . . , z n each sampled independently following a prior p(z) for each object k, background included. Since the pixel values of the different regions are considered as independent given the segmentation, our generator can be decomposed in n generators denoted G k (M k , z k ), each one being in charge of deciding the pixel values for one specific region. The complete image generation process thus operates in three steps:</p><formula xml:id="formula_2">1) M 1 , . . . , M n ← F(I) (composition step) 2) V k ← G k (M k , z k ) for k ∈ {1, . . . , n} (drawing step) 3) G F (I, z 1 , . . . , z n ) = n k=1 M k V k (assembling step).</formula><p>Provided the functions F and G k are differentiable, they can thus be learned by solving the following adversarial problem:</p><formula xml:id="formula_3">min GF max D L = E I∼p data log D(I) + E I∼p data ,z1,...zn∼p(z) log(1 -D(G F (I, z 1 , . . . , z n ))) .</formula><p>Therefore, in practice we have F output soft masks in [0, 1] instead of binary masks. Also, in line with recent GAN literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref>, we choose to use the hinge version of the adversarial loss <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b49">50]</ref> instead, and obtain the following formulation:</p><formula xml:id="formula_4">max GF L G =E I∼p data ,z1,...,zn∼p(z) D(G F (I, z 1 , . . . , z n )) max D L D =E I∼p data min(0, -1 + D(I)) + E I∼p data ,z1,...,zn∼p(z) min(0, -1 -D(G F (I, z 1 , . . . , z n ))) .</formula><p>Still, as it stands, the learning process of this model may fail for two reasons. First, it does not have to extract a meaningful segmentation in regards to the input I. Indeed, since the values of all the output pixels will be generated, I can be ignored entirely to generate plausible pictures. For instance, the segmentation could be the same for all the inputs regardless of input I. Second, it naturally converges to a trivial extractor F that puts the whole image into a single region, the other regions being empty. We thus have to add additional constraints to our model.</p><p>Constraining mask extraction by redrawing a single region. The first constraint aims at forcing the model to extract meaningful region masks instead of ignoring the image. To this end, we take advantage of the assumption that the different objects are independently generated. We can, therefore, replace only one region at each iteration instead of regenerating all the regions. Since the generator now has to use original pixel values from the image in the reassembled image, it cannot make arbitrary splits. The generation process becomes as follows:</p><formula xml:id="formula_5">1) M 1 , . . . , M n ← F(I) (composition step) 2) V k ← I for k ∈ {1, . . . , n} \ {i} V i ← G i (M i , z i ) (drawing step) 3) G F (I, z i , i) = n k=1 M k V k (assembling step),</formula><p>where i designates the index of the only region to redraw and is sampled from U(n), the discrete uniform distribution on {1, . . . , n}. The new learning objectives are as follows:</p><formula xml:id="formula_6">max GF L G = E I∼p data ,i∼U (n),z i ∼p(z) D(G F (I, z i , i)) max D L D = E I∼p data [min(0, -1 + D(I))] + E I∼p data ,i∼U (n),z i ∼p(z) [min(0, -1 -D(G F (I, z i , i)))].</formula><p>Conservation of Region Information. The second constraint is that given a region i generated from a latent vector z i , the final image G F (I, z i , i) must contain information about z i . This constraint is designed to prevent the mask extractor F to produce empty regions. Indeed, if region i is empty, i.e. M i x,y = 0 for all x, y, then z i cannot be retrieved from the final image. Equivalently, if z i can be retrieved, then region i is not empty. This information conservation constraint is implemented through an additional term in the loss function. Let us denote δ k a function which objective is to infer the value of z k given any image I. One can learn such a function simultaneously to promote conservation of information by the generator. This strategy is similar to the mutual information maximization used in InfoGAN. <ref type="bibr" target="#b8">[9]</ref>. The final complete process is illustrated in Figure <ref type="figure">1</ref> and correspond to the following learning objectives:</p><formula xml:id="formula_7">max GF,δ L G = E I∼p data ,i∼U (n),z i ∼p(z) D(G F (I, z i , i)) -λ z ||δ i (G F (I, z i , i)) -z i || 2 2 max D L D = E I∼p data min(0, -1 + D(I) + E I∼p data ,i∼U (n),z i ∼p(z) min(0, -1 -D(G F (I, z i , i)) ,</formula><p>where λ z is a fixed hyper-parameter that controls the strength of the information conservation constraint. Note that the constraint is necessary for our model to find non trivial solutions, as otherwise, putting the whole image into a single region is both optimal and easy to discover for the neural networks. The final learning algorithm follows classical GAN schema <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b56">57]</ref> by updating the generator and the discriminator alternatively with the update functions presented in Algorithm 1. sample noise vector z i ∼ p(z)</p><p>5:</p><formula xml:id="formula_8">I gen ← G f (I, z i , i) generate image 6: L z ← -||δ i (I gen ) -z i || 2 compute information conservation loss 7: L G ← D(I gen ) compute adversarial loss 8: update G f with ∇ G f L G + L z 9:</formula><p>update δ i with ∇ δ i L z 10: procedure DISCRIMINATORUPDATE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We now provide some information about the architecture of the different components (additional details are given in Supplementary materials). As usual with GAN-based methods, the choice of a good architecture is crucial. We have chosen to build on the GAN and the image segmentation literature and to take inspiration from the neural network architectures they propose.</p><p>For the mask generator F, we use an architecture inspired by PSPNet <ref type="bibr" target="#b57">[58]</ref>. The proposed architecture is a fully convolutional neural network similar to one used in image-to-image translation <ref type="bibr" target="#b59">[60]</ref>, to which we add a Pyramid Pooling Module <ref type="bibr" target="#b57">[58]</ref> whose goal is to gather information on different scales via pooling layers. The final representation of a given pixel is thus encouraged to contain local, regional, and global information at the same time.</p><p>The region generators G k , the discriminator D and the network δ that reconstructs z are based on SAGAN <ref type="bibr" target="#b56">[57]</ref> that is frequently used in recent GAN literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>. Notably, we use spectral normalization <ref type="bibr" target="#b38">[39]</ref> for weight regularization for all networks except for the mask provider F, and we use self-attention <ref type="bibr" target="#b56">[57]</ref> in G k and D to handle non-local relations. To both promote stochasticity in our generators and encourage our latent code z to encode for texture and colors, we also use conditional batch-normalization in G k . The technique has emerged from style modeling for style transfer tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> and has since been used for GANs as a mean to encode for style and to improve stochasticity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54]</ref>. All parameters of the different δ k functions are shared except for their last layers.</p><p>As it is standard practice for GANs <ref type="bibr" target="#b2">[3]</ref>, we use orthogonal initialization <ref type="bibr" target="#b46">[47]</ref> for our networks and ADAM <ref type="bibr" target="#b30">[31]</ref> with β = (0, .9) as optimizer. Learning rates are set to 10 -4 except for the mask network F which uses a smaller value of 10 -5 . We sample noise vectors z i of size 32 (except for MNIST where we used vectors of size 16) from N (0, I d ) distribution. We used mini-batches of size 25 and ran each experiment on a single NVidia Tesla P100 GPU. Despite our conservation of information loss, the model can still collapse into generating empty masks at the early steps of the training. While the regularization does alleviate the problem, we suppose that the mask generator F can collapse even before the network δ learns anything relevant and can act as a stabilizer. As the failures happen early and are easy to detect, we automatically restart the training should the case arise.</p><p>We identified λ z and the initialization scheme as critical hyper-parameters and focus our hyperparameters search on those. More details, along with specifics of the implementation and complete source code used in our experiments are provided as Supplementary materials. The code is also available open-source<ref type="foot" target="#foot_0">1</ref> .</p><p>5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We present results on three natural image datasets and one toy dataset. All images have been resized and then cropped to 128 × 128.</p><p>Flowers dataset <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> is composed of 8189 images of flowers. The dataset is provided with a set of masks obtained via an automated method built specifically for flowers <ref type="bibr" target="#b39">[40]</ref>. We split into sets of 6149 training images, 1020 validation and 1020 test images and use the provided masks as ground truth for evaluation purpose only.</p><p>Labeled Faces in the Wild dataset <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref> is a dataset of 13233 faces. A subpart of the funneled version <ref type="bibr" target="#b23">[24]</ref> has been segmented and manually annotated <ref type="bibr" target="#b26">[27]</ref>, providing 2927 groundtruth masks. We use the non-annotated images for our training set. We split the annotated images between validation and testing sets so that there is no overlap in the identity of the persons between both sets. The test set is composed of 1600 images, and the validation set of 1327 images.</p><p>The Caltech-UCSD Birds 200 2011 (CUB-200-2011) dataset <ref type="bibr" target="#b52">[53]</ref> is a dataset containing 11788 photographs of birds. We use 10000 images for our training split, 1000 for the test split, and the rest for validation.</p><p>As a sanity check, we also build a toy dataset colored-2-MNIST in which each sample is composed of an uniform background on which we draw two colored MNIST <ref type="bibr" target="#b33">[34]</ref> numbers: one odd number and one even number. Odd and even numbers have colors sampled from different distributions so that our model can learn to differentiate them. For this dataset, we set n = 3 as there are three components.</p><p>As an additional experiment, we also build a new dataset by fusing Flowers and LFW datasets. This new Flowers+LFW dataset has more variability, and contains different type of objects. We used this dataset to demonstrate that ReDO can work without label information on problems with multiple categories of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>To evaluate our method ReDO, we use two metrics commonly used for segmentation tasks. The pixel classification accuracy (Acc) measures the proportion of pixels that have been assigned to the correct region. The intersection over union (IoU) is the ratio between the area of the intersection between the inferred mask and the ground truth over the area of their union. In both cases, higher is better.</p><p>Because ReDO is unsupervised and we can't control which output region corresponds to which object or background in the image, we compute our evaluation based on the regions permutation that matches the ground truth the best. For model selection, we used IoU computed on a held out labeled validation set. When available, we present our evaluation on both the training set and a test set as, in an unsupervised setting, both can be relevant depending on the specific use case. Results are presented in Table <ref type="table" target="#tab_0">1</ref> and show that ReDO achieves reasonable performance on the three real-world datasets.  We also compared the performance of ReDO, which is unsupervised, with a supervised method, keeping the same architecture for F in both cases. We analyze how many training samples are needed to reach the performance of the unsupervised model (see Figure <ref type="figure" target="#fig_5">4</ref>). One can see that the unsupervised results are in the range of the ones obtained with a supervised method, and usually outperform supervised models trained with less than around 50 or 100 examples depending on the dataset. For instance, on the LFW Dataset, the unsupervised model obtains about 92% of accuracy and 79% IoU and the supervised model needs 50-60 labeled examples to reach similar performance.</p><p>We provide random samples of extracted masks (Figure <ref type="figure" target="#fig_2">2</ref>) and the corresponding generated images with a redrawn object or background. Note that our objective is not to generate appealing images but to learn an object segmentation function. Therefore, ReDO generates images that are less realistic than the ones generated by state-of-the-art GANs. Focus is, instead, put on the extracted masks, and we can see the good quality of the obtained segmentation in many cases. Best and worst masks, as well as more random samples, are displayed in Supplementary materials.</p><p>We also trained ReDO on the fused Flowers+LFW dataset without labels. We re-used directly the hyper-parameters we have used to fit the Flowers dataset without further tuning and obtained, as preliminary results, a reasonable accuracy of 0.856 and an IoU of 0.691. This shows that ReDO is able to infer class information from masks even in a fully unsupervised setup. Samples are displayed in Figure <ref type="figure" target="#fig_3">3</ref>. provided along the original Flowers dataset <ref type="bibr" target="#b40">[41]</ref> have been obtained using an automated method. We display samples with top disagreement masks between ReDO and ground truth in Supplementary materials. In those cases, we find ours to provide better masks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel method called ReDO for unsupervised learning to segment images. Our proposal is based on the assumption that if a segmentation model is accurate, then one could edit any real image by replacing any segmented object in a scene by another one, randomly generated, and the result would still be a realistic image. This principle allows casting the unsupervised learning of image segmentation as an adversarial learning problem. Our experimental results obtained on three datasets show that this principle works. In particular, our segmentation model is competitive with supervised approaches trained on a few hundred labeled examples.</p><p>Our future work will focus on handling more complex and diverse scenes. As mentioned in Section 2, our model could generalize to an arbitrary number of objects and objects of unknown classes via iterative design and/or class agnostic generators. Currently, we are mostly limited by our ability to effectively train GANs on those more complicated settings but rapid advances in image generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> make it a reasonable goal to pursue in a near future. Meanwhile, we will be investigating the use of the model in a semi-supervised or weakly-supervised setup. Indeed, additional information would allow us to guide our model for harder datasets while requiring fewer labels than fully supervised approaches. Conversely, our model could act as a regularizer by providing a prior for any segmentation tasks. Code and dataset splits are included in Supplementary material.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Algorithm 1</head><label>11</label><figDesc>Figure 1: Example generation with G f (I, z i , i) with i = 1 and n = 2. Learned functions are in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>11 :</head><label>11</label><figDesc>sample datapoints I real , I input ∼ p data 12: sample region i ∼ Uniform({1, . . . , n}) 13: sample noise vector z i ∼ p(z) 14: I gen ← G f (I input , z i , i) generate image 15: L D ← min(0, -1 + D(I real )) + min(0, -1 -D(I gen ) compute adversarial loss 16: update D with ∇ D L D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Generated samples (not cherry-picked, zoom in for better visibility). For each dataset, the columns are from left to right: 1) input images, 2) ground truth masks, 3) masks inferred by the model for object one, 4-7) generation by redrawing object one, 8-11) generation by redrawing object two. As we keep the same z i on any given column, the color and texture of the redrawn object is kept constant across rows. More samples are provided in Supplementary materials.</figDesc><graphic coords="8,109.47,161.22,194.02,88.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on LFW + Flowers dataset, arranged as in Figure2. As z is kept constant on a column across all rows, we can observe that z codes for different textures depending on the class of the image even though the generator is never given this information explicitly.</figDesc><graphic coords="8,109.47,353.09,194.04,70.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison with supervised baseline as a function of the number of available training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of ReDO in accuracy (Acc) and intersection over union (IoU) on retrieved masks. Means and standard deviations are based on five runs with fixed hyper-parameters. LWF train set scores are not available since we trained on unlabeled images. *Please note that segmentations</figDesc><table><row><cell></cell><cell>Train Acc</cell><cell>Train IoU</cell><cell>Test Acc</cell><cell>Test IoU</cell></row><row><cell>LFW</cell><cell>-</cell><cell>-</cell><cell cols="2">0.917 ± 0.002 0.781 ± 0.005</cell></row><row><cell>CUB</cell><cell cols="4">0.840 ± 0.012 0.423 ± 0.023 0.845 ± 0.012 0.426 ± 0.025</cell></row><row><cell>Flowers*</cell><cell cols="4">0.886 ± 0.008 0.780 ± 0.012 0.879 ± 0.008 0.764 ± 0.012</cell></row><row><cell>Flowers+LFW</cell><cell>-</cell><cell>-</cell><cell>0.856</cell><cell>0.691</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/mickaelChen/ReDO</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the French project LIVES ANR-15-CE23-0026-03.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented cyclegan: Learning many-to-many mappings from unpaired data</title>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Loic</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view data generation without view supervision</title>
		<author>
			<persName><forename type="first">Mickael</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Artières</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantically decomposing the latent spaces of generative adversarial networks</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophane</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3225" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Babak</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alican</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Willem</forename><surname>Meent</surname></persName>
		</author>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2525" to="2534" />
		</imprint>
	</monogr>
	<note>Structured disentangled representations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Lopez Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Draw: a recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-attention cnns for unsupervised object cosegmentation</title>
		<author>
			<persName><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep instance co-segmentation by copeak search and co-saliency detection</title>
		<author>
			<persName><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidit</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Invariant information distillation for unsupervised image segmentation and clustering</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Augmenting CRFs with Boltzmann machine shape priors for image labeling</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by backpropagation</title>
		<author>
			<persName><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2654" to="2663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: Updates and new reporting procedures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2014-003</idno>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jae</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02271</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Michael F Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Delving into the whorl of flower segmentation</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Seigan: Towards compositional image generation by simultaneously learning to segment, enhance, and inpaint</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Ostyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Khomenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07630</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harm</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scenecut: joint geometric and object segmentation for indoor scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh-Toan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to segment via cut-and-paste</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs</title>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Vector image generation by learning parametric layer decomposition</title>
		<author>
			<persName><forename type="first">Othman</forename><surname>Sbai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05484</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A case for object compositionality in deep generative models of images</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10340</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">W-net: A deep model for fully unsupervised image segmentation</title>
		<author>
			<persName><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08506</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">LR-GAN: layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
