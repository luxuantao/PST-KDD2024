<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification in the Presence of Label Noise: a Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
						</author>
						<title level="a" type="main">Classification in the Presence of Label Noise: a Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08940F8262030D0B62B61E39DF02D3FF</idno>
					<idno type="DOI">10.1109/TNNLS.2013.2292894</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Class noise</term>
					<term>classification</term>
					<term>label noise</term>
					<term>mislabeling</term>
					<term>robust methods</term>
					<term>survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label noise is an important issue in classification, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the field lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to fill this gap. First, the definitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular field of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g. confidences on labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C LASSIFICATION has been widely studied in machine learning. In that context, the standard approach consists in learning a classifier from a labeled dataset, to predict the class of new samples. However, real-world datasets may contain noise, which is defined in <ref type="bibr" target="#b0">[1]</ref> as anything that obscures the relationship between the features of an instance and its class. In <ref type="bibr" target="#b1">[2]</ref>, noise is also described as consisting of nonsystematic errors. Among other consequences, many works have shown that noise can adversely impact the classification performances of induced classifiers <ref type="bibr" target="#b2">[3]</ref>. Hence, the ubiquity of noise seems to be an important issue for practical machine learning e.g. in medical applications where most of the medical diagnosis tests are not 100% accurate and cannot be considered a gold standard <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Indeed, classes are not always as easy to distinguish as lived and died <ref type="bibr" target="#b3">[4]</ref>. It is therefore necessary to implement techniques that eliminate noise or reduce its consequences. It is all the more necessary since reliably labeled data are often expensive and time consuming to obtain <ref type="bibr" target="#b3">[4]</ref>, what explains the commonness of noise <ref type="bibr" target="#b6">[7]</ref>. The authors are with the Institute of Information and Communication Technologies, Electronics and Applied Mathematics, Université catholique de Louvain, Louvain-la-Neuve 1348, Belgium (e-mail: benoit.frenay@uclouvain.be; michel.verleysen@uclouvain.be).</p><p>Digital Object Identifier 10.1109/TNNLS.2013.2292894</p><p>In the literature, two types of noise are distinguished: feature (or attribute) and class noises <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>. On the one hand, feature noise affects the observed values of the feature, e.g. by adding a small Gaussian noise to each feature during measurement. On the other hand, class noise alters the observed labels assigned to instances, e.g. by incorrectly setting a negative label on a positive instance in binary classification. In <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b8">[9]</ref>, it is shown that class noise is potentially more harmful than feature noise, what highlights the importance of dealing with this type of noise. The prevalence of the impact of label noise is explained by the fact: 1) that there are many features, whereas there is only one label and 2) that the importance of each feature for learning is different, whereas labels always have a large impact on learning. Similar results are obtained in <ref type="bibr" target="#b1">[2]</ref>: feature noise appears to be less harmful than class noise for decision trees, except when a large number of features are polluted by feature noise.</p><p>Even if there exists a large literature about class noise, the field still lacks a comprehensive survey on different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to cover the class noise literature. In particular, the different definitions and consequences of class noise are discussed, as well as the different families of algorithms that have been proposed to deal with class noise. As in outlier detection, many techniques rely on noise detection and removal algorithms, but it is shown that more complex methods have emerged. Existing datasets and data generation methods are also discussed, as well as experimental considerations.</p><p>In this paper, class noise refers to observed labels that are incorrect. It is assumed that no other information is available, contrarily to other contexts where experts can e.g. provide a measure of confidence or uncertainty on their own labeling or answer with sets of labels. It is important to make clear that only the observed label of an instance is affected, not its true class. For this reason, here, class noise is called label noise.</p><p>This paper is organized as follows. Section II discusses several definitions and sources of label noise, as well as a new taxonomy inspired by <ref type="bibr" target="#b9">[10]</ref>. The potential consequences of label noise are depicted in Section III. Section IV distinguishes three types of approaches to deal with label noise: label noise-robust, label noise cleansing, and label noise-tolerant methods. The three families of methods are discussed in Sections V-VII, respectively. Section VIII discusses the design of experiments in the context of label noise and Section IX concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEFINITION, SOURCES, AND TAXONOMY</head><p>OF LABEL NOISE Label noise is a complex phenomenon, as shown in this section. First, Section II-A defines the label noise and specifies the scope of this survey. Similarities and differences with outliers and anomalies are also highlighted, since outlier detection methods can be used to detect mislabeled instances. Next, Section II-B reviews various sources of label noise, including insufficient information, expert labeling errors, subjectivity of the classes, and encoding and communication problems. Eventually, a taxonomy of the types of label noise is proposed in Section II-C to facilitate further discussions. The proposed taxonomy highlights the potentially complex relationships between the features of instances, their true class and observed label. This complexity should be considered when designing algorithms to deal with label noise, as they should be adapted to the characteristics of label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition of Label Noise and Scope of the Survey</head><p>Classification consists in predicting the class of new samples, using a model inferred from training data. In this paper, it is assumed that each training sample is associated with an observed label. This label often corresponds to the true class of the sample, but it may be subjected to a noise process before being presented to the learning algorithm <ref type="bibr" target="#b10">[11]</ref>. It is therefore important to distinguish the true class of an instance from its observed label. The process that pollutes labels is called label noise and must be separated from feature (or attribute) noise <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref> that affects the value of features. Some authors also consider outliers that are correctly labeled as label noise <ref type="bibr" target="#b11">[12]</ref>, what is not done here.</p><p>In this survey, label noise is considered to be a stochastic process, i.e., the case where the labeling errors may be intentionally (like e.g. in the food industry <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>) and maliciously induced by an adversary agent <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b25">[26]</ref> is not considered. Moreover, labeling errors are assumed to be independent from each other <ref type="bibr" target="#b10">[11]</ref>. Edmonds <ref type="bibr" target="#b26">[27]</ref> shows that noise in general is a complex phenomenon. In some very specific contexts, stochastic label noise can be intentionally introduced e.g. to protect people privacy, in which case its characteristics are completely under control <ref type="bibr" target="#b27">[28]</ref>. However, a fully specified model of label noise is usually not available, what explains the need for automated algorithms that are able to cope with label noise. Learning situations where label noise occurs can be called imperfectly supervised, i.e. pattern recognition applications where the assumption of label correctness does not hold for all the elements of the training sample <ref type="bibr" target="#b28">[29]</ref>. Such situations are between supervised and unsupervised learning.</p><p>Dealing with label noise is closely related to outlier detection <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref> and anomaly detection <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>. Indeed, mislabeled instances may be outliers, if their label has a low probability of occurrence in their vicinity. Similarly, such instances may also look anomalous, with respect to the class that corresponds to their incorrect label. Hence, it is natural that many techniques in the label noise literature are very close to outlier and anomaly detection techniques; this is detailed in Section VI. Many of the methods that have been developed to deal with outliers and anomalies can also be used to deal with label noise (see e.g. <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>). However, it must be highlighted that mislabeled instances are not necessarily outliers or anomalies, which are subjective concepts <ref type="bibr" target="#b40">[41]</ref>. For example, if labeling errors occur in a boundary region where all classes are equiprobable, the mislabeled instances neither are rare events nor look anomalous. Similarly, an outlier is not necessarily a mislabeled sample <ref type="bibr" target="#b41">[42]</ref>, since it can be due to feature noise or simply be a low-probability event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sources of Label Noise</head><p>As outlined in <ref type="bibr" target="#b0">[1]</ref>, the identification of the source(s) of label noise is not necessarily important, when the focus of the analysis is on the consequences of label noise. However, when a label noise model has to be embedded directly into the learning algorithm, it may be important to choose a modelling that accurately explains the actual label noise.</p><p>Label noise naturally occurs when human experts are involved <ref type="bibr" target="#b42">[43]</ref>. In that case, possible causes of label noise include imperfect evidence, patterns that may be confused with the patterns of interest, perceptual errors or even biological artifacts. See <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b44">[45]</ref> for a philosophical account on probability, imprecision, and uncertainty. More generally, potential sources of label noise include four main classes.</p><p>First, the information that is provided to the expert may be insufficient to perform reliable labeling <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b45">[46]</ref>. For example, the results of several tests may be unknown in medical applications <ref type="bibr" target="#b11">[12]</ref>. Moreover, the description language may be too limited <ref type="bibr" target="#b46">[47]</ref>, what reduces the amount of available information. In some cases, the information is also of poor or variable quality. For example, the answers of a patient during anamnesis may be imprecise or incorrect or even may be different if the question is repeated <ref type="bibr" target="#b47">[48]</ref>.</p><p>Second, as above-mentioned, errors can occur in the expert labeling itself <ref type="bibr" target="#b0">[1]</ref>. Such classification errors are not always due to human experts, since automated classification devices are used nowadays in different applications <ref type="bibr" target="#b11">[12]</ref>. In addition, since collecting reliable labels is a time consuming and costly task, there is an increasing interest in using cheap, easy-to-get labels from nonexpert using frameworks like e.g. the Amazon Mechanical Turk<ref type="foot" target="#foot_0">1</ref> [49]- <ref type="bibr" target="#b51">[52]</ref>. Labels provided by nonexpert are less reliable, but <ref type="bibr" target="#b48">[49]</ref> shows that the wealth of available labels may alleviate this problem.</p><p>Thirdly, when the labeling task is subjective, like e.g. in medical applications <ref type="bibr" target="#b52">[53]</ref> or image data analysis <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, there may exist an important variability in the labeling by several experts. For example, in electrocardiogram analysis, experts seldom agree on the exact boundaries of signal patterns <ref type="bibr" target="#b55">[56]</ref>. The problem of interexpert variability was also noticed during the labeling of the Penn Treebank, an annotated corpus of over 4.5 million words <ref type="bibr" target="#b57">[57]</ref>.</p><p>Eventually, label noise can also simply come from data encoding or communication problems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b45">[46]</ref>. For example, in spam filtering, sources of label noise include misunderstanding the feedback mechanisms and accidental click <ref type="bibr" target="#b58">[58]</ref>. Real-world databases are estimated to contain around five percents of encoding errors, all fields taken together, when no specific measures are taken <ref type="bibr" target="#b59">[59]</ref>- <ref type="bibr" target="#b61">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Taxonomy of Label Noise</head><p>In the context of missing values, Schafer and Graham <ref type="bibr" target="#b9">[10]</ref> discuss a taxonomy that is adapted below to provide a new taxonomy for label noise. Similarly, Nettleton et al. <ref type="bibr" target="#b62">[62]</ref> characterize noise generation in terms of its distribution, the target of the noise (features, label, etc.) and whether its magnitude depends on the data value of each variable. Since it is natural to consider label noise from a statistical point of view, Fig. <ref type="figure" target="#fig_1">1</ref> shows three possible statistical models of label noise. To model the label noise process, four random variables are depicted: X is the vector of features, Y is the true class, Ỹ is the observed label, and E is a binary variable telling whether a labeling error occurred (Y = Ỹ ). The set of possible feature values is X , whereas the set of possible classes (and labels) is Y. Arrows report statistical dependencies: for example, Ỹ is assumed to always depend on Y (otherwise, there is no sense in using the labels).</p><p>1) Noisy Completely at Random Model: In Fig. <ref type="figure" target="#fig_1">1</ref>(a), the relationship between Y and Ỹ is called noisy completely at random (NCAR): the occurrence of an error E is independent of the other random variables, including the true class itself. In the NCAR case, the observed label is different from the true class with a probability p e = P(E = 1) = P(Y = Ỹ ) <ref type="bibr" target="#b10">[11]</ref>, sometime called the error rate or the noise rate <ref type="bibr" target="#b63">[63]</ref>. In the case of binary classification, NCAR noise is necessarily symmetric: the same percentage of instances are mislabeled in both classes. When p e = 1/2, the labels are useless, since they no longer carry any information <ref type="bibr" target="#b10">[11]</ref>. The NCAR setting is similar to the absent-minded professor discussed in <ref type="bibr" target="#b64">[64]</ref>.</p><p>In the case of multiclass classification, it is usually assumed that the incorrect label is chosen at random in Y \ {y} when E = 1 <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b65">[65]</ref>. In other words, a biased coin is first flipped to decide whether the observed label is correct or not. If the label is wrong, a fair dice with |Y| -1 faces (where |Y| is the number of classes) is tossed to choose the observed, wrong label. This particularly simple model is called the uniform label noise.</p><p>2) Noisy at Random Model: In Fig. <ref type="figure" target="#fig_1">1</ref>(b), it is assumed that the probability of error depends on the true class Y , what is called here noisy at random (NAR). E is still independent of X, but this model allows modeling asymmetric label noise, i.e. when instances from certain classes are more prone to be mislabeled. For example, in medical case-control studies, control subjects may be more likely to be mislabeled. Indeed, the test that is used to label case subjects may be too invasive (e.g., a biopsy) or too expensive to be used on control subjects and is therefore replaced by a suboptimal diagnostic test for control subjects <ref type="bibr" target="#b66">[66]</ref>. Since one can define the labeling probabilities</p><formula xml:id="formula_0">P( Ỹ = ỹ|Y = y) = e∈{0,1} P( Ỹ = ỹ|E = e, Y = y)P(E = e|Y = y), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>the NAR label noise can equivalently be characterized in terms of the labeling (or transition) matrix <ref type="bibr" target="#b67">[67]</ref>, <ref type="bibr" target="#b68">[68]</ref> </p><formula xml:id="formula_2">γ = ⎛ ⎜ ⎝ γ 11 • • • γ 1n Y . . . . . . . . . γ n Y 1 • • • γ n Y n Y ⎞ ⎟ ⎠ = ⎛ ⎜ ⎝ P( Ỹ = 1|Y = 1) • • • P( Ỹ = n Y |Y = 1) . . . . . . . . . P( Ỹ = 1|Y = n Y ) • • • P( Ỹ = n Y |Y = n Y ) ⎞ ⎟ ⎠<label>(2)</label></formula><p>where n Y = |Y| is the number of classes. Each row of the labeling matrix must sum to 1, since ỹ∈Y P( Ỹ = ỹ| Y = y) = 1. For example, the uniform label noise corresponds to the labeling matrix ⎛ ⎜ ⎝</p><formula xml:id="formula_3">1 -p e • • • p e n Y -1 . . . . . . . . . p e n Y -1 • • • 1 -p e ⎞ ⎟ ⎠ . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Notice that NCAR label noise is a special case of NAR label noise. When true classes are known, the labeling probabilities can be directly estimated by the frequencies of mislabeling in data, but it is seldom the case <ref type="bibr" target="#b47">[48]</ref>. Alternately, one can also use the incidence-of-error matrix <ref type="bibr" target="#b47">[48]</ref> </p><formula xml:id="formula_5">⎛ ⎜ ⎝ π 1 γ 11 • • • π 1 γ 1n Y . . . . . . . . . π n Y γ n Y 1 • • • π n Y γ n Y n Y ⎞ ⎟ ⎠ = ⎛ ⎜ ⎝ P(Y = 1, Ỹ = 1) • • • P(Y = 1, Ỹ = n Y ) . . . . . . . . . P(Y = n Y , Ỹ = 1) • • • P(Y = n Y , Ỹ = n Y ) ⎞ ⎟ ⎠<label>(4)</label></formula><p>where π y = P(Y = y) is the prior of class y. The entries of the incidence-of-error matrix sum to one and may be of more practical interest.</p><p>With the exception of uniform label noise, NAR label noise is the most commonly studied case of label noise in the literature. For example, Lawrence and Schölkopf <ref type="bibr" target="#b67">[67]</ref> consider arbitrary labeling matrices. In <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b69">[69]</ref>, pairwise label noise is introduced: 1) two classes c 1 and c 2 are selected and 2) each instance of class c 1 has a probability to be incorrectly labeled as c 2 and vice versa. For this label noise, only two nondiagonal entries of the labeling matrix are nonzero.</p><p>In the case of NAR label noise, it is no longer trivial to decide whether the labels are helpful or not. One solution is to compute the expected probability of error <ref type="bibr" target="#b4">(5)</ref> and to require that p e &lt; 1/2, similarly to NCAR label noise. However, this condition does not prevent the occurrence of very small correct labeling probabilities P( Ỹ = y|Y = y) for some class y ∈ Y, in particular if the prior probability P(y) of this class is small. Instead, conditional error probabilities p e (y) = P(E = 1|Y = y) can also be used.</p><formula xml:id="formula_6">p e = P(E = 1) = y∈Y P(Y = y)P(E = 1|Y = y)</formula><p>3) Noisy Not at Random Model: Most works on label noise consider that the label noise affects all instances with no distinction. However, it is not always realistic to assume the two above types of label noise <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b70">[70]</ref>. For example, samples may be more likely mislabeled when they are similar to instances of another class <ref type="bibr" target="#b70">[70]</ref>- <ref type="bibr" target="#b76">[76]</ref>, as illustrated in <ref type="bibr" target="#b77">[77]</ref> where empirical evidence is given that more difficult samples in a text entailment dataset are labeled randomly. It also seems natural to expect less reliable labels in regions of low density <ref type="bibr" target="#b78">[78]</ref>- <ref type="bibr" target="#b80">[80]</ref>, where experts predictions may be actually based on a very small number of similar previously encountered cases.</p><p>Let us consider a more complex and realistic model of label noise. In Fig. <ref type="figure" target="#fig_1">1(c</ref>), E depends on both variables X and Y , i.e., mislabeling is more probable for certain classes and in certain regions of the X space. This noisy not at random (NNAR) model is the most general case of label noise <ref type="bibr" target="#b81">[81]</ref>, <ref type="bibr" target="#b82">[82]</ref>. For example, mislabeling near the classification boundary or in low density regions can only be modeled in terms of NNAR label noise. Such a situation occurs e.g. in speech recognition where automatic speech recognition is more difficult in the case of phonetic similarity between the correct and the recognized words <ref type="bibr" target="#b83">[83]</ref>. The context of each word can be considered to detect incorrect recognitions. Notice that the medical literature distinguishes differential (feature dependent, i.e. NNAR) label noise and nondifferential (feature independent, i.e. NCAR or NAR) label noise <ref type="bibr" target="#b84">[84]</ref>.</p><p>The reliability of labels is even more complex to estimate that for NCAR or NAR label noise. Indeed, the probability of error also depends in that case on the value of X. As before, one can define an expected probability of error that becomes</p><formula xml:id="formula_7">p e = P(E = 1) = y∈Y P(Y = y) × x∈X P(X = x|Y = y)P(E = 1|X = x, Y = y)dx (6)</formula><p>if X is continuous. However, this quantity does not reflect the local nature of label noise: in some cases, p e can be almost zero although the density of labeling errors shows important peaks in certain regions. The quantity p e (x, y) = P(E = 1|X = x, Y = y) may therefore be more appropriate to characterize the reliability of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CONSEQUENCES OF LABEL NOISE ON LEARNING</head><p>In this section, the potential consequences of label noise are described to show the necessity to consider label noise in learning problems. Section III-A reviews the theoretical and empirical evidences of the impact of label noise on classification performances, which is the most frequently reported issue. Section III-B shows that the presence of label noise also increases the necessary number of samples for learning, as well as the complexity of models. Label noise may also pose a threat for related tasks, like e.g. class frequencies estimation and feature selection, which are discussed in Section III-C and III-D, respectively.</p><p>This section presents the negative consequences of label noise, but artificial label noise also has potential advantages. For example, label noise can be added in statistical studies to protect people privacy: it is used in <ref type="bibr" target="#b27">[28]</ref> to obtain statistics for questionnaires, while making impossible to recover individual answers. In <ref type="bibr" target="#b85">[85]</ref>- <ref type="bibr" target="#b89">[89]</ref>, label noise is added to improve classifier results. Whereas bagging produces different training sets by resampling, these works copy the original training set and switch labels in new training sets to increase the variability in data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deterioration of Classification Performances</head><p>The more frequently reported consequence of label noise is a decrease in classification performances, as shown in the theoretical and experimental works described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Theoretical Studies of Simple Classifiers:</head><p>There exist several theoretical studies of the consequences of label noise on prediction performances. For simple problems and symmetric label noise, the accuracy of classifiers may remain unaffected. Lachenbruch <ref type="bibr" target="#b71">[71]</ref> consider e.g. the case of binary classification when both classes have Gaussian distribution with identical covariance matrix. In such a case, a linear discriminant function can be used. For a large number of samples, the consequence of uniform noise is noticeable only if the error rates α 1 and α 2 in each class are different. In fact, the change in decision boundary is completely described in terms of the difference α 1 -α 2 . These results are also discussed asymptotically in <ref type="bibr" target="#b90">[90]</ref>.</p><p>The results of Lachenbruch <ref type="bibr" target="#b71">[71]</ref> are extended in <ref type="bibr" target="#b91">[91]</ref> for quadratic discriminant functions, i.e. Gaussian conditional distributions with unequal covariance matrices. In that case, prediction is affected even when label noise is symmetric among the classes (α 1 = α 2 ). Consequences worsen when differences in covariance matrices or misclassification rates increase. Michalek and Tripathi <ref type="bibr" target="#b92">[92]</ref> and Bi and Jeske <ref type="bibr" target="#b93">[93]</ref> show that label noise affects normal discriminant and logistic regression: their error rates are increased and their parameters are biased. Logistic regression seems to be less affected.</p><p>In <ref type="bibr" target="#b64">[64]</ref>, the single-unit perceptron is studied in the presence of label noise. If the teacher providing learning samples is absent minded, i.e. labels are flipped with a given probability (uniform noise), the performances of a learner who takes the labels for granted are damaged and even get worse than the performances of the teacher.</p><p>Classification performances of the k nearest neighbors (kNN) classifier are also affected by label noise <ref type="bibr" target="#b94">[94]</ref>, <ref type="bibr" target="#b95">[95]</ref>, in particular when k = 1 <ref type="bibr" target="#b96">[96]</ref>. Okamoto and Nobuhiro <ref type="bibr" target="#b96">[96]</ref> present an average-case analysis of the kNN classifier. When k is optimized, the consequences of label noise are reduced and remain small unless a large amount of label noise is added. The optimal value of k depends on both the number of training instances and the presence of label noise. For small noise-free training sets, 1NN classifiers are often optimal. However, as soon as label noise is added, the optimal number of neighbors k is shown to monotonically increase with the number of instances even for small training sets, what seems natural since 1NN classifiers are particularly affected by label noise.</p><p>2) Experimental Assessment of Specific Models: Apart from theoretical studies, many works show experimentally that label noise may be harmful. First, the impact of label noise is not identical for all types of classifiers. As detailed in Section V, this fact can be used to cope (at least partially) with label noise. For example, Nettleton et al. <ref type="bibr" target="#b62">[62]</ref> compare the impact of label noise on four different supervised learners: naive Bayes, decision trees induced by C4.5, kNNs, and support vector machines (SVMs). In particular, naive Bayes achieves the best results, what is attributed to the conditional independence assumption and the use of conditional probabilities. This should be contrasted with the results in <ref type="bibr" target="#b11">[12]</ref>, where naive Bayes is sometime dominated by C4.5 and kNNs. The poor results of SVMs are attributed to its reliance on support vectors and the feature interdependence assumption.</p><p>In text categorization, Zhang and Yang <ref type="bibr" target="#b97">[97]</ref> consider the robustness of regularized linear classification methods. Three linear methods are tested by randomly picking and flipping labels: linear SVMs, ridge regression, and logistic regressions. The experiments show that the results are dramatically affected by label noise for all three methods, which obtain almost identical performances. Only 5% of flipped labels already leads to a dramatic decrease of performances, what is explained by the presence of a relatively very small classes with only a few samples in their experiments.</p><p>Several studies have shown that boosting <ref type="bibr" target="#b98">[98]</ref> is affected by label noise <ref type="bibr" target="#b99">[99]</ref>- <ref type="bibr" target="#b102">[102]</ref>. In particular, the adaptive boosting algorithm AdaBoost tends to spend too much efforts on learning mislabeled instances <ref type="bibr" target="#b100">[100]</ref>. During learning, successive weak learners are trained and the weights of instance that are misclassified at one step are increased at the next step. Hence, in the late stages of learning, AdaBoost tends to increase the weights of mislabeled instances and starts overfitting <ref type="bibr" target="#b103">[103]</ref>, <ref type="bibr" target="#b104">[104]</ref>. Dietterich <ref type="bibr" target="#b100">[100]</ref> clearly shows that the mean weight per training sample becomes larger for mislabeled samples than for correctly labeled samples as learning goes on. Interestingly, it has been shown in <ref type="bibr" target="#b105">[105]</ref>- <ref type="bibr" target="#b108">[108]</ref> that AdaBoost tends to increase the margins of the training examples <ref type="bibr" target="#b109">[109]</ref> and achieves asymptotically a decision with hard margin very similar to the one of SVMs for the separable case <ref type="bibr" target="#b108">[108]</ref>. This may not be a good idea in the presence label noise and may explain why AdaBoost overfits noisy training instances. In <ref type="bibr" target="#b110">[110]</ref>, it is also shown that ensemble methods can fail simply because the presence of label noise affects the ensembled models. Indeed, learning through multiple models becomes harder for large levels of label noise, where some samples become more difficult for all models and are therefore seldom correctly classified by an individual model.</p><p>In systems that learn Boolean concepts with disjuncts, Weiss <ref type="bibr" target="#b111">[111]</ref> explains that small disjuncts (which individually cover only a few examples) are more likely to be affected by label noise than large disjuncts covering more instances. However, only large levels of label noise may actually be a problem. For decision trees, it appears in <ref type="bibr" target="#b1">[2]</ref> that destroying class information produces a linear increase in error. Taking logic to extremes, when all class information is noise, the resulting decision tree classifies objects entirely randomly.</p><p>Another example studied in <ref type="bibr" target="#b58">[58]</ref> is spam filtering where performances are decreased by label noise. Spam filters tend to overfit label noise, due to aggressive online update rules that are designed to quickly adapt to new spam.</p><p>3) Additional Results for More Complex Types of Label Noise: The above works deal with NAR label noise, but more complex types of label noise have been studied in the literature. For example, in the case of linear discriminant analysis (LDA), i.e. binary classification with normal class distributions, Lachenbruch <ref type="bibr" target="#b70">[70]</ref> considers that mislabeling systematically occurs when samples are too far from the mean of their true class. In that NNAR label noise model, the true probabilities of misclassification are only slightly affected, whereas the populations are better separated. This is attributed to the reduction of the effects of outliers. However, the apparent error rate <ref type="bibr" target="#b112">[112]</ref> of LDA is highly influenced, what may cause the classifier to overestimate its own efficiency.</p><p>LDA is also studied in the presence of label noise by <ref type="bibr" target="#b72">[72]</ref>, which generalizes the results of <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b90">[90]</ref>- <ref type="bibr" target="#b92">[92]</ref>. Let us define: 1) the misallocation rate α y for class y, i.e. the number of samples with label y that belong to the other class and 2) a z-axis that passes through the center of both classes and is oriented toward the positive class, such that each center is located at z = ± /2. In <ref type="bibr" target="#b72">[72]</ref>, three label noise models are defined and characterized in terms of the probability of misallocation g y (z), which is a monotone decreasing (increasing) function of z for positive (negative) samples. In random misallocation, g y (z) = α y is constant for each class, what is equivalent to the NAR label noise. In truncated label noise, g(z) is zero as long as the instance is close enough to the mean of its class. Afterward, the mislabeling probability is equal to a small constant. This type of NNAR label noise is equivalent to the model of <ref type="bibr" target="#b70">[70]</ref> when the constant is equal to one. Eventually, in the exponential model, the probability of misallocation becomes for the negative class</p><formula xml:id="formula_8">g y (z) = 0 i f z ≤ -2 1 -exp -1 2 k y z + 2 2 if z &gt; -2<label>(7)</label></formula><p>where is the distance between the centers of both classes and k y = (1 -2α y ) -2 . A similar definition is given for the positive class. For equivalent misallocation rates α y , random misallocation has more consequences than truncated label noise, in terms of influence on the position and variability of the discriminant boundary. In turn, truncated label noise itself has more consequences than exponential label noise. The same ordering appears when comparing misclassification rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Consequences on Learning Requirements and Model Complexity</head><p>Label noise can affect learning requirements (e.g., number of necessary instances) or the complexity of learned models. For example, Quinlan <ref type="bibr" target="#b1">[2]</ref> warns that the size of decision trees may increase in case of label noise, making them overly complicated, what is confirmed experimentally in <ref type="bibr" target="#b45">[46]</ref>. Similarly, Abellán and Masegosa <ref type="bibr" target="#b104">[104]</ref> show that the number of nodes of decision trees induced by C4.5 for bagging is increased, while the resulting accuracy is reduced. Reciprocally, Brodley and Friedl <ref type="bibr" target="#b45">[46]</ref> and Libralon et al. <ref type="bibr" target="#b113">[113]</ref> and show that removing mislabeled samples reduces the complexity of SVMs (number of support vectors), decision trees induced by C4.5 (size of trees) and rule-based classifiers induced by RIPPER (number of rules). Postpruning also seems to reduce the consequences of label noise <ref type="bibr" target="#b104">[104]</ref>. Noise reduction can therefore produce models that are easier to understand, what is desirable in many circumstances <ref type="bibr" target="#b114">[114]</ref>- <ref type="bibr" target="#b116">[116]</ref>.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, it is shown that the presence of uniform label noise in the probably approximately correct (PAC) framework <ref type="bibr" target="#b117">[117]</ref> increases the number of necessary samples for PAC identification. An upper bound for the number of necessary samples is given, which is strengthened in <ref type="bibr" target="#b118">[118]</ref>. Similar bounds are also discussed in <ref type="bibr" target="#b65">[65]</ref> and <ref type="bibr" target="#b119">[119]</ref>. Also, Angluin and Laird <ref type="bibr" target="#b10">[11]</ref> discuss the feasibility of PAC learning in the presence of label noise for propositional formulas in conjunctive normal form, what is extended in <ref type="bibr" target="#b120">[120]</ref> for Boolean functions represented by decision trees and in <ref type="bibr" target="#b73">[73]</ref> and <ref type="bibr" target="#b121">[121]</ref> for linear perceptrons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distortion of Observed Frequencies</head><p>In medical applications, it is often necessary to perform medical tests for disease diagnosis, to estimate the prevalence of a disease in a population or to compare (estimated) prevalence in different populations. However, label noise can affect the observed frequencies of medical test results, what may lead to incorrect conclusions. For binary tests, Bross <ref type="bibr" target="#b3">[4]</ref> shows that mislabeling may pose a serious threat: the observed mean and variance of the test answer is strongly affected by label noise. Let us consider a simple example taken from <ref type="bibr" target="#b3">[4]</ref>: if the minority class represents 10% of the dataset and 5% of the test answers are incorrect (i.e. patients are mislabeled), the observed proportion of minority cases is 0.95×10%+0.05×90% = 14% and is therefore overestimated by 40%. Significance tests that assess the difference between the proportions of both classes in two populations are still valid in case of mislabeling, but their power may be strongly reduced. Similar problems occur e.g. in consumer survey analysis <ref type="bibr" target="#b122">[122]</ref>.</p><p>Frequency estimates are also affected by label noise in multiclass problems. Hout and Heijden <ref type="bibr" target="#b27">[28]</ref> discuss the case of artificial label noise which can be intentionally introduced after data collection to preserve privacy. Since the label noise is fully specified in this case, it is possible to adjust the observed frequencies. When a model of the label noise is not available, Tenenbein <ref type="bibr" target="#b123">[123]</ref> proposes to solve the problem pointed by <ref type="bibr" target="#b3">[4]</ref> using double sampling, which uses two labelers: an expensive, reliable labeler and a cheap, unreliable labeler. The model of mislabeling can thereafter be learned from both sets of labels <ref type="bibr" target="#b124">[124]</ref>, <ref type="bibr" target="#b125">[125]</ref>. In <ref type="bibr" target="#b47">[48]</ref>, the case of multiple experts is discussed in the context of medical anamnesis; an algorithm is proposed to estimate the error rates of the experts.</p><p>Evaluating the error rate of classifiers is also important for both model selection and model assessment. In that context, Lam and Stork <ref type="bibr" target="#b126">[126]</ref> show that label noise can have an important impact on the estimated error rate, when test samples are also polluted. Hence, mislabeling can also bias model comparison. As an example, a spam filter with a true error rate of 0.5%, for example, might be estimated to have an error rate between 5.5% and 6.5% when evaluated using labels with an error rate of 6.0%, depending on the correlation between filter and label errors <ref type="bibr" target="#b127">[127]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Consequences for Related Tasks</head><p>The aforementioned consequences are not the only possible consequences of label noise. For example, Zhang et al. <ref type="bibr" target="#b128">[128]</ref> show that the consequences of label noise are important in feature selection for microarray data. In an experiment, only one mislabeled sample already leads to about 20% of not identified discriminative genes. Notice that in microarray data, only few data are available. Similarly, Shanab et al. <ref type="bibr" target="#b129">[129]</ref> show that label noise decreases the stability of feature rankings. The sensitivity of feature selection to label noise is also illustrated for logistic regression in <ref type="bibr" target="#b130">[130]</ref>. A methodology to achieve feature selection for classification problems polluted by label noise is proposed in <ref type="bibr" target="#b131">[131]</ref>, based on a probabilistic label noise model combined with a nearest neighbors-based estimator of the mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conclusion</head><p>This section shows that the consequences of label noise are important and diverse: decrease in classification performances, changes in learning requirements, increase in the complexity of learned models, distortion of observed frequencies, difficulties to identify relevant features, etc. The nature and the importance of the consequences depend, among others, on the type and the level of label noise, the learning algorithm, and the characteristics of the training set. Hence, it seems important for the machine learning practitioner to deal with label noise and to consider these factors, prior to the analysis of polluted data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODS TO DEAL WITH LABEL NOISE</head><p>In light of the various consequences detailed in Section III, it seems important to deal with label noise. In the literature, there exist three main approaches to take care of label noise <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b132">[132]</ref>- <ref type="bibr" target="#b137">[137]</ref>; these approaches are described below. Manual review of training samples is not considered in this survey, because it is usually prohibitively costly and time consuming, if not impossible in the case of large datasets.</p><p>A first approach relies on algorithms that are naturally robust to label noise. In other words, the learning of the classifier is assumed to be not too sensitive to the presence of label noise. Indeed, several studies have shown that some algorithms are less influenced than others by label noise, what advocates for this approach. However, label noise is not really considered in this type of approach. Label noise handling is entrusted to overfitting avoidance <ref type="bibr" target="#b132">[132]</ref>- <ref type="bibr" target="#b134">[134]</ref>.</p><p>Second, one can try to improve the quality of training data using filter approaches. In such a case, noisy labels are typically identified and being dealt with before training occurs. Mislabeled instances can either be relabeled or simply removed <ref type="bibr" target="#b138">[138]</ref>. Filter approaches are cheap and easy to implement, but some of them are likely to remove a substantial amount of data.</p><p>Eventually, there exist algorithms that directly model label noise during learning or which have been modified to consider label noise in an embedded fashion. The advantage of this approach is to separate the classification model and the label noise model, what allows using information about the nature of label noise.</p><p>The literature for the three above trends of approaches is reviewed in the three next sections. In some cases, it is not always clear whether an approach belongs to one category or the other. For example, some of the label noise-tolerant variants of SVMs could also be observed as filtering. Table <ref type="table" target="#tab_0">I</ref> shows an overview of the main methods considered in this paper. At the end of each section, a short discussion of the strengths and weaknesses of the described techniques is proposed, to help the practitioner in its choice. The three following sections are strongly linked with Section III. Indeed, the knowledge of the consequences of label noise allows one to avoid some pitfalls and to design algorithms that are more robust or tolerant to label noise. Moreover, the consequences of label noise themselves can be used to detect mislabeled instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LABEL NOISE-ROBUST MODELS</head><p>This section describes models that are robust to the presence of label noise. Even if label noise is neither cleansed nor modeled, such models have been shown to remain relatively effective when training data are corrupted by small amounts of label noise. Label noise-robustness is discussed from a theoretical point of view in Section V-A. Then, the robustness of ensembles methods and decision trees are considered in Section V-B and V-C, respectively. Eventually, various other methods are discussed in Section V-D and V-E concludes about the practical use of label noise-robust methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Considerations on the Robustness of Losses</head><p>Before we turn to empirical results, a first, fundamental question is whether it is theoretically possible (and under what circumstances) to achieve perfect label noise-robustness. To have a general view of label noise-robustness, Manwani and Sastry <ref type="bibr" target="#b82">[82]</ref> study learning algorithms in the empirical risk minimization (ERM) framework for binary classification. In ERM, the cost of wrong predictions is measured by a loss and classifiers are learned by minimizing the expected loss for future samples, which is called the risk. The more natural loss is the 0-1 loss, which gives a cost of 1 in case of error and is zero otherwise. However, the 0-1 loss is neither convex nor differentiable, what makes it intractable for real learning algorithms. Hence, others losses are often used in practice, which approximate the 0-1 loss by a convex function, called a surrogate <ref type="bibr" target="#b139">[139]</ref>.</p><p>In <ref type="bibr" target="#b82">[82]</ref>, risk minimization under a given loss function is defined as label noise-robust if the probability of misclassification of inferred models is identical, irrespective of label noise presence. It is demonstrated that the 0-1 loss is label noiserobust for uniform label noise <ref type="bibr" target="#b140">[140]</ref> or when it is possible to achieve zero error rate <ref type="bibr" target="#b81">[81]</ref>; see e.g. <ref type="bibr" target="#b74">[74]</ref> for a discussion in the case of NNAR label noise. The least-square loss is also robust to uniform label noise, which guarantees the robustness of the Fisher linear discriminant in that specific case. Other well-known losses are shown to be not robust to label noise, even in the uniform label noise case: 1) the exponential loss, which leads to AdaBoost; 2) the log loss, which leads to logistic regression; and 3) the hinge loss, which leads to SVMs. In other words, one can expect most of the recent learning algorithms in machine learning to be not completely label noise-robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ensemble Methods: Bagging and Boosting</head><p>In the presence of label noise, bagging achieves better results than boosting <ref type="bibr" target="#b100">[100]</ref>. On the one hand, mislabeled instances are characterized by large weights in AdaBoost, which spends too much effort in modeling noisy instances <ref type="bibr" target="#b104">[104]</ref>. On the other hand, mislabeled samples increase the variability of the base classifiers for bagging. Indeed, since each mislabeled sample has a large impact on the classifier and bagging repeatedly selects different subsets of training instances, each resampling leads to a quite different model. Hence, the diversity of base classifiers is improved in bagging, whereas the accuracy of base classifiers in AdaBoost is severely reduced.</p><p>Several algorithms have been shown to be more label noiserobust than AdaBoost <ref type="bibr" target="#b101">[101]</ref>, <ref type="bibr" target="#b102">[102]</ref>, e.g., LogitBoost <ref type="bibr" target="#b141">[141]</ref>, and BrownBoost <ref type="bibr" target="#b142">[142]</ref>. In <ref type="bibr" target="#b108">[108]</ref> and <ref type="bibr" target="#b143">[143]</ref>- <ref type="bibr" target="#b145">[145]</ref>, boosting is casted as a margin maximization problem and slack variables are introduced to allow a given fraction of patterns to stand in the margin area. Similar to soft-margin SVMs, these works propose to allow boosting to misclassify some of the training samples, what is not directly aimed at dealing with label noise but robustifies boosting. Moreover, this approach can be used to find difficult or informative patterns <ref type="bibr" target="#b145">[145]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decision Trees</head><p>It is well known that decision trees are greatly impacted by label noise <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b104">[104]</ref>. In fact, their instability makes them well suited for ensemble methods <ref type="bibr" target="#b146">[146]</ref>- <ref type="bibr" target="#b148">[148]</ref>. In <ref type="bibr" target="#b148">[148]</ref>, different node split criteria are compared for ensembles of decision trees in the presence of label noise. The imprecise info-gain <ref type="bibr" target="#b149">[149]</ref> is shown to improve accuracy, with respect to the information gain, the information gain ratio and the Gini index. Compared with ensembles of decision trees inferred by C4.5, Abellán and Masegosa <ref type="bibr" target="#b104">[104]</ref> also show that the imprecise info-gain allows reducing the size of the decision trees. Eventually, they observe that postpruning of decision trees can reduce the impact of label noise. The approach is extended for continuous features and missing data in <ref type="bibr" target="#b150">[150]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Methods</head><p>Most of the studies on label noise robustness have been presented in Section III. They show that complete label noise robustness is seldom achieved, as discussed in Section V-A. An exception is <ref type="bibr" target="#b81">[81]</ref>, where the 0-1 loss is directly optimized using a team of continuous-action learning automata: 1) a probability distribution is defined on the weights of a linear classifier; 2) weights are repetitively drawn from the distribution to classify training samples; and 3) the 0-1 losses for the training samples are used at each iteration as a rein-forcement to progressively tighten the distribution around the optimal weights. In the case of separable classes, the approach converges to the true optimal separating hyperplane, even in the case of NNAR label noise. In <ref type="bibr" target="#b151">[151]</ref>, 11 classifiers are compared on imbalanced datasets with asymmetric label noise. In all cases, the performances of the models are affected by label noise. Random forests <ref type="bibr" target="#b147">[147]</ref> are shown to be the most robust among the eleven methods, what is also the case in another study by the same authors <ref type="bibr" target="#b152">[152]</ref>. C4.5, radial basis function networks and rule-based classifiers obtain the worst results. The sensitivity of C4.5 to label noise is confirmed in <ref type="bibr" target="#b153">[153]</ref>, where multilayer perceptrons are shown to be less affected. In <ref type="bibr" target="#b135">[135]</ref>, a new artificial immune recognition system is proposed, called RWTSAIRS, which is shown to be less sensitive to label noise. In <ref type="bibr" target="#b154">[154]</ref>, two procedures based on argumentation theory are also shown to be robust to label noise. In <ref type="bibr" target="#b11">[12]</ref>, it is shown that feature extraction can help to reduce the impact of label noise. Also, Sàez et al. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b155">[155]</ref> show that using one-versus-one decomposition in multiclass problems can improve the robustness, which could be due to the distribution of the noisy examples in the subproblems, the increase of the separability of the classes, and collecting information from different classifiers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>Theoretically, common losses in machine learning are not completely robust to label noise <ref type="bibr" target="#b139">[139]</ref>. However, overfitting avoidance techniques like, e.g., regularization can be used to partially handle label noise <ref type="bibr" target="#b132">[132]</ref>- <ref type="bibr" target="#b134">[134]</ref>, even if label noise may interfere with the quality of the classifier, whose accuracy might suffer and the representation might be less compact <ref type="bibr" target="#b132">[132]</ref>. Experiments in the literature show that the performances of classifiers inferred by label noise-robust algorithms are still affected by label noise. Label noise-robust methods seem to be adequate only for simple cases of label noise that can be safely managed by overfitting avoidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DATA CLEANSING METHODS FOR LABEL NOISE-POLLUTED DATASETS</head><p>When training data are polluted by label noise, an obvious and tempting solution consists in cleansing the training data themselves, what is similar to outlier or anomaly detection. However, detecting mislabeled instances is seldom trivial: Weiss and Hirsh <ref type="bibr" target="#b156">[156]</ref> show e.g. in the context of learning with disjuncts that true exceptions may be hard to distinguish from mislabeled instances. Hence, many methods have been proposed to cleanse training sets, with different degrees of success. The whole procedure is shown by Fig. <ref type="figure" target="#fig_2">2</ref>, which is inspired by <ref type="bibr" target="#b45">[46]</ref>. This section describes several methods that detect, remove, or relabel mislabeled instances. First, simple methods based on thresholds are presented in Section VI-A. Model prediction-based filtering methods are discussed in Section VI-B, which includes classification, voting, and partition filterings. Methods based on measures of the impact of label noise and introspection are considered in Section VI-C. Section VI-D, VI-E, and VI-F address methods based on nearest neighbors, graphs and ensembles. Eventually, several other methods are discussed in Section VI-G and a general discussion about data cleansing methods is proposed in Section VI-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Measures and Thresholds</head><p>Similar to outlier detection <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref> and anomaly detection <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>, several methods in label noise cleansing are based on ad hoc measures. Instances can e.g. be removed when the anomaly measure exceeds a predefined threshold. For example, in <ref type="bibr" target="#b157">[157]</ref>, the entropy of the conditional distribution P(Y |X) is estimated using a probabilistic classifier. Instances with a low entropy correspond to confident classifications. Hence, such instances for which the classifier disagrees with the observed label are relabeled using the predicted label.</p><p>As discussed in Section III, label noise may increase the complexity of inferred models. Therefore, complexity measures can be used to detect mislabeled instances that disproportionately increase model complexity when added to the training set. In <ref type="bibr" target="#b158">[158]</ref>, the complexity measure for inductive concept learning is the number of literals in the hypothesis. A cleansing algorithm is proposed, which: 1) finds for each literal the minimal set of training samples whose removal would allow going without the literal and 2) awards one point to each sample in the minimal set. Once all literals have been reviewed, the sample with the higher score is removed, if the score is high enough. This heuristic produces less complex models. Similarly, Gamberger and Lavrač <ref type="bibr" target="#b159">[159]</ref> measure the complexity of the least complex correct hypothesis (LCCH) for a given training set. Each training set is characterized by a LCCH value and is saturated if its LCCH value is equal to the complexity of the target hypothesis. Mislabeled samples are removed to obtain a saturated training set. Gamberger et al. <ref type="bibr" target="#b160">[160]</ref>- <ref type="bibr" target="#b162">[162]</ref> elaborate on the above notions of complexity and saturation, which results in the so-called saturation filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Predictions-Based Filtering</head><p>Several data cleansing algorithms rely on the predictions of classifiers: classification, voting, and partition filterings. In <ref type="bibr" target="#b163">[163]</ref>, such methods are extended in the context of costsensitive learning, whereas Khoshgoftaar and Rebours <ref type="bibr" target="#b164">[164]</ref> propose a generic algorithms that can be specialized to classification, voting, or partition filterings by a proper choice of parameters.</p><p>1) Classification Filtering: The predictions of classifiers can be used to detect mislabeled instances, what is called classification filtering <ref type="bibr" target="#b161">[161]</ref>, <ref type="bibr" target="#b164">[164]</ref>. For example, <ref type="bibr" target="#b165">[165]</ref> learns a SVM using the training data and removes all instances that are misclassified by the SVM. A similar method is proposed in <ref type="bibr" target="#b166">[166]</ref> for neural networks. Miranda et al. <ref type="bibr" target="#b167">[167]</ref> extend the approach of <ref type="bibr" target="#b165">[165]</ref>: four classifiers are induced by different machine learning techniques and are combined by voting to detect mislabeled instances. The above methods can be applied to any classifier, but it eliminates all instances that on the wrong side of the classification boundary, what be can dangerous <ref type="bibr" target="#b168">[168]</ref>, <ref type="bibr" target="#b169">[169]</ref>. In fact, as discussed in <ref type="bibr" target="#b170">[170]</ref>, classification filtering (and data cleansing in general) suffers from a chicken-and-egg dilemma, since: 1) good classifiers are necessary for classification filtering and 2) learning in the presence of label noise may precisely produce poor classifiers. An alternative is proposed in <ref type="bibr" target="#b169">[169]</ref>, which: 1) defines a pattern as informative if it is difficult to predict by a model trained on previously seen data and 2) send a pattern to the human operator for checking if its informativeness is above a threshold found by cross-validation. Indeed, such patterns can either be atypical patterns that are actually informative or garbage patterns. The level of surprise is considered to be a good indication of how informative a pattern is, what is quantified by the information gainlog P(Y = y|X = x).</p><p>In <ref type="bibr" target="#b171">[171]</ref>, an iterative procedure called robust-C4.5 is introduced. At each iteration: 1) a decision tree is inferred and pruned by C4.5 and 2) training samples that are misclassified by the pruned decision tree are removed. The procedure is akin to regularization, in that the model is repeatedly made simpler. Indeed, each iteration removes training samples, what in turn allows C4.5 to produce smaller decision trees. Accuracy is slightly improved, whereas the mean and variance of the tree size are decreased. Hence, smaller and more stable decision trees are obtained, which also perform better. Notice that caution is advised when comparing sizes of decision trees in data cleansing <ref type="bibr" target="#b172">[172]</ref>, <ref type="bibr" target="#b173">[173]</ref>. Indeed, Oates and Jensen <ref type="bibr" target="#b172">[172]</ref> show that the size of decision trees naturally tends to increase linearly with the number of instances. It means that the removal of randomly selected training samples already leads to a decrease in tree sizes. Therefore, <ref type="bibr" target="#b172">[172]</ref> proposes the measure 100 × initial tree size-tree size with random filtering initial tree size-tree size with studied filtering <ref type="bibr" target="#b7">(8)</ref> to estimate the percentage of decrease in tree size which is simply due to a reduction in the number of samples. For example, Oates and Jensen <ref type="bibr" target="#b172">[172]</ref> shows experimentally for robust-C4.5 that 42% of the decrease in tree size can be imputed to the sole reduction in training set size, whereas the remaining 58% are due to an appropriate choice of the instances to be removed. A similar analysis could be done for other methods in this section.</p><p>Local models <ref type="bibr" target="#b174">[174]</ref> can also be used to filter mislabeled training samples. Such models are obtained by training a standard model like e.g. LDA <ref type="bibr" target="#b175">[175]</ref> or a SVM <ref type="bibr" target="#b176">[176]</ref>, <ref type="bibr" target="#b177">[177]</ref> on a training set consisting of the k nearest neighbors of the sample to be classified. Many local models have to be learnt, but the respective local training sets are very small. In <ref type="bibr" target="#b116">[116]</ref>, local SVMs are used to reject samples for which the prediction is not confident enough. In <ref type="bibr" target="#b115">[115]</ref>, the local SVM noise reduction method is extended for large datasets, by reducing the number of SVMs to be trained. In <ref type="bibr" target="#b178">[178]</ref>, a sample is removed if it is misclassified by a k nearest centroid neighbors classifier <ref type="bibr" target="#b179">[179]</ref> trained when the sample itself is removed from the training set.</p><p>2) Voting Filtering: Classification filtering faces the risk to remove too many instances. To solve this problem, ensembles of classifiers are used in <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b138">[138]</ref>, and <ref type="bibr" target="#b180">[180]</ref> to identify mislabeled instances, what is inspired by outlier removal in regression <ref type="bibr" target="#b181">[181]</ref>. The first step consists in using a K -fold cross-validation scheme, which creates K pairs of distinct training and validation datasets. For each pair of sets, m learning algorithms are used to learn m classifiers using the training set and to classify the samples in the validation set. Therefore, m classifications are obtained for each sample, since each instance belongs to exactly one validation set. The second step consists in inferring from the m predictions whether a sample is mislabeled or not, what is called voting filtering in <ref type="bibr" target="#b173">[173]</ref> or ensemble filtering in <ref type="bibr" target="#b164">[164]</ref>. Two possibilities are studied in <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b138">[138]</ref>, and <ref type="bibr" target="#b180">[180]</ref>: a majority vote and a consensus vote. Whereas majority vote classifies a sample as mislabeled if a majority of the m classifiers misclassified it, the consensus vote requires that all classifiers have misclassified the sample. One can also require high agreement of classifiers, i.e. misclassification by more than a given percentage of the classifiers <ref type="bibr" target="#b182">[182]</ref>. The consensus vote is more conservative than the majority vote and results in a few removed samples. The majority vote tends to throw out too many instances <ref type="bibr" target="#b183">[183]</ref>, but performs better than consensus vote, because keeping mislabeled instances seems to harm more than removing too many correctly labeled samples.</p><p>The K -fold cross-validation is also used in <ref type="bibr" target="#b161">[161]</ref>. For each training set, a classifier is learnt and directly filters its corresponding validation set. The approach is intermediate between <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b138">[138]</ref>, <ref type="bibr" target="#b165">[165]</ref>, <ref type="bibr" target="#b180">[180]</ref> and has been shown to be nonselective, i.e. too many samples are detected as being potentially noisy <ref type="bibr" target="#b161">[161]</ref>. Eventually, Verbaeten <ref type="bibr" target="#b173">[173]</ref> performs an experimental comparison of some of the above methods and proposes several variants. In particular, m classifiers from the same type are learnt using all combinations of the K -1 parts in the training set. Voting filters are also iterated until no more samples are removed. In <ref type="bibr" target="#b184">[184]</ref>, voting filters are obtained by generating the m classifiers using bagging: m training sets are generated by resampling and the inferred classifiers are used to classify all instances in the original training set.</p><p>3) Partition Filtering: Classification filtering is adapted for large and distributed datasets in <ref type="bibr" target="#b69">[69]</ref> and <ref type="bibr" target="#b185">[185]</ref>, which propose a partition filter. In the first step, samples are partitioned and rules are inferred for each partition. A subset of good rules are chosen for each partition using two factors that measure the classification precision and coverage for the partition. In the second step, all samples are compared to the good rules of all partitions. If a sample is not covered by a set of rules, it is not classified; otherwise, it is classified according to these rules. This mechanism allows distinguishing between the exceptions (not covered by the rules) and mislabeled instances (covered by the rules, but misclassified). Majority or consensus vote is used to detect mislabeled instances. Privacy is preserved in distributed datasets, since each site (or partition) only shares its good rules. The approach is experimentally shown to be less aggressive than <ref type="bibr" target="#b161">[161]</ref>. In <ref type="bibr" target="#b186">[186]</ref>, partitioning is repeated and several classifiers are learned for each partition. If all classifiers predict the same label that is different from the observed label, the instance is considered as potentially mislabeled. Votes are summed over all iterations and can be used to order the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Influence and Introspection</head><p>Mislabeled instances can be detected by analyzing their impact on learning. For example, Malossini et al. <ref type="bibr" target="#b52">[53]</ref> define the leave-one-out perturbed classification (LOOPC) matrix, where the (i, j ) entry is the label predicted for the j th training sample if: 1) the j th sample itself is removed from the training set and 2) the label of the i th sample is flipped. The LOOPC matrix is defined only for binary classification. Two algorithms are proposed to analyze the LOOPC matrix in search for wrong labels. The classification-stability algorithm (CLstability) analyses each column to detect suspicious samples: good samples are expected to be consistently classified even in the case of small perturbation in training data. The leaveone-out-error-sensitivity (LOOE-sensitivity) algorithm detects samples whose label flip improves the overall results of the classifier. The computation of the LOOPC matrix is expensive, but it can be afforded for small datasets. Experiments show that CL-stability dominates LOOE-sensitivity. The approach is extended in <ref type="bibr" target="#b187">[187]</ref> and <ref type="bibr" target="#b188">[188]</ref>.</p><p>Based on introspection, Heskes <ref type="bibr" target="#b64">[64]</ref> proposes an online learning algorithm for the single-unit perceptron, when labels coming from the teacher are polluted by uniform noise. The presented samples are accepted only when the confidence of the learner in the presented labeled sample is large enough. The propensity of the learner to reject suspicious labels is called the stubbornness: the learner only accepts to be taught when it does not contradict its own model too much. The stubbornness of the learner has to be tuned, since discarding too many samples may slow the learning process. An update rule is proposed for the student self-confidence: the stubbornness is increased by learner-teacher contradictions, whereas learner-teacher agreements decrease stubbornness. The update rule itself depends on the student carefulness that reflects the confidence of the learner and can be chosen to outperform any absent-minded teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. kNN-Based Methods</head><p>The kNN classifiers <ref type="bibr" target="#b189">[189]</ref>, <ref type="bibr" target="#b190">[190]</ref> are sensitive to label noise <ref type="bibr" target="#b94">[94]</ref>, <ref type="bibr" target="#b95">[95]</ref>, in particular for small neighborhood sizes <ref type="bibr" target="#b96">[96]</ref>. Hence, it is natural that several methods have emerged in the kNN literature for cleansing training sets. Among these methods, many are presented as editing methods <ref type="bibr" target="#b191">[191]</ref>, what may be a bit misleading: most of these methods do not edit instances, but rather edit the training set itself by removing instances. Such approaches are also motivated by the particular computational and memory requirements of kNN methods for prediction, which linearly depend on the size of the training set. See e.g. <ref type="bibr" target="#b192">[192]</ref> for a discussion on instance selection methods for case-based reasoning.</p><p>Wilson and Martinez <ref type="bibr" target="#b95">[95]</ref>, <ref type="bibr" target="#b193">[193]</ref> provide a survey of kNN-based methods for data cleansing, propose several new methods and perform experimental comparisons. Wilson and Martinez <ref type="bibr" target="#b95">[95]</ref> show that mislabeled training instances degrade the performances of both the kNN classifiers built on the full training set and the instance selection methods that are not designed to take care of label noise. This section presents solutions from the literature and is partially based on <ref type="bibr" target="#b95">[95]</ref> and <ref type="bibr" target="#b193">[193]</ref>. See e.g. <ref type="bibr" target="#b194">[194]</ref> for a comparison of several instance-based noise reduction methods.</p><p>kNN-based instance selection methods are mainly based on heuristics. For example, the condensed nearest neighbors (CNN) rule <ref type="bibr" target="#b195">[195]</ref> builds a subset of training instances that allows classifying correctly all other training instances. However, such a heuristic systematically keeps mislabeled instances in the training set. There exist other heuristics that are more robust to label noise. For example, the reduced nearest neighbors (RNN) rule <ref type="bibr" target="#b196">[196]</ref> successively removes instances whose removal do not cause other instances to be misclassified, i.e. it removes noisy and internal instances. The blame-based noise reduction (BBNR) algorithm <ref type="bibr" target="#b197">[197]</ref> removes all instances that contribute to the misclassification of another instance and whose removal does not cause any instance to be misclassified. In <ref type="bibr" target="#b198">[198]</ref> and <ref type="bibr" target="#b199">[199]</ref>, instances are ranked based on a score rewarding the patterns that contribute to a correct classification and punishing those that provide a wrong one. An important danger of instance selection is to remove too many instances <ref type="bibr" target="#b200">[200]</ref>, if not all instances in some pathological cases <ref type="bibr" target="#b95">[95]</ref>.</p><p>More complex heuristics exist in the literature; see e.g. <ref type="bibr" target="#b113">[113]</ref> and <ref type="bibr" target="#b201">[201]</ref> for an experimental comparison for gene expression data. For example, Wilson <ref type="bibr" target="#b202">[202]</ref> removes instances whose label is different from the majority label in its k = 3 nearest neighbors. This method is extended in <ref type="bibr" target="#b203">[203]</ref> by the all-k nearest neighbors method. In <ref type="bibr" target="#b95">[95]</ref> and <ref type="bibr" target="#b193">[193]</ref>, six heuristics are introduced and compared with other methods: DROP1-6. For example, DROP2 is designed to reduce label noise using the notion of instance associates, which have the instance itself in their k nearest neighbors. DROP2 removes an instance if its removal does not change the number of its associates that are incorrectly classified in the original training set. This algorithm tends to retain instances that are close to the classification boundary. In <ref type="bibr" target="#b200">[200]</ref>, generalized edition (GE) checks whether there are at least k samples in the locally majority class among the k neighbors of an instance. In such a case, the instance is relabeled with the locally majority label, otherwise it is simply removed from the training set. This heuristic aims at keeping only instances with strong support for their label. Barandela and Gasca <ref type="bibr" target="#b28">[29]</ref> show that a few repeated applications of the GE algorithm improves results in the presence of label noise.</p><p>Other instance selection methods designed to deal with label noise include e.g. IB3, which employs a significance test to determine which instances are good classifiers and which ones are believed to be noisy <ref type="bibr" target="#b204">[204]</ref>, <ref type="bibr" target="#b205">[205]</ref>. Lorena et al. <ref type="bibr" target="#b206">[206]</ref> propose to use Tomek links <ref type="bibr" target="#b207">[207]</ref> to filter noisy instances for splice junction recognition. Different instance selection methods are compared in <ref type="bibr" target="#b114">[114]</ref>. In <ref type="bibr" target="#b192">[192]</ref>, a set of instances are selected using Fisher discriminant analysis, while maximizing the diversity of the reduced training set. The approach is shown to be robust to label noise for a simple artificial example. In <ref type="bibr" target="#b208">[208]</ref>, different heuristics are used to distinguish three types of training instances: normal instances, border samples and instances that should be misclassified (ISM). ISM instances are such that, based on the information in the dataset, the label assigned by the learning algorithm is the most appropriate even though it is incorrect. For example, one of the heuristics uses a nearest neighbors approach to estimate the hardness of a training sample, i.e. how hard it is to classify correctly. ISM instances are simply removed, what results in the so-called PRISM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Graph-Based Methods</head><p>Several methods in the data cleansing literature are similar to kNN-based editing methods, except that they represent training sets by neighborhood graphs <ref type="bibr" target="#b209">[209]</ref> where the instances (or nodes) are linked to other close instances. The edge between the two instances can be weighted depending on the distance between them. Such methods work directly on the graphs to detect noisy instances. For example, Sánchez et al. <ref type="bibr" target="#b94">[94]</ref> propose variants of kNN-based algorithms which use Gabriel graphs and relative neighborhood graphs <ref type="bibr" target="#b210">[210]</ref>, <ref type="bibr" target="#b211">[211]</ref>. In <ref type="bibr" target="#b212">[212]</ref> and <ref type="bibr" target="#b213">[213]</ref>, mode filters, which preserve edges and remove impulsive noise in images, are extended to remove label noise in datasets represented by a graph. In <ref type="bibr" target="#b209">[209]</ref> and <ref type="bibr" target="#b214">[214]</ref>, the i th instance is characterized by its local cut edge weight statistic J i , which is the sum of the weights of edges linking the instance to its neighbors with a different label. Three types of instances are distinguished: good samples with a small J i , doubtful samples with an intermediate J i , and bad samples with a large J i . Two filtering policies are considered: 1) to relabel doubtful samples and to remove bad samples or 2) to relabel doubtful and bad samples using the majority class in good neighbors (if any) and to remove doubtful and bad samples which have no good neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ensemble and Boosting-Based Methods</head><p>As discussed in Section III-A.2, AdaBoost is well known to overfit noisy datasets. Indeed, the weights of mislabeled instances tend to become much larger than the weights of normal instances in the late iterations of AdaBoost. Several works presented below show that this propensity to overfitting can be exploited to remove label noise.</p><p>A simple data cleansing method is proposed in <ref type="bibr" target="#b184">[184]</ref>, which removes a given percentage of the samples with the highest weights after m iterations of AdaBoost. Experiments show that the precision of this boosting-based algorithm is not very good, what is attributed to the dynamics of Adaboost. In the first iterations, mislabeled instances quickly obtain large weights and are correctly spotted as mislabeled. However, therefore, several correctly labeled instances then obtain large weights in late iterations, what explains that they are incorrectly removed from the training set by the boosting filter.</p><p>A similar approach is pursued in <ref type="bibr" target="#b215">[215]</ref>. Outlier removal boosting (ORBoost) is identical to AdaBoost, except that instance weights which are above a certain threshold are set to zero during boosting. Hence, data cleansing is performed while learning and not after learning as in <ref type="bibr" target="#b184">[184]</ref>. ORBoost is sensitive to the choice of the threshold, which is performed using validation. In <ref type="bibr" target="#b216">[216]</ref>, mislabeled instances are also removed during learning if they are misclassified by the ensemble with high confidence.</p><p>In <ref type="bibr" target="#b217">[217]</ref>, edge analysis is used to detect mislabeled instances. The edge of an instance is defined as the sum of the weights of weak classifiers that misclassified the instance <ref type="bibr" target="#b218">[218]</ref>. Hence, an instance with a large edge is often misclassified by the weak learners and is classified by the ensemble with a low confidence, what is the contrary of the margin defined in <ref type="bibr" target="#b106">[106]</ref>. Wheway <ref type="bibr" target="#b217">[217]</ref> observes a homogenization of the edge as the number of weak classifiers increases: the mean of the edge stabilizes and its variance goes to zero. It means that observations which were initially classified correctly are classified incorrectly in later rounds to classify harder observations correctly, what is consistent with results in <ref type="bibr" target="#b106">[106]</ref> and <ref type="bibr" target="#b218">[218]</ref>. Mislabeled data have edge values which remain high due to persistent misclassification. It is therefore proposed to remove the instances corresponding e.g. to the 5% top edge values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Others Methods</head><p>There exist other methods for data cleansing. For example, in ECG segmentation, Hughes et al. <ref type="bibr" target="#b55">[56]</ref> delete the label of the instances (and not the instances themselves) that are close to classification boundaries, since experts are known to be less reliable in that region. Thereafter, semisupervised learning is performed using both the labeled and the (newly) unlabeled instances. In <ref type="bibr" target="#b219">[219]</ref>, a genetic algorithm approach based on a class separability criterion is proposed. In <ref type="bibr" target="#b220">[220]</ref> and <ref type="bibr" target="#b221">[221]</ref>, the automatic data enhancement (ADE) method and the automatic noise reduction (ANR) method are proposed to relabel mislabeled instances with a neural network approach. A similar approach is proposed in <ref type="bibr" target="#b222">[222]</ref> for decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Discussion</head><p>One of the advantages of label noise cleansing is that removed instances have absolutely no effects on the model inference step <ref type="bibr" target="#b158">[158]</ref>. In several works, it has been observed that simply removing mislabeled instances is more efficient than relabeling them <ref type="bibr" target="#b167">[167]</ref>, <ref type="bibr" target="#b223">[223]</ref>. However, instance selection methods may remove too many instances <ref type="bibr" target="#b132">[132]</ref>- <ref type="bibr" target="#b134">[134]</ref>, <ref type="bibr" target="#b200">[200]</ref>, if not all instances in some pathological cases <ref type="bibr" target="#b95">[95]</ref>. On the one hand, Matic et al. <ref type="bibr" target="#b168">[168]</ref> show that overcleansing may reduce the performances of classifiers. On the other hand, it is suggested in <ref type="bibr" target="#b45">[46]</ref> that keeping mislabeled instances may harm more than removing too many correctly labeled samples. Therefore, a compromise has to be found. The overcleansing problem is of particular importance for imbalanced datasets <ref type="bibr" target="#b224">[224]</ref>. Indeed, minority instances may be more likely to be removed by e.g. classification filtering (because they are also more likely to be misclassified), what makes learning even more difficult. In <ref type="bibr" target="#b225">[225]</ref>, it is shown that dataset imbalance can affect the efficiency of data cleansing methods. Label noise cleansing can also reduce the complexity of inferred models, but it is not always trivial to know if this reduction is not simply due to the reduction of the training set size <ref type="bibr" target="#b172">[172]</ref>, <ref type="bibr" target="#b173">[173]</ref>.</p><p>Surprisingly, to the best of our knowledge, the method in <ref type="bibr" target="#b55">[56]</ref> has not been generalized to other label noise cleansing methods, what would be easy to do. Indeed, instead of completely removing suspicious instances, one could only delete their labels and perform semi-supervised learning on the resulting training set. The approach in <ref type="bibr" target="#b55">[56]</ref> has the advantage of keeping the distribution of the instances unaltered (what is not the case for their conditional distributions, though), what is of particular interest for generative approaches. An interesting open research question is whether this method would improve the results with respect to the classical solution of simply removing suspicious instances. Another alternative would be to resubmit the suspicious samples to a human expert for relabeling as proposed in <ref type="bibr" target="#b168">[168]</ref>. However, this may reveal too costly or even impossible in most of the applications, and there is no guarantee that the new labels will actually be noise free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. LABEL NOISE-TOLERANT LEARNING ALGORITHMS</head><p>When some information is available about label noise or its consequences on learning, it becomes possible to design models that consider label noise. Typically, one can learn a label noise model simultaneously with a classifier, what uncouples both components of the data generation process and improves the resulting classifier. In a nutshell, the resulting classifier learns to classify instances according to their true, unknown class. Other approaches consist in modifying the learning algorithm to reduce the influence of label noise. Data cleansing can also be embedded directly into the learning algorithm, like e.g. for SVMs. Such techniques are described in this section and are called label noise-tolerant, since they can tolerate label noise by modeling it. Section VII-A reviews probabilistic methods, whereas model-based methods are discussed in Section VII-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Probabilistic Methods</head><p>Many label noise-tolerant methods are probabilistic, in a broad sense. They include Bayesian and frequentist methods, as well as methods based on clustering or belief functions. An important issue that is highlighted by these methods is the identifiability of label noise. The four families of methods are discussed in the following four sections.</p><p>1) Bayesian Approaches: Detecting mislabeled instances is a challenging problem. Indeed, there are identifiability issues <ref type="bibr" target="#b226">[226]</ref>- <ref type="bibr" target="#b228">[228]</ref>, as illustrated in <ref type="bibr" target="#b122">[122]</ref>, where consumers answer a survey with some error probability. Under the assumption that it results in a Bernoulli process, it is possible to obtain an infinite number of maximum likelihood solutions for the true proportions of answers and the error probabilities. In other words, in this simple example, it is impossible to identify the correct model for observed data. Several works claim that prior information is strictly necessary to deal with label noise. In particular, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b122">[122]</ref>, <ref type="bibr" target="#b227">[227]</ref>, and <ref type="bibr" target="#b228">[228]</ref> propose to use Bayesian priors on the mislabeling probabilities to break ties. Label noise identifiability is also considered for inductive logic programming in <ref type="bibr" target="#b226">[226]</ref>, where a minimal description length principle prevents the model to overfit on label noise.</p><p>Several Bayesian methods to take care of label noise are reviewed in <ref type="bibr" target="#b68">[68]</ref> and summarized here. In medical applications, it is often necessary to assess the quality of binary diagnosis tests with label noise. Three parameters must be estimated: the population prevalence (i.e., the true proportion of positive samples) and the sensitivity and specificity of the test itself <ref type="bibr" target="#b4">[5]</ref>. Hence, the problem has one degree of freedom in excess, since only two data-driven constraints can be obtained (linked to the observed proportions of positive and negative samples). In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b229">[229]</ref> and <ref type="bibr" target="#b230">[230]</ref>, it is proposed to fix the degree of freedom using a Bayesian approach: setting a prior on the model parameters disambiguates maximum likelihood solutions. Indeed, whereas the frequentist approach considers that parameters have fixed values, the Bayesian approach considers that all unknown parameters have a probability distribution that reflects the uncertainty in their values and that prior knowledge about unknown parameters can be formally included <ref type="bibr" target="#b231">[231]</ref>. Hence, the Bayesian approach can be interpreted as a generalization of constraints on the parameters values, where the uncertainty on the parameters is considered through priors.</p><p>Popular choices for Bayesian priors for label noise are Beta priors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b128">[128]</ref>, <ref type="bibr" target="#b229">[229]</ref>, <ref type="bibr" target="#b230">[230]</ref>, <ref type="bibr" target="#b232">[232]</ref>- <ref type="bibr" target="#b236">[236]</ref> and Dirichlet priors <ref type="bibr" target="#b237">[237]</ref>, <ref type="bibr" target="#b238">[238]</ref>, which are the conjugate priors of binomial and multinomial distributions, respectively. Bayesian methods have also been designed for logistic regression <ref type="bibr" target="#b130">[130]</ref>, <ref type="bibr" target="#b236">[236]</ref>, <ref type="bibr" target="#b239">[239]</ref>- <ref type="bibr" target="#b241">[241]</ref>, hidden Markov models <ref type="bibr" target="#b84">[84]</ref>, and graphical models for medical image segmentation <ref type="bibr" target="#b242">[242]</ref>. In the Bayesian approaches, although the posterior distribution of parameters may be difficult (or impossible) to calculate directly, efficient implementations are possible using Markov chain Monte Carlo methods, which allow approximating the posterior of model parameters <ref type="bibr" target="#b68">[68]</ref>. A major advantage of using priors is the ability to include any kind of prior information in the learning process <ref type="bibr" target="#b68">[68]</ref>. However, the priors should be chosen carefully, for the results obtained depend on the quality of the prior distribution used <ref type="bibr" target="#b243">[243]</ref>, <ref type="bibr" target="#b244">[244]</ref>.</p><p>In the spirit of the above Bayesian approaches, an iterative procedure is proposed in <ref type="bibr" target="#b128">[128]</ref> to correct labels. For each sample, Rekaya et al. <ref type="bibr" target="#b235">[235]</ref> define an indicator variable α i , which is equal to 1 if the label of the i th instance was switched. Hence, each indicator follows a Bernoulli distribution parameterized by the mislabeling rate (which itself follows a Beta prior). In <ref type="bibr" target="#b128">[128]</ref>, the probability that α i = 1 is estimated for each sample and the sample with the higher mislabeling probability is relabeled. The procedure is repeated as long as the test is significant. Indicators are also used in <ref type="bibr" target="#b245">[245]</ref> for Alzheimer disease prediction, where four out of 16 patients are detected as potentially misdiagnosed. The correction of the supposedly incorrect labels leads to a significant increase in predictive ability. A similar approach is used in <ref type="bibr" target="#b246">[246]</ref> to robustify multiclass Gaussian process classification. If the indicator for a given sample is zero, then the label of that sample is assumed to correspond to a latent function. Otherwise, the label is assumed to be randomly chosen. The same priors as in <ref type="bibr" target="#b235">[235]</ref> are used and the approach is shown to yield better results than other methods that assume that the latent function is polluted by a random Gaussian noise <ref type="bibr" target="#b247">[247]</ref> or which use Gaussian processes with heavier tails <ref type="bibr" target="#b248">[248]</ref>.</p><p>2) Frequentist Methods: Since label noise is an inherently stochastic process, several frequentist methods have emerged to deal with it. A simple solution consists in using mixture models, which are popular in outlier detection <ref type="bibr" target="#b31">[32]</ref>. In <ref type="bibr" target="#b249">[249]</ref>, each sample is assumed to be generated either from a majority (or normal) distribution or an anomalous distribution, with respective priors 1 -λ and λ. The expert error probability λ is assumed to be relatively small. Depending on prior knowledge, any appropriate distribution can be used to model the majority and anomalous distributions, but the anomalous distribution may be simply chosen as uniform. The set of anomalous samples is initially empty, i.e. all samples initially belong to the majority set. Samples are successively tested and added to the anomalous set whenever the increase in log-likelihood due to this operation is higher than a predefined threshold. Mansour and Parnas <ref type="bibr" target="#b250">[250]</ref> also consider the mixture model and propose an algorithm to learn conjunctions of literals.</p><p>Directly linked with the definition of NAR label noise in Section II-C, Lawrence and Schölkopf <ref type="bibr" target="#b67">[67]</ref> propose another probabilistic approach to label noise. The label of an instance is assumed to correspond to two random variables (Fig. <ref type="figure" target="#fig_3">3</ref>, inspired by <ref type="bibr" target="#b67">[67]</ref>): the true hidden label Y and the observed label Ỹ that is possibly noisy. Ỹ is assumed to depend only on the true label Y , whose relationship is described by a labeling matrix (see Section II-C.2). Using this simple model of label noise, a Fisher discriminant is learned using an EM approach. Eventually, the approach is kernelised and is shown to effectively deal with label noise. Interestingly, the probabilistic modeling also leads to an estimation of the noise level. Later, Li et al. <ref type="bibr" target="#b251">[251]</ref> extended this model by relaxing the Gaussian distribution assumption and carried out extensive experiments on more complex datasets, which convincingly demonstrated the value of explicit label noise modeling. More recently, the same model has been extended to multiclass datasets <ref type="bibr" target="#b252">[252]</ref> and sequential data <ref type="bibr" target="#b253">[253]</ref>. Asymmetric label noise is also considered in <ref type="bibr" target="#b66">[66]</ref> for logistic regression. It is shown that conditional probabilities are altered by label noise and that this problem can be solved by considering a model of label noise. A similar approach was developed for neural networks in <ref type="bibr" target="#b254">[254]</ref> and <ref type="bibr" target="#b255">[255]</ref> for uniform label noise. Repeatedly, a neural network is trained to predict the conditional probability of each class, what allows optimizing the mislabeling probability before retraining the neural network. The mislabeling probability is optimized either using a validation set <ref type="bibr" target="#b254">[254]</ref> or a Bayesian approach with a uniform prior <ref type="bibr" target="#b255">[255]</ref>. In <ref type="bibr" target="#b256">[256]</ref>, Gaussian processes for classification are also adapted for label noise by assuming that each label is potentially affected by a uniform label noise. It is shown that label noise modeling increases the likelihood of observed labels when label noise is actually present.</p><p>Valizadegan and Tan <ref type="bibr" target="#b257">[257]</ref> propose a method based on a weighted kNN. Given the probability p i that the i th training example is mislabeled, the binary label y i is replaced by its expected valuep i y i +(1p i )y i = (1-2 p i )y i . Then, the sum of the consistencies</p><formula xml:id="formula_9">δ i = (1 -2 p i )y i j ∈N(x i ) w i j (1 -2 p j )y j j ∈N(x i ) w i j (9)</formula><p>between the expected value of y i and the expected value of the weighted kNN prediction is maximized, where N(x i ) contains the neighbors of x i and w i j is the weight of the j th neighbor.</p><p>To avoid declaring all the examples from one of the two classes as mislabeled, a L1 regularization is enforced on the probabilities p i . Contrarily to the methods described in Section VII-A.1, Bayesian priors are not used in the above frequentist methods. We hypothesize that the identifiability problem discussed in Section VII-A.1 is solved using a generative approach and setting constraints on the conditional distribution of X. For example, in <ref type="bibr" target="#b67">[67]</ref>, Gaussian distributions are used, whereas Li et al. <ref type="bibr" target="#b251">[251]</ref> consider mixtures of Gaussian distributions. The same remark applies to Section VII-A. <ref type="bibr" target="#b2">3</ref>.</p><p>3) Clustering-Based Methods: In the generative statistical models of Section VII-A.2, it is assumed that the distribution of instances can help to solve classification problems. Classes are not arbitrary: they are linked to a latent structure in the distribution of X. In other words, clusters in instances can be used to build classifiers, what is done in <ref type="bibr" target="#b136">[136]</ref>. First, a clustering of the instances <ref type="bibr" target="#b258">[258]</ref> is performed using an unsupervised algorithm. Labels are not used and the procedure results in a mixture of K models p k (x) with priors π k for components k = 1 . . . K . Second, instances are assumed to follow the density</p><formula xml:id="formula_10">p(x) = y∈Y K k=1 r yk π k p k (x)<label>(10)</label></formula><p>where r yk can be interpreted as the probability that the kth cluster belongs to the yth class. The coefficients r yk are learned using a maximum likelihood approach. Eventually, classification is performed by computing the conditional probabilities P(Y = y|X = x) using both the unsupervised (clusters) and supervised (r yk probabilities) parts of the model. When a Gaussian mixture model is used to perform clustering, the mixture model can be interpreted as a generalization of mixture discriminant analysis (MDA, see <ref type="bibr" target="#b259">[259]</ref>). In this case, the model is called robust MDA and is shown to improve classification results with respect to MDA <ref type="bibr" target="#b136">[136]</ref>, <ref type="bibr" target="#b260">[260]</ref>. In <ref type="bibr" target="#b261">[261]</ref>, the method is adapted to discrete data for DNA barcoding and is called robust discrete discriminant analysis. In that case, data are modeled by a multivariate multinomial distribution. A clustering approach is also used in <ref type="bibr" target="#b262">[262]</ref> to estimate a confidence on each label, where each instance inherits the distribution of classes within its assigned cluster. Confidences are averaged over several clusterings and a weighted training set are obtained. In this spirit, El Gayar et al. <ref type="bibr" target="#b263">[263]</ref> propose a method that is similar to <ref type="bibr" target="#b136">[136]</ref>. Labels are converted into soft labels to reflect the uncertainty on labels. First, a fuzzy clustering of the training instances is performed, which gives a set of cluster and the membership of each instance to each cluster. Then, the membership L yk of the kth cluster to the yth class is estimated using the fuzzy memberships. Each instance with label y increases the membership L yk by its own membership to cluster k. Eventually, the fuzzy label of each instance is computed using the class memberships of the clusters where the instance belongs. Experiments show improvements with respect to other label fuzzification methods like kNN soft labels and Keller soft labels <ref type="bibr" target="#b264">[264]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Belief Functions:</head><p>In the belief function theory, each possible subset of classes is characterized by a belief mass, which is the amount of evidence which supports the subset of classes <ref type="bibr" target="#b265">[265]</ref>. For example, let us consider an expert who: 1) thinks that a given case is positive, but 2) has a very low confidence in its own prediction. In the formalism of belief functions, one can translate the above judgment by a belief mass function (BMF), also called basic probability assignment m such that m({-1, +1}) = 0.8, m({-1}) = 0, and m({+1}) = 0.2. Here, there is no objective uncertainty on the class itself, but rather a subjective uncertainty on the judgment itself. For example, if a coin is flipped, the BMF would simply be m({head, tail}) = 1, m({head}) = 0, and m({tail}) = 0 when the bias of the coin is unknown. If the coin is known to be unbiased, the BMF becomes m({head, tail}) = 0, m({head}) = 1/2, and m({tail}) = 1/2. Again, this simple example shows how the belief function theory allows distinguishing subjective uncertainty from objective uncertainty. Notice that Smets <ref type="bibr" target="#b266">[266]</ref> argues that it is necessary to fall back to classical probabilities to make decisions. Different decision rules are analyzed in <ref type="bibr" target="#b79">[79]</ref>. Interestingly, the belief function formalism can be used to modify standard machine learning methods like e.g. kNN classifiers <ref type="bibr" target="#b78">[78]</ref>, neural networks <ref type="bibr" target="#b80">[80]</ref>, decision trees <ref type="bibr" target="#b267">[267]</ref>, mixture models <ref type="bibr" target="#b268">[268]</ref>, <ref type="bibr" target="#b269">[269]</ref>, or boosting <ref type="bibr" target="#b270">[270]</ref>.</p><p>In the context of this paper, belief functions cannot be used directly, since the belief masses are not available. Indeed, they are typically provided by the expert itself as an attempt to quantify its own (lack of) confidence, but we made the hypothesis in Section I that such information is not available. However, several works have proposed heuristics to infer belief masses directly from data <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b271">[271]</ref>.</p><p>In <ref type="bibr" target="#b78">[78]</ref>, a kNN approach based on Dempster-Shafer theory is proposed. If a new sample x s has to be classified, each training sample (x i , y i ) is considered as an evidence that the class of x s is y i . The evidence is represented by a BMF m s,i such that m s,i ({y i }) = α, m s,i (Y) = 1 -α and m s,i is zero for all other subsets of classes, where α = α 0 (d s,i ) <ref type="bibr" target="#b10">(11)</ref> such that 0 &lt; α 0 &lt; 1 and is a monotonically decreasing function of the distance d s,i between both instances. There are many possible choices for ;</p><formula xml:id="formula_11">(d) = exp -γ d β (12)</formula><p>is chosen in <ref type="bibr" target="#b78">[78]</ref>, where γ &gt; 0 and β ∈ {1, 2}. Heuristics are proposed to select proper values of α 0 and γ . For the classification of the new sample x s , each training sample provides an evidence. These evidences are combined using the Dempster rule and it becomes possible to take a decision (or to refuse to take a decision if the uncertainty is too high). The case of mislabeling is experimentally studied in <ref type="bibr" target="#b78">[78]</ref> and <ref type="bibr" target="#b272">[272]</ref> and the approach is extended to neural networks in <ref type="bibr" target="#b80">[80]</ref>.</p><p>In <ref type="bibr" target="#b271">[271]</ref>, a kNN approach is also used to infer BMFs. For a given training sample, the frequency of each class in its k nearest neighbors is computed. Then, the sample is assigned to a subset of classes containing: 1) the class with the maximum frequency and 2) the classes whose frequency is not too different from the maximum frequency. A neural network is used to compute beliefs for test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model-Based Methods</head><p>Apart from probabilistic methods, specific strategies have been developed to obtain label noise-tolerant variants of popular learning algorithms, including e.g. SVMs, neural networks, and decision trees. Many publications also propose label noise-tolerant boosting algorithms, since boosting techniques like AdaBoost are well known to be sensitive to label noise. Eventually, label noise is also tackled in semisupervised learning. These five families of methods are discussed in the following five sections.</p><p>1) SVMs and Robust Losses: SVMs are not robust to label noise <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b82">[82]</ref>, even if instances are allowed to be misclassified during learning. Indeed, instances which are misclassified during learning are penalized in the objective using the hinge loss</p><formula xml:id="formula_12">[1 -y i x i , w ] +<label>(13)</label></formula><p>where [z] + = max(0, z) and w is the weight vector. The hinge loss increases linearly with the distance to the classification boundary and is therefore significantly affected by mislabeled instance that stand far from the boundary. Data cleansing can be directly implemented into the learning algorithm of SVMs. For example, instance that correspond to a very large dual weights can be identified as potentially mislabeled <ref type="bibr" target="#b273">[273]</ref>. In <ref type="bibr" target="#b274">[274]</ref>, k samples are allowed to be not considered in the objective function. For each sample, a binary variable (indicating whether or not to consider the sample) is added and the sum of the indicators is constrained to be equal to k. An opposite approach is proposed in <ref type="bibr" target="#b275">[275]</ref> for aggregated training sets that consists of several distinct training subsets labeled by different experts. The percentage of support vectors in training samples is constrained to be identical in each subset, to decrease the influence of low-quality teachers, which tend to require more support vectors due to more frequent mislabeling. In <ref type="bibr" target="#b276">[276]</ref> and <ref type="bibr" target="#b277">[277]</ref>, SVMs are adapted by weighting the contribution of each training sample in the objective function. The weights (or fuzzy memberships) are computed using heuristics. Similar work is done in <ref type="bibr" target="#b278">[278]</ref> for relevance vector machines. Empathetic constraints SVMs <ref type="bibr" target="#b279">[279]</ref> relax the constraints of suspicious samples in the SVM optimization problem.</p><p>Xu et al. <ref type="bibr" target="#b280">[280]</ref> propose a different approach, which consists in using the loss</p><formula xml:id="formula_13">η i [1 -y i x i , w ] + + (1 -η i ) (<label>14</label></formula><formula xml:id="formula_14">)</formula><p>where 0 ≤ η i ≤ 1 indicates whether the i th sample is an outlier. The η i variables must be optimized together with the weights vector, what is shown to be equivalent to using the robust hinge loss min(1, [1y i x i , w ] + ). <ref type="bibr" target="#b14">(15)</ref> Notice that there exist other bounded, nonconvex losses <ref type="bibr" target="#b281">[281]</ref>- <ref type="bibr" target="#b284">[284]</ref>, which could be used similarly. A nonconvex loss is also used in <ref type="bibr" target="#b285">[285]</ref> to produce label noise-tolerant SVMs without filtering. For binary classification with y ∈ {-1, +1}, the loss is</p><formula xml:id="formula_15">K p e (1 -p e (-y i )) [1 -y i x i , w ] + -p e (y i ) [1 + y i x i , w ] +<label>(16)</label></formula><p>where K p e = 1/1p e (+1)p e (-1). Interestingly, the expected value of the proposed loss (with respect to all possible mislabelings of the noise-free training set) is equal to the hinge loss computed on the noise-free training set. In other words, it is possible to estimate the noise-free [. . .] errors from the noisy data. Theoretical guarantees are given and the proposed approach is shown to outperform SVMs, but error probabilities must be known a priori.</p><p>2) Neural Networks: Different label noise-tolerant variants of the perceptron algorithm are reviewed and compared experimentally in <ref type="bibr" target="#b286">[286]</ref>. In the standard version of this algorithm, samples are presented repeatedly (on-line) to the classifier. If a sample is misclassified with</p><formula xml:id="formula_16">y i [wx i + b] &lt; 0 (<label>17</label></formula><formula xml:id="formula_17">)</formula><p>where w is the weight vector and b is the bias, then the weight vector is adjusted toward this sample. Eventually, the perceptron algorithm converges to a solution.</p><p>Since the solution of the perceptron algorithm can be biased by mislabeled samples, different variants have been designed to reduce the impact of mislabeling. With the λ-trick <ref type="bibr" target="#b287">[287]</ref>, <ref type="bibr" target="#b288">[288]</ref>, if an instance has already been misclassified, the adaptation criterion becomes</p><formula xml:id="formula_18">y i [wx i + b] + λ x i 2 2 &lt; 0.</formula><p>Large values of λ may prevent mislabeled instances to trigger updates. Another heuristic is the α-bound <ref type="bibr" target="#b289">[289]</ref>, which does not update w for samples that have already been misclassified α times. This simple solution limits the impact of mislabeled instances. Although not directly designed to deal with mislabeling, Khardon and Wachman <ref type="bibr" target="#b286">[286]</ref> also describe the perceptron algorithm using margins (PAM) <ref type="bibr" target="#b290">[290]</ref>. PAM updates w for instances with y i [wx i + b] &lt; τ, similarly to support vector classifiers and to the λ-trick.</p><p>3) Decision Trees: Decision trees can easily overfit data, if they are not pruned. In fact, learning decision trees involves a tradeoff between the accuracy and simplicity, which are two requirements for good decision trees in real-world situations <ref type="bibr" target="#b291">[291]</ref>. It is particularly important to balance this tradeoff in the presence of label noise, what makes the overfitting problem worse. For example, Clark and Niblett <ref type="bibr" target="#b291">[291]</ref> propose the CN2 algorithm, which learns a disjunction of logic rules while avoiding too complex ones.</p><p>4) Boosting Methods: In boosting, an ensemble of weak learners h t with weights α t is formed iteratively using a weighted training set. At each step t, the weights w (t ) i of misclassified instances are increased (respectively, decreased for correctly classified samples), what progressively reduces the ensemble training error because the next weak learners focus on the errors of the previous ones. As discussed in Section III, boosting methods tend to overfit label noise. In particular, AdaBoost obtains large weights for mislabeled instances in late stages of learning. Hence, several methods propose to update weights more carefully to reduce the sensitivity of boosting to label noise. In <ref type="bibr" target="#b292">[292]</ref>, MadaBoost imposes an upper bound for each instance weight, which is simply equal to the initial value of that weight. The AveBoost and AveBoost2 <ref type="bibr" target="#b293">[293]</ref>, <ref type="bibr" target="#b294">[294]</ref> algorithms replace the weight w (t +1) i of the i th instance at step t + 1 by</p><formula xml:id="formula_19">tw (t ) i + w (t +1) i t + 1 . (<label>18</label></formula><formula xml:id="formula_20">)</formula><p>With respect to AdaBoost, AveBoost2 obtains larger training errors, but smaller generalization errors. In other words, AveBoost2 is less prone to overfitting than AdaBoost, what improves results in the presence of label noise. Kim <ref type="bibr" target="#b295">[295]</ref> proposes another ensemble method called Averaged Boosting (A-Boost), which: 1) does not consider instances weights to compute the weights of the successive weak classifiers and 2) performs similarly to bagging on noisy data. Other weighting procedures have been proposed in <ref type="bibr" target="#b296">[296]</ref>, but they were not assessed in the presence of label noise.</p><p>In <ref type="bibr" target="#b297">[297]</ref>, two approaches are proposed to reduce the consequences of label noise in boosting. First, AdaBoost can be early-stopped: limiting the number of iterations prevents AdaBoost from overfitting. A second approach consists in smoothing the results of AdaBoost. The proposed BB algorithm combines bagging and boosting: 1) K training sets consisting of ρ percents of the training set (subsampled with replacement) are created; 2) K boosted classifiers are trained for M iterations; and 3) the predictions are aggregated. In <ref type="bibr" target="#b297">[297]</ref>, it is advised to use K = 15, M = 15, and ρ = 1/2. The BB algorithm is shown to be less sensitive to label noise than AdaBoost. A similar approach is proposed in <ref type="bibr" target="#b298">[298]</ref>: the multiple boosting (MB) algorithm.</p><p>A reverse boosting algorithm is proposed in <ref type="bibr" target="#b299">[299]</ref>. In adaptive boosting, weak learners may have difficulties to obtain good separation frontiers because correctly classified samples get lower and lower weights as learning goes on. Hence, safe, noisy, and borderline patterns are distinguished, whose weights are respectively increased, decreased and unaltered during boosting. Samples are classified into these three categories using parallel perceptrons, a specific type of committee machine whose margin allows to separate the input space into three regions: a safe (beyond the margin), a noisy (before the margin), and a borderline regions (inside the margin). The approach improves the results of parallel perceptrons in the presence of label noise, but is most often dominated by classical perceptrons.</p><p>5) Semisupervised Learning: In <ref type="bibr" target="#b6">[7]</ref>, a particle competitionbased algorithm is proposed to perform semisupervised learning in the presence of label noise. First, the dataset is converted into a graph, where instances are nodes with edges between the similar instances. Each labeled node is associated with a labeled particle. Particles walk through the graph and cooperate with identically labeled particles to label unlabeled instances, while staying in the neighborhood of their home node. What interests us in <ref type="bibr" target="#b6">[7]</ref> is the behavior of mislabeled particles: they are pushed away by the particles of near instances with different labels, what prevents a mislabeled instance to influence the label of close unlabeled instances. In <ref type="bibr" target="#b300">[300]</ref>, unlabeled instances are first labeled using a semisupervised learning algorithm, and then the new labels are used to filter instances. Similarly, context-sensitive semisupervised SVMs <ref type="bibr" target="#b301">[301]</ref>, <ref type="bibr" target="#b302">[302]</ref> first use labeled instances to label unlabeled instances that are spatially close (e.g. in images) to them and second these new semilabels are used to reduce the effect of mislabeled training instances. Other works on label noise for semisupervised learning include <ref type="bibr" target="#b303">[303]</ref> or <ref type="bibr" target="#b304">[304]</ref>- <ref type="bibr" target="#b306">[306]</ref>, which are particular because they model the label noise induced by the labeling of unlabeled samples. A similar problem occur in <ref type="bibr" target="#b307">[307]</ref>- <ref type="bibr" target="#b309">[309]</ref> where two different views are available for each instance, like e.g. the text in a web page and the text attached to the hyperlinks pointing to this page. In the seminal work of Blum and Mitchell <ref type="bibr" target="#b307">[307]</ref>, co-training consists in: 1) learning two distinct weak predictors from labeled data with each of the two views; 2) predicting labels with the weak predictors for a random subset of the unlabeled data; and 3) keeping the most confident labels to enlarge the pool of labeled instances. See <ref type="bibr" target="#b310">[310]</ref>- <ref type="bibr" target="#b314">[314]</ref> for examples of studies on the effectiveness of co-training. Co-training allows each weak predictor to provide labels to improve the other weak predictor, but the problem is that each weak predictor is likely to make prediction errors. Incorrect labels are a source of label noise, which has to be considered, like e.g. in <ref type="bibr" target="#b308">[308]</ref> and <ref type="bibr" target="#b309">[309]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The probabilistic methods to deal with label noise are grounded in a more theoretical approach than robust or data cleansing methods. Hence, probabilistic models of label noise can be directly used and allow to take advantage of prior knowledge. Moreover, the model-based label noise-tolerant methods allow us to use the knowledge gained by the analysis of the consequences of label noise. However, the main problem of the approaches described in this section is that they increase the complexity of learning algorithms and can lead to overfitting, because of the additional parameters of the training data model. Moreover, the identifiability issue discussed in Section VII-A.1 must be addressed, what is done explicitly in the Bayesian approach (using Bayesian priors) and implicitly in the frequentist approach (using generative models).</p><p>As highlighted in <ref type="bibr" target="#b0">[1]</ref>, different models should be used for training and testing in the presence of label noise. Indeed, a complete model of the training data consists of a label noise model and a classification model. Both parts are used during training, but only the classification model is useful for prediction: one has no interest in making noisy predictions. Dropping the label noise model is only possible when label noise is explicitly modeled, as in the probabilistic approaches discussed in Section VII-A. For other approaches, the learning process of the classification model is supposed to be robust or tolerant to label noise and to produce a good classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. EXPERIMENTS IN THE PRESENCE OF LABEL NOISE</head><p>This section discusses how experiments are performed in the label noise literature. In particular, existing datasets, label noise generation techniques, and quality measures are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets with Identified Mislabeled Instances and Label Noise Generation Techniques</head><p>There exist only a few datasets where incorrect labels have been identified. Among them, Lewis et al. <ref type="bibr" target="#b315">[315]</ref> provide a version of the Reuters dataset with corrected labels and Malossini et al. <ref type="bibr" target="#b52">[53]</ref> propose a short analysis of the reliability of instances for two microarray datasets. In spam filtering, where the expert error rate is usually between 3% and 7%, the TREC datasets have been carefully labeled by experts adhering to the same definition of spam, with a resulting expert error rate of about 0.5% <ref type="bibr" target="#b127">[127]</ref>. Mislabeling is also discussed for a medical image processing application in <ref type="bibr" target="#b316">[316]</ref> and Alzheimer disease prediction in <ref type="bibr" target="#b245">[245]</ref>. However, artificial label noise is more common in the literature. Most studies on label noise use NCAR label noise that is introduced in real datasets by: 1) randomly selecting instances and 2) changing their label into one of the other remaining labels <ref type="bibr" target="#b135">[135]</ref>. In this case, label noise is independent of Y . In <ref type="bibr" target="#b317">[317]</ref>, it is also proposed to simulate label noise for artificial datasets by: 1) computing the membership probabilities P(Y = y|X = x) for each training sample x; 2) adding a small uniform noise to these values; and 3) choosing the label corresponding to the largest polluted membership probability.</p><p>Several methods have been proposed to introduce NAR label noise. For example, in <ref type="bibr" target="#b62">[62]</ref>, label noise is artificially introduced by changing the labels of some randomly chosen instances from the majority class. In <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b69">[69]</ref> and <ref type="bibr" target="#b301">[301]</ref>, label noise is introduced using a pairwise scheme. Two classes c 1 and c 2 are selected, then each instance of class c 1 has a probability P e to be incorrectly labeled as c 2 and vice versa. In other words, this label noise models situations where only certain types of classes are mislabeled. In <ref type="bibr" target="#b0">[1]</ref>, label noise is introduced by increasing the entropy of the conditional mass function P( Ỹ |X). The proposed procedure is called majorization: it leaves the probability of the majority class unchanged, but the remaining probability is spread more evenly on the other classes, with respect to the true conditional mass function P(Y |X). In <ref type="bibr" target="#b151">[151]</ref> and <ref type="bibr" target="#b153">[153]</ref>, the percentage of mislabeled instances is first chosen; then, the proportions of mislabeled instances in each class are fixed.</p><p>NNAR label noise is considered in much less works than NCAR and NAR label noise. For example, Chhikara and McKeon <ref type="bibr" target="#b72">[72]</ref> introduce the truncated and the exponential label noise models which are detailed in Section III-A.3 and where the probability of mislabeling depends on the distance to the classification boundary. A special case of truncated label noise is studied in <ref type="bibr" target="#b70">[70]</ref>. In <ref type="bibr" target="#b81">[81]</ref>, two features are randomly picked and the probability of mislabeling depends on which quadrant (with respect to the two selected features) the sample belongs to.</p><p>In practice, it would be very interesting to obtain more real-world datasets where mislabeled instances are clearly identified. Also, an important open research problem is to find what the characteristics of real-world label noise are. Indeed, it is not yet clear in the literature if and when NCAR, NAR, or NNAR label noise is the most realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Validation and Test of Algorithms in the Presence of Label Noise</head><p>An important issue for methods which deal with label noise is to prove their efficiency. Depending on the consequence of label noise that is targeted, different criteria can be used. In general, a good method must either: 1) maintain the value of the quality criterion when label noise is introduced or 2) improve the value of the criterion with respect to other methods in the presence of label noise. In the literature, most experiments assess the efficiency of methods to take care of label noise in terms of accuracy (see e.g. <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b138">[138]</ref>, <ref type="bibr" target="#b160">[160]</ref>, <ref type="bibr" target="#b161">[161]</ref>, <ref type="bibr" target="#b171">[171]</ref>, <ref type="bibr" target="#b180">[180]</ref>, and <ref type="bibr" target="#b184">[184]</ref>), since a decrease in accuracy is one of the main consequences of label noise, as discussed in Section III-A.</p><p>Another common criterion is the model complexity <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b138">[138]</ref>, <ref type="bibr" target="#b184">[184]</ref>, e.g. the number of nodes for decision trees or the number of rules in inductive logic. Indeed, as discussed in Section III-B, some inference algorithms tend to overfit in the presence of label noise, what results in overly complex models. Less complex models are considered better, since they are less prone to overfitting.</p><p>In some contexts, the estimated parameters of the models themselves can also be important, as discussed in Section III-C. Several works focus on the estimation of true frequencies from observed frequencies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b122">[122]</ref>, <ref type="bibr" target="#b123">[123]</ref>, <ref type="bibr" target="#b126">[126]</ref>, what is important e.g. in disease prevalence estimation.</p><p>Eventually, in the case of data cleansing methods, one can also investigate the filter precision. In other words, do the removed instances actually correspond to mislabeled instances and conversely? Different measures are used in the literature, which can be explained using Fig. <ref type="figure">4</ref> inspired by <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b138">[138]</ref>. In <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b180">[180]</ref>, <ref type="bibr" target="#b184">[184]</ref> and <ref type="bibr" target="#b318">[318]</ref>, two types of errors are distinguished. Type 1 errors are correctly labeled instances that are erroneously removed. The corresponding measure is ER 1 = # of correctly labelled instances which are removed # of correctly labelled instances .</p><p>(</p><formula xml:id="formula_21">)<label>19</label></formula><p>Type 2 errors are mislabeled instances which are not removed. The corresponding measure is ER 2 = # of mislabelled instances which are not removed # of mislabelled instances .</p><p>(</p><formula xml:id="formula_22">)<label>20</label></formula><p>The percentage of removed samples that are actually mislabeled is also computed in <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b180">[180]</ref>, <ref type="bibr" target="#b183">[183]</ref>, <ref type="bibr" target="#b184">[184]</ref> and <ref type="bibr" target="#b318">[318]</ref>, what is given by the noise elimination precision NEP = # of mislabeled instances which are removed # of removed instances . (</p><p>A good data cleansing method must find a compromise between ER 1 , ER 2 , and NEP <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b180">[180]</ref>, <ref type="bibr" target="#b184">[184]</ref>. On the one hand, conservative filters remove few instances and are therefore precise (ER 1 is small and NEP is large), but they tend to keep most mislabeled instances (ER 2 is large). Hence, classifiers learnt with data cleansed by such filters achieve low accuracies. On the other hand, aggressive filters remove more mislabeled instances (ER 2 is small) to increase the Fig. <ref type="figure">4</ref>. Types of errors in data cleansing for label noise, inspired by <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b138">[138]</ref>.</p><p>classification accuracy, but they also tend to remove too many instances (ER 1 is large and NEP is small). Notice that Verbaeten and Van Assche <ref type="bibr" target="#b184">[184]</ref> also compute the percentage of mislabeled instances in the cleansed training set.</p><p>Notice that a problem that is seldom mentioned in the literature is that model validation can be difficult in the presence of label noise. Indeed, since validation data are also polluted by label noise, methods like e.g. cross-validation or bootstrap may poorly estimate generalization errors and choose metaparameters that are not optimal (with respect to clean data). For example, the choice of the regularization constant in regularized logistic regression will probably be affected by the presence of mislabeled instances far from the classification boundary. We think that this is an important open research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>This survey shows that label noise is a complex phenomenon with many potential consequences. Moreover, there exist many different techniques to address label noise, which can be classified as label noise-robust, label noise cleansing, or label noise-tolerant methods. As discussed in Section VII-A.1, an identification problem occurs in practical inference: mislabeled instances are difficult to distinguish from correctly labeled instances. In fact, without additional information beyond the main data, it is not possible to take into account the effect of mislabeling <ref type="bibr" target="#b84">[84]</ref>. A solution is to make assumptions that allow selecting a compromise between naively using instances as they are and seeing any instance as possibly mislabeled.</p><p>All methods described in this survey can be interpreted as making particular assumptions. First, in label noise-robust methods described in Section V, overfitting avoidance is assumed to be sufficient to deal with label noise. In other words, mislabeled instances are assumed to cause overfitting in the same way as any other instance would. Second, in data cleansing methods presented in Section VI, different heuristics are used to distinguish mislabeled instances from exceptions. Each heuristic is a definition of what is label noise. Third, label noise-tolerant methods described in Section VII impose different constraint using e.g. Bayesian priors or structural constraints (i.e. in generative methods) or attempt to make existing methods less sensitive to the consequences of label noise.</p><p>In conclusion, the machine learning practitioner has to choose the method whose definition of label noise seems more relevant in his particular field of application. For example, if experts can provide prior knowledge about the values of the parameters or the shape of the conditional distributions, probabilistic methods should be used. On the other hand, if label noise is only marginal, label noise-robust methods could be sufficient. Eventually, most data cleansing methods are easy to implement and have been shown to be efficient and to be good candidates in many situations. Moreover, underlying heuristics are usually intuitive and easy-to-interpret, even for the nonspecialist who can look at removed instances.</p><p>There are many open research questions related to label noise and many avenues remain to be explored. For example, to the best of our knowledge, the method in <ref type="bibr" target="#b55">[56]</ref> has not been generalized to other label noise cleansing methods. Hughes et al. <ref type="bibr" target="#b55">[56]</ref> delete the label of the instances (and not the instances themselves) whose labels are less reliable and perform semisupervised learning using both the labeled and the (newly) unlabeled instances. This approach has the advantage of not altering the distribution of the instances and it could be interesting to investigate whether this improve the results with respect to simply removing suspicious instances. Also, it would be very interesting to obtain more real-world datasets where mislabeled instances are clearly identified, since there exist only a few such datasets <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b127">[127]</ref>, <ref type="bibr" target="#b245">[245]</ref>, <ref type="bibr" target="#b315">[315]</ref>, <ref type="bibr" target="#b316">[316]</ref>. It is also important to find what the characteristics of real-world label noise are, since it is not yet clear if and when NCAR, NAR, or NNAR label noise is the most realistic. Answering this question could lead to more complex and realistic models of label noise in the line of e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b67">[67]</ref>, <ref type="bibr" target="#b70">[70]</ref>- <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b90">[90]</ref>, <ref type="bibr" target="#b91">[91]</ref>, <ref type="bibr" target="#b122">[122]</ref>, <ref type="bibr" target="#b227">[227]</ref>- <ref type="bibr" target="#b230">[230]</ref>, <ref type="bibr" target="#b235">[235]</ref>, <ref type="bibr" target="#b251">[251]</ref>. Label noise should be also be studied in more complex settings than standard classification, like, e.g., image processing <ref type="bibr" target="#b242">[242]</ref>, <ref type="bibr" target="#b301">[301]</ref>, <ref type="bibr" target="#b302">[302]</ref> and sequential data analysis <ref type="bibr" target="#b84">[84]</ref>, <ref type="bibr" target="#b253">[253]</ref>. The problem of metaparameter selection in the presence of label noise is also an important open research problem, since estimated error rates are also biased by label noise <ref type="bibr" target="#b112">[112]</ref>, <ref type="bibr" target="#b126">[126]</ref>, <ref type="bibr" target="#b127">[127]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received March 22, 2013; revised August 22, 2013; accepted November 21, 2013. Date of publication December 17, 2013; date of current version April 10, 2014.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Statistical taxonomy of label noise inspired by [10]: (a) noisy completely at random (NCAR), (b) noisy at random (NAR), and (c) noisy not at random (NNAR). Arrows report statistical dependencies. Notice the increasing complexity of statistical dependencies in the label noise generation models, from left to right. The statistical link between X and Y is not shown for clarity.</figDesc><graphic coords="3,105.95,58.73,92.66,92.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. General procedure for learning in the presence of label noise with training set cleansing, inspired by<ref type="bibr" target="#b45">[46]</ref>.</figDesc><graphic coords="9,125.99,58.37,359.78,53.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Statistical model of label noise, inspired by [67].</figDesc><graphic coords="14,122.99,58.85,102.98,102.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>OF THE METHODS REVIEWED IN SECTIONS V-VII WITH SOME SELECTED EXAMPLES OF TYPICAL METHODS FOR EACH CLASS. THE TABLE HIGHLIGHTS THE STRUCTURE OF EACH SECTION, SUMMARIZES THEIR RESPECTIVE CONTENT AND POINTS TO SPECIFIC REFERENCES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.mturk.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise modelling and evaluating learning from examples</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hickey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="157" to="179" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class noise vs. attribute noise: A quantitative study</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Misclassification in 2 × 2 tables</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="478" to="486" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian estimation of disease prevalence and the parameters of diagnostic tests in the absence of a gold standard</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Gyorkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coupal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The discrepancy in discrepant analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadgu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">9027</biblScope>
			<biblScope unit="page" from="592" to="593" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning from imperfect data through particle cooperation and competition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Breve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Quiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Knowledge Acquisition from Databases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Ablex</publisher>
			<pubPlace>Greenwich, CT, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing the presence of noise in multi-class problems: Alleviating its influence with the onevs-one decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sàez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Missing data: Our view of the state of the art</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="177" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="370" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class noise and supervised learning in medical domains: The effect of feature extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puuronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th IEEE Int. Symp. Comput</title>
		<meeting>19th IEEE Int. Symp. Comput<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FISH-BOL and seafood identification: Geographically dispersed case studies reveal systemic market substitution across Canada</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mitochondr. DNA</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="106" to="122" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Species misidentification in mixed hake fisheries may lead to overexploitation and population bottlenecks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Garcia-Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Machado-Schiaffino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Juanes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fish. Res</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="52" to="55" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of mislabelling in the fresh potato retail market employing microsatellite markers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lopez-Vizcón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Food Control</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="575" to="579" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A high incidence of species substitution and mislabelling detected in meat products sold in South Africa</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Cawthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Steinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Food Control</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="440" to="449" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning disjunction of conjunctions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Joint Conf</title>
		<meeting>9th Int. Joint Conf<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985-08">Aug. 1985</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="560" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning in the presence of malicious errors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Annu. ACM Symp. Theory Comput</title>
		<meeting>20th Annu. ACM Symp. Theory Comput<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-05">May 1988</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical queries and faulty PAC oracles</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Decatur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Annu. Conf. Comput. Learn. Theory</title>
		<meeting>6th Annu. Conf. Comput. Learn. Theory<address><addrLine>Santa Cruz, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-07">Jul. 1993</date>
			<biblScope unit="page" from="262" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning in hybrid noise environments using statistical queries</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Decatur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from Data</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-J</forename><surname>Lenz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Four types of noise in data for PAC learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="162" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On-line learning with malicious noise and the closure algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample-efficient strategies for learning in the presence of noise</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dichterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">U</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="719" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth boosting and learning with malicious noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="633" to="648" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Asian Conf</title>
		<meeting>3rd Asian Conf<address><addrLine>Taoyuan, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial label flips attack on support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Eur. Conf</title>
		<meeting>20th Eur. Conf<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08">Aug. 2012</date>
			<biblScope unit="page" from="870" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The nature of noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Epistemological Aspects of Computer Simulation in the Social Sciences</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Squazzoni</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Randomized response, statistical disclosure control and misclassification: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G M</forename><surname>Van Der Heijden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Statist. Rev</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="288" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decontamination of training samples for supervised pattern recognition methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint IAPR Int. Workshops Adv. Pattern Recognit</title>
		<meeting>Joint IAPR Int. Workshops Adv. Pattern Recognit<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">Aug./Sep. 2000</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Identification of Outliers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="149" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note type="report_type">Outlier. . . . . . . . . .s</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">Outliers in Statistical Data</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999-09">Aug./Sep. 1999</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Support vector novelty detection applied to jet engine vibration spectra</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anuzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000-11">Nov. 2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="946" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kernel PCA for novelty detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="874" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhancing data analysis with noise removal</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="319" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using one-class SVM outliers detection for verification of collaboratively tagged image training sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jun./Jul. 2009</date>
			<biblScope unit="page" from="682" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The subjective nature of outlier rejection procedures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Collett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. C, Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="228" to="237" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Analyzing outliers cautiously</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="437" />
			<date type="published" when="2002-04">Mar./Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What are statistical decisions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcnicol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Primer of Signal Detection Theory</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Allen</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Imperfect information: Imprecision and uncertainty</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smets</surname></persName>
		</author>
		<editor>A. Motro and P. Smets</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="225" to="254" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>in Uncertainty Management in Information Systems</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Philosophical Lectures on Probability: Collected, Edited, and Annotated by Alberto Mura</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Finetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from imperfect data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Meta-Reasoning and Logics</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brazdil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="207" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. C, Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cheap and fast-But is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quality management on Amazon mechanical turk</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Workshop Human Comput</title>
		<meeting>ACM SIGKDD Workshop Human Comput<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A survey of crowdsourcing systems</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 3rd Int</title>
		<meeting>IEEE 3rd Int<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="page" from="766" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detecting potential labeling errors in microarrays by data perturbation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malossini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2114" to="2121" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Inferring ground truth from subjective labelling of venus images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, CO, USA; Nov</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994-12">Dec. 1994</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bounds on the mean classification error rate of multiple experts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1253" to="1257" />
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of probabilistic models for ECG segmentation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th</title>
		<meeting>26th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Int. Conf. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="434" to="437" />
			<date type="published" when="2004-09">Sep. 2004</date>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993-06">Jun. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Filtering email spam in the presence of noisy user feedback</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th CEAS</title>
		<meeting>5th CEAS<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Data quality and systems theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Orr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="71" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The impact of poor data quality on the typical enterprise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Redman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="82" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Data cleansing: Beyond integrity analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Maletic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Inf. Qual</title>
		<meeting>Conf. Inf. Qual<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A study of the effect of different types of noise on the precision of supervised learning techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nettleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orriols-Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="306" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Boosting in the presence of noise</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="266" to="290" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The use of being stubborn and introspective</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ZiF Conf. Adapt. Behavior Learn</title>
		<meeting>ZiF Conf. Adapt. Behavior Learn<address><addrLine>Bielefeld, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-04">Apr. 1994</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On the sample complexity of noise-tolerant learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="189" to="195" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Accounting for control mislabeling in case-control biomarker studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rantalainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5562" to="5567" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Estimating a kernel fisher discriminant in the presence of label noise</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn</title>
		<meeting>18th Int. Conf. Mach. Learn<address><addrLine>Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">Jun./Jul. 2001</date>
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Misclassified multinomial data: A Bayesian approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rojano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. R. Acad. Cien. A, Mat</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Eliminating class noise in large datasets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Mach. Learn</title>
		<meeting>20th Int. Conf. Mach. Learn<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="920" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Discriminant analysis when the initial samples are misclassified II: Non-random misclassification models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lachenbruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="424" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Discriminant analysis when the initial samples are misclassified</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lachenbruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="662" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Linear discriminant analysis with misallocation in training samples</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chhikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mckeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">388</biblScope>
			<biblScope unit="page" from="899" to="906" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning noisy perceptrons by a perceptron in polynomial time</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Annu</title>
		<meeting>38th Annu</meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="page" from="514" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning with annotation noise</title>
		<author>
			<persName><forename type="first">E</forename><surname>Beigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Klebanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang. Process</title>
		<meeting>Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang. ess<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08">Aug. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">From annotator agreement to noise models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Klebanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="503" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Genre-based decomposition of email class noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jun./Jul. 2009</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Some empirical evidence for annotation noise in a benchmarked dataset</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Klebanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Lang. Technol., Annu. Conf. North Amer. Chapter ACL</title>
		<meeting>Human Lang. Technol., Annu. Conf. North Amer. Chapter ACL<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="438" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A k-nearest neighbor classification rule based on Dempster-Shafer theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="804" to="813" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Analysis of evidence-theoretic decision rules for pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1095" to="1107" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A neural network classifier based on Dempster-Shafer theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="150" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A team of continuousaction learning automata for noise-tolerant learning of half-spaces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Context-based speech recognition error detection and correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Lang. Technol. Conf./North Amer. Chapter AACL Annu. Meeting</title>
		<meeting>Human Lang. Technol. Conf./North Amer. Chapter AACL Annu. Meeting<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Correcting for misclassification for a monotone disease process with an application in dental research</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>García-Zattera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mutsvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Declerckc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lesaffrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="3103" to="3117" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Randomizing outputs to increase prediction accuracy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Switching class labels to generate classification ensembles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Martínez-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suárez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1483" to="1494" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Building ensembles of neural networks with classswitching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Martínez-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suárez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Artif</title>
		<meeting>16th Int. Conf. Artif<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Class-switching neural network ensembles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Martínez-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suárez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="2521" to="2528" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Label alteration to improve underwater mine classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="488" to="492" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Asymptotic results for discriminant analysis when the initial samples are misclassified</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="422" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Note on initial misclassification effects on the quadratic discriminant function</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lachenbruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The effect of errors in diagnosis and measurement on the estimation of the probability of an event</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Michalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">371</biblScope>
			<biblScope unit="page" from="713" to="721" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The efficiency of logistic regression compared to normal discriminant analysis under class-conditional classification noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jeske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivariate Anal</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1622" to="1637" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Prototype selection for the nearest neighbour rule through proximity graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="507" to="513" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Reduction techniques for instancebased learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">An average-case analysis of the k-nearest neighbor classifier for noisy domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nobuhiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Joint Conf</title>
		<meeting>15th Int. Joint Conf<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08">Aug. 1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Robustness of regularized linear classification methods in text categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. ACM SIGIR Conf</title>
		<meeting>26th Annu. Int. ACM SIGIR Conf<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">Jul./Aug. 2003</date>
			<biblScope unit="page" from="190" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Jpn. Soc. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="771" to="780" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">An empirical comparison of three boosting algorithms on real data sets with artificial class noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Eckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Workshop Multiple Classifier Syst</title>
		<meeting>4th Int. Workshop Multiple Classifier Syst<address><addrLine>Guilford, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Experiments on ensembles with missing and noisy data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Workshop Multi Classifier Syst</title>
		<meeting>5th Int. Workshop Multi Classifier Syst<address><addrLine>Cagliari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Some theoretical aspects of boosting in the presence of noisy data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn</title>
		<meeting>18th Int. Conf. Mach. Learn<address><addrLine>Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">Jun./Jul. 2001</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Bagging decision trees on data sets with classification noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Masegosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Found</title>
		<meeting>6th Int. Conf. Found<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-02">Feb. 2010</date>
			<biblScope unit="page" from="248" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Boosting the margin: A new explanation for the effectiveness of voting methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Mach. Learn</title>
		<meeting>14th Int. Conf. Mach. Learn<address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">Jul. 1997</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Boosting the margin: A new explanation for the effectiveness of voting methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1651" to="1686" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of AdaBoost in the binary classification case</title>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Neural Netw</title>
		<meeting>Int. Conf. Artif. Neural Netw<address><addrLine>Skövde, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-09">Sep. 1998</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Soft margins for AdaBoost</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="320" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Error reduction through learning multiple descriptions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="173" to="202" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning with rare cases and small disjuncts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Mach. Learn</title>
		<meeting>12th Int. Conf. Mach. Learn<address><addrLine>Tahoe City, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-07">Jul. 1995</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Allocation rules and their error rates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Pre-processing for noise detection in gene expression classification data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Libralon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P</forename><surname>De Leon Ferreira De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Lorena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Brazil. Comput. Soc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Evaluation of noise reduction techniques in the splice junction recognition problem</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Lorena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genet. Molecular Biol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="672" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A scalable noise reduction technique for large case-based systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Segata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Case, Based Reason</title>
		<meeting>8th Int. Conf. Case, Based Reason<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jul. 2009</date>
			<biblScope unit="page" from="328" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Noise reduction for instance-based learning with a local maximal margin approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Segata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="331" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Annu. ACM Symp. Theory Comput</title>
		<meeting>16th Annu. ACM Symp. Theory Comput<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984-05">Apr. May 1984</date>
			<biblScope unit="page" from="436" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Learning from Good and Bad Data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Laird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Improved lower bounds for learning from noisy examples: An information-theoretic approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf. Comput. Learn. Theory</title>
		<meeting>11th Annu. Conf. Comput. Learn. Theory<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07">Jul. 1998</date>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Noise-tolerant Occam algorithms and their applications to learning decision trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="62" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Learning linear threshold functions in the presence of classification noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bylander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Annu. Workshop Comput. Learn. Theory</title>
		<meeting>7th Annu. Workshop Comput. Learn. Theory<address><addrLine>New Brunswick, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07">Jul. 1994</date>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Implications of errors in survey data: A Bayesian model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manage. Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="913" to="925" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A double sampling scheme for estimating from binomial data with misclassifications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tenenbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">331</biblScope>
			<biblScope unit="page" from="1350" to="1361" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Estimating genomic category probabilities from fluorescent in situ hybridization counts with misclassification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Thall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. C, Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="431" to="446" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Adjustment of cancer incidence rates for ethnic misclassification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Swallen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Horn-Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Evaluating classifiers by means of test data with noisy labels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Joint Conf</title>
		<meeting>18th Int. Joint Conf<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="513" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Spam filter evaluation with imprecise ground truth</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. ACM SIGIR Conf. Res. Develop</title>
		<meeting>32nd Int. ACM SIGIR Conf. Res. Develop<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jul. 2009</date>
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A method for predicting disease subtypes in presence of misclassification among training samples using gene expression: Application to human breast cancer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rekaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="325" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Robustness of threshold-based feature rankers with data sampling on noisy and imbalanced data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Shanab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int</title>
		<meeting>25th Int<address><addrLine>Marco Island, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Bayesian model selection for logistic regression with misclassified outcomes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stamey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Model</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="273" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Feature selection with imprecise labels: Estimating mutual information in the presence of label noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doquire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="832" to="848" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Evaluating noise correction</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Pacific Rim Int. Conf</title>
		<meeting>6th Pacific Rim Int. Conf<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">Aug./Sep. 2000</date>
			<biblScope unit="page" from="188" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A comparison of noise handling techniques</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int</title>
		<meeting>14th Int<address><addrLine>Key West, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="269" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Dealing with data corruption in remote sensing</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Symp. Adv. Intell. Data Anal</title>
		<meeting>6th Int. Symp. Adv. Intell. Data Anal<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
			<biblScope unit="page" from="452" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">The effect of noise on RWTSAIRS classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Golzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doraisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Sulaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Udzir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Sci. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="632" to="641" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Robust supervised classification with mixture models: Learning from data with uncertain labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2649" to="2658" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">The problem of noise in classification: Past, current and future work</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 3rd Int. Conf</title>
		<meeting>IEEE 3rd Int. Conf<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="412" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Identifying and eliminating mislabeled training instances</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Nat. Conf</title>
		<meeting>13th Nat. Conf<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">Aug. 1996</date>
			<biblScope unit="page" from="799" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Networks of Learning Automata: Techniques for Online Stochastic Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thathachar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Additive logistic regression: A statistical view of boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="374" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">An adaptive version of the boost by majority algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Regularizing AdaBoost</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998-12">Nov./Dec. 1998</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="564" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">An improvement of AdaBoost to avoid overfitting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Neural Inf. Process</title>
		<meeting>5th Int. Conf. Neural Inf. ess<address><addrLine>Kitakyushu, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
			<biblScope unit="page" from="506" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Robust ensemble learning for data mining</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Pacific-Asia Conf. Knowl. Discovery Data Mining</title>
		<meeting>4th Pacific-Asia Conf. Knowl. Discovery Data Mining<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-04">Apr. 2000</date>
			<biblScope unit="page" from="341" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Mach. Learn</title>
		<meeting>13th Int. Conf. Mach. Learn<address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-07">Jul. 1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">An experimental study about simple decision trees for bagging ensemble on datasets with classification noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Masegosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Symbolic Quant. Approaches Reason. Uncertainty</title>
		<meeting>10th Eur. Conf. Symbolic Quant. Approaches Reason. Uncertainty<address><addrLine>Verona, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jul. 2009</date>
			<biblScope unit="page" from="446" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Building classification trees using the total uncertainty criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1215" to="1225" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Bagging schemes on the presence of class noise in classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Masegosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6827" to="6837" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Identifying learners robust to low quality data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Folleco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="259" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Software quality modeling: The impact of class noise on the random forest classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Folleco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Bullard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Congr</title>
		<meeting>IEEE Congr<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="3853" to="3859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Supervised neural network modeling: An empirical investigation into learning from imbalanced data with labeling errors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Arguing from experience to classifying noisy data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bench-Capon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Data Warehous</title>
		<meeting>11th Int. Conf. Data Warehous<address><addrLine>Linz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Aug./Sep. 2009</date>
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A first study on decomposition strategies with data with class noise using decision trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sàez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int</title>
		<title level="s">Conf. Hybrid Artif. Intell. Syst.</title>
		<meeting>7th Int<address><addrLine>Salamanca, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">The problem with noise and small disjuncts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07">Jul. 1998</date>
			<biblScope unit="page" from="574" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Identifying and correcting mislabeled training instances</title>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Future Generat</title>
		<meeting>Future Generat<address><addrLine>Jeju-Island, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="244" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Noise elimination in inductive concept learning: A case study in medical diagnosis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Workshop Algorithmic Learn. Theory</title>
		<meeting>7th Int. Workshop Algorithmic Learn. Theory<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Conditions for Occam&apos;s razor applicability and noise elimination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Mach. Learn</title>
		<meeting>9th Eur. Conf. Mach. Learn<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">Apr. 1997</date>
			<biblScope unit="page" from="108" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Noise detection and elimination applied to noise handling in a KRK chess endgame</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Workshop Inductive Logic Program</title>
		<meeting>5th Int. Workshop Inductive Logic Program<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-09">Sep. 1997</date>
			<biblScope unit="page" from="59" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Experiments with noise filtering in a medical domain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boskovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Groselj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Mach. Learn</title>
		<meeting>16th Int. Conf. Mach. Learn<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">Jun. 1999</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Noise detection and elimination in data preprocessing: Experiments in medical domains</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="223" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Class noise handling for effective cost-sensitive learning by cost-guided iterative classification filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1435" to="1440" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Generating multiple noise elimination filters with the ensemble-partitioning filter</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rebours</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Inf. Reuse Integr</title>
		<meeting>IEEE Int. Conf. Inf. Reuse Integr<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">Nov. 2004</date>
			<biblScope unit="page" from="369" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Support vector machine for outlier detection in breast cancer survivability prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thongkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<editor>Advanced Web and Network Technologies, and Applications, Y. Ishikawa, J. He, G. Xu, Y. Shi, G. Huang, C. Pang, et al.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="99" to="109" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Data cleaning for classification using misclassification analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jeatrakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Adv. Comput. Intell. Intell. Inf</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Use of classification algorithms in noise detection and elimination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Lorena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf</title>
		<meeting>4th Int. Conf<address><addrLine>Salamanca, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Computer aided cleaning of large databases for character recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Matic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IAPR Int. Conf. Pattern Recognit., Conf. B, Pattern Recognit</title>
		<meeting>11th IAPR Int. Conf. Pattern Recognit., Conf. B, Pattern Recognit<address><addrLine>The Hague, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-09">Aug./Sep. 1992</date>
			<biblScope unit="page" from="330" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Discovering informative patterns and data cleaning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Matic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><surname>Uthurusamy</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="181" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Pruning training sets for learning of object categories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Robust decision trees: Removing outliers from databases</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>1st Int. Conf. Knowl. Discovery Data Mining<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-08">Aug. 1995</date>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">The effects of training set size on decision tree complexity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Mach. Learn</title>
		<meeting>14th Int. Conf. Mach. Learn<address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">Jul. 1997</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training examples in ILP classification problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verbaeten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Belgian-Dutch Conf</title>
		<meeting>12th Belgian-Dutch Conf<address><addrLine>Utrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Local learning algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="888" to="900" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Discriminant adaptive nearest neighbor classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="607" to="616" />
			<date type="published" when="1996-06">Jun. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">An adaptive SVM nearest neighbor classifier for remotely sensed imagery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Conf. Geosci. Remote Sens. Symp<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">Jul./Aug. 2006</date>
			<biblScope unit="page" from="3931" to="3934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Nearest neighbor classification of remote sensing images with the maximal margin principle</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1804" to="1811" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Analysis of new techniques to obtain quality training sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Badenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1015" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A new definition of neighborhood of a point in multidimensional space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Improving automated land cover mapping by identifying and eliminating mislabeled observations from training data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Geosci. Remote Sens. Symp</title>
		<meeting>Int. Geosci. Remote Sens. Symp<address><addrLine>Lincoln, NE, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Applied Linear Regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weisberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Advances in class noise detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sluban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Eur. Conf</title>
		<meeting>19th Eur. Conf<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="1105" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Ensemble of classifiers for noise detection in PoS tagged corpora</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berthelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Megyesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Workshop Text, Speech Dialogue</title>
		<meeting>3rd Int. Workshop Text, Speech Dialogue<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Ensemble methods for noise elimination in classification problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verbaeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Assche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Multiple Classifier Syst</title>
		<meeting>4th Int. Conf. Multiple Classifier Syst<address><addrLine>Guildford, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Bridging local and global data cleansing: Identifying class noise in large, distributed data datasets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="275" to="308" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">The partitioning-and rulebased filter for noise detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Inf. Reuse Integr</title>
		<meeting>IEEE Int. Conf. Inf. Reuse Integr<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">Aug. 2005</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Methods for labeling error detection in microarrays based on the effect of data perturbation on the regression model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2708" to="2714" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">A fast algorithm for outlier detection in microarray</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf<address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
			<biblScope unit="page" from="513" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967-01">Jan. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dasarathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">Pattern Recognition: A Statistical Approach</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Mining competent case bases for casebased reasoning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="1039" to="1068" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Instance pruning techniques</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn<address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">Jul. 1997</date>
			<biblScope unit="page" from="403" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Profiling instances in noise reduction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Namee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">The condensed nearest neighbor rule</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="515" to="516" />
			<date type="published" when="1968-05">May 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">The reduced nearest neighbor rule (Corresp.)</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Gates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="1972-05">May 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">An analysis of case-base editing in a spam filtering system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Eur. Conf. Case Based Reasoning</title>
		<meeting>7th Eur. Conf. Case Based Reasoning<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">Aug./Sep. 2004</date>
			<biblScope unit="page" from="128" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Data pre-processing through reward-punishment editing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Reduced reward-punishment editing for building ensembles of classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2395" to="2400" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">On the relation of performance to editing in nearest neighbor rules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koplowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="255" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Ensembles of pre-processing techniques for noise detection in gene expression data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Libralon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lorena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. Adv. Neuro-Inf. Process</title>
		<meeting>15th Int. Conf. Adv. Neuro-Inf. ess<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="486" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Asymptotic properties of nearest neighbor rules using edited data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="421" />
			<date type="published" when="1972-07">Jul. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">An experiment with the edited nearest-neighbor rule</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tomek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="448" to="452" />
			<date type="published" when="1976-06">Jun. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Noise-tolerant instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. sJoint Conf</title>
		<meeting>11th Int. sJoint Conf<address><addrLine>Detroit, MI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-08">Aug. 1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="794" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">The influence of noisy patterns in the performance of learning methods in the splice junction recognition problem</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Lorena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P L F</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Brazilian Symp</title>
		<meeting>7th Brazilian Symp<address><addrLine>Recife, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Two modifications of CNN</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tomek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="769" to="772" />
			<date type="published" when="1976-11">Nov. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Improving classification accuracy by identifying and removing instances that should be misclassified</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08">Jul./Aug. 2011</date>
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Identifying and handling mislabelled instances</title>
		<author>
			<persName><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Relative sensitivity of a family of closest-point graphs in computer vision applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tüceryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chorzempa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="361" to="373" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Relative neighborhood graphs and their relatives</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Jaromczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1992-09">Sep. 1992</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1502" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Error-correcting semi-supervised learning with mode-filter on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Urahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th ICCV Workshops</title>
		<meeting>IEEE 12th ICCV Workshops<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="2095" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Error-correcting semi-supervised pattern recognition with mode filter on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Urahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Symp. Aware Comput</title>
		<meeting>2nd Int. Symp. Aware Comput<address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Improving classification by removing or relabeling mislabeled instances</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int</title>
		<meeting>13th Int<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">A boosting approach to remove class label noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karmaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hybrid Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Improved boosting algorithm with adaptive filtration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th World Congr</title>
		<meeting>8th World Congr<address><addrLine>Jinan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="3173" to="3178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Using boosting to detect noisy data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wheway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Intelligence. PRICAI 2000 Workshop Reader</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Kowalczyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Loke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Arcing the edge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<date type="published" when="1997">1997</date>
			<pubPlace>Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Statist., Univ. California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Automatic ground-truth validation with genetic algorithms for multispectral image classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghoggalii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2172" to="2181" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">An algorithm for correcting mislabeled data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">A noise filtering method using neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int</title>
		<meeting>IEEE Int<address><addrLine>Provo, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Using decision trees and soft labeling to filter mislabeled data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="354" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Automatic labeling inconsistencies detection and correction for sentence unit segmentation in conversational speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cuendet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf</title>
		<meeting>4th Int. Conf<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="144" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Knowledge discovery from imbalanced and noisy data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1513" to="1542" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">An empirical study of the classification performance of learners on imbalanced and noisy software quality data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Folleco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Inf. Reuse Integr</title>
		<meeting>IEEE Int. Conf. Inf. Reuse Integr<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Distinguishing exceptions from noise in non monotonic learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Workshop Induct. Logic Program</title>
		<meeting>2nd Int. Workshop Induct. Logic Program<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-06">Jun. 1992</date>
			<biblScope unit="page" from="97" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary data subject to misclassification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guttman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Haitovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Swartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Analysis in Statistics and Econometrics: Essays in Honor of</title>
		<editor>
			<persName><forename type="first">Arnold</forename><surname>Zellner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Berry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaloner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Geweke</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="67" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Bayesian identifiability and misclassification in multinomial data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Haitovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Statist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="302" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Inferences with an unknown noise level in a Bernoulli process</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manage. Sci</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1227" to="1237" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Information loss in noisy and dependent processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Lindley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Bayesian statistics for parasitologists</title>
		<author>
			<persName><forename type="first">M.-G</forename><surname>Basànez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Carabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gyorkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Parasitol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="91" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Bayesian inference for medical screening tests: Approximations useful for the analysis of acquired immune deficiency syndrome</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gastwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="427" to="439" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Inferences for likelihood ratios in the absence of a &apos;gold standard</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Gyorkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Decis. Making</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="412" to="417" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Case-control analysis with partial knowledge of exposure misclassification probabilities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="598" to="609" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Threshold model for misclassified binary responses with applications to animal breeding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rekaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Weigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gianola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1123" to="1129" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Binomial regression with misclassification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Paulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neuhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="670" to="675" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">A Bayesian model for multinomial sampling with misclassified data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Girón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rojano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Bayesian analysis of a matched case-control study with expert prior information on both the misclassification of exposure and the exposure-disease association</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Burstyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="3411" to="3423" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Binary data in the presence of misclassifications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Achcar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Louzada-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Symp</title>
		<meeting>16th Symp<address><addrLine>Praga, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="581" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Modelling risk when binary outcomes are subject to error</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcinturff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Med</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1095" to="1109" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Bayesian analysis of correlated misclassified binary data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Paulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Achcar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1120" to="1131" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Comparative validation of graphical models for learning tumor segmentations from noisy manual annotations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O</forename><surname>Kaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. MICCAI Conf</title>
		<meeting>Int. MICCAI Conf<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Evaluation of nucleic acid amplification tests in the absence of a perfect gold-standard test: A review of the statistical and epidemiologic issues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadgu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dendukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="604" to="612" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Robustness of prevalence estimates derived from misclassified data from administrative databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ladouceur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="272" to="279" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Classification of incipient alzheimer patients using gene expression data: Dealing with potential misdiagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rekaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bertrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online J. Bioinformat</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Robust multi-class Gaussian process classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Bayesian Gaussian process classification with the EM-EP algorithm</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1948" to="1959" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Heavy-tailed process priors for selective shrinkage</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Wauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010-12">Dec. 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2406" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Detecting errors within a corpus using anomaly detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st NAACL Conf</title>
		<meeting>1st NAACL Conf<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Learning conjunctions with noise under product distributions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="189" to="196" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Classification in the presence of class noise using a probabilistic kernel fisher method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Ridder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3349" to="3357" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Multi-class classification in the presence of labelling errors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bootkrajang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Eur. Symp. Artif</title>
		<meeting>19th Eur. Symp. Artif<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="345" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Label noise-tolerant hidden Markov models for segmentation: Application to ECGS</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Lannoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Eur. Conf. Mach. Learn. Knowl. Discovery Databases<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Design of robust neural network classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hintz-Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="1205" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Outlier estimation and detection: Application to skin lesion classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Wulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>Int. Conf. Acoust., Speech Signal ess<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="1049" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Outlier robust Gaussian process classification</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint IAPR Int. Workshop Structural, Syntactic, Statist. Pattern Recognit</title>
		<meeting>Joint IAPR Int. Workshop Structural, Syntactic, Statist. Pattern Recognit<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="896" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Kernel based detection of mislabeled training examples</title>
		<author>
			<persName><forename type="first">H</forename><surname>Valizadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Conf. Data Mining</title>
		<meeting>SIAM Conf. Data Mining<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">Apr. 2007</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="678" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Discriminant analysis by Gaussian mixtures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Weakly-supervised classification with mixture models for cervical cancer detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Work-Conf. Artif</title>
		<meeting>10th Int. Work-Conf. Artif<address><addrLine>Salamanca, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Supervised classification of categorical data with uncertain labels for DNA barcoding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Eur. Symp. Artif</title>
		<meeting>17th Eur. Symp. Artif<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04">Apr. 2009</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Class noise mitigation through instance weighting</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rebbapragada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Eur. Conf. Machine Learning</title>
		<meeting>18th Eur. Conf. Machine Learning<address><addrLine>Warsaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="page" from="708" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">A study of the robustness of KNN classifiers trained using soft labels</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">El</forename><surname>Gayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Artif. Neural Netw. Pattern Recognit</title>
		<meeting>2nd Int. Conf. Artif. Neural Netw. Pattern Recognit<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Aug./Sep. 2006</date>
			<biblScope unit="page" from="67" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">A fuzzy K-nearest neighbor algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J A</forename><surname>Givens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="580" to="585" />
			<date type="published" when="1985-08">Jul./Aug. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<title level="m" type="main">A Mathematical Theory of Evidence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Princeton Univ. Press</publisher>
			<pubPlace>Princeton, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Decision making in the TBM: The necessity of the pignistic transformation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reason</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="147" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Handling uncertain labels in multiclass problems using belief decision trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vannoorenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Inf. Process. Manag. Uncertainty</title>
		<meeting>9th Int. Conf. Inf. ess. Manag. Uncertainty<address><addrLine>Annecy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">Jul. 2002</date>
			<biblScope unit="page" from="1919" to="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Mixture model estimation with soft labels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Côme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oukhellou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aknin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft Methods for Handling Variability and Imprecision</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lubiano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Prade</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Grzegorzewski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Hryniewicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Learning from partially supervised data using mixture models and belief functions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Côme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oukhellou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aknin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="348" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Learning from data with uncertain labels by boosting credal classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Quost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ACM SIGKDD Workshop Knowl</title>
		<meeting>1st ACM SIGKDD Workshop Knowl<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="38" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Knitted fabric defect classification for uncertain labels based on Dempster-Shafer theory of evidence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tabassian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghaderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5259" to="5267" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Evidential multi-label classification approach to learning from data with imprecise labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Inf. Process. Manag. Uncertainty</title>
		<meeting>13th Int. Conf. Inf. ess. Manag. Uncertainty<address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jun./Jul. 2010</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Support vector machines for automatic data cleanup</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganapathiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>State</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Spoken Lang. Process</title>
		<meeting>6th Int. Conf. Spoken Lang. ess<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="page" from="210" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Automatic discrimination of mislabeled training points for large margin classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Snowbird Mach. Learn. Workshop</title>
		<meeting>Snowbird Mach. Learn. Workshop<address><addrLine>Clearwater, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04">Apr. 2009</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Good learners for evil teachers</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Ann. Int. Conf. Mach. Learn</title>
		<meeting>26th Ann. Int. Conf. Mach. Learn<address><addrLine>Montreal, PQ, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Training algorithms for fuzzy support vector machines with noisy data</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1647" to="1656" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Fuzzy support vector machine based on within-class scatter for classification problems with outliers or noises</title>
		<author>
			<persName><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Fuzzy relevance vector machine for learning from unbalanced data and noise</title>
		<author>
			<persName><forename type="first">D.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1181" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Emphatic constraints support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabzekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Yazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naghibzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Effati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="306" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Robust support vector machine training via convex outlier ablation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Nat. Conf</title>
		<meeting>21st Nat. Conf<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">Jul. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Functional gradient techniques for combining hypotheses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="221" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Leveraging the margin more carefully</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Mach. Learn</title>
		<meeting>21st Int. Conf. Mach. Learn<address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">Jul. 2004</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">On the design of loss functions for classification: Theory, robustness to outliers, and SavageBoost</title>
		<author>
			<persName><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="1049" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">On the design of robust classifiers for computer vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Learning SVMs from sloppily labeled data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stempfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Conf. Artif</title>
		<meeting>19th Int. Conf. Artif<address><addrLine>Limassol, Cyprus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="884" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Noise tolerant variants of the perceptron algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khardon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227" to="248" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Kernel machines and boolean functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">The relaxed online maximum margin algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="361" to="387" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Learning algorithms with optimal stability in neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A, Math. General</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="745" to="L752" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">The CN 2 induction algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="283" />
			<date type="published" when="1989-03">Mar. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">MadaBoost: A modification of AdaBoost</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Ann. Conf. Comput. Learn. Theory</title>
		<meeting>13th Ann. Conf. Comput. Learn. Theory<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">Jun. 2000</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Boosting with averaged weight vectors</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Multiple Classifier Syst</title>
		<meeting>4th Int. Conf. Multiple Classifier Syst<address><addrLine>Guildford, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">AveBoost2: Boosting for noisy data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Multiple Classifier Syst</title>
		<meeting>5th Int. Conf. Multiple Classifier Syst<address><addrLine>Cagliari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Averaged boosting: A noise-robust ensemble method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Pacific, Asia Conf. Adv. Knowl. Discovery Data Mining</title>
		<meeting>7th Pacific, Asia Conf. Adv. Knowl. Discovery Data Mining<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">Apr./May 2003</date>
			<biblScope unit="page" from="388" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Boosting by weighting critical and erroneous samples</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gómez-Verdejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega-Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arenas-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Figueiras</surname></persName>
		</author>
		<author>
			<persName><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="679" to="685" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Boosting noisy data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn</title>
		<meeting>18th Int. Conf. Mach. Learn<address><addrLine>Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">Jun./Jul. 2001</date>
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">MultiBoosting: A technique for combining boosting and wagging</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="196" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Boosting parallel perceptrons for label noise reduction in classification problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Dorronsoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Work, Conf. Interplay Between Natural Artif</title>
		<meeting>1st Int. Work, Conf. Interplay Between Natural Artif<address><addrLine>Las Palmas, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="586" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data with the aid of unlabeled data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">A novel context-sensitive semisupervised SVM classifier robust to mislabeled training samples</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Persello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2142" to="2154" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">A spatial-contextual support vector machine for remotely sensed image classification</title>
		<author>
			<persName><forename type="first">C.-T</forename><forename type="middle">L C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="784" to="799" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Semi-supervised classification and noise detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Fuzzy Syst</title>
		<meeting>6th Int. Conf. Fuzzy Syst<address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08">Aug. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="277" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with explicit misclassification modeling</title>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Joint Conf</title>
		<meeting>18th Int. Joint Conf<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with an imperfect supervisor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="413" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Semisupervised document classification with a mislabeling error model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Eur. Conf. IR Res</title>
		<meeting>28th Eur. Conf. IR Res<address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-04">Apr. 2008</date>
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with Co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf. Comput. Learn. Theory</title>
		<meeting>11th Annu. Conf. Comput. Learn. Theory<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07">Jul. 1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Bayesian Co-training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2649" to="2680" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">CoTrade: Confident Co-training with data editing</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1612" to="1626" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Analyzing the effectiveness and applicability of Co-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Inf. Knowl. Manag</title>
		<meeting>9th Int. Conf. Inf. Knowl. Manag<address><addrLine>McLean, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Co-trained support vector machines for large scale unstructured document classification using unlabeled data and syntactic information</title>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="421" to="439" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Semi-supervised protein subcellular localization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">When does cotraining work in real data?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="788" to="799" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of alternatively spliced exons using Co-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tangirala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE BIBM</title>
		<meeting>IEEE BIBM<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Generalized linear discriminant analysis: A unified framework and efficient model selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1768" to="1782" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Handling uncertainties in SVM classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Niaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lartizien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Statist. Signal Process</title>
		<meeting>IEEE Workshop Statist. Signal ess<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="757" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">An algorithm for detecting noise on supervised classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Congr</title>
		<meeting>World Congr<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="701" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">His current research interests include machine learning include support vector machines, label noise, extreme learning, graphical models, classification, data clustering, probability density estimation, and feature selection</title>
	</analytic>
	<monogr>
		<title level="m">Benoît Frénay received the M.S. and Ph.D. degrees from</title>
		<meeting><address><addrLine>Louvain, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007 and 2013</date>
		</imprint>
		<respStmt>
			<orgName>Université Catholique de Louvain</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">He is the author or co-author of more than 250 scientific papers in international journals and books or communications to conferences with a reviewing committee. He is the co-author of the scientific popularization book on artificial neural networks in the series Que Sais-Je</title>
	</analytic>
	<monogr>
		<title level="m">Verleysen is an Editor-in-Chief of the Neural Processing Letters (Springer), a Chairman of the Annual ESANN Conference (European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting><address><addrLine>Louvain, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987 and 1992. 2007</date>
		</imprint>
	</monogr>
	<note>French, and the book Nonlinear Dimensionality Reduction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
