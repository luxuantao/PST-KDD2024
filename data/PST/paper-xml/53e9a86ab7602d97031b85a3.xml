<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PLSA-based Image Auto-Annotation: Constraining the Latent Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Florent</forename><surname>Monay</surname></persName>
							<email>monay@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute Rue du Simplon 4</orgName>
								<address>
									<postCode>592 1920</postCode>
									<settlement>Martigny</settlement>
									<region>CP</region>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute Rue du Simplon 4</orgName>
								<address>
									<postCode>592 1920</postCode>
									<settlement>Martigny</settlement>
									<region>CP</region>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PLSA-based Image Auto-Annotation: Constraining the Latent Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AEFAF319FA5D109277D44823FAA07F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Indexing methods Algorithms</term>
					<term>Theory</term>
					<term>Languages Automatic Annotation of Images</term>
					<term>Semantic Indexing</term>
					<term>PLSA</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The potential value of large image collections can be fully realized only when effective methods for access and search exist. Image users often prefer to formulate intuitive textbased queries to retrieve relevant images <ref type="bibr" target="#b1">[1]</ref>, which requires the annotation of each image in the collection. Automatic image annotation has thus emerged as one of the key research areas in multimedia information retrieval <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b2">2]</ref>, as an alternative to costly, labor-intensive manual captioning.</p><p>Motivated by the success of latent space models in text analysis, generative probabilistic models for auto-annotation have been proposed, including variations of PLSA <ref type="bibr" target="#b5">[5]</ref>, and Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">[2]</ref>. Such models use a latent variable representation for unsupervised learning of co-occurrences between image features and words in an annotated image collection, and later employ the learned models to predict words for unlabeled images <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b6">6]</ref>. The latent space representation can capture high-level relations within and across the textual and visual modalities.</p><p>Specific assumptions introduce variations in the ways in which co-occurrence information is captured. However, with a few exceptions <ref type="bibr" target="#b2">[2]</ref>, most previous works assume that words and visual features should have the same importance in defining the latent space <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b6">6]</ref>. There are limitations with this view. First, the semantic level of words is much higher than the one of visual features extracted even by state-ofthe-art methods. Second, in practice, visual feature cooccurrences across images often do not imply a semantic relation between them. This results in a severe degree of visual ambiguity that in general cannot be well handled by existing joint models. For auto-annotation, we are ultimately interested in defining a latent space that is consistent in semantic terms, while able to capture multimodal co-occurrences.</p><p>We present a novel approach to achieve the above goal, based on a linked pair of PLSA models. We constrain the definition of the latent space by focusing on textual features first, and then learning visual variations conditioned on the space learned from text. Our model consistently outperforms previous latent space models <ref type="bibr" target="#b6">[6]</ref>, while retaining the elegant formulation of annotation as probabilistic inference.</p><p>The paper is organized as follows. Section 2 describes our representation of annotated images. Section 3 presents the key PLSA concepts. Section 4 introduces our approach, motivated by the limitations of previous models. Section 5 presents experiments. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA REPRESENTATION</head><p>Annotated images are documents combining two complementary modalities, each one referring to the other: while an image potentially illustrates hundreds of words, its caption specifies the context. Both textual and visual modalities are represented in a discrete vector-space form.</p><p>Caption. The set of captions of an annotated image collection defines a keywords vector-space of dimension W , where each component indexes a particular keyword w that occurs in an image caption. The textual modality of a particular document d is thus represented as a vector t d = (t d1 , . . . , t dw , . . . , t dW ) of size W , where each element tw is the count of the corresponding word w in document d.</p><p>Image. We use two common image representations. RGB <ref type="bibr" target="#b6">[6]</ref>: 6 * 6 * 6 RGB histograms are computed from three distinct regions in the image, and only values higher than a threshold value are kept. This amounts at keeping only the dominant colors. The RGB vector-space is then built from the bin values found in the whole image set with respect to the three regions. The visual modality of document d is then v d = (v d1 , . . . , v db , . . . , v dB ), a vector of size B = 6 3 * 3.</p><p>Blobs <ref type="bibr" target="#b3">[3]</ref> : The normalized cut segmentation algorithm is applied to the image set, and the resulting regions are represented by color, texture, shape, size, and position descriptors. The K-means clustering algorithm is applied to all the computed descriptors, quantizing the image regions into a B-dimensional blob vector-space (same notation as RGB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE PLSA MODEL</head><p>In a collection of discrete data such as the annotated image dataset described in Section 2, a fundamental problem might occur: different elements from the vector-space can express the same concept (synonymy) and one element might have different meanings depending on the context (polysemy). If this semantic issue is well known for text, visual data share similar ambiguities: one color might have different meanings if occurring with different sets of color and two colors could represent the same concept.</p><p>When this ambiguities occur, a disambiguate latent space representation could potentially be extracted from the data, which is the goal of PLSA <ref type="bibr" target="#b5">[5]</ref>. This model assumes the existence of a latent variable z (aspect) in the generative process of each element xj in a particular document di. Given this unobserved variable, each occurence xj is independent from the document it was generated from, which corresponds to the following joint probability:</p><formula xml:id="formula_0">P (xj, z k , di) = P (di)P (z k | di)P (xj | z k ).</formula><p>The joint probability of the observed variables is obtained by marginalization over the K latent aspects z k ,</p><formula xml:id="formula_1">P (xj, di) = P (di) K X k P (z k | di)P (xj | z k ). (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Model parameters. The PLSA parameters are the two conditional distributions in equation 1, and are computed by an Expectation-Maximization algorithm on a set of training documents <ref type="bibr" target="#b5">[5]</ref>. For a vector-space representation of size N , P (x | z) is a N -by-K table that stores the parameters of the K multinomial distributions P (x | z k ). To give an intuition of P (x | z), Figure <ref type="figure" target="#fig_5">3</ref> (b) shows the posterior distribution of the 10 most probable words for a given aspect, for a model trained on a set of image captions. The keywords distribution refers to a people and costume-related set of keywords. P (x | z) characterizes the aspect, and is valid for documents out of the training set <ref type="bibr" target="#b5">[5]</ref>.</p><p>On the contrary, the other K-by-M table P (z | d) is only relative to the M training documents. Storing the parameters of the M multinomial distributions P (z | di), it does not carry any a priori information about the probability of aspect z k beeing expressed in any unseen document.</p><p>Learning. The standard Expectation-Maximization approach is used to compute the model parameters P (x | z) and P (z | d) by maximizing the data likelihood.</p><formula xml:id="formula_3">L = M Y i N Y j P (di) K X k P (z k | di)P (xj | z k ) n(d i ,x j ) ,<label>(2)</label></formula><p>where n(di, xj) is the count of element xj in document di. Inference: PLSA of a new document. For an unseen document dnew, the conditional distribution over aspects P (z | dnew) has to be computed. The method proposed in <ref type="bibr" target="#b5">[5]</ref> consist in maximizing the likelihood of the document dnew with a partial version of the EM algorithm described above, where P (x | z) is kept fixed (not updated at each M-step). In doing so, P (z | dnew) maximizes the likelihood of document dnew with respect to the previously trained P (x | z) parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PLSA-BASED ANNOTATION</head><p>PLSA has been recently proposed as a model for automatic image annotation <ref type="bibr" target="#b6">[6]</ref>. Referred here as PLSA-mixed, it somewhat showed surprisingly poor annotation performance with respect to very basic non probabilistic methods <ref type="bibr" target="#b6">[6]</ref>. We propose here a new application of PLSA to automatic image annotation and motivate our approach by an analysis of PLSA-mixed, which then leads to the new method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PLSA-mixed</head><p>The PLSA-mixed system applies a standard PLSA on a concatenated representation of the textual and the visual modalities of a set of annotated images d:</p><formula xml:id="formula_4">x d = (t d , v d ).</formula><p>Using a training set of captioned images, P (x | z) is learned for both textual and visual co-occurrences, which is an attempt to capture simultaneous occurrence of visual features (regions or dominant colors) and words. Once P (x | z) has been learned, those parameters can be used for the autoannotation of a new image.</p><p>The new image dnew is represented in the concatenated vector space, where all keywords elements are zero (no annotation): xnew = (0, vnew). The multinomial distribution over aspects given the new image P (z | dnew) is then computed with the partial PLSA steps described in Section 3, and allows the computation of P (x | dnew). From P (x | dnew), the marginal distribution over the keyword vector-space P (t | dnew) is easily extracted. The annotation of dnew results from this distribution, either by selecting a predefined number of the most probable keywords or by thresholding the distribution P (t | dnew).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problems with PLSA-mixed</head><p>Using a concatenated representation, PLSA-mixed attempts to simultaneously model visual and textual modalities with PLSA. It means that intrinsically, PLSA-mixed assumes that the two modalities have an equivalent importance in defining the latent space. This has traditionally been the assumption in most previous work <ref type="bibr" target="#b4">[4]</ref>. However, an analysis of the captions and the image features in the Corel dataset (described in Section 5) emphasizes the difference between the keywords and the visual features occurrences. Figure <ref type="figure" target="#fig_1">1</ref> shows two similarity matrices for a set of annotated images ordered by topics, as in human-based CD organization provided by Corel. They represent the cosine similarity between each document in the keyword space (left), and the visual feature space (Right). The keywords similarity matrix has sharp block-diagonal structure, each corresponding to a consistent cluster of images, while the second similarity matrix (visual features) consist in a less contrasted pattern. Of course, Figure <ref type="figure" target="#fig_1">1</ref> does not prove that no latent representation exists for the visual features, but it strongly suggests that in general, two PLSA separately applied on each modality would define two distinct latent representations of the same document. For example, color co-occurrence happens across images, but does not necessarily mean that the corresponding images are semantically related. PLSA-mixed thus might model aspects mainly based on visual features, which results in a prediction of almost random keywords if these aspects have high probabilities given the image to annotate. Moreover, assuming that no particular importance is given to any modality, the amount of visual and textual information need to be balanced in the concatenate representation of an annotated image. This constrains the size of the visual representation, as the number of keywords per image is usually limited (an average of 3 for the data we used). A typical aspect from PLSA-mixed where images are relatively consistent in terms of visual features, but not semantically (dominant colors: green, red, yellow, black) is shown in Figure <ref type="figure" target="#fig_3">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Our approach: PLSA-words</head><p>Given the above observations, we propose to model a set of documents d with two linked PLSA models sharing the same distribution over aspects P (z | d). Contrarily to PLSAmixed, this formulation allows to treat each modality differently and give more importance to the captions in the latent space definition. The idea is to capture meaningful aspects in the data and use those for annotation. Both parameters estimation and annotation inference involve two linked PLSA steps<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning parameters 1.</head><p>A first PLSA model is completely trained on the set of image captions to learn both P (t | z) and P (z | d) parameters. Figure <ref type="figure" target="#fig_5">3</ref> illustrates one aspect automatically learned on the textual modality, with its most probable training images (a ) and their corresponding distribution over keywords P (t | z) (b ). This example 2 shows that this first PLSA can capture meaningful aspects from the data.</p><p>2 . We then consider that the aspects have been observed for this set of documents d and train a second PLSA on the visual modality to compute P (v | z), keeping P (z | d) from above fixed. Note that this technique is very similar to the process described in Section 3, where P (x | z) was kept fixed and P (z | d) was computed by likelihood maximization.  2. The posterior probability of keywords given this new image is then inferred by:</p><formula xml:id="formula_5">P (t | dnew) = K X k P (t | z k ) * P (z k | dnew)<label>(3)</label></formula><p>If a new image has a high probability of belonging to one aspect, then a consistent set of keywords will be predicted. The PLSA-words method thus automatically builds a kind of language model for the set of training images, which is then applied for auto-annotation. It is also interesting to notice that PLSA is applied here on very small textual documents, given that each annotation is about 3 words long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>The data used for experiments are comprised of roughly 16000 Corel images split in 10 overlapping subsets, each devided in training (∼5200 images) and testing sets (∼1800 images). The average vocabulary size per subset is 150 keywords, and the average caption size is 3. Both RGB and Blobs features described in Section 2 are tested. Blob features were downloaded from Kobus Barnard's website <ref type="bibr" target="#b4">[4]</ref>. 2 Find more examples at www.idiap.ch/∼monay/acmm04/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance measures</head><p>No commonly agreed image auto-annotation measure exists. We evaluated our method on three different measures, but restrict the discussion to the two measures described below for space reasons 3 .</p><p>Annotation accuracy : When predicting exactly the same number of keywords as the ground truth, the annotation accuracy for one image is defined as Acc = r/n, where r is the number of correctly predicted keywords and n is the size of the ground truth caption. The average annotation accuracy is computed over a set of images.</p><p>Normalized Score <ref type="bibr" target="#b4">[4]</ref> : Sharing the same r and n values with the above definition, the normalized score is defined as: N score = r/n -(p -r)/(N -n), where N is the vocabulary size and p is the number of predicted keywords. The average normalized score is computed over a set of images for a varying number of predicted keywords and the maximum is reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We compare the two PLSA-based methods described in Section 4.1 and 4.3, and three other methods : Empirical, LSA and PLSA-split. Empirical simply uses the empirical keywords distribution from the training set to predict the same set of keywords regardless of the image content; LSA was the best method reported in <ref type="bibr" target="#b6">[6]</ref> in term of normalized score, better than PLSA-mixed; and PLSA-split is the unlinked equivalent of PLSA-words, for which two distinct sets of parameters Pt(z | d) and Pv(z | d) are learned for each modality. The latent space dimensionality K = 100 has been used for all the reported results (except Empirical). The average annotation accuracy results are presented in Table <ref type="table">1</ref>   <ref type="table">1</ref>: Average annotation accuracy computed over the 10 subsets. These values correspond to an average number of 3.1 predicted keywords per image. The variance is given in parantheses.</p><p>The RGB and Blobs features give similar annotation performance for both measures. This suggests that the blob representation is equivalent to the much simpler RGB features when applied to this annotation task. One explanation could be that the k-means algorithm applied on the concatenated color and texture representation of the image regions converges to a color-only driven clustering.</p><p>As originally reported <ref type="bibr" target="#b6">[6]</ref>, the PLSA-mixed maximum normalized score is lower than the non-probabilistic LSA one, while PLSA shows better performance than LSA for textual data modeling <ref type="bibr" target="#b5">[5]</ref>. Annotation accuracy, which measures the quality of smaller but more realistic annotation, gives PLSA-mixed as the best performing method.</p><p>The ranking of the three PLSA-based methods emphasizes the importance of a well defined link between textual and visual modalities. PLSA-split naively assumes no link between captions and images and models them separately. No 3 Prec./Recall measures at www.idiap.ch/∼monay/acmm04/ match between the two latent space definitions exist, which explains why PLSA-split performs worse than the simplest Empirical method. The PLSA-mixed method introduces a determining yet unclear interaction between text and image by concatenating the two modalities. This connexion translates in significant improvement over PLSA-split in both annotation and normalized score measures.</p><p>PLSA-words outperforms both PLSA-split and PLSAmixed, therefore justifing its design. PLSA-words makes an explicit link between visual features and keywords, learning the latent aspects distribution in the keywords space and fixing these parameters to learn the distribution of visual features. This results in the definition of semantically meaningfull clusters, and forces the system to predict consistent sets of keywords. Performing significantly better than all the other methods for all the measures, it improves the performance of the PLSA-mixed and LSA methods for both normalized score and annotation accuracy measures. The relative annotation accuracy improvement for the Blobs features is 108% with respect to LSA and 32% with respect to PLSA-mixed (respectively 66% and 33% for the RGB case). Table <ref type="table">2</ref>: Average maximum normalized score value over the 10 subsets. The variance is given in parantheses and the corresponding average number of keywords predicted is in brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We proposed a new PLSA-based image auto-annotation system, which uses two linked PLSA models to represent the textual and visual modalites of an annotated image. This allows a different processing of each modality while learning the parameters and makes a truely semantic latent space definition possible. We compared this method to previously proposed systems using different performance measures and showed that this new latent space modeling significantly improves the previous latent space methods based on a concatenated textual+visual representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>E</head><label></label><figDesc>-step : P (z | d, x), the probabilities of latent aspects given the observations are computed from the previous estimate of the model parameters (randomly initialized). M-step : The parameters P (x | z) and P (z | d) are updated with the new expected values P (z | d, x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Similarity matrices for a set of manually ordered documents (9 CDs from Corel). The left matrix is the textual modality, the right matrix is the visual modality (Blobs features are used).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: One semantically meaningless aspect from PLSA-MIXED: the 9 most probable images in the training set and the 10 most probable keywords with their corresponding probability P (t | z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: One aspect from PLSA learned on words: the 9 most probable images in the training set (from P (z | d)) and the 10 most probable keywords with their corresponding probability P (t | z). Annotation by inference 1. Given new visual features vnew and the previously calculated P (v | z) parameters, P (z | dnew) is computed for a new image dnew using the standard PLSA procedure for a new document (Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc>and Table2contains the maximum normalized scores values. All results are averaged over the 10 subsets.</figDesc><table><row><cell>Method</cell><cell>BLOBS</cell><cell>RGB</cell></row><row><cell>Empirical</cell><cell cols="2">0.191 (0.012) 0.191 (0.012)</cell></row><row><cell>LSA</cell><cell cols="2">0.140 (0.009) 0.178 (0.009)</cell></row><row><cell>PLSA-split</cell><cell cols="2">0.113 (0.017) 0.121 (0.019)</cell></row><row><cell>PLSA-mixed</cell><cell cols="2">0.221 (0.011) 0.217 (0.024)</cell></row><row><cell cols="3">PLSA-words 0.292 (0.011) 0.288 (0.014)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>PLSA-words 0.570 (0.013) [31.2] 0.571 (0.013) [31.3]</figDesc><table><row><cell></cell><cell>BLOBS</cell><cell>RGB</cell></row><row><cell>Empirical</cell><cell cols="2">0.427 (0.016) [36.2] 0.427 (0.016) [36.2]</cell></row><row><cell>LSA</cell><cell cols="2">0.521 (0.013) [40.6] 0.540 (0.011) [37.9]</cell></row><row><cell>PLSA-split</cell><cell cols="2">0.273 (0.020) [43.8] 0.298 (0.022) [36.3]</cell></row><row><cell>PLSA-mixed</cell><cell cols="2">0.463 (0.018) [37.2] 0.473 (0.020) [36.4]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Computational complexity is discussed at www.idiap.ch/∼monay/acmm04/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been carried out in the framework of the Swiss NCCR project (IM)2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of user need in image archives</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Armitage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Enser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="299" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling annotated data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. on Research and Development in Information Retrieval (ACM SIGIR)</title>
		<meeting>ACM Int. Conf. on Research and Development in Information Retrieval (ACM SIGIR)</meeting>
		<imprint>
			<date type="published" when="2003-08">Aug 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On image auto-annotation with latent space models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. on Multimedia (ACM MM)</title>
		<meeting>ACM Int. Conf. on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2003-11">Nov 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
