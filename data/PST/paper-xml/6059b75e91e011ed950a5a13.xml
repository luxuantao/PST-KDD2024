<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Predictive Substructures with Subgraph Information Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-20">20 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yu</surname></persName>
							<email>yujunchi2019@ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<email>tingyangxu@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<email>yataobian@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>joehuang@tencent.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Tingyang Xu</orgName>
								<address>
									<settlement>Yu Rong, Yatao Bian</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tencent AI LAB</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Predictive Substructures with Subgraph Information Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-20">20 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.11155v1[cs.LG]</idno>
					<note type="submission">received April 19, 2005; revised August 26, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Convolutional Network</term>
					<term>Subgraph Information Bottleneck</term>
					<term>Graph Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The emergence of Graph Convolutional Network (GCN) has greatly boosted the progress of graph learning. However, two disturbing factors, noise and redundancy in graph data, and lack of interpretation for prediction results, impede further development of GCN. One solution is to recognize a predictive yet compressed subgraph to get rid of the noise and redundancy and obtain the interpretable part of the graph. This setting of subgraph is similar to the information bottleneck (IB) principle, which is less studied on graph-structured data and GCN. Inspired by the IB principle, we propose a novel subgraph information bottleneck (SIB) framework to recognize such subgraphs, named IB-subgraph. However, the intractability of mutual information and the discrete nature of graph data makes the objective of SIB notoriously hard to optimize. To this end, we introduce a bilevel optimization scheme coupled with a mutual information estimator for irregular graphs. Moreover, we propose a continuous relaxation for subgraph selection with a connectivity loss for stabilization. We further theoretically prove the error bound of our estimation scheme for mutual information and the noise-invariant nature of IB-subgraph. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior property of IB-subgraph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M ANY real-world data such as social networks, drug molecules, and point clouds can be viewed as graphs. Classifying the labels or analyzing the underlying properties of graph-structured data is the fundamental problem in the deep graph learning area. Recently, there is a surge of interest to employ Graph Convolutional Network (GCN) <ref type="bibr" target="#b0">[1]</ref> to cope with such irregular graphs. Many GCN variants <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> have achieved new state-of-the-art performance in various graph-level tasks. Apart from the tremendous progress in deep graph learning, the main concern is that real-world graphs are likely to contain redundant even noisy structure information <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. This is detrimental to various GCN variants as they are incapable of recognizing the poisoned structure <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Moreover, since GCN variants generally leverage all structure information for prediction <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, the interpretation for their prediction results has lagged compared to the significant gain in performance. These two challenges for the literature of graph learning trigger an interesting idea to recognize a subgraph which is predictive in terms of the graph label and discards unnecessary information.</p><p>Recognizing the predictive yet compressed subgraph sheds lights on various tasks as it discovers the vital substructure and filters out irrelevant parts. For example, in drug discovery, when viewing molecules as graphs with atoms as nodes and chemical bonds as edges, biochemists are interested in identifying the subgraphs that mostly represent certain properties of the molecules, namely the functional groups <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In graph representation learning, the predictive subgraph highlights the vital substructure for graph classification, and provides an alternative way for yielding graph representation besides mean/sum aggregation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref> and pooling aggregation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In graph attack and defense, it is vital to purify a perturbed graph and mine the robust structures for classification <ref type="bibr" target="#b14">[15]</ref>. In 3D object detection and segmentation, viewing point clouds as K-NN graphs, researchers seek for substructures which share the same categories <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Along with wide applications, the major difficulty is that it is laborious and time-consuming to obtain explicitly subgraph-level annotations for supervised learning. For example, there are 250 thousand molecules in ZINC250K dataset <ref type="bibr" target="#b20">[21]</ref>. It requires extra expertise and experience to label every functional group in each molecule. In the absence of explicit supervision, one is blind to recognize the predictive yet compressed subgraph. Recently, the mechanism of self-attentive aggregation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> employs attention scores to validate the significance of nodes and somehow discovers a vital substructure at node level with a well-selected threshold. However, the discovered substructure is highly influenced by the threshold. Moreover, this method only identifies isolated important nodes but ignores the topological information at the subgraph level. Consequently, it leads to a novel challenge as subgraph recognition: how can we recognize a compressed subgraph with minimum information loss in terms of predicting the graph labels/properties?</p><p>Recalling the above challenge, there is a similar problem setting in information theory called information bottleneck (IB) principle <ref type="bibr" target="#b23">[24]</ref>, which aims to juice out a compressed code from the original data that keeps most predictive information of labels or properties. Recently, there is a growing tendency to incorporate IB principle with deep learning due to its capability of extracting informative representation from regular data in the fields of computer vision <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, reinforcement learning <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> and natural language processing <ref type="bibr" target="#b29">[30]</ref>. However, current IB methods, like VIB <ref type="bibr" target="#b25">[26]</ref>, are still incapable for irregular graph data. Another work, Graph Information Bottleneck <ref type="bibr" target="#b30">[31]</ref>, attempts to bind IB with node representation learning. It follows the Gaussian prior assumption in <ref type="bibr" target="#b25">[26]</ref>, iteratively sample neighbors and learning node representation at each layer of the network, which is still far from subgraph recognition. Hence, it is still challenging for IB to compress irregular graph data, like a subgraph from an original graph, with a minimum information loss.</p><p>To this end, we advance the IB principle for irregular graph data to solve the proposed subgraph recognition problem, which leads to a novel principle, Subgraph Information Bottleneck (SIB). SIB directly recognizes a compressed subgraph which is most predictive of a certain graph label without any subgraph annotations. This idea significantly distinguishes from prior researches in IB in two aspects. First of all, SIB directly reveals the vital substructure at the subgraph level instead of learning an optimal representation of the input data in the hidden space. Secondly, SIB deals with discrete and irregular graph data rather than regular data. We first i) leverage the mutual information estimator from Deep Variational Information Bottleneck (VIB) <ref type="bibr" target="#b25">[26]</ref> for irregular graph data as the SIB objective. However, VIB is intractable to compute the mutual information without knowing the distribution forms, especially on graph data. To tackle this issue, ii) we adopt a bi-level optimization scheme to maximize the SIB objective. Meanwhile, the continuous relaxation that we adopt to approach the discrete selection of subgraph will lead to unstable optimization process. To further stabilize the training process and encourage a compact subgraph, iii) we propose a novel connectivity loss to assist SIB to effectively discover the maximally informative but compressed subgraph, which is defined as IB-Subgraph. By optimizing the above SIB objective and connectivity loss, one can recognize the IB-Subgraph only with the input graph and its label/property. On the other hand, iv) SIB is model-agnostic and can be easily plugged into various Graph Neural Networks (GNNs).</p><p>We evaluate SIB in four application scenarios: improvement of graph classification, graph interpretation, graph denoising, and 3D relevant structure extraction. Extensive experiments on both synthetic and real-world datasets demonstrate that the information-theoretic IB-Subgraph, recognized by the proposed Subgraph Information Bottleneck, enjoys superior graph properties compared to the subgraphs found by SOTA baselines.</p><p>This paper is an extension of the conference work <ref type="bibr" target="#b31">[32]</ref>. There are three major improvements over the previous one: 1). We theoretically analyze with provable guarantees that the IB-subgraph, found by SIB framework, is noiseinvariant. In the previous version, we empirically analyze that the significant performance gain in various tasks is because SIB is able to preserve the informative substructure and discard noise and redundancy. In this paper, we further indicate that optimizing SIB objective is equivalent to minimize the upper bound of the mutual information of the IBsubgraph and noise, and thus leads to the noise-invariance nature of IB-subgraph. 2). We provide more theoretical analysis on our optimization scheme. We give the error bound when estimating the mutual information following the PAC-Bayes framework. 3). We provide more experiment results on computer vision tasks. Despite of graph interpretation, graph classification and graph denoising, we further evaluate the proposed method on S3DIS point cloud dataset <ref type="bibr" target="#b32">[33]</ref>. Experiment results show the proposed method is compatible to the large GNN models and efficiently extracts label-relevant structures on large-scale graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Subgraph Discovery. Traditional subgraph discovery mainly includes dense subgraph discovery and frequent subgraph mining. Dense subgraph discovery aims to find the subgraph with the highest density (e.g. the number of edges over the number of nodes <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>). Frequent subgraph mining is to look for the most common substructure among graphs <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Recently, it is popular to select a neighborhood subgraph of a central node to do message passing in node representation learning. DropEdge <ref type="bibr" target="#b38">[39]</ref> relieves the over-smoothing phenomenon in deep GCNs by randomly dropping a portion of edges in graph data. Similar to DropEdge, DropNode <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> principle is also widely adopted in node representation learning. FastGCN <ref type="bibr" target="#b39">[40]</ref> and ASGCN <ref type="bibr" target="#b41">[42]</ref> accelerate GCN training via node sampling. GraphSAGE <ref type="bibr" target="#b40">[41]</ref> leverages neighborhood sampling for inductive node representation learning. NeuralSparse <ref type="bibr" target="#b42">[43]</ref> select Top-K (K is a hyper-parameter) task-relevant 1-hop neighbors of a central node for robust node classification. Similarly, researchers discover the vital substructure at node level via the attention mechanism <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b43">[44]</ref> further identifies the important computational graph for node classification. <ref type="bibr" target="#b44">[45]</ref> discovers subgraph representations with specific topology given subgraph-level annotation. However, the above methods are far from subgraph recognition, as they are incapable of discovering a compressed yet predictive subgraph in graph data.</p><p>Graph Classification. In recent literature, there is a surge of interest in adopting graph neural networks (GNN) in graph classification. The core idea is to aggregate all the node information for graph representation. A typical implementation is the mean/sum aggregation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, which is to average or sum up the node embeddings. An alternative way is to leverage the hierarchical structure of graphs, which leads to the pooling aggregation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b45">[46]</ref>. When tackling redundant and noisy graphs, these approaches will likely to result in the sub-optimal graph representation. Recently, InfoGraph <ref type="bibr" target="#b46">[47]</ref> maximize the mutual information between graph representations and multilevel local representations to obtain more informative global representations.</p><p>Information Bottleneck. Information bottleneck (IB), originally proposed for signal processing, attempts to find a short code of the input signal but preserves maximum information of the code <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b25">[26]</ref> firstly bridges the gap between IB and deep learning, and proposed variational information bottleneck (VIB). Nowadays, IB and VIB have been wildly employed in computer vision <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, reinforcement learning <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, natural language processing <ref type="bibr" target="#b29">[30]</ref>, and speech and acoustics <ref type="bibr" target="#b47">[48]</ref> due to the capability of learning compact and meaningful representations. However, IB is less researched on irregular graphs due to the intractability of mutual information. A parallel work, named Graph Information Bottleneck, recently incorporates IB with node representation learning. However, it is still far from directly recognizing a compressed but informative subgraph in graph data.</p><p>Point Cloud Segmentation. Point cloud segmentation is to identify the category of each element in the set of points, a type of 3D geometric data. Traditional convolutional network can not consume such irregular data. PointNet <ref type="bibr" target="#b48">[49]</ref> first proposes a unified framework to process the point cloud. Recent breakthroughs in graph learning innovate researchers to view the point cloud as a K-NN graph, thus naturally leading to GCN-based solutions <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Beyond that, as it is expensive to label the point cloud, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> leverage a tiny portion of labeled point for weakly supervised training. Here, in the subgraph recognition phenomenon, one is required to infer the underlying substructure of a certain category with no point-level attribution, which is a huge challenge for existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NOTATIONS AND PRELIMINARIES</head><p>Let {(G 1 , Y 1 ), . . . , (G N , Y N )} be a set of N graphs with their real value properties or categories, where G n refers to the n-th graph and Y n refers to the corresponding properties or labels. We denote by G n = (V, E, A, X) the n-th graph of size M n with node set V = {V i |i = 1, . . . , M n }, edge set E = {(V i , V j )|i &gt; j; V i , V j is connected}, adjacent matrix A ∈ {0, 1} Mn×Mn , and feature matrix X ∈ R Mn×d of V with d dimensions, respectively. Denote the neighborhood of V i as N (V i ) = {V j |(V i , V j ) ∈ E}. We use G sub as a specific subgraph and G sub as the complementary structure of G sub in G. Let f : G → R/[0, 1, • • • , n] be the mapping from graphs to the real value property or category, Y . G is the domain of graphs. I(X, Y ) refers to the Shannon mutual information of two random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph convolutional network</head><p>Graph convolutional network (GCN) is widely adopted to graph classification. Given a graph G = (V, E) with node feature X and adjacent matrix A, GCN outputs the node embeddings X from the following process:</p><formula xml:id="formula_0">X = GCN(A, X; W ) = ReLU(D − 1 2 ÂD − 1 2 XW ),<label>(1)</label></formula><p>where D refers to the diagonal matrix with nodes' degrees and Â = A + I is the adjacent matrix after adding the selfloop. W refers to the model parameters.</p><p>One can simply sum up the node embeddings to get a fixed-length graph embeddings <ref type="bibr" target="#b12">[13]</ref>. Recently, researchers attempt to exploit an hierarchical structure of graphs, which leads to various graph pooling methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. <ref type="bibr" target="#b21">[22]</ref> enhances the graph pooling with selfattention mechanism to leverage the importance of different nodes contributing to the results. Finally, the graph embedding is obtained by multiplying the node embeddings with the normalized attention scores:</p><formula xml:id="formula_1">E = Att(X ) = softmax(Φ 2 tanh(Φ 1 X T ))X ,<label>(2)</label></formula><p>where Φ 1 and Φ 2 refer to the model parameters of selfattention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Bottleneck</head><p>Given the input data X and the label Y , the information bottleneck principle aims to discover the latent representation Z which is maximally informative in terms of Y (Sufficient) and contains as little information of the input data X as possible (Minimal). Formally, the sufficient and minimal representation, denoted as Z s and Z m , can be obtained by the following objectives respectively:</p><formula xml:id="formula_2">Z s = arg max Z I(Z, Y ) Z m = arg min Z I(Z, X)<label>(3)</label></formula><p>where I(A, B) is the mutual information between random variable A and B:</p><formula xml:id="formula_3">I(A, B) = a∈A b∈B p(a, b) log p(a, b) p(a)p(b) dadb<label>(4)</label></formula><p>Built upon the above intuition, one can learn the minimally sufficient Z by maximizing the information bottleneck objective:</p><formula xml:id="formula_4">L IB = I(Z, Y ) − βI(X, Z)<label>(5)</label></formula><p>where β refers to a hyper-parameter trading off informativeness and compression. Optimizing this objective will lead to a minimally sufficient Z, which is less prone to overfitting and less sensitive to noise. However, the IB objective is notoriously difficult to optimize as it is troublesome to compute the mutual information. <ref type="bibr" target="#b25">[26]</ref> optimize a tractable lower bound of the IB objective:</p><formula xml:id="formula_5">L V IB = 1 N N i=1 p(z|x i ) log q φ (y i |z)dz − βKL(p(z|x i )|r(z)),<label>(6)</label></formula><p>where q φ (y i |z) is the variational approximation to p φ (y i |z) and r(z) is the prior distribution of Z. KL is the Kullback-Leibler divergence. However, it is hard to estimate the mutual information in high dimensional space when the distribution forms are inaccessible, especially for irregular graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZING THE SUBGRAPH INFORMATION BOTTLENECK OBJECTIVE FOR SUBGRAPH RECOG-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NITION</head><p>In this section, we will elaborate on the proposed method in detail. We first formally define the subgraph information bottleneck and IB-Subgraph. Then, we introduce a novel framework for SIB to effectively find the IB-Subgraph. Moreover, we propose a bi-level optimization scheme and a graph mutual information estimator for SIB optimization.</p><p>We further elaborate a continuous relaxation strategy with a connectivity loss to stabilize the training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subgraph Information Bottleneck</head><p>To begin with, we formally define the Minimal Subgraph and the Sufficient Subgraph: Definition 4.1. G sub is the Minimal Subgraph if it contains minimally usable information of G. We denote it as</p><formula xml:id="formula_6">G m sub = arg min G sub I(G, G sub ).</formula><p>Intuitively, the minimal subgraph compresses the information via dropping a portion of the topological structures of the corresponding graph. By the definition of mutual information, we have:</p><formula xml:id="formula_7">I(G sub , G) = H(G sub ) − H(G sub |G) ≤ H(G sub ) ≤ log |G sub |,<label>(7)</label></formula><p>where | • | denotes the cardinally of a set. Eq. 7 provides a natural bound of I(G sub , G). That is, given a finite set G, one can minimize the number of the elements in the induced subgraph set G sub . One possible way is to discover the most common and frequent substructures among graphs <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. However, it is troublesome to directly optimize the above bound since it is quite loose and hard to optimize with gradient methods. As a consequence, we can achieve comparable performance in the downstream tasks when replacing G with G sub . However, there is a trivial solution where G s sub = G. In this case, we are unable to distinguish the essential subgraphs and the noise and redundancy.</p><p>To this end, we generalize the information bottleneck principle to recognize minimal sufficient subgraphs in irregular graphs, which leads to the subgraph information bottleneck (SIB) principle.  </p><formula xml:id="formula_8">G sub I(Y, G sub ) − βI(G, G sub ).<label>(8)</label></formula><p>We denote the subgraph G sub induced by Eq. 8 as the IB-Subgraph. As is shown in Eq. 8, learning the IB-subgraph needs no subgraph-level annotations. One is supposed to obtain the IB-subgraph only with the input graphs and their labels, thanks to the information-theoretic SIB objective. Statistically, IB-Subgraph is minimal sufficient concerning the original graph. Intuitively, the IB-Subgraphs only preserve label-relevant substructure in original graphs, and thus reduce the effect of noise and redundancy to the downstream tasks. Specifically, let G n be the noisy substructure which is irrelevant to the graph property Y , we show the IB-Subgraph G sub is invariant to the noise G n .</p><p>Proposition 1 (Noise-invariance). Suppose the noisy structure G n is independent of Y , the mutual information</p><formula xml:id="formula_9">I(G n , G sub ) is upper bounded by I(G sub , G) − I(G sub , Y ): I(G n , G sub ) ≤ I(G, G sub ) − I(Y, G sub ).<label>(9)</label></formula><p>Proof. We prove the above proposition following the Markov chain assumption in <ref type="bibr" target="#b55">[56]</ref>. Suppose G n interacts with G sub only through G and G is defined by Y and G n . We can define the following Markov chain (Y, G n ) → G → G sub . By the Data Processing Inequality, we obtain:</p><formula xml:id="formula_10">I(G; G sub ) ≥ I(Y, G n ; G sub ) = I(G n ; G sub ) + I(Y ; G sub |G n ) = I(G n ; G sub ) + H(Y |G n ) − H(Y |G n ; G sub ),<label>(10)</label></formula><p>since G n and Y are independent, we obtain</p><formula xml:id="formula_11">H(Y |G n ) = H(Y ). Moreover, it is straightforward that H(Y |G n ; G sub ) ≤ H(Y |G sub ).</formula><p>Plug the above equality and inequality into Eq. 10, and we derive:</p><formula xml:id="formula_12">I(G; G sub ) ≥ I(G n ; G sub ) + H(Y ) − H(Y |G sub ) = I(G n ; G sub ) + I(Y ; G sub ),<label>(11)</label></formula><p>thus we obtain</p><formula xml:id="formula_13">I(G n , G sub ) ≤ I(G, G sub ) − I(Y, G sub )</formula><p>and conclude the proof. QED Proposition 1 indicates that optimizing the SIB objective in Eq. 8 is equivalent to minimize the mutual information between the IB-Subgraph and the noisy substructure, leading to the noise-invariance property of the IB-Subgraph. This property is appealing as it provides theoretical guarantees that one can effectively discover the vital substructure which mostly influences the property of the original graph by compressing the information in the IB-Subgraph, even in the absence of explicit subgraph-level annotations. Meanwhile, as the SIB objective also restrict the IB-Subgraph to be predictive, it is sufficient to plug SIB objective into various baseline models to enhance their performances, which shows SIB is model-agnostic.</p><p>However, the SIB objective in Eq. 8 is notoriously hard to optimize due to the intractability of mutual information and the discrete nature of irregular graph data. We then introduce approaches on how to optimize such objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bi-level optimization for the SIB objective</head><p>The SIB objective in Eq. 8 consists of two parts. We examine the first term I(Y, G sub ) in Eq. 8. This term measures the relevance between G sub and Y : with a variational approximation q φ1 (Y |G sub ), we obtain a tractable lower bound of the first term in Eq. 8:</p><formula xml:id="formula_14">I(Y, G sub ) = p(Y, G sub ) log p(Y |G sub )dyY dG sub + H(Y ).</formula><formula xml:id="formula_15">I(Y, G sub ) = H(Y |G sub ) + H(Y ) = p(Y, G sub ) log q φ1 (Y |G sub )dY + KL[p(Y |G sub )|q φ1 (Y |G sub ))] ≥ p(Y, G sub ) log q φ1 (Y |G sub )dY dG sub ≈ 1 N N i=1 log q φ1 (Y i |G subi ) =: −L cls (q φ1 (Y |G sub ), Y gt ),<label>(13)</label></formula><p>where Y gt is the ground truth label of the graph. Eq. 13 indicates that maximizing I(Y, G sub ) is achieved by the minimization of the classification loss between Y and G sub as L cls . Intuitively, minimizing L cls encourages the subgraph to be predictive of the graph label, and thus leads to the relevance between G sub and Y . In practice, we choose the cross entropy loss for categorical Y and the mean squared loss for continuous Y , respectively. Then we introduce the PAC-Bayes bound: <ref type="figure">and N (t, F, d</ref>) is the covering number of F . Then, for any δ ∈ (0, 0.5), with probability p ≥ 1 − δ we have the following inequality , up to a constant:</p><formula xml:id="formula_16">Theorem 2. Suppose G sub and Y take value in G and Y respectively. Let f (Y, G sub ) = log q φ1 (Y |G sub ) ∈ [−B, B], and F is the family of f (Y, G sub ). let (F, d) be a metric space, d = || • || 2 . Define F t = {(f 1 , f 2 , • • • , f n )|∀f k ∈ F, ∃i ∈ 1 • • • , n, d(f k , f i ) ≤ t} is a t-cover of F,</formula><formula xml:id="formula_17">|I(Y, G sub ) − 1 N N i=1 log q φ1 (Y i |G subi )| ≤ B 2 log 1 δ N + inf t 2(t + log N (t, F, d) N ) + KL[p(Y |G sub )|q φ1 (Y |G sub ))].<label>(14)</label></formula><p>Theorem 2 indicates that we can reduce the estimating error with large training sets and simple prediction models. Moreover, the KL term decreases when the variational approximation approaches p(Y |G sub ). This also reduces the estimating error. The proof is in supplementary materials. Then, we consider the minimization of I(G, G sub ) which is the second term of Eq. 8. Remind that <ref type="bibr" target="#b25">[26]</ref> introduces a tractable prior distribution r(Z) in Eq. 6, and thus results in a variational upper bound. However, this setting is troublesome as it is hard to find a reasonable prior distribution for p(G sub ), which is the distribution of graph substructures instead of latent representation. Thus we go for another route. Directly applying the DONSKER-VARADHAN representation <ref type="bibr" target="#b54">[55]</ref> of the KL-divergence, we have:</p><formula xml:id="formula_18">I(G, G sub ) = sup f φ 2 :G×G→R E G,G sub ∈p(G,G sub ) f φ2 (G, G sub ) − log E G∈p(G),G sub ∈p(G sub ) e f φ 2 (G,G sub ) ,<label>(15)</label></formula><p>where f φ2 is the statistics network that maps from the graph set to the set of real numbers. In order to approximate I(G, G sub ) using Eq. 15, we design a statistics network based on modern GNN architectures as shown by Figure θ ← θ 0 , φ 1 ← φ 0 1 3:</p><formula xml:id="formula_19">for i = 0 → N do 4: φ 2 ← φ 0 2 5:</formula><p>for t = 0 → T do 6:</p><formula xml:id="formula_20">φ t+1 2 ← φ t 2 + η 1 ∇ φ t 2 L MI 7:</formula><p>end for 8:</p><formula xml:id="formula_21">θ i+1 ← θ i − η 2 ∇ θ i L(θ i , φ i 1 , φ T 2 )</formula><p>9:</p><formula xml:id="formula_22">φ i+1 1 ← φ i 1 − η 2 ∇ φ i 1 L ( θ i , φ i 1 , φ T 2 )</formula><p>10:</p><p>end for 11:</p><formula xml:id="formula_23">G sub ← g(G; θ N ) 12:</formula><p>return G sub 13: end function 1: first, we use a GNN to extract embeddings from both G and G sub (parameter shared with the subgraph generator, which will be elaborated in Section 4.3), then concatenate G and G sub embeddings and feed them into a Multi-Layer Perceptron (MLP), which finally produces the real number. In conjunction with the sampling method to approximate p(G, G sub ), p(G) and p(G sub ), we reach the following optimization problem to approximate 1 I(G, G sub ):</p><formula xml:id="formula_24">max φ2 L MI (φ 2 , G sub ) = 1 N N i=1 f φ2 (G i , G subi ) − log 1 N N i=1,j =i e f φ 2 (Gi,G sub j ) .<label>(16)</label></formula><p>With the approximation to the MI in graph data, we combine Eq. 8 , Eq. 13 and Eq. 16, and formulate the optimization process of SIB as a tractable bi-level optimization problem:</p><formula xml:id="formula_25">min G sub ,φ1 L(G sub , φ 1 , φ * 2 ) = L cls + βL MI<label>(17)</label></formula><formula xml:id="formula_26">s.t. φ * 2 = arg max φ2 L MI .<label>(18)</label></formula><p>We first derive a sub-optimal φ 2 notated as φ * 2 by optimizing Eq. 18 for T steps as inner loops. After the T-step optimization of the inner-loop ends, Eq. 16 is a proxy for MI minimization for the SIB objective as an outer loop. Then, the parameter φ 1 and the subgraph G sub are optimized to yield IB-Subgraph. However, in the outer loop, the discrete nature of G and G sub hinders applying the gradient-based method to optimize the bi-level objective and find the IB-Subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Subgraph Generator</head><p>To alleviate the discreteness in Eq. 17, we propose the continuous relaxation to the subgraph recognition and propose a loss to stabilize the training process.</p><p>1. Notice that the MINE estimator <ref type="bibr" target="#b56">[57]</ref> straightforwardly uses the DONSKER-VARADHAN representation to derive an MI estimator between the regular input data and its vectorized representation/encoding. It cannot be applied to estimate the mutual information between G and G sub since both G and G sub are irregular graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgraph generator:</head><p>For the input graph G, we generate its IB-Subgraph with the node assignment S which indicates the node is in G sub or G sub . Then, we introduce a continuous relaxation to the node assignment with the probability of nodes belonging to the G sub or G sub . For example, the i-th</p><formula xml:id="formula_27">row of S is a 2-dimensional vector [p(V i ∈ G sub |V i ), p(V i ∈ G sub |V i )].</formula><p>We first use an l-layer GNN to obtain the node embedding and employ a multi-layer perceptron (MLP) to output S :</p><formula xml:id="formula_28">X l = GNN(A, X l−1 ; θ 1 ), S = Sof tmax(MLP(X l ; θ 2 )).<label>(19)</label></formula><p>S is an n × 2 matrix, where n is the number of nodes. We add row-wise Softmax to the output of MLP to ensure the nodes are either in or out of the subgraph. For simplicity, we compile the above modules as the subgraph generator, denoted as g(; θ) with θ := (θ 1 , θ 2 ). When S is well-learned, the assignment of nodes is supposed to saturate to 0/1. The representation of G sub , which is employed for predicting the graph label, can be obtained by taking the first row of S T X l .</p><p>Connectivity loss: When directly generating the IBsubgraph with the above subgraph generator, poor initialization will lead to the assignment p(V i ∈ G sub |V i ) and p(V i ∈ G sub |V i ) of node to be close. Therefore, aggregation of the subgraph embedding by taking the first row of S T X l is likely to contain excessive information of G sub . Moreover, as S outputs the subgraph with a node selection manner, we hope our model to have an inductive bias to better leverage the topological information. That is, the found subgraph is supposed to be compact. Therefore, we propose the following connectivity loss:</p><formula xml:id="formula_29">L con = ||Norm(S T AS) − I 2 || F ,<label>(20)</label></formula><p>where Norm(•) is the row-wise normalization, || • || F is the Frobenius norm, and I 2 is a 2 × 2 identity matrix. L con not only leads to distinguishable node assignment but also encourages the subgraph to be compact. Take (S T AS) 1: for example, denote a 11 , a 12 the element 1,1 and the element 1,2 of S T AS,</p><formula xml:id="formula_30">a 11 = i,j A ij p(V i ∈ G sub |V i )p(V j ∈ G sub |V j ), a 12 = i,j A ij p(V i ∈ G sub |V i )p(V j ∈ G sub |V j ).<label>(21)</label></formula><p>Minimizing L con results in a11 a11+a12 → 1. This occurs if V i is in G sub , the elements of N (V i ) have a high probability in G sub . Minimizing L con also encourages a12 a11+a12 → 0. This encourages p(V i ∈ G sub |V i ) → 0/1 and less cuts between G sub and G sub . This also holds for G sub when analyzing a 21 and a 22 . <ref type="bibr" target="#b57">[58]</ref> In a word, L con encourages distinctive S to stabilize the training process and a compact topology in the subgraph. Therefore, the overall loss is:</p><formula xml:id="formula_31">min θ,φ1 L(θ, φ 1 , φ * 2 ) = L cls + αL con + βL MI s.t. φ * 2 = arg max φ2 L MI .<label>(22)</label></formula><p>We provide the pseudo code in the Algorithm 1 to better illustrate how to optimize the above objective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the proposed SIB method on four scenarios, including improvement of graph classification, graph interpretation, graph denoising, and relevant 3D structure extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines and settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement of graph classification:</head><p>We first examine SIB's capability of improving graph classification. Different from prior methods which aggregate all node information for the graph representation, SIB only aggregates the node message in the corresponding IB-subgraph. We plug SIB into various backbones including GCN <ref type="bibr" target="#b0">[1]</ref>, GAT <ref type="bibr" target="#b9">[10]</ref>, GIN <ref type="bibr" target="#b12">[13]</ref> and GraphSAGE <ref type="bibr" target="#b40">[41]</ref>. That is, we first employ the backbones to extract node representations and then yield the graph representation by aggregating the node representation in the IB-Subgraph. We compare the proposed method with the mean/sum aggregation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b40">[41]</ref> and pooling aggregation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b53">[54]</ref> in terms of classification accuracy. Moreover, we apply DropEdge <ref type="bibr" target="#b38">[39]</ref> to GAT, namely GAT+DropEdge, which randomly drop 30% edges in message-passing at node-level. Similarly, we apply SIB to GAT+DropEdge, resulting in GAT+SIB+DropEdge. This is to examine the flexibility of SIB to the recent proposed dropping-based regularization in node message-passing.</p><p>For fair comparisons, all the backbones for different methods consist of the same 2-layer GNN with 16 hidden-size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Interpretation:</head><p>As the IB-subgraph preserves the predictive substructure for the graph property, it is interesting to see if we could identify the substructures which best represent some chemical properties of molecules. The goal of graph interpretation is to find the substructure which shares the most similar property to the molecule. If the substructure is disconnected, we evaluate its largest connected part. We compare SIB with the attention mechanism <ref type="bibr" target="#b21">[22]</ref>. That is, we attentively aggregate the node information with the normalized attention scores for graph prediction. The interpretable subgraph is generated by choosing the nodes with top 50% and 70% attention scores, namely Att05 and Att07. SIB outputs the interpretation with the IB-Subgraph, without a manually-selected threshold. Then, we evaluate the absolute property bias (the absolute value of the difference between the property of graph and subgraph) between the graph and its interpretation. Similarly, for fare comparisons, all the backbones for different methods consist of the same 2-layer GNN with 16 hidden-size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Denoising:</head><p>We further evaluate the robustness of SIB to the graphs with noisy structures. We translate the permuted graph into the line-graph and use SIB and attention to infer the real structure of the graph. Then, we classify the permuted graph via the inferred structure respectively. We further compare the performance of GCN and DiffPool on the permuted graphs. Similarly, for fare comparisons, all the backbones for different methods consist of the same 2-layer GNN with 16 hidden-size.</p><p>Relevant 3D Structure Extraction: Most work on weakly supervised 3D segmentation still requires a tiny portion of labeled points for training <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Here we consider a more challenging scenario. That is, to extract relevant 3D structures which belong to the same category without any node-level annotation. This poses a huge challenge for SIB to effectively process large scale graphs. We use the GCN with residual graph connections between layers and GCN with dense graph connections which concatenates representation from previous layers in <ref type="bibr" target="#b49">[50]</ref> as our backbones. After each layer, we dynamically updating the constructed K-NN graphs. We refer to these two backbones as ResDyGCN and DenseDyGCN. Since computing the connective loss in large graphs is prone to exceed the GPU memory, we employ Gumbel-Softmax strategy <ref type="bibr" target="#b58">[59]</ref> for continuous relaxation in learning assignment matrix. We compare SIB with attention.</p><p>For fair comparisons, all the backbones for different methods consist of the same 4-layer model with 1024 hidden-size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement of Graph Classification:</head><p>We evaluate different methods on the datasets of MUTAG <ref type="bibr" target="#b59">[60]</ref>, PROTEINS <ref type="bibr" target="#b60">[61]</ref>, IMDB-BINARY and DD <ref type="bibr" target="#b61">[62]</ref> datasets<ref type="foot" target="#foot_0">2</ref> . The statistics of the datasets are available in Table <ref type="table" target="#tab_2">3</ref> <ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Interpretation:</head><p>We construct the datasets for graph interpretation on four molecule properties based on ZINC dataset <ref type="bibr" target="#b20">[21]</ref>], which contains 250K molecules. QED measures the drug likeness of a molecule, which is bounded within the range (0, 1.0). DRD2 measures the probability that a molecule is active against dopamine type 2 receptor, which is bounded with (0, 1.0). HLM-CLint and MLM-CLint are estimated values of in vitro human and mouse liver microsome metabolic stability (base 10 logarithm of mL/min/g). We sample the molecules with QED ≥ 0.85, DRD2 ≥ 0.50, HLM-CLint ≥ 2, MLM-CLint ≥ 2 for each task. We use 85% of these molecules for training, 5% for validating, and 10% for testing. The statistics of the datasets are available in Table <ref type="table">4</ref>. We label the point cloud with the category of its main component and leverage the color and coordinates as the feature of each point. Then we convert the point cloud into a K-NN graph, where K equals 16. SIB further takes graph data as input and extracts the structure which is most relevant to its main component. We evaluate the result by computing the mIoU of the output and the ground truth (not available in the training procedure) in the testing set. This example is from Area 5 in S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 5</head><p>Quantitative results on graph denoising. We report the classification accuracy (Acc), the number of real edges over total real edges (Recall), and the number of real edges over total edges in subgraphs (Precision) on the test set. Graph Denoising: We generate a synthetic dataset by adding 30% redundant edges for each graph in MUTAG dataset. We use 70% of these graphs for training, 5% for validating, and 25% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method GCN DiffPool GCN+Att05 GCN+Att07</head><p>Relevant 3D Structure Extraction: We construct the datasets for relevant 3D Structure Extraction based on S3DIS dataset <ref type="bibr" target="#b32">[33]</ref>. It contains over 23000 point clouds of 6 indoor areas. Every point cloud consists of 4096 points. Each point is labeled with one of 13 categories, and it has a 9-dimensional feature which indicates the 3D coordinates, normalized 3D coordinates, and RGB color. To enable SIB training, we first choose the point clouds with the percent of main categories greater than 50%, and label the point clouds with the main categories. Then we choose floor and wall for relevant structure extraction as these two categories contain sufficient training samples. We use Area 1-5 for training and Area 6 for testing, which yield 637 and 2099 training samples and 411 and 1270 testing samples for floor and wall respectively. We train these two categories separately and randomly add the same number of negative samples from the rest 12 categories in training. All point clouds are transformed into K-NN graphs, where K equals 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Improvement of Graph Classification</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we comprehensively evaluate the proposed method and baselines on improvement of graph classification. We train SIB on various backbones and aggregate the graph representations only from the IB-subgraphs. As is shown, SIB outperforms the backbones by a large margin. This is because SIB can recognize the predictive yet  compressed substructures and thus relieves the side-effect of noisy and redundant structures in graph data, which is detrimental to graph classification. SIB also exceeds many powerful pooling-based methods, which is able to leverage the hierarchical topological information in graphs. Notice that SIB operates on node features rather than hierarchically coarsened features, therefore, binding SIB with poolingbased methods may be problematic. However, it is sufficient to say that SIB can improve graph classification with the information theoretic IB-subgraph.</p><p>Recent work shows that regularization on messagepassing can lead to better node representation <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b42">[43]</ref>. This is orthogonal to our work as these methods either relieves over-smoothing in GCN or select representative neighborhoods on node-level tasks. Our SIB recognizes a predictive yet compressed subgraph, namely IB-subgraph, to enhance the performance of existing models on graph-level tasks. As SIB also relies on informative node representations, we further plug DropEdge into our model and also obtain a significant gain in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Graph Interpertation</head><p>We further examine the capability of IB-subgraph on interpreting the property of molecules. To this end, we first extract the substructures which mostly affect the chemical property of molecules by different models. Then we compare the property of learned substructures with the input molecules. To ensure the chemical validity, when the substructure is disconnected, we evaluate the property of its largest connected part 4 .</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the quantitative performance of different methods on the graph interpretation task. SIB is able to generate precise graph interpretation (IB-Subgraph), as the substructures found by SIB have the most similar property to the input molecules. Moreover, it is noticed that the interpretation found by attention-based method is highly influenced by the manually selected threshold. For example, GCN+Att07 generates subgraphs with more similar properties compared to GCN+att05. However, GCN+Att05 outperforms GCN+Att07 on HLM-CLint and MLM-CLint properties. Therefore, one needs to carefully choose the threshold for different tasks. In contrast, SIB does require a threshold to select the interpretation thanks to the information theoretic objective. In Fig. <ref type="figure" target="#fig_7">3</ref>, SIB generates a more compact and reasonable interpretation of the property of molecules confirmed by chemical experts. More visualization results are provided in Fig. <ref type="figure" target="#fig_8">4</ref> We further compare the compactness of the found subgraphs of different methods as a more compact subgraph 4. We obtain QED and DRD2 values of molecules with the toolkit on https://www.rdkit.org/. And we evaluate HLM-CLint and MLM-CLint value of molecules on https://drug.ai.tencent.com/ has a higher probability to be a functional group and easier for chemists to interpret the property of a molecule. In Table <ref type="table" target="#tab_4">6</ref>, we compare the average number of disconnected substructures per graph. SIB generates more compact subgraphs to better interpret the graph property. Moreover, compared to the baselines, SIB does not require a hyperparameter to control the sizes of subgraphs, thus being more adaptive to different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Graph Denoising</head><p>Table <ref type="table">5</ref> shows the performance of different methods on noisy graph classification. GCN and DiffPool are vulnerable to structure perturbation since they are unable to distinguish the real structure and noise. However, SIB is able to better reveal the real structure of permuted graphs in terms of precision and recall rate of true edges, even in the absence of explicit annotations. Therefore, SIB is more robust to perturbations and outperforms the baselines on classification accuracy by a large margin. We provide visualization results in Fig. <ref type="figure" target="#fig_9">5</ref>. It is noticed that SIB recognizes more similar structures to the ground truth (not provided in the training process) than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Relevant 3D Structure Extraction</head><p>This task is rather difficult since no point label is provided in the scene point cloud. To the best of our knowledge, few prior methods focus on this extreme scenario. We evaluate different methods in terms of mean intersection over union (mIoU) between the learned substructure and the ground truth <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. We employ two backbones, namely ResDyGCN and DenseDyGCN, in <ref type="bibr" target="#b49">[50]</ref> to extensively compare different methods. As is shown in Table <ref type="table" target="#tab_5">7</ref>, SIB also outperforms the attentive methods since it is capable of effectively recognize relevant structures in large scale point clouds. The results of attentive methods are highly influenced by the threshold. We also notice using ResDyGCN as backbone leads to better results. This is because DenseDyGCN concatenates all previous point representation at each layer, which potentially brings superfluous information. Meanwhile, the residual connection between layers avoids gradient vanishing, and thus leads to more stable training and better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Ablation Study</head><p>To further understand the roles of L con and L M I , we derive two variants of our method by deleting L con and L M I , namely SIB w/o L con and SIB w/o L M I . Note that SIB w/o L M I is similar to InfoGraph <ref type="bibr" target="#b46">[47]</ref> and GNNExplainer <ref type="bibr" target="#b43">[44]</ref>, as they only consider maximizing MI between latent embedding and global summarization and ignore compression. When adapted to subgraph recognition, it is likely to be G = G sub . We evaluate the variants with 2-layer GCN and 16 hidden sizes on graph interpretation. In practice, we find that the training process of SIB w/o L con is unstable as discussed in Section 4.3. Moreover, we find that SIB w/o L M I is very likely to output G sub = G, as it does not consider compression. Therefore, we try several initiations for SIB w/o L con and L M I to get the current results. As shown in Table <ref type="table" target="#tab_6">8</ref>, SIB also outperforms the variants, and thus indicates that every part of our model does contribute to the improvement of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">More Discussions</head><p>L con is proposed for stabilizing the training process and resulting in compact subgraphs. As it poses regularization for the subgraph generation, we are interested in its potential influence on the sizes of the chosen IB-Subgraphs. Therefore, we show the influence of different hyper-parameters of L con to the sizes of the chosen IB-Subgraphs. We implement the experiments with α varies in {1, 3, 5, 10} on QED dataset and compute the mean and deviation of the sizes of IB-Subgraphs (All) and their largest connected parts (Max). As shown in Table <ref type="table" target="#tab_7">9</ref>, we observe that different values of α result in similar sizes of IB-Subgraphs. Therefore, its influence on the size of chosen subgraphs is weak.</p><p>As the initialization of our model may potentially influence the final chosen subgraphs, we rerun our model five times on the QED dataset for graph interpretation task. Then, we employ the intersection over union (IoU ) to measure the overlap between the subgraphs in 5 different runs and the results reported in Table <ref type="table" target="#tab_1">2</ref>. Similarly, we compute the IoU between the chosen subgraphs and their largest connected parts separately, which refer to IoU all and IoU max . We finally report the mean and standard deviation of IoU all , IoU max on the testing set in Table <ref type="table" target="#tab_8">10</ref>. We notice that different initialization has limited influence on the chosen subgraphs, as all the results of five additional runs have high portions of common nodes with the initial run.</p><p>In the graph interpretation task, the hyper-parameter of L con , α, is set to be 5 on four datasets. We show the mean and standard deviation of the sizes of subgraphs in percent in Table <ref type="table" target="#tab_0">11</ref> and Table <ref type="table" target="#tab_1">12</ref>. Note that the sizes of chosen subgraphs mainly depend on task relevant information. For example, as DRD2 measures the probability of being active against dopamine type 2 receptor, it depends on almost the whole structure of a molecule. In contrast, HLM-CLint measures vitro human microsome metabolic stability, which is greatly influenced by small motifs. As shown in Table <ref type="table" target="#tab_0">11</ref> and Table <ref type="table" target="#tab_1">12</ref>, GCN+SIB can recognize the subgraphs with adaptive sizes on different tasks, leading to better performance. However, in GCN+Att05 and GCN+Att07, the size of subgraphs is explicitly controlled by the hyper-parameter (preserve top 50% or 70 % nodes with the highest attention scores). Therefore, the performances of these methods are limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have studied a subgraph recognition problem to extract a predictive yet compressed subgraph, termed the IB-subgraph. To effectively recognize the IBsubgraph in a weakly supervised manner, we propose a novel subgraph information bottleneck (SIB) framework. Unlike the prior work in information bottleneck, SIB directly recognizes the predictive IB-subgraph for the graph label and operates on the irregular graph-structured data. We further propose a bi-level scheme to efficiently optimize SIB with a mutual information estimator for irregular graph </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Illustration of the proposed subgraph information bottleneck (SIB) framework. We employ a bi-level optimization scheme to optimize the SIB objective and thus yielding the IB-Subgraph. In the inner optimization phase, we estimate I(G, G sub ) by optimizing the statistics network of the DONSKER-VARADHAN representation<ref type="bibr" target="#b54">[55]</ref>. Given a good estimation of I(G, G sub ), in the outer optimization phase, we maximize the SIB objective by optimizing the mutual information, the classification loss L cls and connectivity loss Lcon.</figDesc><graphic url="image-1.png" coords="4,79.20,49.37,453.61,259.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 . 2 .</head><label>42</label><figDesc>G sub is the Sufficient Subgraph if it maximizes the mutual information I(G sub , Y ), where Y is the label of the original graph G. We denote it as G s sub = arg max G sub I(Y, G sub ). A sufficient subgraph ensures that almost all substructures which are essential to the graph property are pre-served. Moreover, when defining a Markov Chain Y → G → G sub , we have I(Y, G sub ) ≤ I(G, Y ) by the data processing inequality. This leads to I(G s sub , Y ) = I(G, Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 4 . 3 (</head><label>43</label><figDesc>Subgraph Information Bottleneck). Given a graph G and its label Y , the Subgraph Information Bottleneck seeks for the most informative yet compressed subgraph G sub by optimizing the following objective:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b11">(12)</ref> H(Y ) is the entropy of Y and thus can be ignored. In practice, we approximate p(Y, G sub ) with an empirical distribu-tion p(Y, G sub ) ≈ 1 N N i=1 δ Y (y i )δ G sub (G subi ),where δ() is the Dirac function to sample training data. G subi and Y i are the output subgraph and graph label corresponding to i-th training data. By substituting the true posterior p(Y |G sub )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Optimizing the subgraph information bottleneck. Input: Graph G = {A, X}, graph label Y , inner-step T , outer-step N . Output: Subgraph G sub 1: function SIB(G = {A, X}, Y, T, N ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The pipeline of relevant 3D structure extraction. We label the point cloud with the category of its main component and leverage the color and coordinates as the feature of each point. Then we convert the point cloud into a K-NN graph, where K equals 16. SIB further takes graph data as input and extracts the structure which is most relevant to its main component. We evaluate the result by computing the mIoU of the output and the ground truth (not available in the training procedure) in the testing set. This example is from Area 5 in S3DIS dataset.</figDesc><graphic url="image-2.png" coords="8,312.00,43.70,252.00,108.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The molecules with their interpretable subgraphs discovered by different methods. These subgraphs exhibit similar chemical properties compared to the molecules on the left.</figDesc><graphic url="image-4.png" coords="9,312.00,308.78,251.99,99.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The molecules with its interpretation found by SIB. These subgraphs exhibit similar chemical properties compared to the molecules on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. We show the blindly denoising results on permuted graphs. Each method operates on the line-graphs and tries to recover the true topology by removing the redundant edges. Columns 4,5,6 shows results obtained by different methods, where "miss: m, wrong: n" means missing m edges and there are n wrong edges in the output graph. SIB always recognizes more similar structure to the ground truth (not provided in the training process) than other methods.</figDesc><graphic url="image-5.png" coords="10,48.00,43.70,251.99,135.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Classification accuracy. The pooling methods yield pooling aggregation while the backbones yield mean aggregation. The proposed SIB method with backbones yields subgraph embedding by aggregating the nodes in subgraphs.</figDesc><table><row><cell>Method</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>IMDB-BINARY</cell><cell>DD</cell></row><row><cell>SortPool</cell><cell>0.844 ± 0.141</cell><cell>0.747 ± 0.044</cell><cell>0.712 ± 0.047</cell><cell>0.732 ± 0.087</cell></row><row><cell>ASAPool</cell><cell>0.743 ± 0.077</cell><cell>0.721 ± 0.043</cell><cell>0.715 ± 0.044</cell><cell>0.717 ± 0.037</cell></row><row><cell>DiffPool</cell><cell>0.839 ± 0.097</cell><cell>0.727 ± 0.046</cell><cell>0.709 ± 0.053</cell><cell>0.778 ± 0.030</cell></row><row><cell>EdgePool</cell><cell>0.759 ± 0.077</cell><cell>0.723 ± 0.044</cell><cell>0.728 ± 0.044</cell><cell>0.736 ± 0.040</cell></row><row><cell>AttPool</cell><cell>0.721 ± 0.086</cell><cell>0.728 ± 0.041</cell><cell>0.722 ± 0.047</cell><cell>0.711 ± 0.055</cell></row><row><cell>GCN</cell><cell>0.743±0.110</cell><cell>0.719±0.041</cell><cell>0.707 ± 0.037</cell><cell>0.725 ± 0.046</cell></row><row><cell>GraphSAGE</cell><cell>0.743±0.077</cell><cell>0.721 ± 0.042</cell><cell>0.709 ± 0.041</cell><cell>0.729 ± 0.041</cell></row><row><cell>GIN</cell><cell>0.825±0.068</cell><cell>0.707 ± 0.056</cell><cell>0.732 ± 0.048</cell><cell>0.730 ± 0.033</cell></row><row><cell>GAT</cell><cell>0.738 ± 0.074</cell><cell>0.714 ± 0.040</cell><cell>0.713 ± 0.042</cell><cell>0.695 ± 0.045</cell></row><row><cell>GAT + DropEdge</cell><cell>0.743±0.081</cell><cell>0.711±0.043</cell><cell>0.710±0.041</cell><cell>0.717±0.035</cell></row><row><cell>GCN+SIB</cell><cell>0.776 ± 0.075</cell><cell>0.748 ± 0.046</cell><cell>0.722 ± 0.039</cell><cell>0.765 ± 0.050</cell></row><row><cell>GraphSAGE+SIB</cell><cell>0.760 ± 0.074</cell><cell>0.734 ± 0.043</cell><cell>0.719 ± 0.052</cell><cell>0.781 ± 0.042</cell></row><row><cell>GIN+SIB</cell><cell>0.839 ± 0.064</cell><cell>0.749 ± 0.051</cell><cell>0.737 ± 0.070</cell><cell>0.747 ± 0.039</cell></row><row><cell>GAT+SIB</cell><cell>0.749 ± 0.097</cell><cell>0.737 ± 0.044</cell><cell>0.729 ± 0.046</cell><cell>0.769 ± 0.040</cell></row><row><cell>GAT+SIB+DropEdge</cell><cell>0.754±0.085</cell><cell>0.737±0.037</cell><cell>0.731±0.003</cell><cell>0.776±0.034</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>The mean and standard deviation of absolute property bias between the graphs and the corresponding subgraphs.</figDesc><table><row><cell>Method</cell><cell>QED</cell><cell>DRD2</cell><cell cols="2">HLM-CLint MLM-CLint</cell></row><row><cell cols="4">GCN+Att05 0.48± 0.07 0.20± 0.13 0.90± 0.89</cell><cell>0.92± 0.61</cell></row><row><cell cols="4">GCN+Att07 0.41± 0.07 0.16± 0.11 1.18± 0.60</cell><cell>1.69± 0.88</cell></row><row><cell cols="4">GCN+SIB 0.38± 0.12 0.06± 0.09 0.37± 0.30</cell><cell>0.72± 0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Statistics of datasets in improvement of graph classification. ×10 −5 1.7 ×10 −4 2.0 ×10 −3 3.0 ×10 −5</figDesc><table><row><cell></cell><cell cols="5">MUTAG PROTEINSIMDB-BINARY</cell><cell>DD</cell></row><row><cell>Nodes</cell><cell cols="2">97.9K</cell><cell cols="2">43.5K</cell><cell>19.8K</cell><cell>334.9K</cell></row><row><cell>Edges</cell><cell cols="2">202.5K</cell><cell cols="2">162.1K</cell><cell>386.1K</cell><cell>1.7M</cell></row><row><cell cols="2">Density 4.2 Maximum degree</cell><cell>20</cell><cell cols="2">50</cell><cell>540</cell><cell>38</cell></row><row><cell>Minimum degree</cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell>4</cell><cell>2</cell></row><row><cell>Average degree</cell><cell></cell><cell>4</cell><cell>7</cell><cell></cell><cell>39</cell><cell>10</cell></row><row><cell cols="3">Number of triangles 2.8K</cell><cell cols="2">366K</cell><cell>18.8M</cell><cell>7.1M</cell></row><row><cell>Maximum k-core</cell><cell></cell><cell>5</cell><cell>9</cell><cell></cell><cell>117</cell><cell>15</cell></row><row><cell>Average number of triangles</cell><cell></cell><cell>0</cell><cell>8</cell><cell></cell><cell>951</cell><cell>21</cell></row><row><cell>Maximum number of triangles</cell><cell></cell><cell>12</cell><cell cols="2">136</cell><cell>17.8K</cell><cell>160</cell></row><row><cell>Average clustering coefficient</cell><cell cols="4">0.001965 0.316645</cell><cell>0.831934</cell><cell>0.413379</cell></row><row><cell>Fraction of closed triangles</cell><cell cols="4">0.003160 0.315106</cell><cell>0.803561</cell><cell>0.410832</cell></row><row><cell>Lower bound of Maximum Clique</cell><cell></cell><cell>6</cell><cell>5</cell><cell></cell><cell>18</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell cols="3">TABLE 4</cell></row><row><cell cols="6">Statistics of datasets in graph interpretation.</cell></row><row><cell></cell><cell></cell><cell cols="4">QED DRD2 HLM-CLint MLM-CLint</cell></row><row><cell cols="2">Number of graphs</cell><cell cols="3">35000 3000</cell><cell>25850</cell><cell>16666</cell></row><row><cell cols="4">Maximum number of nodes 29</cell><cell>66</cell><cell>37</cell><cell>37</cell></row><row><cell cols="4">Minimum number of nodes 12</cell><cell>13</cell><cell>9</cell><cell>7</cell></row><row><cell cols="5">Average number of nodes 21.82 27.43</cell><cell>25.14</cell><cell>22.44</cell></row><row><cell cols="4">Maximum number of edges 34</cell><cell>74</cell><cell>42</cell><cell>42</cell></row><row><cell cols="4">Minimum number of edges 12</cell><cell>14</cell><cell>9</cell><cell>7</cell></row><row><cell cols="5">Average number of edges 23.43 30.24</cell><cell>22.23</cell><cell>24.19</cell></row><row><cell cols="3">Dimension of node features 9</cell><cell></cell><cell>8</cell><cell>9</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6</head><label>6</label><figDesc>Average number of disconnected substructures per graph selected by different methods.</figDesc><table><row><cell>Method</cell><cell cols="4">QED DRD2 HLM MLM</cell></row><row><cell>GCN+Att05</cell><cell>3.38</cell><cell>1.94</cell><cell>3.11</cell><cell>5.16</cell></row><row><cell>GCN+Att07</cell><cell>2.04</cell><cell>1.76</cell><cell>2.75</cell><cell>3.00</cell></row><row><cell>GCN+SIB</cell><cell>1.57</cell><cell>1.08</cell><cell>2.29</cell><cell>2.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7</head><label>7</label><figDesc>We compare the capability of different methods on extraction label-relevant substructures in 3D point cloud. Notice that no point label is available in the training process.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>floor</cell><cell></cell><cell>wall</cell></row><row><cell>Backbone</cell><cell>Model</cell><cell cols="2">mIoU size</cell><cell cols="2">mIoU size</cell></row><row><cell></cell><cell>Att05</cell><cell>0.46</cell><cell>2048</cell><cell>0.384</cell><cell>2048</cell></row><row><cell>ResDyGCN</cell><cell>Att07 SIB</cell><cell>0.464 0.519</cell><cell>2867 2819.628</cell><cell>0.465 0.492</cell><cell>2867 2032.613</cell></row><row><cell></cell><cell>Att05</cell><cell>0.27</cell><cell>2048</cell><cell>0.289</cell><cell>2048</cell></row><row><cell>DenseDyGCN</cell><cell>Att07 SIB</cell><cell>0.378 0.397</cell><cell>2867 1978.479</cell><cell>0.358 0.419</cell><cell>2867 1784.214</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>Ablation study on Lcon and L M I . Note that we try several initiations for SIB w/o Lcon and L M I to get the current results due to the instability of optimization process. Lcon 0.46± 0.07 0.15± 0.12 0.45± 0.37 1.58± 0.86 SIB w/o L M I 0.43± 0.15 0.21± 0.13 0.48± 0.34</figDesc><table><row><cell>Method</cell><cell>QED</cell><cell>DRD2</cell><cell cols="2">HLM-CLint MLM-CLint</cell></row><row><cell cols="5">SIB w/o 1.20± 0.97</cell></row><row><cell>SIB</cell><cell cols="3">0.38± 0.12 0.06± 0.09 0.37± 0.30</cell><cell>0.72± 0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 9</head><label>9</label><figDesc>The influence of the hyper-parameter α of Lcon to the size of subgraphs.</figDesc><table><row><cell>α</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>All</cell><cell cols="4">0.483±0.143 0.496±0.150 0.494±0.147 0.466±0.150</cell></row><row><cell cols="5">Max 0.387±0.173 0.413±0.169 0.411±0.169 0.391±0.172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 10 The</head><label>10</label><figDesc>overlap between the chosen subgraphs with different initialization. IoU all 0.848±0.163 0.765±0.106 0.784±0.112 0.829±0.166 0.813±0.186 IoUmax 0.779±0.330 0.696±0.310 0.742±0.333 0.757±0.335 0.762±0.304 TABLE 11 Size of the chosen subgraphs on four datasets in percent. TABLE 12 Size of largest connected parts used for graph interpretation in percent. We introduce continuous relaxation to enable training SIB with the gradient-based optimizer and a connectivity loss to stabilize the training process. We evaluate the modelagnostic SIB framework on both graph learning and computer vision scenarios, including the improvement of graph classification, graph interpretation, graph denoising, and relevant 3D structure extraction. Experiment results verify the superior properties of IB-subgraphs.</figDesc><table><row><cell>Run</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">. We follow the protocol in https://github.com/rusty1s/pytorch geometric/tree/master/benchmark/kernel</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">. The statistics of datasets in improvement of graph classification are collected from http://networkrepository.com</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Representation Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asap: Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph-revised convolutional network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1911">1911.07123, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Re. OpenReview.net</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A restricted black-box adversarial framework towards attacking graph embedding models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3389" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-objective molecule generation using interpretable substructures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations, ser. ICLR &apos;19</title>
				<meeting>the 7th International Conference on Learning Representations, ser. ICLR &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2005">2005.10203. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">620</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1800" to="1809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zinc-a free database of commercially available compounds for virtual screening</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Shoichet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="182" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised graph classification: A hierarchical graph perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Wed Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4204" to="4214" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing</title>
				<meeting>the 37-th Annual Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Representation Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Representation Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6777" to="6786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Infobot: Transfer and exploration via the information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Strouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Representation Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning with selective noise injection and information bottleneck</title>
		<author>
			<persName><forename type="first">M</forename><surname>Igl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning efficient multi-agent communication: An information bottleneck approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph information bottleneck</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12811</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph information bottleneck for subgraph recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05563</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient algorithms for densest subgraph discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V S</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VLDB Endowment</title>
				<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1719" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dense subgraph discovery: Kdd 2015 tutorial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2313" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">gspan: graph-based substructure pattern mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subdue: compressionbased frequent pattern discovery in graph data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Ketkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficiently mining frequent embedded unordered trees</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Information</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkx1qkrKPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05343</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Subgraph neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10538</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised speech decomposition via triple information bottleneck</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic point cloud segmentation: Towards 10x fewer labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">715</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Label-efficient point cloud semantic segmentation: An active learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06931</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Edge contraction pooling for graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905">1905.10990, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schanauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>Supplement of Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The network data repository with interactive graph analytics and visualization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
