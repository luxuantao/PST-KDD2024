<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Last-Touch Correlated Data Streaming</title>
				<funder ref="#_wev4GEK">
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder ref="#_sTHdr7d">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_vCKPNvZ">
					<orgName type="full">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
							<email>mferdman@ece.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Last-Touch Correlated Data Streaming</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research advocates address-correlating predictors to identify cache block addresses for prefetch. Unfortunately, address-correlating predictors require correlation data storage proportional in size to a program's active memory footprint. As a result, current proposals for this class of predictor are either limited in coverage due to constrained on-chip storage requirements or limited in prediction lookahead due to long off-chip correlation data lookup.</p><p>In this paper, we propose Last-Touch Correlated Data Streaming (LT-cords), a practical address-correlating predictor.</p><p>The key idea of LT-cords is to record correlation data off chip in the order they will be used and stream them into a practicallysized on-chip table shortly before they are needed, thereby obviating the need for scalable on-chip tables and enabling low-latency lookup. We use cycle-accurate simulation of an 8-way out-oforder superscalar processor to show that: (1) LT-cords with 214KB of on-chip storage can achieve the same coverage as a last-touch predictor with unlimited storage, without sacrificing predictor lookahead, and (2) LT-cords improves performance by 60% on average and 385% at best in the benchmarks studied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in semiconductor fabrication, along with circuit and microarchitectural innovation, have led to an unprecedented performance gap between microprocessors and memory. Today's processors attempt to reduce this performance gap by incorporating a hierarchy of cache memories. Further expansion of these hierarchies has reached the point of diminishing returns because accesses to distant (e.g., off-chip) hierarchy levels require hundreds of processor cycles, an order of magnitude higher than the lowest on-chip level. Today's wide-issue out-of-order superscalar processors with non-blocking caches can at best tolerate access latencies to nearby cache levels, and incur performance penalties upon long-latency memory accesses.</p><p>There is a myriad of techniques proposed to address the memory-access performance bottleneck. A number of proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> advocate microarchitectural enhancements to the speculation window and allow instructions to flow speculatively beyond long-latency memory accesses. These techniques rely primarily on accurate control flow and load-value speculation in the presence of pending memory accesses to avoid stalling. However, these enhancements do not benefit pointerchasing applications with little or no memory-level parallelism. Some researchers and vendors have explored prefetching to mitigate the processor-memory performance gap. Many proposals are inherently limited in scope, targeting only specific access patterns, such as strided accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, pointer dereference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, or accesses to linked data structures <ref type="bibr" target="#b16">[17]</ref>. Although frequently accurate for the subset of memory accesses they target, these prefetchers have inherently low coverage overall. More recently, researchers have proposed generalizing stride predictors to target miss sequences that exhibit recurring patterns of (non-constant) strides, an approach called delta correlation. The delta-correlating Global History Buffer (GHB) prefetcher <ref type="bibr" target="#b14">[15]</ref> was recently shown to outperform a variety of other hardware prefetching schemes <ref type="bibr" target="#b8">[9]</ref>. Although delta correlation subsumes many previous approaches, it is not effective for data structures with irregular access patterns. Furthermore, current delta-correlating prefetchers cannot track repetitive deltas if several individually-correlated access sequences are interleaved.</p><p>Another proposed class of prefetchers utilizes address correlation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>, which promises wider applicability across a diverse spectrum of workloads because they target generalized memory access patterns. Rather than detecting patterns in data layout, these prefetchers correlate data addresses to predict future misses. Ideally, address-correlating prefetchers are able to predict sufficiently early to tolerate long off-chip access latency while predicting a large fraction of cache misses. Unfortunately, no prior design is able to achieve both long lookahead and high predictor coverage. To achieve long lookahead, a predictor requires high-bandwidth on-chip access to correlation data, which precludes large storage. To achieve high coverage, address-correlating predictors require storage proportional in size to a program's memory footprint, which precludes fast prediction.</p><p>A recent proposal, the Dead-Block Correlating Prefetcher (DBCP) <ref type="bibr" target="#b11">[12]</ref>, achieves maximal prefetch lookahead through correlation to "last touch" accesses-the last access to each cache block prior to eviction. However, capacity constraints of DBCP's on-chip correlation table limit its coverage. Conversely, the designs of Solihin et al. <ref type="bibr" target="#b19">[20]</ref> and Wenisch et al. <ref type="bibr" target="#b23">[24]</ref> record address correlation data in off-chip DRAM. Although these mechanisms have abundant correlation data storage, lookup is performed off chip, drastically increasing prediction latency.</p><p>We propose Last-Touch Correlated Data Streaming (LT-cords), the first address-correlating predictor design to combine the prediction timeliness of last-touch on-chip lookup with the high coverage enabled by off-chip predictor storage. The key idea of LT-cords is to record correlation data off chip in the order they will be used, and stream them into a practically-sized on-chip table shortly before they are needed. Just as prior predictors rely on a repetitive sequence of cache misses, LT-cords depends on a repetitive sequence of predictions. In this paper, we demonstrate that the order in which DBCP-like prediction data are used (the last-touch order) closely matches the order these entries are discovered (the blocks' eviction order).</p><p>We use trace-driven and cycle-accurate simulation of an 8way out-of-order superscalar processor running SPEC CPU2000 and Olden benchmarks to show:</p><p>? Last-touch order is correlated to block eviction order. The observed sequence of block evictions exactly matches the sequence of last touch accesses for 21% of evictions. Allowing for local reorderings, over 98% of last touch accesses match eviction order, enabling a hardware mechanism that uses block eviction order to approximate last-touch order. ? High coverage and timely prediction with practical onchip storage. LT-cords eliminates 69% of L1D misses using only 214KB of on-chip storage, while increasing off-chip traffic for pin-bandwidth-hungry applications by at most 15% and less than 4% on average. In contrast, DBCP requires on average 80MB of on-chip storage to eliminate the same fraction of L1D misses. ? Speedup. LT-cords achieves a performance improvement of 60%, compared to 123% achieved by a perfect L1D. In contrast, the delta-correlating GHB prefetcher achieves only 31% performance improvement. DBCP with 2MB on-chip storage <ref type="bibr" target="#b11">[12]</ref> achieves only 17% performance improvement. Finally, increasing L2 cache size by a factor of four achieves only 16% performance improvement. The rest of this paper is organized as follows. Section 2 provides background on dead-block correlated prefetching. Section 3 investigates the temporal correlation of last touches and how it can be exploited to create sequences of last-touch signatures. Section 4 describes a LT-cords design based on last-touch signature sequences. Section 5 details our evaluation methodology and results. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: DBCP Prefetching</head><p>Lai et al. proposed Dead-Block Correlated Prefetching <ref type="bibr" target="#b11">[12]</ref> to predict the last touch to a cache block prior to that block's eviction, and replace the block by prefetching a subsequently accessed block. By triggering the prefetch at the last touch, DBCP provides maximal lookahead among address-correlating prefetchers.</p><p>DBCP correlates each last touch of a cache block to the address of the block that replaces it. Figure <ref type="figure" target="#fig_0">1</ref> depicts an example of prefetching using a DBCP. We assume a direct-mapped L1D for this example. Cache blocks A1, A2, and A3 all map to the same cache index. The predictor tracks all instructions {PC i , PC j , PC k } accessing block A2 from the miss until A2 is evicted. Upon a miss to block A3, block A2 is evicted, and the predictor records the trace that led to the eviction and replacement address as a fixed-size last-touch signature; the history trace is represented by a hash of the address history (e.g., {A1, A2}) with the instruction trace (e.g., {PC i , PC j , PC k }). On subsequent encounters of the trace {PC i , PC j , PC k } accessing A2, the predictor identifies the access at PC k as a last touch and initiates a prefetch to retrieve A3 directly into L1D to replace A2.</p><p>Because cache blocks remain in the cache for long periods of time after the last access, DBCP attains increased lookahead as compared to conventional prefetcher proposals. Figure <ref type="figure">2</ref> shows the cumulative distribution of L1 block dead-times-the number of cycles between a last touch to a block until its eventual eviction. 1 The figure corroborates prior results <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>, showing that over 85% of all cache-block dead-times are longer than the memory access latency. Therefore, prefetch requests issued at last touch can complete before the next access to the same cache index, eliminating the entire off-chip miss latency. The figure also indicates that potential remains in future generation systems, even if memory latencies increase.</p><p>The DBCP mechanism correlates data addresses to last-touch accesses, enabling DBCP to increase memory-level parallelism for dependent memory accesses. Modern out-of-order processors often stall, unable to overlap the latency of dependent L1D misses <ref type="bibr" target="#b4">[5]</ref>. Correlating miss addresses with last-touch instruction traces enables the predictor to retrieve data-dependent blocks in parallel and enhances memory level parallelism for pointerdependent traversals (e.g., linked lists or trees).</p><p>Unlike delta-correlating prefetchers, DBCP does not require a regular layout to repeat over many memory locations. Instead, like all address-correlating prefetchers, DBCP learns correlations between arbitrary address pairs. Moreover, DBCP can exploit these correlations even if several access sequences are interleaved when the sequences recur.  Finally, DBCP prefetches blocks directly into the cache rather than into an auxiliary prefetch buffer. Many prefetchers deliver data into prefetch buffers to avoid cache pollution <ref type="bibr" target="#b18">[19]</ref>. However, due to lookup complexity and size restrictions, prefetch buffers limit the amount of speculatively fetched data and can affect the cache's critical access path. By accurately predicting dead blocks in L1D, DBCP can place prefetched blocks directly in the cache without incurring cache pollution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Why is DBCP Impractical?</head><p>Figure <ref type="figure">3</ref> depicts the anatomy of DBCP <ref type="bibr" target="#b11">[12]</ref>. On every memory access, DBCP computes a last-touch history trace by combining the PC and address of that access with history table data. History traces for previously observed replacements are stored in the on-chip correlation table. If the history trace for an access is found in the correlation table, a request for the corresponding replacement block is issued.</p><p>Although address-correlating prefetchers like DBCP show great promise, they are impractical to implement because they require on-chip storage proportional in size to the number of blocks referenced by the application. To prefetch effectively, DBCP and similar address-correlating predictors must store correlation data across long (e.g., billions of instructions) recurring program phases <ref type="bibr" target="#b17">[18]</ref> with little temporal reuse. Because last-touch signatures are calculated and looked up on every L1D access, the correlation table must reside on chip to provide high lookup bandwidth. Figure <ref type="figure">4</ref> plots the average prefetch coverage (fraction of L1D misses eliminated) of DBCP with a finite-size correlation table, normalized to coverage of DBCP with an unlimited-size correlation table; worst-case benchmark (wupwise) is shown in light gray. To realize its full potential, DBCP needs a 160MB correlation table and in the worst-case achieves negligible coverage with less than 80MB. Our results corroborate prior work showing that DBCP with practically-sized storage is ineffective <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LT-cords</head><p>In this paper, we propose Last-Touch Correlated Data Streaming (LT-cords), a practical address-correlating predictor. LT-cords exploits the inherent temporal address correlation of data accesses to enable accurate and timely streaming of data into the L1D cache. The key innovation of LT-cords is in using offchip storage to record long repetitive sequences of consecutivelyused last-touch signatures, while retaining only a single "head" signature per sequence on chip, thus drastically reducing the on-chip storage requirements of the predictor. When an access sequence recurs, LT-cords streams the corresponding sequence of last-touch signatures from off chip into a practically-sized on-chip table. Large last-touch prediction lookahead enables timely signature retrieval from off-chip, while contiguous packing of signatures in off-chip memory enables bandwidth-efficient retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LT-cords Example</head><p>To illustrate the operation of LT-cords, we consider an application that executes an outer loop many times to perform computation on a large data set. Thousands of L1D cache misses are encountered during the execution of each loop iteration. Because data structures remain mostly static throughout program execution, similar access sequences repeat during each loop iteration. Hence, each iteration repeats the same sequence of cache misses.</p><p>When LT-cords first encounters a particular cache miss sequence during an early loop iteration, it learns the entire sequence of last-touch signatures that corresponds to these misses. LT-cords stores the head signature of the sequence on chip, and stores the remainder of the sequence in off-chip DRAM. When the processor issues a memory reference matching the onchip head signature (corresponding to the start of another loop iteration), LT-cords retrieves the beginning of the signature sequence from off-chip memory into an on-chip table that functions like the correlation table in DBCP. As in DBCP, signatures that recur in the program trigger prefetches into the L1 cache. In addition, as signatures in the on-chip table are used, LT-cords replaces them with the subsequent signatures from off-chip memory. In this fashion, LT-cords keeps just the necessary portion of the signature sequence on chip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Order Disparity</head><p>At the time of prediction, LT-cords requires last-touch signatures to be made available on chip in the order that the signatures are to be used by the predictor. However, the mechanism that discovers last-touch signatures and records sequences off chip must do so in cache-miss order. Although the last touch and cache miss order are generally similar, localized reordering is abundant in typical access patterns (see Section 5.1). For example, consider the sequence of references to addresses {A1,B1,B2,A2} made to cache indices A and B in a direct-mapped cache. Accesses A1 and B1 are last touches prior to those blocks being evicted from the cache, and A2 and B2 are the corresponding misses that cause the two evictions. Although the cache miss to address B2 happens before the miss to A2, the corresponding last touches, A1 and B1, occur in the reverse order. Therefore, the sequence discovered by LT-cords {B2,A2} does not match the last-touch order {A1,B1} in which predictions must be made. LT-cords must tolerate order disparity between the recorded sequence and last-touch sequence to predict successfully. Control-flow irregularities and data structure modification can cause last-touch signatures in the middle of previouslyrecorded sequences to become stale (no longer valid). Stale signatures in a sequence can prevent LT-cords from accurately tracking the sequence to its end. Avoiding coverage loss from stale signatures in the middle of a sequence requires a sequence tracking mechanism that supports skipping stale signatures.</p><p>To tolerate reordering and stale signatures, previous temporal streaming designs <ref type="bibr" target="#b23">[24]</ref> temporarily buffer data in a small fullyassociative structure (e.g., 32 entries). LT-cords requires a similar mechanism to allow on-chip random access to signatures to tolerate order disparity. However, unlike temporal streaming, LT-cords typically follows several signature sequences in parallel. Furthermore, the disparity between last-touch and cache miss order is larger than the reorderings typically observed in temporal streams. As such, LT-cords must buffer many signatures on chip (upwards of 1000 signatures, see Section 5.2) which precludes using a fully-associative structure. Instead, LT-cords places signatures from simultaneous sequences into a set-associative on-chip table. In Section 5.4, we present results indicating that, in practice, tolerating reorder with a moderately-sized (e.g., ~200KB) signature cache enables accurate tracking of last-touch sequences recorded in cache miss order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lookahead for Sequence Retrieval</head><p>Cache misses are frequently clustered <ref type="bibr" target="#b11">[12]</ref>, corresponding to bursts of last touches. During such bursts, last touches may remain undetected by LT-cords if their corresponding last-touch signatures have not yet arrived from off-chip memory. To avoid losing prediction opportunity due to long off-chip access latency, LT-cords must initiate the retrieval of signatures from off chip sufficiently early to bring the signatures on chip before their corresponding last touches take place. Furthermore, because predictions must take place during signature retrieval, a LT-cords implementation must maintain enough signatures in the on-chip table to overlap long off-chip signature retrieval latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LT-cords Implementation</head><p>Figure <ref type="figure">5</ref> depicts the anatomy of a LT-cords implementation. LT-cords comprises three on-chip hardware structures and a section of main memory for off-chip signature sequence storage. The on-chip structures include: (1) a history table, organized like the L1D tag array, that maintains last-touch history trace encodings and previous tags for each L1D cache index, (2) a sequence tag array, which tracks the contents of the off-chip sequence storage, and (3) a signature cache, a small set-associative correlation table that temporarily holds signatures on chip for prediction. The signature cache must store enough signatures to tolerate signature reordering and overlap signature retrieval latency for all active sequences. We assume that the sequence storage in main memory is managed by the operating system. Due to the long training time of the predictor (potentially billions of instructions), contents of LT-cords structures must persist across context switches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recording Last-Touch Signatures</head><p>As in DBCP <ref type="bibr" target="#b11">[12]</ref>, LT-cords constructs signatures using the history table. Each history table entry maintains a PC trace of committed memory instructions that access the corresponding L1D set. The trace is incrementally constructed from program counter values of all instructions that access the set and is reset on every eviction from the set. The history table entry also contains the tags of the last two blocks evicted from the set. Upon an eviction, LT-cords constructs a last-touch signature from the PC trace hash, the previous and evicted tags, a 2-bit confidence counter, and the replacement address. Constructed signatures are stored off chip in eviction order. To optimize off-chip write bandwidth, LTcords buffers a small number of consecutive signatures and transfers them as a single unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Indexing Off-Chip Sequence Storage</head><p>The signature sequences that LT-cords exploits are arbitrarily long. To manage main memory sequence storage, LT-cords divides sequences into fixed-length fragments, each holding a sub-sequence of consecutive last-touch signatures. As the application follows an access sequence, the signatures from corresponding fragments are streamed from off chip into the signature cache. To predict when a particular fragment will be needed, LT-cords associates each fragment with a signature, called the head signature, that precedes the fragment in the signature sequence. When the head signature recurs, LT-cords begins retrieving the corresponding fragment. Because of the long latency required to access off-chip signatures, the head signature must precede the fragment by several hundred signatures (see Section 3.3). Together, the sequence tag array and main-memory sequence storage act as a direct mapped cache of sequence fragments. LTcords divides the main-memory sequence storage into frames that each store a sequence fragment. Fragments map to frames based on the low-order bits of the head signature. The sequence tag array stores the head hash of the fragment in each frame.</p><p>No explicit sequence start or stop criteria exists; LT-cords continues to record the sequence of last-touch signatures off chip as long as cache misses occur. LT-cords appends signatures to a fragment until the frame becomes full. Then, a new frame is allocated for the subsequent fragment. As in a direct mapped cache, an existing fragment is overwritten by a new fragment that maps to the same frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Last-Touch Prediction and Sequences</head><p>LT-cords uses the signature cache to predict addresses for prefetch. As memory accesses commit, signatures stored in the history table are updated. LT-cords looks for each updated signature in the on-chip signature cache. Presence of the corresponding signature with a high confidence count indicates that the memory access is a last touch. LT-cords retrieves the data at the predicted address; when it arrives, data are placed into the L1D.</p><p>LT-cords must buffer enough signatures in the signature cache to tolerate reordering (see Section 3.2). However, to efficiently utilize signature cache capacity and allow space for several simultaneously active fragments, LT-cords cannot load fragments in their entirety at once; fragment size is chosen to optimize storage efficiency and greatly exceeds the number of signatures required to tolerate reordering.</p><p>Instead, LT-cords keeps a sliding window of signatures from each active fragment in the signature cache. As signatures are used, the sliding window is advanced (win. pos. in the sequence tag array of Figure <ref type="figure">5</ref>) and new signatures are streamed into the signature cache. The signature cache is organized as a set-associative structure with signatures replaced in FIFO order. Along with the signature, signature cache entries store a pointer to the location of the signature in the off-chip sequence storage. When a signature is accessed, LT-cords uses this pointer to identify the signature's frame, advance its corresponding fragment's sliding window, and load additional signatures. The location of a frag-ment's sliding window is tracked in the sequence tag array entry for the fragment. To utilize bandwidth efficiently, LT-cords advances the window and transfers signatures in the same size units used to record sequences (see Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Updating the Signature Confidence</head><p>LT-cords uses 2-bit saturating confidence counters for each last-touch signature to avoid premature eviction of L1D cache blocks by signatures that become invalid. The pointer kept along with each signature in the signature cache provides an exact location of the signature in off-chip sequence storage, allowing for a direct update of the counter value on confidence changes. Confidence updates are not frequent, and incur minimal overhead by utilizing otherwise-unused bus cycles. Because most signatures are valid immediately after creation, confidence counters are initialized to the value "2" to expedite training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We use SimpleScalar 3.0/Alpha for both trace-driven and cycle-accurate simulation of an aggressive out-of-order processor and cache hierarchy. Table <ref type="table" target="#tab_0">1</ref> specifies the simulated system configuration. We extend SimpleScalar to model MSHR contention and queuing accurately at both the L1/L2 and L2/memory busses. We model two channels between the L1 and L2, allowing for an L2 request to be issued while an L1 fill is in progress.</p><p>Results in Section 5.1 through Section 5.6 are derived from trace-driven simulation of each benchmark in its entirety. Speedup and bandwidth (Sections 5.7 and 5.8) are obtained by cycle-accurate simulation using SMARTS statistical sampling and checkpointing <ref type="bibr" target="#b24">[25]</ref>. We use trace-driven simulation to create many evenly spaced checkpoints of the cache hierarchy and predictor state for each application. We launch cycle-accurate simulation from each checkpoint, and aggregate the results. For each checkpoint, we measure 10M instruction regions; prior to measurement of each region, we perform 10M instruction warm-up. Sample sizes are chosen to maintain a 95% confidence interval of ?3% on performance change (except for results with greater than 50% performance improvement, where the confidence interval is less than one tenth of the performance improvement).</p><p>For trace-driven results, we use 32-bit last-touch signatures to minimize the effects of hash collisions. In cycle-accurate tim-  <ref type="table" target="#tab_1">2</ref> lists our benchmarks and their L1 and L2 miss rates and IPCs for our baseline processor configuration. We run all SPEC CPU2000 benchmarks except vpr; SimpleScalar 3.0/Alpha is unable to correctly execute vpr. The first reference input is used for all benchmarks except perl, for which the "splitmail" input is used. We include results for three pointer-intensive Olden <ref type="bibr" target="#b1">[2]</ref> benchmarks (indicated by * in Table <ref type="table" target="#tab_1">2</ref>) because they represent memory intensive applications with access patterns that are not amenable to simple address predictors (e.g., stride predictors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Temporal Correlation Opportunity</head><p>We begin our evaluation by quantifying the temporal correlation of L1D cache misses. We investigate the presence of correlation by determining the degree of reordering between each pair of consecutive cache misses and the previous occurrence of the same two misses. We employ a Temporal Correlation Distance metric similar to that introduced in <ref type="bibr" target="#b23">[24]</ref>. Temporal correlation distance between two consecutive misses is the distance between the previous occurrence of the same two misses in the sequence of all cache misses. 1 A temporal correlation distance of +1 implies perfect correlation, where the two most recent occurrences of a pair of consecutive misses appeared in exactly the same order; a distance of -1 corresponds to a reversal of the two misses compared to previous occurrence, as in the sequence {A,B,...,B,A}.</p><p>Figure <ref type="figure">6</ref> (left) shows absolute temporal correlation distances as a cumulative distribution of all cache misses. The figure indicates that many applications (15 out of 28) exhibit nearly perfect temporal correlation, with most cache misses repeating in exactly the same order. Of the remaining applications with imperfect correlation, many present significant opportunity for LT-cords. Ammp, apsi, galgel, and parser contain approximately 60% perfectly correlated misses; mcf, perlbmk, and vortex contain over 40%. Not surprisingly, we note that gzip, bzip2, and twolf, applications that rely heavily on hashed or randomized memory accesses, exhibit little temporal correlation.</p><p>Figure <ref type="figure">6</ref> (left) shows strong pair-wise correlation of cache misses (very high percentage of miss pairs have +1 correlation distance), suggesting that repetitive sequences of cache misses exist. However, the temporal correlation distance metric says little about the length of the existing sequences. We measured the lengths of sequences with temporal correlation distances up to ?16. Our results show that benchmarks exhibiting strong temporal correlation have long sequences, ranging from over 2K misses for apsi to 16M misses for fma3d. For applications that exhibit imperfect correlation, Figure <ref type="figure">6</ref> (right) presents the cumulative distribution of correlated misses, grouped by sequence length.</p><p>The plot shows that even for a narrow temporal correlation distance range (?16), a large fraction of correlated cache misses belong to long sequences; for example, 80% of mcf's correlated cache misses belong to sequences longer than 2K, and over 30% of ammp's correlated misses are found in sequences greater than 4K in length. In practice, LT-cords tolerates a greater degree of reordering than ?16, potentially enabling even greater coverage.</p><p>1. We define a cache miss as a point in time when a cache replacement occurs. In our trace study, we label cache misses with the tuple (miss PC, miss block address, evicted block address). The previous occurrence of a miss is the nearest preceding miss with the same label.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 6. Absolute temporal correlation distance of all cache misses; correlation distance of 1 implies perfect repetition (left). Lengths of sequences of temporally correlated misses in applications that</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cache Miss vs Last-Touch Ordering</head><p>LT-cords stores last-touch signatures in the order they are discovered, the order of cache misses. However, when predicting, LT-cords follows these sequences in last-touch order. Using the temporal correlation distance metric from Section 5.1, we present the degree of reordering that LT-cords must tolerate to remain effective. We report the difference between the sequences of last touches and corresponding cache misses. Correlation distance of +1 indicates that cache misses corresponding to consecutive last touches appear in the same order, with no intervening misses.</p><p>Figure <ref type="figure" target="#fig_1">7</ref> shows that, on average, only 21% of cache misses are perfectly correlated with the last touches that precede them. Although this is a significant fraction of misses, this result indicates that LT-cords must tolerate reordering to achieve maximum potential. Figure <ref type="figure" target="#fig_1">7</ref> shows that LT-cords must hold on chip up to 1K signatures per sequence to tolerate reordering in over 98% of cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">LT-cords Coverage and Accuracy</head><p>In this section, we demonstrate the ability of LT-cords with realistic on-chip storage to approximate an "oracle" correlation table. We configure LT-cords parameters based on the sensitivity study presented in Section 5.4; LT-cords is compared to a Dead-Block Correlating Prefetcher with an unlimited-capacity correlation table . 
Figure <ref type="figure">8</ref> expresses results as percentages of the prediction opportunity-the total number of all L1D cache misses that would occur without a predictor. Correct represents eliminated cache misses, incorrect represents mispredicted replacement addresses, and train represents the fraction of cache misses not predicted due to training or low confidence. Cache misses that form prediction opportunity are either eliminated (correct), or not (incorrect and train); these percentages add up to 100%. Early represents premature evictions induced by the predictor, expressed above 100% as a percentage of the base case misses.</p><p>We compare LT-cords to a DBCP with unlimited-capacity storage to present an upper bound for prediction opportunity. Figure <ref type="figure">8</ref> indicates that, for most of the applications studied, LTcords exhibits coverage and accuracy that closely track an oracle predictor.</p><p>The most pronounced differences in coverage are in applications that have a large fraction of uncorrelated cache misses. Imperfect correlation in these applications (ammp, apsi, bzip2, gzip, parser, and twolf) diminishes the ability to bring last-touch signatures on-chip. For example, apsi exhibits sequences of hundreds to thousands of last touches that do not recur. Non-repetitive signatures pollute sequences, allowing successful tracking of only short correlated sequences (Figure <ref type="figure">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Storage Size Sensitivity</head><p>We first examine predictor sensitivity to signature cache size. We conduct an experiment with an unlimited number of 512-signature sequence fragments in off-chip memory. To reduce bias due to signature conflict at small signature cache sizes, we use an 8-way set associative signature cache. Figure <ref type="figure" target="#fig_2">9</ref> depicts normalized LT-cords coverage as a function of signature cache size.</p><p>Ideally, only one in-progress sequence is necessary. In practice, dynamic control flow and data structures result in multiple parallel sequences. Multiple sequences introduce a larger number of signatures that the cache must hold and require the cache size to account for conflicts. Based on results in Figure <ref type="figure" target="#fig_2">9</ref>, we estimate that a signature cache with 32K signatures is sufficient for our benchmarks, enabling approximately 20 simultaneously active sequences with up to ?1024 reordering and necessary sequence retrieval lookahead. At this size, 2-way associativity is sufficient.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> presents a breakdown of the necessary off-chip sequence storage size for the benchmarks with the largest storage requirements. The results show predictor coverage improvement up to 32M signatures for many applications. With 32M signatures, we found minimal sensitivity to fragment size up to 8K signatures (on average less than 2% decrease in LT-cords coverage). We therefore select this sequence fragment size to minimize the on-chip storage requirements of the sequence tag array.  The results of these experiments further stress that on-chip tables are insufficient for last-touch signature storage, even for applications with small active memory footprints. Applications such as facerec, mcf, and art, which have the smallest storage requirements, still require approximately 2M signatures (10MB of storage, 5 bytes per signature), exceeding reasonable on-chip structure sizes. For lucas, mgrid, and applu, less than half of the coverage is attainable with 16M sequence storage, and full predictor potential is only possible with 32M signatures (over 150MB of storage, comparable to application memory footprint). These widely-varying and workload-dependent storage requirements suggest that LT-cords storage is best managed by the operating system, to allow scaling predictor DRAM usage based on the application's needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multi-Programmed Environments</head><p>LT-cords records miss sequences that span billions of instructions and therefore must be preserved across context switches. Consequently, both the on-chip and off-chip storage structures must be shared among multiple contexts. In this section, we investigate the effect of multiple programs utilizing shared LT-cords structures by simulating a multi-programmed environment. We alternate execution between pairs of benchmarks, mimicking context-switches. To estimate appropriate duration of execution, we assume IPC of 1.5 for integer applications, and 3.0 for floating point applications, resulting in respective simulated quantums of 60M and 120M instructions with a 4GHz clock. The addresses accessed by one application in each pair were shifted to simulate non-overlapping physical address ranges. We present results from simulating the first 60 context switches.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> presents LT-cords coverage results for applications simulated on their own and in a multi-programmed environment with one other application. The benchmarks were selected as a representative subset of integer and floating point applications with comparatively high and low LT-cords coverages.</p><p>The results indicate that as long as predictor state is preserved across context switches, and off-chip sequence storage has ample space for recording sequences, the effect of multiple, simultaneously running programs on LT-cords coverage is negligible. On the other hand, pairings of lucas with applu and mgrid demonstrate the effect of insufficient combined sequence storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Cycle-Accurate Simulation Parameters</head><p>Based on our sensitivity analyses, for cycle-accurate timing results we simulate LT-cords with 160MB of off-chip sequence storage, a 204KB signature cache, and a 10KB sequence tag array. The sequence storage is partitioned into 4K frames, each frame holding a fragment of 8K last-touch signatures. Each last-touch signature consists of a 23-bit last-touch history trace, a 2-bit confidence counter, and 15-bit prediction address tag. The signature cache is organized as a 2-way set-associative structure, indexed by the low-order 14 bits and tagged by the 9 high-order bits of the last-touch signatures. Each signature cache entry is 42-bits, consisting of the 15-bit prediction address tag, 2-bit confidence counter, and 25-bit pointer to itself (addressing 32M signatures) in off-chip storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Speedup</head><p>We first evaluate LT-cords performance against a baseline processor augmented with a perfect L1D cache. Table <ref type="table" target="#tab_11">3</ref> presents the percent performance improvement of all benchmarks over the baseline configuration of Table <ref type="table" target="#tab_0">1</ref>. On average, we see 123% performance improvement opportunity if all accesses hit in L1D. LTcords is able to achieve a large fraction of this opportunity, attaining 60% average performance improvement across all applications. LT-cords achieves speedups that are roughly proportional to its coverage for the benchmarks with significant performance opportunity (e.g., 242% of the possible 338% for swim). However, in several cases, LT-cords speedup is lower than trace coverage suggests. For these benchmarks, a significant fraction of the speedup opportunity arises from block accesses that hit in L2 that are partially hidden by the out-of-order core. LT-cords correctly identifies and initiates fetches for these blocks, but only a small number of cycles ahead of the demand-fetch.</p><p>Table <ref type="table" target="#tab_11">3</ref> compares LT-cords performance with the program counter / delta correlation variant of the Global History Buffer (GHB PC/DC, subsumes stride prefetching), a realistic DBCP implementation, and a baseline processor with a larger L2 cache. GHB uses 256-entry index and history tables, as recommended for SPEC applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip correlation table as in <ref type="bibr" target="#b11">[12]</ref>. For comparison with a larger L2, we quadruple the size of the L2 cache of the baseline processor to 4MB, conservatively assuming the same access latency as the base 1MB cache. We corroborate prior results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> showing that GHB, an advanced stride/delta correlating predictor, can eliminate a large fraction of L2 cache misses in many applications, attaining on average 31% performance improvement across the applications we studied. Delta correlation is effective when the data layout is regular and accesses to distinct addresses follow a repeating pattern. For example, many SPECfp applications include array accesses that GHB can capture. Delta correlation can also be effective for pointer-intensive data structures if systematic heap allocation results in a regular layout (e.g., as in treeadd).</p><p>LT-cords can also predict the accesses captured by delta correlation if the same addresses are revisited more than once. In addition, LT-cords can predict repetitive accesses that do not follow a regular pattern, common in pointer-chasing accesses (e.g., in bh and em3d). Hence, LT-cords eliminates on average 59% of off-chip misses, while GHB eliminates 26%. However, delta correlation can outperform address correlation for applications with regular data layouts, but little data reuse (e.g., gap).</p><p>In addition to its off-chip coverage advantage, LT-cords further outperforms GHB by reducing the stalls associated with dependent chains of L1D misses that hit in L2. Unlike GHB, LTcords is able to prefetch directly into L1D without pollution because last-touch prediction ensures that useful data is not replaced. Furthermore, to achieve substantial coverage, GHB must be highly aggressive, fetching many blocks that are never accessed. GHB's aggressiveness can lead to significant bandwidth and L2 contention, degrading performance (e.g., as in twolf).</p><p>LT-cords also outperforms address-correlating predictors that use on-chip predictor storage. DBCP coverage is restricted by its limited on-chip signature storage, achieving an average performance improvement of only 17%. DBCP is able to achieve speedup on benchmarks with small memory footprints (e.g., bh) and benchmarks with large memory footprints but small working sets (e.g. mcf). Although both techniques use the same prediction signatures, DBCP marginally outperforms LT-cords in mcf and ammp because DBCP's on-chip signature lookup is always timely. However, the majority of applications with substantial memory system improvement opportunity incur misses to many distinct addresses and actively utilize most allocated memory. The signature storage requirements for these applications therefore scale with the application footprint. LT-cords allows the entire set of signatures to fit into its off-chip sequence storage, enabling LTcords to achieve four times the average DPCP speedup.</p><p>Finally, LT-cords offers superior speedup compared to quadrupling the base system's L2 cache size. A 4MB L2 cache cannot mitigate the impacts of: (1) an application's working set that is larger than 4MB, or (2) dependent chains of L1D misses, even if L2 accesses hit. The larger cache achieves a 16% performance improvement on average, mainly in applications with small working sets or non-repetitive (hash lookup) access patterns (e.g., bzip2, twolf). For most applications with substantial memory system opportunity, LT-cords is therefore significantly more effective in reducing the processor's memory stall cycles than a larger L2.</p><p>Many SPEC benchmarks exhibit little sensitivity to memory system improvements; they are included only for completeness. These benchmarks do not benefit significantly from any prediction technique. In these cases, LT-cords attains only slight performance improvements (3% of possible 10% for sixtrack, 3% of 9% for mesa, and 9% of 26% for apsi). LT-cords does not adversely affect performance of these benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Memory Bandwidth Overhead</head><p>To be effective, LT-cords must operate with practical memory bandwidth demands. Intuitively, LT-cords bus overhead is inversely proportional to the program's off-chip data traffic. LTcords transfers, on average, one signature from off-chip storage for every L1D miss. When the L2 miss rate is high, LT-cords overhead constitutes a small fraction of total bus utilization; the 5byte LT-cords signature is small relative to the cache block transferred as a result of the cache miss. Conversely, when L2 miss rate is low, LT-cords traffic constitutes a significant fraction of bus bandwidth. However, in this case, LT-cords overhead is unimportant, because a low L2 miss rate implies an under-utilized bus.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> presents the bus utilization of both the base system and LT-cords overhead for all benchmarks. Bus utilization is expressed as average bytes per instruction to ensure a meaningful comparison that is independent of the application runtimes. Per- With the exception of four benchmarks, the average memory bus bandwidth overhead of LT-cords is small: 17% for applications that exceed 1 byte per instruction off-chip traffic. Ammp, art, gcc, and mcf experience significant traffic overhead from incorrect predictions. For these applications, LT-cords incorrectly predicts up to 12.5% of the replacement addresses. Incorrect predictions do not directly degrade application performance (no new cache misses are introduced); however, an increase in memory system utilization is observed. Despite higher bus utilization, these applications experience over 22% performance improvement with LT-cords (385% in the case of mcf), indicating that the overhead of bringing signatures on chip is outweighed by LTcords' ability to reduce CPU stalls from cache misses.</p><p>In many situations, LT-cords can parallelize dependent cache misses that must be incurred in series in a demand-fetch system. Consequently, some benchmarks (applu, art, swim) begin to approach the peak bandwidth available in the system, becoming memory bandwidth-bound and not latency-bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Power</head><p>Although the LT-cords on-chip storage structures are larger than the L1D and are accessed as frequently, LT-cords implementation should contribute significantly less to the chip power dissipation than the L1D. First, despite lookup on every L1D access, the signature cache and sequence tag array read out data less frequently than the L1D cache. LT-cords structures contain signatures only for cache misses. It therefore follows that the majority of accesses to LT-cords structures require only a tag check and not a data read operation. By employing a serial lookup policy in the implementation of the LT-cords structures, less energy must be spent for performing data reads compared to the L1 data cache.</p><p>Next, the actual data width of the LT-cords structures is substantially narrower than the L1D. To exploit spatial locality, a cache line will contain on the order of 512 bits, of which only a single word must be selected and read or written. Conversely, each sequence tag array entry contains only a single counter and each signature cache entry is only 42 bits. The narrower datapath of the LT-cords structures therefore allows for lower read and write energy compared to the L1D.</p><p>To get a sense for the relative contribution of LT-cords compared to the L1D of our architecture, we used CACTI 4.2 <ref type="bibr" target="#b22">[23]</ref> to estimate the energy of the LT-cords storage structures and an L1D-like 64KB cache in a 70nm technology. CACTI estimates 18pJ dynamic energy to read a cache block from the data array of the L1D-like fast cache. Due to considerably narrower width of the data array and lack of selection logic, signature read energy is estimated at below 6pJ, despite the larger size of this structure.</p><p>The L1D tag and data array accesses must be performed in parallel to minimize access latency. As a result, total dynamic tag lookup and data read energy of a fast four-port L1D-like cache is approximately 73pJ, while using cache structures with serial lookup for the sequence tag array and signature cache results in a combined energy of 30pJ and only infrequently, once per L1D miss, incurs an additional 6.5pJ to read signature data. Conservatively estimating a 20% L1D cache miss rate, the average power dissipation of LT-cords structures is about 48% of L1D dissipation if implemented in the same technology.</p><p>CACTI estimates combined leakage power of the sequence tag array and signature cache at 800mW, while the L1D data cache will only leak approximately 230mW. This drastic difference arises because CACTI assumes that these structures will all be implemented using similar transistors. Lookup in the LT-cords structures does not require low latency and is not on the critical path, enabling a deeply pipelined design using high-Vt and/or long channel length transistors, which significantly reduce leakage compared to the highly latency-sensitive L1D cache.</p><p>Finally, LT-cords substantially raises chip performance on many workloads. By increasing the IPC, LT-cords may allow meeting target performance at a lower clock rate and lower power supply voltage, providing a way to offset the power dissipation overhead of LT-cords.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Technological advances in microarchitecture, circuits, and semiconductor fabrication all contribute to the long-term trend of a widening performance gap between microprocessors and memory. Larger on-chip caches can only partially mitigate the effect of growing memory latency. To facilitate efficient data transfer to the processor, recent research advocates using last-touch correlating prefetchers. Unlike delta-correlation-based prediction schemes, these prefetchers can eliminate most data cache misses despite interleaved access sequences and irregular memory layouts. However, existing designs are inefficient and impractical to implement, requiring prohibitive on-chip storage.</p><p>In this paper, we observed that last-touch signatures are temporally correlated and proposed Last-Touch Correlated Data Streaming (LT-cords), a practical design for accurate and timely streaming of data to the L1 cache. LT-cords leverages the temporal correlation of last-touch signatures to obviate the need for large, high-bandwidth, on-chip storage. LT-cords keeps last-touch signatures in off-chip DRAM, in the order they are discovered, and brings them on-chip when needed. Using only 214KB of onchip storage and with little impact on memory bandwidth, LTcords achieves coverage and lookahead similar to an "oracle" DBCP, a last-touch correlating prefetcher with unlimited on-chip resources. Our results show that LT-cords can eliminate 69% of all cache misses, achieving an average performance improvement of 60%, four times better than DBCP with 2MB on-chip storage, and two times better than a GHB PC/DC prefetcher.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Example of dead-block correlated prefetching; last access to A2 triggers a request for its replacement with A3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 7 .</head><label>7</label><figDesc>FIGURE 7. Last-Touch to Cache Miss correlation, plotted as a cumulative percentage of all misses up to a given absolute (negative or positive) correlation distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 9 .</head><label>9</label><figDesc>FIGURE 9. Coverage sensitivity to signature cache size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Presented deadtimes are an average across benchmarks listed in</figDesc><table><row><cell></cell><cell cols="3">CPU memory references</cell></row><row><cell></cell><cell>PC h :</cell><cell>ld/st A1 hit .</cell><cell></cell></row><row><cell></cell><cell>PC i :</cell><cell>ld/st A2 miss</cell><cell></cell></row><row><cell></cell><cell>PC j :</cell><cell>ld/st A2 hit .</cell><cell></cell></row><row><cell>lookahead</cell><cell>PC k :</cell><cell>ld/st A2 hit . . .</cell><cell>last touch prefetch A3</cell></row><row><cell></cell><cell>PC l :</cell><cell>ld/st A3 hit</cell><cell>prefetched</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1. Simulation system parameters listed in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . FIGURE 2. Cumulative distribution of dead-times (cycles between last access to a block and its eviction).</head><label>2</label><figDesc></figDesc><table><row><cell>Memory</cell><cell>latency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Dead-Tim e (cycles) CDF of Cache Blocks</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>A2 A1,A2,PC i ,PC j ,PC k FIGURE 3. Previously-proposed DBCP hardware.</head><label></label><figDesc></figDesc><table><row><cell>CPU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell cols="3">average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ld/st A2 @ PC k</cell><cell></cell><cell cols="2">on-chip correlation table</cell><cell>80%</cell><cell></cell><cell cols="3">w orst-case</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>history table</cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>A1,PC i ,PC j</cell><cell>?</cell><cell>A3</cell><cell>11</cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1</cell><cell>evict A2</cell><cell></cell><cell></cell><cell>%</cell><cell>160KB</cell><cell>320KB</cell><cell>640KB</cell><cell>1MB</cell><cell>2MB</cell><cell>5MB</cell><cell>10MB</cell><cell>20MB</cell><cell>40MB</cell><cell>80MB</cell><cell>160MB</cell><cell>320MB</cell></row><row><cell>A3</cell><cell></cell><cell></cell><cell cols="2">prefetch A3</cell><cell></cell><cell></cell><cell cols="8">On-Chip Correlation Table Size</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>of Achievable Coverage FIGURE 4. Sensitivity of DBCP to on-chip correlation table size, normalized to DBCP with unlimited storage.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>A1,PC i ,PC j FIGURE 5. The anatomy of a LT-cords implementation.</head><label></label><figDesc></figDesc><table><row><cell>on chip</cell><cell></cell><cell cols="3">sequence tag array</cell><cell></cell><cell>sequence frames</cell></row><row><cell>CPU</cell><cell></cell><cell cols="3">head hist-hash win. pos.</cell><cell>?</cell><cell>sig 1 sig 2 ..........</cell><cell>sig n</cell></row><row><cell>history table</cell><cell></cell><cell cols="2">signature cache</cell><cell></cell><cell></cell><cell>main memory</cell></row><row><cell>L1</cell><cell>?</cell><cell>hist-hash</cell><cell>A3</cell><cell>11</cell><cell>ptr</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 1 . System configuration. Processor Caches</head><label>1</label><figDesc></figDesc><table><row><cell>Clock rate</cell><cell>4 GHz</cell><cell>L1 D</cell><cell>64KB, 64-byte line</cell></row><row><cell>Issue/retire</cell><cell>8 instructions/cycle</cell><cell></cell><cell>2-way, 2-cycle</cell></row><row><cell>Reorder buffer</cell><cell>256 entries</cell><cell>L1 I</cell><cell>64KB, 64-byte line</cell></row><row><cell cols="2">Load/store queue 128 entries</cell><cell></cell><cell>4-way, 2-cycle</cell></row><row><cell cols="2">Branch predictor 8K/8K hybrid</cell><cell>L1 D ports</cell><cell>4</cell></row><row><cell></cell><cell>12-cycle penalty</cell><cell>L1 D MSHRs</cell><cell>64</cell></row><row><cell>Functional Units</cell><cell>8 IALU (1)</cell><cell>L2 (unified)</cell><cell>1MB, 8-way</cell></row><row><cell>(latencies)</cell><cell>2 FALU (2)</cell><cell></cell><cell>20-cycle</cell></row><row><cell></cell><cell>2 I-MUL/DIV (3/19)</cell><cell>L2 ports</cell><cell>1</cell></row><row><cell></cell><cell>2 F-MUL/DIV (4/12)</cell><cell>L1/L2 bus</cell><cell>1-cycle request</cell></row><row><cell></cell><cell>all pipelined except</cell><cell></cell><cell>32 byte per-cycle</cell></row><row><cell></cell><cell>IDIV and FDIV</cell><cell>TLB</cell><cell>256 entry, 4-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell>600-cycle miss</cell></row><row><cell cols="2">Predictors</cell><cell></cell><cell>Memory</cell></row><row><cell>DBCP</cell><cell>2MB correlation</cell><cell>Size</cell><cell>1GB (30-bit space)</cell></row><row><cell></cell><cell>table, 18-cycle</cell><cell>Latency</cell><cell>200 cycles first 32B</cell></row><row><cell>GHB</cell><cell>PC/DC, 4-deep</cell><cell></cell><cell>3 cycles each 32B</cell></row><row><cell></cell><cell>256-entry IT</cell><cell>Bus</cell><cell>32-byte wide</cell></row><row><cell></cell><cell>256-entry GHB</cell><cell></cell><cell>1333 MHz</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 . Benchmarks, base miss rates and IPCs.</head><label>2</label><figDesc>all DBCP and LT-cords requests are placed into a 128-entry circular queue. When the request queue is full, new requests replace old (unissued) ones at the queue head. Requests are only issued when the L1/L2 bus is free.Table</figDesc><table><row><cell></cell><cell>L1 miss %</cell><cell>L2 miss %</cell><cell>IPC</cell><cell></cell><cell>L1 miss %</cell><cell>L2 miss %</cell><cell>IPC</cell></row><row><cell>ammp</cell><cell>15</cell><cell>24</cell><cell>1.07</cell><cell>gcc</cell><cell>38</cell><cell>3</cell><cell>2.71</cell></row><row><cell>applu</cell><cell>34</cell><cell>68</cell><cell>1.53</cell><cell>gzip</cell><cell>5</cell><cell>2</cell><cell>1.55</cell></row><row><cell>apsi</cell><cell>6</cell><cell>16</cell><cell>2.69</cell><cell>lucas</cell><cell>44</cell><cell>67</cell><cell>1.25</cell></row><row><cell>art</cell><cell>60</cell><cell>63</cell><cell>0.72</cell><cell>mcf</cell><cell>53</cell><cell>67</cell><cell>0.08</cell></row><row><cell>bh*</cell><cell>7</cell><cell>94</cell><cell>0.67</cell><cell>mesa</cell><cell>2</cell><cell>25</cell><cell>3.76</cell></row><row><cell>bzip2</cell><cell>4</cell><cell>21</cell><cell>1.56</cell><cell>mgrid</cell><cell>18</cell><cell>49</cell><cell>1.56</cell></row><row><cell>crafty</cell><cell>0</cell><cell>2</cell><cell>2.24</cell><cell>parser</cell><cell>6</cell><cell>17</cell><cell>1.14</cell></row><row><cell>em3d*</cell><cell>67</cell><cell>87</cell><cell>0.50</cell><cell>perlbmk</cell><cell>2</cell><cell>14</cell><cell>1.58</cell></row><row><cell>eon</cell><cell>0</cell><cell>0</cell><cell>1.94</cell><cell>sixtrack</cell><cell>1</cell><cell>74</cell><cell>4.29</cell></row><row><cell>equake</cell><cell>31</cell><cell>85</cell><cell>0.68</cell><cell>swim</cell><cell>49</cell><cell>59</cell><cell>1.18</cell></row><row><cell>facerec</cell><cell>22</cell><cell>42</cell><cell>2.04</cell><cell>treeadd*</cell><cell>5</cell><cell>92</cell><cell>0.24</cell></row><row><cell>fma3d</cell><cell>11</cell><cell>62</cell><cell>1.74</cell><cell>twolf</cell><cell>15</cell><cell>12</cell><cell>0.84</cell></row><row><cell>galgel</cell><cell>17</cell><cell>16</cell><cell>3.13</cell><cell>vortex</cell><cell>4</cell><cell>16</cell><cell>3.11</cell></row><row><cell>gap</cell><cell>2</cell><cell>54</cell><cell>1.07</cell><cell>wupwise</cell><cell>9</cell><cell>72</cell><cell>2.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>exhibit more than 5% uncorrelated misses (right).</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ammp</cell><cell>applu</cell><cell></cell><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDF of Misses</cell><cell>20% 40% 60% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>apsi bh crafty eon facerec galgel gcc lucas mesa</cell><cell>art bzip2 em3d equake fma3d gap gzip mcf mgrid</cell><cell></cell><cell>CDF of Correlated Misses</cell><cell>20% 40% 60% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>1</cell><cell>3</cell><cell>7</cell><cell>15</cell><cell>31</cell><cell>63</cell><cell>127</cell><cell>255</cell><cell>parser sixtrack treeadd</cell><cell>perlbmk swim twolf</cell><cell></cell><cell></cell><cell>0%</cell><cell>128</cell><cell>512</cell><cell>2K</cell><cell>8K</cell><cell>32K</cell><cell>128K</cell><cell>512K</cell><cell>2M</cell><cell>8M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">| Tem p. Correlation Dist. |</cell><cell></cell><cell>vortex</cell><cell>wupwise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Sequence Length</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CDF of Misses</cell><cell>20% 40% 60% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell></cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1K</cell><cell>2K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">| Last-Touch to Miss Correlation Distance |</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Comparing coverage and accuracy of LT-cords to DBCP with unlimited storage.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">correct</cell><cell cols="3">incorrect</cell><cell>train</cell><cell cols="2">early</cell></row><row><cell></cell><cell>120%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">A=LT-cords B=Unlimited DBCP</cell></row><row><cell>% of L1 Misses</cell><cell>20% 40% 60% 80% 100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0% FIGURE 8. AB ammp</cell><cell>applu</cell><cell>apsi</cell><cell>art</cell><cell>bzip2</cell><cell>crafty</cell><cell>eon</cell><cell>equake</cell><cell>facerec</cell><cell>fma3d</cell><cell>galgel</cell><cell>gap</cell><cell>gcc</cell><cell>gzip</cell><cell>lucas</cell><cell>mcf</cell><cell>mesa</cell><cell>mgrid</cell><cell>parser</cell><cell>perlbmk</cell><cell>sixtrack</cell><cell>swim</cell><cell>twolf</cell><cell>vortex</cell><cell>wupwise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>% of Achievable Coverage</cell><cell>0% 20% 40% 60% 80% 100%</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1K</cell><cell>2K</cell><cell>4K</cell><cell>8K</cell><cell>16K</cell><cell cols="2">32K average 64K</cell><cell>128K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Signature Cache Size (entries)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>cords coverage in a multi-programmed environ- ment. The left bar shows standalone application; remaining bars show the application running with one other application.</head><label></label><figDesc></figDesc><table><row><cell>% of Potential Predictions</cell><cell>0% 20% 40% 60% 80% 100%</cell><cell>lucas</cell><cell>mgrid</cell><cell>applu</cell><cell>wupwise</cell><cell>swim</cell><cell>fma3d 32M</cell><cell>ammp</cell><cell>parser 16M</cell><cell>gcc</cell><cell>equake 8M</cell><cell>facerec 4M</cell><cell>mcf</cell><cell>art 2M</cell><cell>% of L1 Misses</cell><cell>0% 20% 40% 60% 80% 100% 120%</cell><cell>standalone w/ mcf w/ gzip w/ swim gcc</cell><cell>standalone correct w/ gcc w/ vortex w/ fma3d mcf</cell><cell>standalone w/ fma3d w/ mesa w/ gcc incorrect standalone w/ swim w/ facerec train w/ mcf sw im fma3d</cell><cell>standalone w/ applu w/ mgrid early lucas</cell></row><row><cell cols="15">FIGURE 10. Off-chip sequence storage size (in millions of signa-</cell><cell cols="3">FIGURE 11. LT-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">tures) needed to achieve given coverage.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 3 .</head><label>3</label><figDesc>Performance comparison. Each value indicates percent performance improvement over the baseline processor configuration. Base data are cache block transfers (including speculative loads) made by the base system without a predictor. Incorrect predictions are extraneous cache block transfers initiated by LT-cords as a result of misprediction. Sequence creation is the LT-cords memory system utilization for writing of off-chip signature sequences and confidence counter updates. Sequence fetch is the traffic overhead introduced by LT-cords retrieving last-touch signatures from off-chip sequence storage. The memory utilization results account for bus request and data transfer cycles.</figDesc><table><row><cell></cell><cell>bzip2</cell><cell>crafty</cell><cell>eon</cell><cell>gap</cell><cell>gcc</cell><cell>gzip</cell><cell>mcf</cell><cell>parser</cell><cell>perlbmk</cell><cell>twolf</cell><cell>vortex</cell><cell>SPECint mean</cell><cell></cell><cell>bh</cell><cell>em3d</cell><cell>treeadd</cell><cell>Olden mean</cell></row><row><cell>Perfect L1</cell><cell>43</cell><cell>3</cell><cell>1</cell><cell>65</cell><cell>29</cell><cell cols="2">17 1637</cell><cell>67</cell><cell>31</cell><cell>89</cell><cell>54</cell><cell>73</cell><cell></cell><cell cols="4">262 439 266 315</cell></row><row><cell>LT-cords</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>22</cell><cell cols="2">0 385</cell><cell>15</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>20</cell><cell></cell><cell cols="4">206 247 224 225</cell></row><row><cell>GHB</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>46</cell><cell>5</cell><cell cols="2">0 143</cell><cell>22</cell><cell>7</cell><cell>-8</cell><cell>0</cell><cell>15</cell><cell></cell><cell>2</cell><cell cols="2">33 179</cell><cell>56</cell></row><row><cell>DBCP</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>6</cell><cell cols="2">0 465</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>3</cell><cell>19</cell><cell></cell><cell>153</cell><cell>0</cell><cell>0</cell><cell>36</cell></row><row><cell>4MB L2</cell><cell>22</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>7</cell><cell cols="2">0 245</cell><cell>28</cell><cell>5</cell><cell>56</cell><cell>1</cell><cell>23</cell><cell></cell><cell>8</cell><cell>12</cell><cell>0</cell><cell>7</cell></row><row><cell></cell><cell>ammp</cell><cell>applu</cell><cell>apsi</cell><cell>art</cell><cell>equake</cell><cell>facerec</cell><cell>fma3d</cell><cell>galgel</cell><cell>lucas</cell><cell>mesa</cell><cell>mgrid</cell><cell>sixtrack</cell><cell>swim</cell><cell>wupwise</cell><cell>SPECfp mean</cell><cell></cell><cell>overall</cell></row><row><cell>Perfect L1</cell><cell cols="2">212 162</cell><cell cols="5">26 301 470 141 155</cell><cell cols="2">67 211</cell><cell cols="2">9 156</cell><cell cols="2">10 338</cell><cell cols="2">93 139</cell><cell></cell><cell>123</cell></row><row><cell>LT-cords</cell><cell>95</cell><cell>39</cell><cell cols="3">9 197 267</cell><cell cols="2">76 108</cell><cell>31</cell><cell>27</cell><cell>3</cell><cell>88</cell><cell cols="2">3 242</cell><cell>40</cell><cell>71</cell><cell></cell><cell>60</cell></row><row><cell>GHB</cell><cell>46</cell><cell>40</cell><cell>2</cell><cell cols="2">16 113</cell><cell>60</cell><cell>65</cell><cell>16</cell><cell>49</cell><cell cols="2">2 114</cell><cell>0</cell><cell>43</cell><cell>51</cell><cell>40</cell><cell></cell><cell>31</cell></row><row><cell>DBCP</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>24</cell><cell>0</cell><cell>58</cell><cell>0</cell><cell>16</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>7</cell><cell>0</cell><cell>0</cell><cell>12</cell><cell></cell><cell>17</cell></row><row><cell>4MB L2</cell><cell>22</cell><cell>4</cell><cell>0</cell><cell>91</cell><cell>2</cell><cell>56</cell><cell>0</cell><cell>47</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>13</cell><cell></cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>LT-cords memory system utilization, normalized to bytes/instruction to remove effect of application speedup.</head><label></label><figDesc></figDesc><table><row><cell>2 4 6 8 10 12 14 16 FIGURE 12. LT-cords overhead ammp applu apsi art bh bzip2 crafty em3d eon equake facerec fma3d galgel gap gcc gzip lucas mcf mesa mgrid parser perlbmk sixtrack base data incorrect predictions sequence creation sequence fetch swim treeadd twolf Memory Bus Utilization (bytes/instruction) application data traffic</cell><cell>vortex</cell><cell>wupwise</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1-4244-1081-9/07/$25.00 ?2007 IEEE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their feedback on drafts of this paper. This work was partially supported by grants and equipment from <rs type="funder">Intel</rs>, a <rs type="grantName">Sloan research fellowship</rs>, an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery Grant</rs>, an <rs type="funder">IBM</rs> faculty partnership award, and <rs type="funder">NSF</rs> grant <rs type="grantNumber">CCR-0509356</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wev4GEK">
					<orgName type="grant-name">Sloan research fellowship</orgName>
				</org>
				<org type="funding" xml:id="_vCKPNvZ">
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
				<org type="funding" xml:id="_sTHdr7d">
					<idno type="grant-number">CCR-0509356</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Supercomputing &apos;91</title>
		<meeting>of Supercomputing &apos;91</meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Early experiences with Olden</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Carlisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Reppy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hendren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Sixth Lang. and Compilers for Parallel Computing</title>
		<meeting>of Sixth Lang. and Compilers for Parallel Computing</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalized correlation-based hardware prefetching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Reeves</surname></persName>
		</author>
		<idno>EE-CEG-95-1</idno>
		<imprint>
			<date type="published" when="1995-02">Feb. 1995</date>
		</imprint>
		<respStmt>
			<orgName>School of Electrical Engineering, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic hot data stream prefetching for general-purpose programs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGPLAN &apos;02 Conf. on Program. Lang. Design and Impl. (PLDI)</title>
		<meeting>of SIGPLAN &apos;02 Conf. on Program. Lang. Design and Impl. (PLDI)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 31st Annual Intl. Symp. on Comp. Arch. (ISCA-31)</title>
		<meeting>of 31st Annual Intl. Symp. on Comp. Arch. (ISCA-31)</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointer cache assisted prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 35th Annual IEEE/ACM Intl. Symp. on Microarch</title>
		<meeting>of 35th Annual IEEE/ACM Intl. Symp. on Microarch<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fong Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 28th Annual Intl. Symp. on Comp. Arch. (ISCA-28)</title>
		<meeting>of 28th Annual Intl. Symp. on Comp. Arch. (ISCA-28)</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A stateless, contentdirected data prefetching mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Tenth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS X)</title>
		<meeting>of Tenth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS X)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microlib: a case for the quantitative comparison of micro-architecture mechanisms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Gracia</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Third Annual Workshop on Duplicating, Deconstructing, and Debunking (WDDD04)</title>
		<meeting>of Third Annual Workshop on Duplicating, Deconstructing, and Debunking (WDDD04)</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Timekeeping in the memory system: predicting and optimizing memory behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 29th Annual Intl. Symp. on Comp. Arch. (ISCA-29)</title>
		<meeting>of 29th Annual Intl. Symp. on Comp. Arch. (ISCA-29)</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 24th Annual Intl. Symp. on Comp. Arch. (ISCA-24)</title>
		<meeting>of 24th Annual Intl. Symp. on Comp. Arch. (ISCA-24)</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 28th Annual Intl. Symp. on Comp. Arch. (ISCA-28)</title>
		<meeting>of 28th Annual Intl. Symp. on Comp. Arch. (ISCA-28)</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modeling live and dead lines in cache memory systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thi'ebaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradhan</surname></persName>
		</author>
		<idno>TR-90-CSE- 14</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Runahead execution: an effective alternative to large instruction windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="2003-12">November/December 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Tenth IEEE Symp</title>
		<meeting>of Tenth IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decoupled kiloinstruction processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pericas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Twelfth IEEE Symp</title>
		<meeting>of Twelfth IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eighth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS VIII)</title>
		<meeting>of Eighth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS VIII)</meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Tenth Intl. Conf. on Arch</title>
		<meeting>of Tenth Intl. Conf. on Arch</meeting>
		<imprint>
			<publisher>ASP-LOS X)</publisher>
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictor-directed stream buffers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 33rd Annual IEEE/ACM Intl. Symp. on Microarch. (MICRO 33)</title>
		<meeting>of 33rd Annual IEEE/ACM Intl. Symp. on Microarch. (MICRO 33)</meeting>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 29th Annual Intl. Symp. on Comp. Arch. (ISCA-29)</title>
		<meeting>of 29th Annual Intl. Symp. on Comp. Arch. (ISCA-29)</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continual flow pipelines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eleventh Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS XI)</title>
		<meeting>of Eleventh Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS XI)</meeting>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slipstream processors: improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Ninth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS IX)</title>
		<meeting>of Ninth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS IX)</meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL-2006-86</idno>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">CACTI 4.0. HP Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 33d Annual Intl. Symp. on Comp. Arch. (ISCA-33)</title>
		<meeting>of 33d Annual Intl. Symp. on Comp. Arch. (ISCA-33)</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simulation sampling with live-points</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intl. Symp. on the Perf</title>
		<meeting>of Intl. Symp. on the Perf</meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model for estimating trace-sample miss ratios</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1991 ACM SIGMETRICS Conf. on Measurement and Modeling of Comp. Syst</title>
		<meeting>of 1991 ACM SIGMETRICS Conf. on Measurement and Modeling of Comp. Syst</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="79" to="89" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
