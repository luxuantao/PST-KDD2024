<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAN-powered Deep Distributional Reinforcement Learning for Resource Management in Network Slicing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiu</forename><surname>Hua</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rongpeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xianfu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">GAN-powered Deep Distributional Reinforcement Learning for Resource Management in Network Slicing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18F842B6DAE2C818782C3AB0A1BBFFAE</idno>
					<idno type="DOI">10.1109/JSAC.2019.2959185</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSAC.2019.2959185, IEEE Journal on Selected Areas in Communications</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>network slicing</term>
					<term>deep reinforcement learning</term>
					<term>distributional reinforcement learning</term>
					<term>generative adversarial network</term>
					<term>GAN</term>
					<term>5G</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network slicing is a key technology in 5G communications system. Its purpose is to dynamically and efficiently allocate resources for diversified services with distinct requirements over a common underlying physical infrastructure. Therein, demand-aware resource allocation is of significant importance to network slicing. In this paper, we consider a scenario that contains several slices in a radio access network with base stations that share the same physical resources (e.g., bandwidth or slots). We leverage deep reinforcement learning (DRL) to solve this problem by considering the varying service demands as the environment state and the allocated resources as the environment action. In order to reduce the effects of the annoying randomness and noise embedded in the received service level agreement (SLA) satisfaction ratio (SSR) and spectrum efficiency (SE), we primarily propose generative adversarial network-powered deep distributional Q network (GAN-DDQN) to learn the actionvalue distribution driven by minimizing the discrepancy between the estimated action-value distribution and the target actionvalue distribution. We put forward a reward-clipping mechanism to stabilize GAN-DDQN training against the effects of widelyspanning utility values. Moreover, we further develop Dueling GAN-DDQN, which uses a specially designed dueling generator, to learn the action-value distribution by estimating the statevalue distribution and the action advantage function. Finally, we verify the performance of the proposed GAN-DDQN and Dueling GAN-DDQN algorithms through extensive simulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The emerging fifth-generation (5G) mobile systems, armed with novel network architecture and emerging technologies, are expected to offer support for a plethora of network services Yuxiu Hua, Rongpeng Li and Honggang Zhang are with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: {21631087, lirongpeng, hong-gangzhang}@zju.edu.cn).</p><p>Zhifeng Zhao is with Zhejiang Lab, Hangzhou, China as well as the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: zhaozf@zhejianglab.com).</p><p>Xianfu Chen is with the VTT Technical Research Centre of Finland, Oulu 90570, Finland (e-mail: xianfu.chen@vtt.fi).</p><p>with diverse performance requirements <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Specifically, it is envisioned that 5G systems cater to a wide range of services differing in their requirements and types of devices, and going beyond the traditional human-type communications to include various kinds of machine-type communications <ref type="bibr" target="#b3">[4]</ref>. According to ITU-R recommendations, it is a consensus that the key technologies of 5G wireless systems will spawn three generic application scenarios: enhanced mobile broadband (eMBB), massive machine-type communications (mMTC), and ultra-reliable and low-latency communications (URLLC) <ref type="bibr" target="#b4">[5]</ref>. Specifically, (a) eMBB supports data-driven use cases requiring high data rates across a wide coverage area; (b) mMTC supports a very large number of devices in a broad area, which may only send data sporadically, such as Internet of Things (IoT) use cases; (c) URLLC supports strict requirements on latency and reliability for missioncritical communications, such as remote surgery, autonomous vehicles or Tactile Internet. Serving a diverse set of use cases over the same network will increase complexity, which must be managed to ensure acceptable service levels. As the services related to different use cases can have very different characteristics, they will impose varying requirements on the network resources. For example, considering mMTC devices for utility metering and parking sensors, they hardly move and do not require mobility management, i.e., the need to track location. On the other hand, sensors related to freight management would commonly move even across countries' borders and would require mobility management, including roaming agreements.</p><p>However, legacy mobile networks are mostly designed to provide services for mobile broadband consumers and merely consist of a few adjustable parameters like priority and quality of service (QoS) for dedicated services. Thus it is difficult for mobile operators to extend their networks into these emerging vertical services because of the different service requirements for network design and development. The concept of network slicing has recently been proposed to address this challenging problem; the physical and computational resources of the network can be sliced to meet the diverse needs of a range of 5G users <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this way, heterogeneous requirements can be served cost-effectively by the same physical infrastructure, since different network slice (NS) instances can be orchestrated and configured according to the specific requirements of the slice tenants.</p><p>As a non-nascent concept, network slicing can be traced back to the Infrastructure as a Service (IaaS) cloud computing model <ref type="bibr" target="#b7">[8]</ref>, whereby different tenants share computing, networking and storage resources to create different isolated fullyfunctional virtual networks on a common infrastructure. In the context of 5G and beyond, network functions virtualization (NFV) and software defined networking (SDN) technologies serve as a basis for the core network slicing by allowing both physical and virtual resources to be used to provide certain services, enabling 5G networks to deliver different kinds of services to various customers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. On the other hand, the Next Generation Mobile Networks (NGMN) alliance puts forward an evolved end-to-end network slicing idea while the Third-Generation Partnership Project (3GPP) also suggests that radio access network (RAN) should not be excluded to "design specific functionality to support multiple slices or even partition of resources for different network slices" <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, in order to provide better-performing and costefficient services, RAN slicing involves more challenging technical issues for the realtime resource management on existing slices, since (a) for RANs, spectrum is a scarce resource and it is essential to guarantee the spectrum efficiency (SE) <ref type="bibr" target="#b8">[9]</ref>; (b) the service level agreements (SLAs) with slice tenants usually impose stringent requirements; and (c) the actual demand of each slice heavily depends on the request patterns of mobile users (MUs) <ref type="bibr" target="#b12">[13]</ref>. Therefore, the classical dedicated resource allocation fails to address these problems simultaneously <ref type="bibr" target="#b10">[11]</ref>. Instead, it is necessary to intelligently allocate the radio resources (e.g., bandwidth or slots) to slices according to the dynamics of service requests from mobile users coherently <ref type="bibr" target="#b13">[14]</ref> with the goal of meeting SLA requirements in each slice, but at the cost of acceptable SE. In this regard, there have been extensive efforts <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr" target="#b14">[15]</ref> proposed an online genetic slicing strategy optimizer for inter-slice resource management. However, <ref type="bibr" target="#b14">[15]</ref> did not consider the explicit relationship between the required resource and SLA on a slice, as one slice might require more resources given its more stringent SLA. <ref type="bibr" target="#b15">[16]</ref> considered the problem of different types of resources (bandwidth, caching, backhaul capacities) being allocated to NS tenants based on user demands. The authors proposed mathematical solutions, but the optimization problem would become intractable when the simulation parameters are scaled up (e.g., increasing the number of NSs or the shareable resources). <ref type="bibr" target="#b16">[17]</ref> mathematically analyzed the joint optimization problem of access control and bandwidth allocation in the multi-base station (BS) multi-NS scenario. However, the solutions therein are based on the assumption that different users have the same fixed demand rate, which is a condition unlikely to be found in practice. From the perspective of bandwidth usage-based pricing (UBP), <ref type="bibr" target="#b19">[20]</ref> used game theory to analyze the relationship between Internet service providers (ISPs) and users, thereby improving the profit of ISPs and solving the peak-time congestion problem. However, the timeslot for bandwidth allocation in <ref type="bibr" target="#b19">[20]</ref> was 1 hour, which is unrealistic in a situation where the number of demands changes drastically in a short period.</p><p>In order to address the demand-aware resource allocation problem, one potential solution is reinforcement learning (RL). RL is an important type of machine learning where an agent learns how to perform optimal actions in an environment from observing state transitions and obtaining feedback (rewards/costs). In RL, the action value, Q(s, a), describes the expected return, or the discounted sum of rewards, when performing action a in state s. Usually, the Q value can be estimated by classic value-based methods such as SARSA <ref type="bibr" target="#b20">[21]</ref> and Q-learning <ref type="bibr" target="#b21">[22]</ref> based on the Bellman equation. <ref type="bibr" target="#b22">[23]</ref> used deep neural networks to approximate the Q function, namely deep Q network (DQN), which demonstrated human-like performance on simple computer games and inspired a research wave of deep reinforcement learning (DRL). Besides modeling Q(s, a), <ref type="bibr" target="#b23">[24]</ref> showed that we could learn the distribution of Q(s, a) by the distributional analogue of Bellman equation; this approach improved the estimation of action values in an inherently randomness environment. Specifically, <ref type="bibr" target="#b23">[24]</ref> proposed C51 algorithm to minimize the Kullback-Leibler (KL) divergence between the approximated Q distribution and the target Q distribution calculated by the distributional Bellman optimality operator. Inspired by the theory of quantile regression <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> proposed the quantile regression DQN (QR-DQN) and thus successfully performed distributional RL over the Wasserstein metric, leading to the state-of-the-art performance. <ref type="bibr" target="#b26">[27]</ref> extended QR-DQN from learning a discrete set of quantiles to learning the full quantile function and put forward the implicit Q network (IQN). Given the success of replacing Q(s, a) by its distribution in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> as well as the reputation of generative adversarial network (GAN) for approximating distributions <ref type="bibr" target="#b27">[28]</ref>, it naturally raises a question of whether GAN is viable for approximating the action-value distribution and thus improving distributional RL.</p><p>In the field of communications and networking, DRL has triggered tremendous research attention to solving resource allocation issues in some specific fields like power control <ref type="bibr" target="#b28">[29]</ref>, green communications <ref type="bibr" target="#b29">[30]</ref>, cloud RANs <ref type="bibr" target="#b30">[31]</ref>, mobile edge computing and caching <ref type="bibr" target="#b31">[32]</ref>. Given the challenging technical issues in resource management on existing NSs, the previous work in <ref type="bibr" target="#b12">[13]</ref> leveraged DQN to find the optimal resource allocation policy and investigated its performance. However, the method proposed in <ref type="bibr" target="#b12">[13]</ref> did not consider the effects of random noise on the calculation of SE and SLA satisfaction ratio (SSR). To mitigate the potential risk of incorrectly estimating the action value due to the randomness in SE and SSR, we intend to introduce the distributional RL to estimate the action-value distribution, thus avoiding the action-value overestimation or underestimation issue that plagues many traditional value-based RL algorithms (e.g., DQN). Meanwhile, the cutting-edge performance of Wasserstein generative adversarial network with gradient penalty (WGAN-GP) in the distribution approximation suggests to us that we might use it to learn the action-value distribution. To this end, we propose a new approach, the GAN-powered deep distributional Q network (GAN-DDQN), based on distributional RL and WGAN-GP, to realize dynamic and efficient resource allocation per slice. The main contributions of this paper are as follows:</p><p>• To find the optimal resource allocation policy under the uncertainty of slice service demands, we design the GAN-DDQN algorithm, where the generator network outputs a fixed number of particles that try to match the actionvalue distribution for each action. Such a design in GAN-DDQN can mitigate the effects of learning from a nonstationary environment and is significantly different from the concurrent yet independent works <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> 1 . • We demonstrate that the widely-spanning system utility values could destabilize GAN-DDQN's training process, and we correspondingly design a reward-clipping mechanism to reduce this negative impact. Specifically, we clip the system utility values to some constant values according to a straightforward rule with several heuristicsguided adjustable thresholds, and then use these constants as the final rewards in RL. • GAN-DDQN suffers from the challenge that only a small part of the generator output is included in the calculation of the loss function during the training. To compensate for this, we further propose Dueling GAN-DDQN, which is a special solution derived from Dueling DQN <ref type="bibr" target="#b34">[35]</ref> and the discrete normalized advantage functions (DNAF) algorithm <ref type="bibr" target="#b35">[36]</ref>. Dueling GAN-DDQN separates the statevalue distribution from the action-value distribution and combines the action advantage function to obtain the action values. In addition, we elaborate on twofold loss functions that further take advantage of the temporal difference (TD) error information to achieve performance gains. The introduction of dueling networks to GAN-DDQN makes the work described in this paper significantly different from our previous work presented in IEEE Globecom 2019 <ref type="bibr" target="#b0">[1]</ref>. • Finally, we perform extensive simulations to demonstrate the superior efficiency of the proposed solutions over the classical methods, such as DQN, and provide insightful numerical results for the implementation details. The remainder of the paper is organized as follows: Section II talks about some necessary mathematical backgrounds and formulates the system model. Section III gives the details of the GAN-DDQN, while Section IV presents the detailed simulation results. Finally, Section V summarizes the paper and offers prospects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES AND SYSTEM MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>Table I lists the important notations used in this paper. An agent tries to find the optimal behavior in a given setting 1 [33] used a generator network that directly outputs action values and did not show any significant improvement of GAN Q-learning over conventional DRL methods. <ref type="bibr" target="#b33">[34]</ref> used the policy iteration method <ref type="bibr" target="#b36">[37]</ref> to loop through a two-step procedure for the value estimation and policy improvement, where GAN was only used to estimate the action-value distribution. Besides, <ref type="bibr" target="#b33">[34]</ref> exploited a totally different framework without the target generator, which is a key component for GAN-DDQN. The number of discriminator updates per training iteration through interaction with the environment, which can be treated as solving an RL problem. This interactive process can be modeled as a Markov Decision Process (S, A, R, P, γ), where S and A denote the state and action spaces, R is the reward, P (•|s, a) is the transition probability, and γ ∈ (0, 1] is a discount factor. A policy π(•|s) maps a state to a distribution over actions. The state-value function of a state s under a policy π(•|s), denoted V π (s), is the expected return when starting in s and following π thereafter. Similarly, we define the value of taking action a in state s under the policy π, denoted Q π (s, a), as the expected return starting from s, taking the action a, and thereafter following policy π.</p><p>Mathematically, the state-value function is</p><formula xml:id="formula_0">V π (s) = E π,P ∞ t=0 γ t R t |S 0 = s ,<label>(1)</label></formula><p>and the action-value function is</p><formula xml:id="formula_1">Q π (s, a) = E π,P ∞ t=0 γ t R t |S 0 = s, A 0 = a ,<label>(2)</label></formula><p>where E denotes the expectation. The relationship between the value of a state and the values of its successor states is expressed by the Bellman equation for</p><formula xml:id="formula_2">V π V π (s) = E π,P [R + γV π (s )] .<label>(3)</label></formula><p>Similarly, the Bellman equation for</p><formula xml:id="formula_3">Q π is Q π (s, a) = E π,P [R + γQ π (s , a )] ,<label>(4)</label></formula><p>where s and a can be derived from the transition probability P (•|s, a) and a policy π(•|s ), respectively. The goal of RL is to find the optimal policy which yields the maximum Q(s, a) for all s and a. Let π * = arg max π Q π (s, a) be the optimal policy and let Q * (s, a) be the corresponding action-value function. Q * (s, a) satisfies the following Bellman optimality equation</p><formula xml:id="formula_4">Q * (s, a) = E π * ,P R + γ max a ∈A Q * (s , a ) .<label>(5)</label></formula><p>Eq. ( <ref type="formula" target="#formula_4">5</ref>) illustrates the temporal consistency of the action-value function, which allows for the design of learning algorithms. Define the Bellman optimality operator T * as</p><formula xml:id="formula_5">T * Q (s, a) = E π,P R + γ max a ∈A Q (s , a ) .<label>(6)</label></formula><p>When γ ∈ (0, 1), starting from any Q t (s, a), iteratively applying the operator</p><formula xml:id="formula_6">Q t+1 (s, a) ← T * Q t (s, a) leads to convergence Q t (s, a) → Q * (s, a) as t → ∞ [37].</formula><p>In high dimensional cases, it is critical to use function approximation as a compact representation of action values. Let Q θ (s, a) denote a function with parameter θ that approximates a table of action values with entry (s, a). The optimization aim is to find θ such that Q θ (s, a) ≈ Q * (s, a), and the optimal solution can be found by iteratively leveraging the Bellman optimality operator T * . In other words, the optimal parameter θ can be approached by minimizing the squared TD error</p><formula xml:id="formula_7">ζ 2 = r + γ max a ∈A Q θ (s , a ) -Q θ (s, a) 2 (7)</formula><p>over samples (s, a, r, s ), which are randomly selected from a replay buffer <ref type="bibr" target="#b37">[38]</ref> that stores transitions which record the interaction between an agent and the environment when following the policy driven by Q θ . In cases where Q θ (s, a) is linear, the iterative process to find Q * (s, a) can be shown to converge <ref type="bibr" target="#b38">[39]</ref>. However, in cases where Q θ (s, a) is nonlinear (e.g., a neural network), Q θ (s, a) becomes more expressive at the cost of no convergence guarantee. A number of DRL algorithms are designed following the above formulation, such as DQN <ref type="bibr" target="#b22">[23]</ref> and Dueling DQN <ref type="bibr" target="#b34">[35]</ref>.</p><p>1) Distributional Reinforcement Learning: The main idea of distributional RL <ref type="bibr" target="#b23">[24]</ref> is to work directly with the distribution of returns rather than their expectation (i.e., Q π ), so as to increase robustness to hyperparameter variation and environment noise <ref type="bibr" target="#b39">[40]</ref>. Let the random variable Z π q (s, a) be the return obtained by following a policy π to perform action a from the state s. Notably, the value of Z π q (s, a) varies due to the unexpected randomness in the environment. Then we have</p><formula xml:id="formula_8">Q π (s, a) = E Z π q (s, a) ,<label>(8)</label></formula><p>and an analogous distributional Bellman equation, that is,</p><formula xml:id="formula_9">Z π q (s, a) D = R + γZ π q (s , a ) ,<label>(9)</label></formula><p>where A D = B denotes that random variable A has the same probability law as B. Therefore, a distributional Bellman optimality operator T * can be defined by</p><formula xml:id="formula_10">T * Z q (s, a) D = R + γZ q s , arg max a ∈A E[Z q (s , a )] . (10)</formula><p>In traditional RL algorithms, we seek the optimal Q function approximator by minimizing a scalar value ζ<ref type="foot" target="#foot_0">2</ref> in Eq. <ref type="bibr" target="#b6">(7)</ref>. In distributional RL, our objective is to minimize a statistical distance:</p><formula xml:id="formula_11">sup s,a dist (T * Z q (s, a), Z q (s, a)) ,<label>(11)</label></formula><p>where dist(A, B) denotes the distance between random variable A and B, which can be measured by many metrics, such as KL divergence <ref type="bibr" target="#b23">[24]</ref>, p-Wasserstein <ref type="bibr" target="#b25">[26]</ref>, etc. In <ref type="bibr" target="#b23">[24]</ref>, Bellemare et al. proved that the distributional Bellman equation is a contraction in p-Wasserstein distance, but the distributional Bellman optimality operator is not necessarily a contraction, which provides a guideline for metric selection.</p><p>C51 algorithm <ref type="bibr" target="#b23">[24]</ref> approximates the distribution over returns using a fixed set of equidistant points and optimizes Eq. ( <ref type="formula" target="#formula_11">11</ref>) by minimizing KL divergence. Different from KL divergence based on the probability density function, p-Wasserstein metric builds on the cumulative distribution function. Assume that there are two real-valued random variables U and V with respective cumulative distribution functions F U and F V , the p-Wasserstein between them is given by 2</p><formula xml:id="formula_12">W p (U, V ) = 1 0 F -1 U (ω) -F -1 V (ω) p dω 1/p . (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>Theoretically, the distributional Bellman optimality operator is a strict contraction in p-Wasserstein distance; that is, minimizing Eq. ( <ref type="formula" target="#formula_11">11</ref>) with p-Wasserstein distance can give the optimal action-value distribution. QR-DQN <ref type="bibr" target="#b25">[26]</ref> used the values on some uniformly distributed quantiles to describe the action-value distribution, leveraging the loss of quantile regression to train the neural network, which is an effective approach to minimizing 1-Wasserstein distance. Therefore, QR-DQN obtains a better balance between theory and practice by working on a special case (i.e., 1-Wasserstein distance, the special case for p-Wasserstein distance with p = 1).</p><p>2) Generative Adversarial Network: GAN <ref type="bibr" target="#b27">[28]</ref> is intended to learn the distribution of data from all domains, mostly image, music, text, etc., to generate convincing data. GAN consists of two neural networks, a generator network G and a discriminator network D, which are engaged in a zero-sum game against each other. The network G takes an input from a random distribution and maps it to the space of real data. The network D obtains input data from both real data and the output of G, and attempts to distinguish the real data from the generated data. The two networks are trained by gradient descent algorithms in alternating steps.</p><p>The classical GAN minimizes Jensen-Shannon (JS) divergence between the real data and generated data distributions. However, <ref type="bibr" target="#b40">[41]</ref> shows that JS metric is not continuous and does not provide a usable gradient all the time. To overcome this shortcoming, <ref type="bibr" target="#b40">[41]</ref> proposed WGAN, in which the JS metric is replaced by 1-Wasserstein distance that provides sufficient gradients almost everywhere. Given that the equation for 1-Wasserstein distance is highly intractable, WGAN uses Kantorovich-Rubinstein duality to simplify the calculation, but introducing an essential constraint that ensures the discriminator is an appropriate 1-Lipschitz function. WGAN satisfies the constraint by clipping the weights of the discriminator to be within a certain range that is governed by a hyperparameter. Furthermore, <ref type="bibr" target="#b41">[42]</ref> proposed WGAN-GP and adopted gradient penalty to enforce the 1-Lipschitz constraint instead of simply clipping weights. Its optimization objective is formulated as follow:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Slices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Allocation</head><note type="other">Time Frequency MUs SDN-Orchestrator Gateway Router</note><formula xml:id="formula_14">min G max D∈D E x∼pdata [D(x)] -E z∼pz(z) [D(G(z))] + p(λ),<label>(13)</label></formula><p>where D denotes the set of 1-Lipschitz functions, x denotes the samples from real data, z denotes the samples from a random distribution, and <ref type="bibr" target="#b0">1)</ref>. Gradient penalty increases the computational complexity but it does make WGAN-GP perform much better than previous GANs.</p><formula xml:id="formula_15">p(λ) = λ 2 ( ∇ xD(x) 2 -1) 2 , x = εx + (1 -ε)G(z), ε ∼ U (0,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Model</head><p>Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the SDN-based system model for dynamic allocation of wireless bandwidth in the RAN scenario with uplink and downlink transmissions. We consider the downlink case in this paper. In this respect, <ref type="bibr" target="#b42">[43]</ref> built an SDN-based C-RAN testbed that realizes the dynamic allocation of radio resources (e.g., wireless bandwidth) by using frequency division duplex scheme, and the demonstration was presented in <ref type="bibr" target="#b43">[44]</ref>. Under the framework of hierarchical network slicing, we consider a RAN scenario with a single BS, where a set N of NSs share the aggregated bandwidth W<ref type="foot" target="#foot_1">3</ref> . The bandwidth is allocated to each NS according to the number of demands for the corresponding type of service. For an NS, say NS n, it provides a single service for a set of users U n . We consider a timeslot model where the slicing decision is updated according to the demand of users periodically (e.g., 1 second). In one timeslot, the number of demands NS n receives is denoted as d n , which partially determines the wireless bandwidth that the BS allocates to this NS, denoted as w n .</p><p>The objective of our work is to find an optimal bandwidthallocation solution that maximizes the system utility, denoted by J, which can be described by the weighted sum of SE and SSR. We now study the two sub-objectives, respectively. Let r un be the downlink rate of user u n served by NS n, which is, for simplicity, defined by Shannon theory as follows</p><formula xml:id="formula_16">r un = w n log(1 + SNR un ), ∀u n ∈ U n ,<label>(14)</label></formula><p>where SNR un is the signal-to-noise-ratio between user u n and the BS. SNR un can be given as</p><formula xml:id="formula_17">SNR un = g un P un N 0 w n ,<label>(15)</label></formula><p>where g un is the average channel gain that captures path loss and shadowing from the BS to the user u n , P un is the transmission power, and N 0 is the single-side noise spectral density. Given the transmission rate, SE can be defined as follows</p><formula xml:id="formula_18">SE = n∈N un∈Un r un W . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>On the other hand, SSR of NS n is obtained by dividing the number of successfully transmitted packets by the total number of arrived packets on NS n. Before formulating this problem, we define Q un as the set of packets sent from the BS to user u n , determined by the actual traffic demand patterns, and define a binary variable x qu n ∈ 0, 1, where x qu n = 1 indicates that the packet q un ∈ Q un is successfully received by user u n , i.e., the downlink data rate r un and the latency l qu n are simultaneously satisfied. Therefore, x qu n = 1 if and only if r un ≥ r n and l qu n ≤ l n , where l qu n denotes the latency that takes account of both queuing delay and transmission delay. r n and l n are the predetermined rate and latency values according to the SLA for service type n. We can formulate the SSR for NS n as:</p><formula xml:id="formula_20">SSR n = un∈Un qu n ∈Qu n x qu n un∈Un |Q un | ,<label>(17)</label></formula><p>where |Q un | denotes the number of packets sent from the BS to user u n .</p><p>The bandwidth allocation problem in the RAN network slicing is formulated as follows</p><formula xml:id="formula_21">max wn αSE + n∈N β n • SSR n = max wn α n∈N un∈Un r un W + n∈N β n • un∈Un qu n ∈Qu n x qu n un∈Un |Q un | ,<label>(18)</label></formula><formula xml:id="formula_22">s.t. n∈N w n = W,<label>(19)</label></formula><p>un∈Un</p><formula xml:id="formula_23">|Q un | = d n ,<label>(20)</label></formula><formula xml:id="formula_24">x pu n = 1, r un ≥ r n &amp; l pu n ≤ l n , 0, otherwise.<label>(21)</label></formula><p>where α and</p><formula xml:id="formula_25">β = [β 1 , β 2 , • • • β n ]</formula><p>are the coefficients that adjust the importance of SE and SSR, and β n refers to the importance weight of SSR n . In this problem, the objective is to maximize two components: (a) the spectral efficiency (i.e., SE), and (b) the proportion of the packets satisfying the constraint of data rate and latency (i.e., SSR). Notably, in the current timeslot, d n depends on both the number of demands and the bandwidth-allocation solution in the previous timeslot, since the maximum transmission capacity of RAN belonging to one service is tangled with the provisioning capabilities for this service. For example, the TCP sending window size is influenced by the estimated channel throughput. Therefore, the traffic demand varies without knowing a prior transition probability, making Eq. ( <ref type="formula" target="#formula_21">18</ref>) difficult to yield a direct solution. However, RL promises to be applicable to tackle this kind of problem. Therefore, we refer to RL to find the optimal policy for network slicing. In particular, consistent with <ref type="bibr" target="#b12">[13]</ref>, we map the RAN scenario to the context of RL by taking the number of arrived packets in each slice within a specific time window as the state, and the bandwidth allocated to each slice as the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GAN-POWERED DEEP DISTRIBUTIONAL Q NETWORK</head><p>In this section, we describe the proposed GAN-DDQN algorithm, shown in Fig. <ref type="figure" target="#fig_2">2</ref>, that address the demand-aware resource allocation problem in network slicing. We then discuss the methods of improving the performance of the algorithm and analyze its convergence. The agent observes S t = s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>The agent samples τ ∼ U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>The agent calculates Q(s, a) = 1 N G (a) (s, τ ) , ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>The agent performs a * ← arg max a Q(s, a), ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>The agent receives the system utility J and observes S t+1 = s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>The agent performs the reward-clipping with respect to J and gets the reward r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>The agent stores transition (s, a * , r, s ) in B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>If B is full, the agent updates the weights of network G and network D every K iterations. The agent samples a minibatch {s, a, r, s } m i=1 from B without replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>The agent samples a minibatch {τ } m i=1 ∼ U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>The agent gets the target action-value particles</p><formula xml:id="formula_26">y i = r i + γ Ĝ(a * i ) (s i , τ i ),</formula><p>where the optimal action is a * i = arg max a 1 N Ĝ(a) (s i , τ i ) , ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>The agent samples a minibatch {ε} m i=1 ∼ U (0, 1), and sets xi = ε i y i + (1 -ε i )G (ai) (s i , τ i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>The agent updates the weights θ D by leveraging gradient descent algorithm to 1 m m i=1 L i , where</p><formula xml:id="formula_27">L i = D(G (ai) (s i , τ i )) -D(y i ) + λ ( ∇ xi D(x i ) 2 -1)</formula><p>2 .</p><p>19:</p><p>The agent updates the weights θ G by leveraging gradient descent algorithm to</p><formula xml:id="formula_28">-1 m m i=1 D(G (ai) (s i , τ i )) 20:</formula><p>until All the transitions in B are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>The agent clones network G to the target network Ĝ every C iterations by resetting θ Ĝ = θ G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>The iteration index is updated by t ← t + 1. 23: until A predefined stopping condition (e.g., the 1 m m i=1 L i , the preset number of iterations, etc.) is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GAN-DDQN Algorithm</head><p>Our previous work <ref type="bibr" target="#b12">[13]</ref> has discussed how to apply RL to the resource slicing problem. However, the DQN algorithm used in that work is based on the expectation of the actionvalue distribution, and thus does not take into account the adverse effects of random noise on the received values of SE and SSR. To overcome this problem, we resort to the combination of the distributional RL and GAN. In this regard, we introduce WGAN-GP to learn the optimal action-value distribution. Specifically, the generator network G outputs a fixed number of samples (we refer to them as particles for clarity) that characterize the estimated action-value distribution learned by network G. Similar to <ref type="bibr" target="#b22">[23]</ref>, we leverage a target generator network Ĝ to obtain the target action-value particles. The discriminator network D realizes the 1-Wasserstein criterion when it attempts to minimize the distance between the estimated action-value particles and the target actionvalue particles calculated by the Bellman optimality operator. GAN-DDQN is able to approximate the optimal action-value distribution by alternately updating networks G and D.</p><p>Before we introduce the details of GAN-DDQN algorithm, it is necessary to describe the structure of the networks G and D. Network G consists of three components, which are responsible for state embedding, sample embedding, and particles generation. The state-embedding and sample-embedding components are both built with two neural layers and process the input state and the quantile samples in parallel. Then, the output of these two components are combined through Hadamard product operation. This step is consistent with <ref type="bibr" target="#b26">[27]</ref> to force interaction between the state embedding and sample embedding. Afterwards, the particles generation component, which contains multiple neural layers, takes the fused information as input, outputting several sets of particles where each set is treated as a representation of the corresponding action-value distribution. On the other hand, network D is a multilayer perceptron (MLP) with one neuron in the output layer. Fig. <ref type="figure">3</ref>(a) further details the structure of GAN-DDQN.</p><p>The GAN-DDQN algorithm can be explained as follows, without loss of generality. At iteration t, the agent feeds the current state S t = s and the samples τ from a uniform distribution (e.g., U (0, 1)) to network G; τ is the quantile values of the action-value distribution <ref type="bibr" target="#b26">[27]</ref>. Network G outputs a set of estimated action-value particles, denoted as G(s, τ ), where the particles belonging to action a are denoted as</p><formula xml:id="formula_29">G (a) (s, τ ); the number of G (a) (s, τ ) is N . Then, the agent calculates Q(s, a) = 1 N G (a)</formula><p>(s, τ ) , ∀a ∈ A, and selects a * = arg max a Q(s, a), ∀a ∈ A to perform. As a result, the agent receives a reward r, and the environment moves to the next state S t+1 = s . The tuple (s, a * , r, s ) is stored into the replay buffer B. When B is full, the agent updates networks G and D using all the transition tuples in B every K iterations.</p><p>In the training and updating process, the agent first randomly selects m transitions from B as a minibatch for training GAN-DDQN. Then, the agent executes the Bellman optimality operator on each transition of the selected minibatch and obtains the target action-value particles. For example, the target action-value particles for the transition i is </p><formula xml:id="formula_30">y i = r i + γ Ĝ(a * i ) (s i , τ i )</formula><formula xml:id="formula_31">L D = E τ ∼U (0,1) (s,a)∼B [D(G (a) (s, τ ))]- E (s,a,r,s )∼B [D(y)] + p(λ),<label>(22)</label></formula><formula xml:id="formula_32">L G = -E τ ∼U (0,1) (s,a)∼B [D(G (a) (s, τ ))]<label>(23)</label></formula><p>where p(λ) is as mentioned in Eq. ( <ref type="formula" target="#formula_14">13</ref>). The training goal for network D is to increase its accuracy in distinguishing the target action-value particles from the action-value particles produced by network G. The goal of training network G, on the other hand, is to improve its ability to generate the actionvalue particles that "fool" network D as much as possible.</p><p>Note that in order to further stabilize the training process, we update the target network Ĝ every C iterations.</p><p>Step by step, we incorporate the aforementioned methods and establish the GAN-DDQN as in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Analysis</head><p>It has been proven in <ref type="bibr" target="#b23">[24]</ref> that the distributional RL can converge when the metric for diverging distributions is p-Wasserstein distance. On the other hand, the fundamental guidance for distinguishing the target and estimated distributions in WGAN-GP is 1-Wasserstein distance. Therefore, the convergence of GAN-DDQN can be analyzed from the perspective of WGAN-GP's convergence on the data sampled from the dynamic RL interaction process. as explored in <ref type="bibr" target="#b45">[46]</ref>, in many currently popular GAN architectures, converging to the target distribution is not guaranteed and oscillatory behavior can be observed. This is a twofold challenge for GAN-DDQN, as we must ensure both the stationarity of the target distribution and the convergence of the WGAN-GP to this target distribution.</p><p>In an idealized WGAN-GP, the generator should be able to learn from the target distribution, and the discriminator should be able to learn any 1-Lipschitz function to produce the exact Wasserstein distance. However, the target distribution will not be stationary as the target network Ĝ regularly updates its weights; thus an idealized WGAN-GP might not be successful in practice. Fortunately, a slight change in the target distribution has little effect on the convergence of WGAN-GP. For example, suppose the real distribution that is the ultimate learning goal of WGAN-GP is a Gaussian distribution with a mean of 100 and a standard deviation of 1, and suppose that the target distribution that the WGAN-GP is expected to approximate at each stage is a Gaussian distribution with a standard deviation of 1 and a mean that starts at 0, increasing periodically by ∆µ. WGAN-GP will need more updates to learn the target distribution if ∆µ is large; the number of updates is difficult to determine. However, if ∆µ is small, a few times of updates is sufficient for WGAN-GP to learn the changed target distribution. Hence, the small ∆µ is more potential to enable WGAN-GP to learn the real distribution smoothly.</p><p>To analyze the convergence characteristic of WGAN-GP while avoiding directly dealing with sophisticated data and WGAN-GP model, <ref type="bibr" target="#b45">[46]</ref> introduces a simple but illustrative model, namely Dirac-WGAN-GP. Specifically, Dirac-WGAN-GP consists of a linear discriminator D ψ (x) = ψ • x and a generator with parameter θ that indicates the position of the Dirac distribution (i.e., δ θ ) output by the generator. Whilst the real data distribution p d is given by a Dirac-distribution concentrated at ξ (i.e., δ ξ ). It is worthy to further investigate the training characteristic of Dirac-WGAN-GP when the real data distribution (i.e., δ ξ ) is varying during the training process, like the typical situation in RL. Consistent with <ref type="bibr" target="#b45">[46]</ref>, we carry out analysis based on Dirac-WGAN-GP, and we have the following Theorem 1 Theorem 1: When trained with gradient descent with a fixed number of the generator and the discriminator updates and a fixed learning rate h &gt; 0, if the value of ξ varies dramatically, Dirac-WGAN-GP needs more learning steps to converge from the old optimal boundary to the new one after the variation of ξ.</p><p>We leave the proof of Theorem 1 in Appendix. As for WGAN-GP, we further have the following Corollary 1 Corollary 1: WGAN-GP could converge to the optimal boundary more rapidly if the real data change by a small amount.</p><p>Corollary 1 reveals that estimating the optimal action-value distribution requires a large amount of training if we directly use the system utility as the reward in RL. Therefore, we put forward a new reward-clipping mechanism to prevent the target action-value distribution from greatly changing. Specifically, assuming that there are T thresholds that partition the system utility, we set T + 1 constants whose values are much smaller than the system utility. Then the system utility can be clipped to these T + 1 constants that are taken as the rewards in RL. For example, if T = 2 and the clipping constants are -η, 0, and η (η &gt; 0), then the clipping strategy can be formulated by Eq. <ref type="bibr" target="#b23">(24)</ref>, where c 1 and c 2 (c 1 &gt; c 2 ) are the manually set thresholds:</p><formula xml:id="formula_33">r =      η, J(w, d) ≥ c 1 , 0, c 2 &lt; J(w, d) &lt; c 1 , -η, J(w, d) ≤ c 2 . (<label>24</label></formula><formula xml:id="formula_34">)</formula><p>However, as T becomes larger, the number of the manually set parameters in the reward-clipping mechanism increases, which makes the parameter setting process more sophisticated. Therefore, we adopt the reward-clipping mechanism defined in Eq. ( <ref type="formula" target="#formula_33">24</ref>) as an experiment. Note that introducing the rewardclipping mechanism to GAN-DDQN algorithm is effortless, and we only need to apply the reward-clipping mechanism to the system utility before storing the transition tuple in the replay buffer, which is described in line 9 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dueling GAN-DDQN</head><p>The training of GAN-DDQN is not a trivial task since it uses the data yielded from a dynamic environment, and only   a tiny portion of the output of the generator is useful for gradient calculation. One intuitive indicating to alleviate the training problem is to carefully adjust the values of GAN-DDQN's hyper-parameters, such as the discount factor γ, the gradient penalty coefficient λ, etc. Nevertheless, we plan to make systemic and architectural changes to the generator and the loss function. Particularly, inspired by <ref type="bibr" target="#b35">[36]</ref>, which uses a specialized dueling Q network to separate the action value into a state-value stream and an advantage stream, we divide the approximation of the action-value distribution into the approximation of the state-value distribution and the approximation of the advantage function for each action. This dueling architecture ignores the trivial variations of the environment and focuses on some crucial states to enhance the stability of DRL algorithms <ref type="bibr" target="#b34">[35]</ref>. In addition, in our improved model, namely Dueling GAN-DDQN, the loss function of the discriminator turns to work on the estimated and target state-value distributions. Moreover, the squared TD error is added to the generator's loss as the criterion that measures the distance of the estimated and target action values.</p><p>The detailed structure of Dueling GAN-DDQN is presented in Fig. <ref type="figure">3(b)</ref>, and we remarkably highlight the key differences from GAN-DDQN. It can be observed that the significant difference compared with GAN-DDQN is the generator or the dueling generator for the sake of distinguishing. In the dueling generator, after Hadamard product operation, we continue to handle the output using multiple neural layers (the common layers). Then, the refined information is separated into two paths, one flowing to a neural network to approximate the state-value distribution, and the other flowing to another neural network to estimate the action advantage function. Accordingly, the dueling generator outputs not only particles from the approximated state-value distribution but also the estimated action advantage values for each action. Note that the dis- The agent observes S t = s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>The agent samples τ ∼ U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>The agent feeds s and τ to network G, getting the state-value particles G v (s, τ ) and each action advantage value G (a) ad (s, τ ), ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>The agent calculates</p><formula xml:id="formula_35">V (s) = 1 N G v (s, τ ). 8:</formula><p>The agent calculates</p><formula xml:id="formula_36">Q(s, a) = V (s) + G (a)</formula><p>ad (s, τ ), ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>The agent performs a * ← arg max a Q(s t , a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>The agent receives the system utility J and observes a new state S t+1 = s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>The agent performs the reward-clipping with respect to J and gets the reward r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>The agent stores transition (s, a * , r, s ) in B. for n = 1 to n critic do 15:</p><p>The agent randomly samples {s, a, r, s } m i=1 from B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>The agent samples {τ } m i=1 and {ε} m i=1 from U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>The agent gets y i = G v (s i , τ i ), and ŷi = r i + γ Ĝv (s i , τ i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>The agent sets xi = ε i ŷi + (1 -ε i )y i .</p><p>19:</p><p>The agent updates the weights θ D by leveraging gradient descent algorithm to 1 m m i=1 L i , where</p><formula xml:id="formula_37">L i = D(y i ) -D(ŷ i ) + λ ( ∇ xi D(x i ) 2 -1)</formula><p>2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>end for</p><p># Train network G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>The agent randomly samples {s, a, r, s } m i=1 from B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>The agent samples {τ } m i=1 from U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>The agent calculates the estimated action value</p><formula xml:id="formula_39">Q i = 1 N G v (s i , τ ) + G (ai)</formula><p>ad (s i , τ ), and the tar-</p><formula xml:id="formula_40">get action value Qi = r i + γ 1 N G v (s i , τ ) + γ max a G (a)</formula><p>ad (s i , τ ), ∀a ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>The agent updates the weights θ G by leveraging gradient descent algorithm to</p><formula xml:id="formula_41">1 m m i=1 [-D(G v (s i )) + 1 2 ( Qi -Q i ) 2 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>The agent clones network G to the target network Ĝ every C iterations by resetting θ Ĝ = θ G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>The iteration index is updated by t ← t + 1. 28: until A predefined stopping condition (e.g., the 1 m m i=1 L i , the preset number of iterations, etc.) is satisfied.</p><p>criminator of Dueling GAN-DDQN has the same structure as GAN-DDQN.</p><p>Similarly to our analysis of the random variable Z π q , we analyze the random variable Z π v (s), which denotes the return obtained by following a policy π from state s. Then we have</p><formula xml:id="formula_42">V π (s) = E [Z π v (s)] ,<label>(25)</label></formula><p>and an analogous distributional Bellman equation for</p><formula xml:id="formula_43">Z v Z π v (s) D = E a∼A s ∼S [R + γZ π v (s )].<label>(26)</label></formula><p>It is difficult to find the distributional Bellman optimality operator for Z π v . Even worse, Eq. ( <ref type="formula" target="#formula_43">26</ref>) indicates that the iterative calculation of Z π v requires a reward from every stateaction pair, which is a noticeable time-consuming operation. Therefore, we introduce a degraded but simplified method to estimate Z π v , which is to minimize the difference between the estimated Z π v and T Z π v calculated by</p><formula xml:id="formula_44">T Z π v D = r + γZ π v (s ),<label>(27)</label></formula><p>where r and s are from the transition (s, a, r, s ) sampled from the replay buffer. This degraded approximation may fail to find the optimal state-value distribution, yet it can significantly reduces computation time. In addition, only considering the 1-Wasserstein loss for the state-value distribution results in the network G weights related to the action advantage function not being trained. Therefore, we leverage the TD error to measure the difference of the estimated and the target action values, where the action value is calculated by adding the corresponding action advantage value to the mean of the statevalue particles. As a consequence, the ultimate loss function for training Dueling GAN-DDQN is composed of the 1-Wasserstein distance and the squared TD error, which can be formulated as follows</p><formula xml:id="formula_45">L D = E τ ∼U (0,1) (s,a)∼B [D(G v (s, τ ))] - E τ ∼U (0,1) (r,s )∼B [D(r + γ Ĝv (s , τ ))] + p(λ),<label>(28)</label></formula><formula xml:id="formula_46">L G = -E τ ∼U (0,1) (s,a)∼B [D(G v (s, τ ))] + 1 2 ζ 2 ,<label>(29)</label></formula><p>where G v denotes the state-value particles output by the dueling generator, and ζ 2 is the squared TD error as defined in Eq <ref type="bibr" target="#b6">(7)</ref>. Algorithm 2 and Fig. <ref type="figure">3</ref>(b) provide the details of Dueling GAN-DDQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION RESULTS AND NUMERICAL ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Environment Settings</head><p>In this part, we verify the performance of GAN-DDQN and Dueling GAN-DDQN in a RAN scenario where there are three types of services (i.e., VoLTE, video, and URLLC) and three corresponding slices in one serving BS, as in <ref type="bibr" target="#b12">[13]</ref>. There exist 100 registered subscribers randomly located within a 40-meterradius circle surrounding the BS. These subscribers generate 0733-8716 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSAC.2019.2959185, IEEE Journal on Selected Areas in Communications  <ref type="bibr" target="#b46">[47]</ref> and TS 22.261 <ref type="bibr" target="#b47">[48]</ref>. The total bandwidth is 10 MHz, and the bandwidth allocation resolution is 1 MHz or 200 KHz. We will show the simulation results for both cases. On the other hand, the packet size of URLLC service has a strong influence on the system utility. For example, it is difficult to meet the latency requirement of URLLC service when the packet size is large, if there is insufficient bandwidth guaranteed for transmission. As a result, SSR degrades, and the system utility is reduced. Therefore, we simulate the network slicing scenario with suitably-sized URLLC packets.</p><p>With the mapping shown in Table <ref type="table" target="#tab_4">III</ref>, RL algorithms can be used to optimize the system utility (i.e., the weighted sum of SE and SSR). Specifically, we perform round-robin scheduling within each slice at 0.5 ms granularity; that is, we sequentially allocate the bandwidth of each slice to the active users within each slice every 0.5 ms. Besides, we adjust the bandwidth allocation to each slice per second. Therefore, the agent updates its neural networks every second. Considering update interval of the bandwidth-allocation process is much larger than the service arrival interval, the number of arrived packets (i.e., the state) is rarely zero when updating the agents. Therefore, it is reasonable to ignore the situation of zero bandwidth for any NS. Moreover, this filter setting narrows the range for the action exploration, as well as enhancing the stability of the training process. Meanwhile, this filter setting does affect our main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simulation Results</head><p>In this part, we show the simulation results of the proposed GAN-DDQN and Dueling GAN-DDQN algorithms, in comparison with the hard slicing method and the standard DQN-based scheme. Hard slicing means that each service is always allocated with 3 of the whole bandwidth (because there are three types of services in total); round-robin scheduling is conducted within each slice. The DQN-based bandwidth allocation scheme was first proposed in <ref type="bibr" target="#b12">[13]</ref>, which directly applied the original DQN algorithm <ref type="bibr" target="#b22">[23]</ref> to the network slicing scenario. Notably, our previous works in <ref type="bibr" target="#b12">[13]</ref> have demonstrated the classical DQN-driven method is superior to other machine learning methods (e.g., long short-term memory (LSTM)-based prediction-before-allocation method). Therefore, due to the space limitation, we put more emphasis on the performance comparison with the classical DQN in <ref type="bibr" target="#b12">[13]</ref>.</p><p>1) Small Packets for URLLC service: We first observe the performance of the proposed algorithms within the scenario that the packet size of URLLC service is small. The traffic parameters are shown in Table <ref type="table" target="#tab_4">II</ref>. We consider two cases: the bandwidth allocation resolution is either 1 MHz or 200 KHz. The importance weights in the optimization objective (i.e., Eq <ref type="bibr" target="#b17">(18)</ref>) are set to = 0.01, β = [1, 1, 1]. The values of the clipping parameters c 1 and c 2 are determined heuristically <ref type="foot" target="#foot_2">4</ref> . In both cases, we set c 1 = 6.5, c 2 = 4.5 to clip the system utility according to Eq. <ref type="bibr" target="#b23">(24)</ref>, where is fixed at 1. The experimental evaluation of the reward-clipping setting is investigated hereinafter. Fig. <ref type="figure" target="#fig_8">4</ref> depicts the variations of the system utility with respect to the iteration index. The left part of Fig. <ref type="figure" target="#fig_8">4</ref> shows that when the bandwidth allocation resolution is 1 MHz, the three RL-based algorithms perform similarly, but Dueling GAN-DDQN is slightly better; DQN is  the most erratic in training. The right part of Fig. <ref type="figure" target="#fig_8">4</ref> illustrates that when the bandwidth allocation resolution becomes 200 KHz, GAN-DDQN and Dueling GAN-DDQN expand the gap to DQN, which demonstrates the performance improvement from distributional RL by the characterization of the action-value or state-value distributions. It's worth noting that Dueling GAN-DDQN improves visibly over GAN-DDQN in both performance and stability, consistent with our previous discussion. Furthermore, it can be observed in Fig. <ref type="figure" target="#fig_8">4</ref> that the system utility obtained by the three RL-based algorithms is significantly greater than that for the hard slicing scheme. The reason for this lies in that the RL-based algorithms can dynamically and reasonably manage bandwidth resources, thereby avoiding wasted resources and improving resources utilization. Moreover, the utilization of the bandwidth resource further gets improved when we slice the bandwidth more finely. Fig. <ref type="figure" target="#fig_8">4</ref> shows that the system utility obtained by the two GAN-based algorithms, especially Dueling GAN-DDQN, becomes significantly larger when the bandwidth allocation resolution changes to 200 KHz from 1 MHz, but that the performance of DQN is slightly degraded. Fig. <ref type="figure" target="#fig_9">5</ref> presents the variations of SE and SSR with respect to the iteration index for both bandwidth allocation resolution settings (i.e., 1 MHz and 200 KHz). It can be observed from Fig. <ref type="figure" target="#fig_9">5</ref> that SE curves are basically consistent with the system utility curves. However, the SSR curves of the three algorithms for the NSs show different patterns. When the bandwidth allocation resolution is 1 MHz, the SSRs for both VoLTE and URLLC services reach 100% with iterative training. Nevertheless, for the SSR of video service, GAN-DDQN and Dueling GAN-DDQN basically converge to 100% after 5000 iterations, while DQN shows no obvious signs of convergence. When the bandwidth allocation resolution is 200 KHz, it can be observed that GAN-DDQN and Dueling GAN-DDQN, by and large, realize 100% of SSR for all three services by the end of training, but DQN shows extreme instability for VoLTE service. Note that the unusual sudden performance drop late in training is caused by the tiny nonzero exploration rate in the -greedy exploration strategy. In Fig. <ref type="figure">6</ref>, we illustrate the policy learned by the three algorithms when the bandwidth allocation resolution is 200 KHz. It can be observed that all three algorithms converge after 15000 iterations. However, there are some differences between the learned bandwidth allocation policies. The DQN agent allocates the least bandwidth to VoLTE service and keeps it unchanged; as a result, SSR of VoLTE service does not always reach 100%. Between the GAN-DDQN agent and the Dueling GAN-DDQN agent, the latter behaves more intelligently, which is prominently manifested in the fact that it maximizes the bandwidth allocated to video service and reduces the bandwidth allocated to the other two services while meeting the SLA. The Dueling GAN-DDQN agent provides as much bandwidth as possible to satisfy the SLA of video service which is requested frequently and bandwidthconsuming, thus improving the SE. Besides, Dueling GAN- DDQN agent provides a policy to better balance the demands of VoLTE and URLLC services that are relatively rarely requested.</p><p>We next investigate the impact of the reward-clipping mechanism. Fig. <ref type="figure">7</ref> shows the differences in system utility during the iterative learning of GAN-DDQN with and without the reward clipping when the bandwidth allocation resolution is 1 MHz. When there is no reward clipping, GAN-DDQN directly takes the system utility as the reward. Note that the values of system utility, although they fluctuate, are much larger than the clipping constants set manually in the rewardclipping mechanism. As a result, if the system utility is directly used as the reward, the target action-value distribution might vary significantly, making it difficult for GAN-DDQN to converge. Therefore, GAN-DDQN without reward clipping requires more training steps to converge from one equilibrium to a new one. It can be observed from Fig. <ref type="figure">7</ref> that the GAN-DDQN performs significantly better with reward clipping than without. The simulation results verify the effectiveness of GAN-DDQN together with reward clipping.</p><p>2) Large Packets for URLLC service: In this part, we consider the case where the packet size of URLLC service is evenly sampled from {0.3, 0.4, 0.5, 0.6, 0.7} MByte, which gives a considerably larger packet size than we just analyzed and requires more bandwidth to guarantee meeting the SLA of URLLC service. In the case where bandwidth allocation resolution is 1 MHz, we set c 1 = 5.7, c 2 = 3 to clip the system utility according to Eq. ( <ref type="formula" target="#formula_33">24</ref>), where η is fixed to 1. Fig. <ref type="figure" target="#fig_11">8</ref> shows the performance of each slice algorithm, from which it can be observed that Dueling GAN-DDQN is way ahead of the others in terms of system utility. However, quite unexpectedly, DQN performs poorly, worse even than hard slicing scheme. Fig. <ref type="figure">9</ref> reveals the details of SE and SSR, from which we can find that Dueling GAN-DDQN agent learned a policy that maximizes system utility by sacrificing the SSR of URLLC service in exchange for higher SE. The reason for this is that when SSR is equally important to all three services (i.e., β = [1, 1, 1]), it is challenging for URLLC service to satisfy its SLA given the large transmission volume and the strictly low latency requirement. Therefore, we further investigate the situation in which URLLC service is more concerned while keeping video service dominating VoLTE service, which is reflected in the value of β changing to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b0">1]</ref>. Fig. <ref type="figure" target="#fig_1">10</ref> presents the results for these different β settings and demonstrates that the adjusted importance weight makes the SLA of URLLC service well guaranteed; meanwhile, the SLA of the other two services can be 100% satisfied as well. However, the requests for URLLC service are scarce in a timeslot-in our simulation, it is the interval between two updates of the agent, defined as 1 second-while the agent has to allocate more bandwidth to URLLC slice to guarantee conformity to SLA until the next update, which wastes the bandwidth to some extent and thus leads to the decrease of SE. There are two lessons that we learn from these simulation results: (a) it is non-trivial to optimize multiple conflicting objectives, even when using cutting-edge RL algorithms; (b) shortening the interval between successive bandwidth allocations may improve performance but it also increases computational costs and raises issues of stability due to more drastic changes in demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have investigated the combination of deep distributional RL and GAN and proposed GAN-DDQN to learn the optimum solution for demand-aware resource management in network slicing. In particular, we have applied GAN to approximate the action-value distribution, so as to avoid the negative impact of randomness and noise on the reward and grasp much more details therein than the conventional DQN. We have also designed a new update procedure that combines the advantages offered by distributional RL with the training algorithm of WGAN-GP. Furthermore, we have adopted the reward-clipping scheme to enhance the training stability of GAN-DDQN. Besides, we have introduced the dueling structure to the generator (i.e., Dueling GAN-DDQN), so as to separate the state-value distribution and the action advantage function from the action-value distribution and thus avoid the inherent training problem of GAN-DDQN. Extensive simulations have demonstrated the effectiveness of GAN-DDQN and Dueling GAN-DDQN with superior performance over the classical DQN algorithm. In the future, we will try to further improve the GAN-DDQN mechanism under various scenarios with multiple-metric constraints as well as nonstationary traffic demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX THE PROOF OF THEOREM 1</head><p>Before the proof of Theorem 1, we give the following lemmas:</p><p>Lemma 1: Because v(θ, ψ) = 0 if and only if (θ, ψ) = (ξ, 0), the unique Nash-equilibrium point of the training objective in Eq. ( <ref type="formula">30</ref>) is given by θ = ξ, ψ = 0.</p><p>Lemma 2: The distance between the optimal boundaries of Dirac-WGAN-GP on D 1 and D 2 is δ.</p><p>Proof : Dirac-WGAN-GP consists of a generator with parameter θ and a linear discriminator D ψ (x) = ψ • x, where the generator outputs a Dirac distribution centralized θ (i.e., δ θ ). Whilst the real data distribution is given by a Dirac distribution concentrated at ξ (i.e., δ ξ ). Therefore, the training objective of Dirac-WGAN-GP is given by L(θ, ψ) = ψθ -ξψ <ref type="bibr" target="#b29">(30)</ref> and the gradient penalty proposed in <ref type="bibr" target="#b41">[42]</ref> is given by</p><formula xml:id="formula_47">p(ψ) = λ 2 E x ( ∇ xD ψ (x) -1) 2 = λ 2 (|ψ| -1) 2<label>(31)</label></formula><p>Inspired by <ref type="bibr" target="#b45">[46]</ref>, we use gradient vector field to analyze convergence, which is defined as follow v(θ, ψ) := -∇ θ L(θ, ψ)</p><formula xml:id="formula_48">∇ ψ L(θ, ψ)<label>(32)</label></formula><p>For Dirac-WGAN-GP, the corresponding gradient vector field is given by v(θ, ψ) = -ψ θ -ξ + sign(ψ)λ (|ψ| -1)</p><p>where sign(•) denotes the signum function and we have Lemma 1.</p><p>Assume that the iteration (θ k , ψ k ) converges towards the equilibrium point (ξ, 0) but (θ k , ψ k ) = (ξ, 0) for all k ∈ N, which implies that v(θ k , ψ k ) ≈ 0 and thus we have</p><formula xml:id="formula_50">-ψ k ≈ θ k -ξ + sign(ψ k )λ (|ψ k | -1)<label>(34)</label></formula><p>in other words,</p><formula xml:id="formula_51">θ k ≈ -ψ k + ξ -sign(ψ k )λ(|ψ k | -1)<label>(35)</label></formula><p>Then, we can get the update amount of parameter θ after the (k + 1)th training as follow</p><formula xml:id="formula_52">|θ k+1 -θ k | ≈ h |-ψ k + ξ -sign (ψ k ) λ(|ψ k | -1) -θ k | ≈ h |-(λ + 1)ψ k + (ξ -θ k ) + sign (ψ k ) λ|<label>(36)</label></formula><p>Therefore, we have lim k→∞ |θ k+1 -θ k | = hλ, which shows that Dirac-WGAN-GP cannot converge to the equilibrium point, and the value of generator's parameter will finally oscillate between ξ -hλ 2 and ξ + hλ 2 . Assume that D 1 and D 2 are two different real data, which are the Dirac distributions concentrated at ξ 1 and ξ 2 , respectively. Let δ = |ξ 1 -ξ 2 |, which indicates the statistic distance between D 1 and D 2 . Note that usually hλ is two or three orders of magnitude smaller than δ, which implies that the optimal boundaries is rarely overlapped, thus further training is required when the real data varies. With the constant learning rate, it is easy to deduce from Lemma 2 that the larger the δ is, the more training steps are required for the Dirac-WGAN-GP to reach the new optimal boundary. Finally, based on Lemma 1 and 2, we obtain the proof of Theorem 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received June 20, 2019; revised October 15, 2019; accepted November 6, 2019. This work was supported in part by National Key R&amp;D Program of China (No. 2017YFB1301003), National Natural Science Foundation of China (No. 61701439, 61731002), Zhejiang Key Research and Development Plan (No. 2019C01002, 2019C03131), the Project sponsored by Zhejiang Lab (2019LC0AB01), Zhejiang Provincial Natural Science Foundation of China (No. LY20F010016), the Fundamental Research Funds for the Central Universities (No. 2019QNA5010). (Corresponding author: Rongpeng Li.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The considered scenario showing uplink and downlink transmissions on different NSs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of GAN-DDQN for resource management in network slicing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>An illustration of GAN-DDQN algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( b )Fig. 3 .</head><label>b3</label><figDesc>Fig. 3. The comparison of GAN-DDQN and Dueling GAN-DDQN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2</head><label>2</label><figDesc>Dueling GAN-DDQN 1: Initialize a dueling generator G and a discriminator D with random weights θ G and θ D respectively, the number of particles N , gradient penalty coefficient λ, batch size m, discount factor γ, n critic = 5. 2: Initialize a target dueling generator Ĝ with weight θ Ĝ ← θ G , a replay buffer B ← ∅, the iteration index t = 0. 3: repeat 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of performance comparison between different slicing schemes (hard slicing, DQN, GAN-DDQN, Dueling GAN-DDQN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. An illustration of SE and SSR in the different cases where the bandwidth allocation resolution is 1 MHz (shown in the top sub-figures) and 200 KHz (shown in the bottom sub-figures).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. An illustration of bandwidth allocation schemes in the case where the bandwidth allocation resolution is 200 KHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. An illustration of performance comparison between different slicing schemes in the case where the packets of URLLC service are large and the bandwidth allocation resolution is 1 MHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. An illustration of SE and SSR in the case where the bandwidth allocation resolution is 1 MHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>NOTATIONS USED IN THIS PAPER</cell></row><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>S</cell><cell>State space</cell></row><row><cell>A</cell><cell>Action space</cell></row><row><cell>P</cell><cell>Transition probability</cell></row><row><cell>V</cell><cell>State-value function</cell></row><row><cell>Q</cell><cell>Action-value function</cell></row><row><cell>Zv</cell><cell>Random variable to statistically model the state values</cell></row><row><cell>Zq</cell><cell>Random variable to statistically model the action values</cell></row><row><cell>T  *</cell><cell>Bellman optimality operator</cell></row><row><cell>s, s</cell><cell>States</cell></row><row><cell>a</cell><cell>An action</cell></row><row><cell>r</cell><cell>A reward</cell></row><row><cell>St</cell><cell>State at time t</cell></row><row><cell>At</cell><cell>Action at time t</cell></row><row><cell>Rt</cell><cell>Reward at time t</cell></row><row><cell>γ</cell><cell>Discount factor</cell></row><row><cell>π</cell><cell>Policy</cell></row><row><cell>J</cell><cell>System utility</cell></row><row><cell>α</cell><cell>Weight of the SE</cell></row><row><cell>β</cell><cell>Weight of the SSR</cell></row><row><cell>τ</cell><cell>Quantile samples</cell></row><row><cell>λ</cell><cell>Gradient penalty coefficient</cell></row><row><cell>n critic</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 GAN-DDQN 1: Initialize a generator G and a discriminator D with random weights θ G and θ D respectively, the number of particles N , gradient penalty coefficient λ, batch size m, discount factor γ. 2: Initialize a target generator Ĝ with weight θ Ĝ ← θ G , a replay buffer B ← ∅, the iteration index t = 0.</figDesc><table /><note><p>3: repeat 4:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSAC.2019.2959185, IEEE Journal on Selected Areas in Communications lowing loss functions to train networks D and G, respectively:</figDesc><table /><note><p>where a * i is the action with the maximum expectation of action-value particles, i.e., a * i = arg max a 1 N Ĝ(a) (s i , τ i ). Finally, the agent uses the fol-0733-8716 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II A</head><label>II</label><figDesc>BRIEF SUMMARY OF KEY SETTINGS FOR TRAFFIC GENERATION PER SLICE</figDesc><table><row><cell></cell><cell></cell><cell>VoLTE</cell><cell>Video</cell><cell>URLLC</cell></row><row><cell cols="2">Bandwidth</cell><cell>20 MHz</cell><cell></cell></row><row><cell cols="2">Scheduling</cell><cell cols="2">Round robin per slot (0.5 ms)</cell></row><row><cell cols="2">Slice Band Adjustment (Q-Value Update)</cell><cell cols="2">1 second (2000 scheduling slots)</cell></row><row><cell cols="2">Channel</cell><cell>Rayleigh fading</cell><cell></cell></row><row><cell cols="2">User No. (100 in all)</cell><cell>46</cell><cell>46</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Truncated</cell><cell>stationary</cell></row><row><cell cols="2">Distribution of Inter-</cell><cell>Uniform [Min = 0, Max =</cell><cell cols="2">distribution [Exponential</cell><cell>Exponential [Mean = 180</cell></row><row><cell cols="2">Arrival Time per User</cell><cell>160ms]</cell><cell cols="2">Para = 1.2, Mean = 6 ms,</cell><cell>ms]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Max = 12.5 ms]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Truncated Pareto [Expo-</cell><cell>Variable constant: {6.4,</cell></row><row><cell cols="2">Distribution of Packet Size</cell><cell>Constant (40 Byte)</cell><cell cols="2">nential Para = 1.2, Mean = 100 Byte, Max = 250</cell><cell>12.8, 19.2, 25.6, 32} KByte or {0.3, 0.4, 0.5,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Byte]</cell><cell>0.6, 0.7} MByte</cell></row><row><cell cols="2">SLA: Rate</cell><cell>51 Kbps</cell><cell>100 Mbps</cell><cell>10 Mbps</cell></row><row><cell cols="2">SLA: Latency</cell><cell>10 ms</cell><cell>10 ms</cell><cell>1 ms</cell></row><row><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="3">THE MAPPING FROM RESOURCE MANAGEMENT FOR NETWORK SLICING</cell><cell></cell></row><row><cell></cell><cell>TO RL ENVIRONMENT</cell><cell></cell><cell></cell></row><row><cell>RL Environment</cell><cell cols="2">Radio Resource Slicing</cell><cell></cell></row><row><cell>State</cell><cell cols="2">The number of arrived packets in each slice within a specific time window</cell><cell></cell></row><row><cell>Action</cell><cell cols="2">bandwidth allocation to each slice</cell><cell></cell></row><row><cell>Reward</cell><cell cols="2">Clipped weighted sum of SE and SSR in 3 sliced bands</cell><cell></cell></row><row><cell cols="3">standard service traffics as summarized in Table II based on</cell><cell></cell></row><row><cell>3GPP TR 36.814</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We further explain the advantage of the Wasserstein metric in the next part.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In fact, such a bandwidth allocation could be realized by physical multiplexing methods<ref type="bibr" target="#b44">[45]</ref>. Meanwhile, the shared resources could be temporal slots as well. However, for simplicity of representation, we take the bandwidth allocation problem as an example.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We first directly regard the system utility as the reward in order to find the range of the system utility, and then try different combinations of the two parameters (i.e., c 1 and c 2 ) to find the suitable values that guarantee both performance and stability.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GAN-based deep distributional reinforcement learning for resource management in network slicing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Globecom</title>
		<meeting>Globecom<address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network slices toward 5G communications: Slicing the LTE network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katsalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikaein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ksentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="146" to="154" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intelligent 5G: When cellular networks meet artificial intelligence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="175" to="183" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network slicing in 5G: Survey and challenges</title>
		<author>
			<persName><forename type="first">X</forename><surname>Foukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patounas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmokashfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Marina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Minimum requirement related to technical performance for IMT-2020 radio interface(s), document ITU-R M</title>
		<imprint>
			<date type="published" when="2017">2410-0, Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network slicing as a service: Enabling enterprises&apos; own software-defined cellular networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="146" to="153" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Network slicing for 5G: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhamare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network slicing and softwarization: A survey on principles, enabling technologies, and solutions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Afolabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Samdanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ksentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Flinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2429" to="2453" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Network slicing based 5G and future mobile networks: Mobility, resource management, and challenges</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghvami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C M</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="138" to="145" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Network slicing for 5G with SDN/NFV: Concepts, architectures, and challenges</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ordonez-Lucena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ameigeiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Ramos-Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Folgueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Impact of network slicing on 5G radio access networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mildh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaloxylos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buracchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trogolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuCNC</title>
		<meeting>EuCNC<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="153" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<idno>document 3GPP TR 22.981</idno>
		<title level="m">Study on new services and markets technology enables</title>
		<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for resource management in network slicing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="74429" to="74441" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The algorithmic aspects of network slicing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gkatzikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Stiakogiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Paschos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="112" to="119" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slice as an evolutionary service: Genetic optimization for inter-slice resource management in 5G networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lianghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Schotten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="33137" to="33147" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slicing the edge: Resource allocation for RAN network slicing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="970" to="973" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User access control and bandwidth allocation for slice-based 5G-and-beyond radio access networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Imran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICC</title>
		<meeting>ICC<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Network slicing management &amp; prioritization in 5G mobile systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Condoluci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahmoodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Wireless Conference</title>
		<meeting>European Wireless Conference<address><addrLine>Oulu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-complexity distributed radio access network slicing: Algorithms and experimental results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Restuccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Melodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2815" to="2828" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bandwidth slicing in software-defined 5G: A stackelberg game approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Veh. Technol. Mag</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems, vo</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge, England</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantile regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Hallock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econ. Perspect</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06923</idno>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intelligent power control for spectrum sharing in cognitive radios: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="25463" to="25473" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning based framework for power-efficient resource allocation in cloud RANs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gursoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICC</title>
		<meeting>ICC<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">hierarchical framework of cloud resource allocation and power management using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDCS</title>
		<meeting>ICDCS<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="372" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C M</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">GAN Q-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04874</idno>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributional multivariate policy evaluation and exploration with the bellman GAN</title>
		<author>
			<persName><forename type="first">D</forename><surname>Freirich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with discrete normalized advantage functions for resource management in network slicing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1337" to="1341" />
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Playing Atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature-based methods for large scale dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="94" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08617</idno>
		<title level="m">Distributed distributional deterministic policy gradients</title>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
	</analytic>
	<monogr>
		<title level="j">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A network slicing prototype for a flexible cloud radio access network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fajjari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aitsaadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Langar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCNC</title>
		<meeting>CCNC<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DEMO: SDN-based network slicing in C-RAN</title>
		<author>
			<persName><forename type="first">S</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fajjari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aitsaadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Langar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCNC</title>
		<meeting>CCNC<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="https://tools.ietf.org/id/draft-geng-netslices-architecture-02.html#rfc.references.2" />
		<title level="m">Network slicing architecture</title>
		<imprint>
			<date type="published" when="2019">Oct. 13. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the convergence properties of GAN training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Evolved Universal Terrestrial Radio Access (E-UTRA); Further advancements for E-UTRA physical layer aspects</title>
		<idno>document 3GPP TR 36.814</idno>
		<imprint>
			<date type="published" when="2010-03">Mar. 2010</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">sponsored by the National Postdoctoral Program for Innovative Talents. His research interests currently focus on Reinforcement Learning, Data Mining and all broad-sense network problems</title>
	</analytic>
	<monogr>
		<title level="m">Service requirements for next generation new services and markets</title>
		<editor>
			<persName><surname>Ltd</surname></persName>
		</editor>
		<editor>
			<persName><surname>Shanghai</surname></persName>
		</editor>
		<meeting><address><addrLine>Hangzhou, China; Hangzhou, China; Hangzhou China; Xi&apos;an, China; Hangzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Huawei Technologies Co</publisher>
			<date type="published" when="2010-06">Mar. 2017. June 2016. June 2015 and June 2010. August 2015 to September 2016</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
		<respStmt>
			<orgName>Science and Electronic Engineering, Zhejiang University ; College of Information Science and Electronic Engineering, Zhejiang University ; D and B.E. from Zhejiang University, Hangzhou, China and Xidian University ; Computer Science and Technologies, Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>His research interest includes deep learning, reinforcement learning, and network slicing. Rongpeng Li (S&apos;12-M&apos;17) is now an assistant pro. and he has authored/coauthored several papers in the related fields. He serves as an Editor of CHINA COMMUNICATIONS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
