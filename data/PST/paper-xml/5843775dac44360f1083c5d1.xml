<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Convolutional Neural Network for Bleeding Detection in Wireless Capsule Endoscopy Images*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">-H</forename><surname>Meng</surname></persName>
						</author>
						<title level="a" type="main">A Deep Convolutional Neural Network for Bleeding Detection in Wireless Capsule Endoscopy Images*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F74A76FA5B833481A3231239F83E75A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the F1 score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved F1 score is of up to 0.9955.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Over the past two decades, there has been a great revolution in the non-invasive approaches to medical diagnosis. The advent of wireless capsule endoscopy (WCE) allows physicians to detect and monitor lesions within the gastrointestinal (GI) tract, without putting their patient through a lengthy, uncomfortable procedure. As a standard way of small bowel evaluation, WCE has now benefited more than 1.5 million patients worldwide.</p><p>GI bleeding detection is essential for WCE examination in that bleeding is not only the most common abnormality of the GI tract, but also an important symptom or syndrome of other GI pathologies such as ulcers, polyps, tumors and Crohn's disease <ref type="bibr" target="#b0">[1]</ref>. GI bleeding can be further categorized as active (i.e., evidence of ongoing bleeding) versus inactive bleeding (see Fig. <ref type="figure" target="#fig_0">1</ref>) and may occur anywhere throughout the GI tract <ref type="bibr" target="#b1">[2]</ref>. However currently, GI bleeding detection is mostly done by physicians reviewing the entire WCE video, an extremely laborious and time consuming process. Therefore, the development of computer-aided diagnosis (CAD) systems for automatic detection of GI bleeding is highly desirable.</p><p>Recently, computerized approaches for GI bleeding detection in WCE videos have received much attention with the goal of relieving the workload of physicians. Literature review shows that existing methods based on shallow network architecture typically start with a manually predefined feature extraction step, followed by a separate training process of the classifier. In the first stage, features such as color [3]- <ref type="bibr" target="#b4">[5]</ref>, texture <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and statistical information <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are manually extracted from the original WCE images. The  generated feature vectors are used to learn a binary or discrete classifier, where both the support vector machine (SVM) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and k-nearest neighbors (KNN) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are commonly used.</p><p>Handcrafted features are useful in that they try to model certain kinds of features that physicians look for when identifying the GI bleeding regions. However, their design circle relies heavily on domain knowledge. Another drawback of manually designed features is that they may lose useful information and not be optimized, because they are not part of an end-to-end learning system (i.e., feature design is separate from training the classifier). These problems motivate us to find a better way for feature generation.</p><p>The affordability of parallel computing resources via GPU, as well as the availability of large amounts of annotated WCE datasets opens the door to the possibility of processing WCE image videos with a deep convolutional neural network (CNN). The CNN has recently drawn great attention to the topic of "deep learning" within the computer vision field and been proved to have remarkable advancement not only in classification tasks of natural images <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, but also in biomedical applications, such as cervical cell segmentation <ref type="bibr" target="#b13">[14]</ref> and mitosis detection <ref type="bibr" target="#b14">[15]</ref>. Contrary to the aforementioned shallow network architecture, CNN replaces the manually feature design with computerized feature learning and is able to find feature patterns that handcrafted features fail to describe. Furthermore, jointly learning features and classifiers makes their integration optimal.</p><p>Hence in this paper, we propose a new method for GI bleeding detection that can automatically and hierarchically learn high-level features via a deep neural network. To the best of our knowledge, it is the first study to report the application of a CNN model for WCE bleeding detection. In summary, key contributions of this work include:</p><p>• An expanded dataset that contains 10,000 labeled WCE images (with 2,850 bleeding samples, both active and inactive).</p><p>• Performing a high level detection for both active and inactive bleeding frames. The organization of the rest of this paper is as follows. In Section II, we describe the details of the new method. In Section III, we present the experiments and results. Finally, in Section IV, we present our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CNN Architecture</head><p>Inspired by the CNN model proposed by Krizhevsky et al. <ref type="bibr" target="#b15">[16]</ref>, we construct an eight-layer convolutional neural network that composes three convolutional layers (C1-C3), three pooling layers (MP1-MP3) and two fully-connected layers (FC1, FC2). A detailed configuration of our CNN architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In our implementation, rectified linear units (ReLUs) are used as the activation function in convolutional layers (C1-C3) and the first fully-connected layer (FC1). Max-pooling is applied in the pooling layers (MP1-MP3) to select the maximal activations over input patches. Finally, the output of the second fully-connected layer (FC2) consists of two neurons (bleeding and normal) and can be activated by a softmax regression function, which is defined as:</p><formula xml:id="formula_0">f θ (x (i) ) =      P (y = 1|x (i) ; θ) P (y = 2|x (i) ; θ)</formula><p>. . .</p><formula xml:id="formula_1">P (y = K|x (i) ; θ)      = 1 K j=1 exp(θ (j) x (i) )     </formula><p>exp(θ (1) x (i) ) exp(θ (2) x (i) ) . . .</p><formula xml:id="formula_2">exp(θ (K) x (i) )     <label>(1)</label></formula><p>where x (i) ∈ n are the input attributes with the corresponding labels y (i) . K is the number of classes. The model parameters θ (1) , θ (2) , . . . , θ (K) ∈ n are trained to minimize the loss function:</p><formula xml:id="formula_3">L(θ) = - m i=1 K k=1 1 y (i) = k log exp(θ (k) x (i) ) K j=1 exp(θ (j) x (i) )<label>(2)</label></formula><p>where m denotes the size of the training set. Particularly, in the binary classification setting, we have y (i) ∈ {0, 1} and</p><formula xml:id="formula_4">K = 2 [17].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN Training</head><p>The CNN training is performed with the Caffe framework <ref type="bibr" target="#b16">[18]</ref>. We train our model using stochastic gradient descent with a batch size of 100 examples, learning rate of 0.001, momentum of 0.9, and weight decay of 0.004. Training iterates 5,000 times before termination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SVM Classifier</head><p>In contrast to CNN models that employ a softmax function to minimize the cross-entropy loss for prediction, learning in the support vector machine (SVM) minimizes a marginbased loss and has been proved more beneficial than the softmax as the top classification layer <ref type="bibr" target="#b17">[19]</ref>. Therefore, we further modify the proposed method by replacing the second fully-connected layer (softmax regression model) with a SVM classifier and complete the bleeding frame detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Preprocessing</head><p>Our dataset consists of 10,000 WCE images, including 2,850 GI bleeding frames and 7,150 normal frames. Apart from the conventional bleeding samples with apparent active bleeding regions (Fig. <ref type="figure" target="#fig_0">1 (b)</ref>), this dataset also collects images that contain small regions of inactive bleeding (Fig. <ref type="figure" target="#fig_0">1 (c</ref>)), making the detection task more challenging. Images containing a bleeding region are labeled as positive (Fig. <ref type="figure" target="#fig_0">1  (b)-(c</ref>)), otherwise, they are labeled as negative (Fig. <ref type="figure" target="#fig_0">1 (a)</ref>). The size of each WCE image is 240 × 240 × 3.</p><p>Before training, WCE images that contain bleeding regions are artificially duplicated with rotations and mirroring to achieve scale and rotational invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Measures</head><p>We measure the performance quantitatively by employing three commonly used metrics <ref type="bibr" target="#b18">[20]</ref>:</p><formula xml:id="formula_5">Recall = T P T P + F N P recision = T P T P + F P F 1 score = 2 × precision × recall precision + recall<label>(3)</label></formula><p>where TP, TN, FP and FN denote the number of true-positive, true-negative, false-positive and false-negative detection results, respectively. The experiments are repeated 10 times with randomly sampled training and test sets. Mean values of each metric are calculated for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussions</head><p>We first construct the training sets with a fixed volume of 8,200 images, including 2,050 positive and 6,150 negative ones. The remaining 1,800 images are gathered for testing, where 800 of them are labeled as positive. All training is performed using a NVIDIA GeForce GTX TITAN X.</p><p>When the CNN model is well trained, Fig. <ref type="figure" target="#fig_2">3</ref> visualizes the 32 learned kernels (filters) of the first convolutional layer (C1). Each pattern represents a 3D kernel of size 5 × 5 × 3, referring to the architecture illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Some training samples and their feature maps after forward passing the first convolutional layer (C1) can be seen in Fig. <ref type="figure" target="#fig_3">4 (a)-(d)</ref>.</p><p>We compare our proposed method with the latest approaches by Fu et al. <ref type="bibr" target="#b2">[3]</ref> and Yuan et al. <ref type="bibr" target="#b19">[21]</ref>, where both of them used the conventional handcrafted features for bleeding   detection. We implement their methods on the same training and test sets as ours to perform direct comparison and report the performance in Table <ref type="table" target="#tab_1">I</ref>. The comparative results illustrate that our proposed method leads to an increase for all three measures, especially in terms of recall (from 0.9538 to 0.9920) and F 1 score (from 0.9757 to 0.9950).</p><p>To evaluate the impact of the size of the training set on detection performance, we further complete the experiment by varying the training size from 200 to 8200 images (fixed ratio sampling, positive : negative = 1 : 3) with a step size of 200 images and plot the comparative results of recall, precision and F 1 score in Fig. <ref type="figure" target="#fig_4">5 (a)-(c)</ref>. One can see that our method outperforms the state-of-the-art approaches in two points: (1) When only a relatively small training size (&lt; 1000) is available, the proposed method yields significantly stronger results compared to <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b19">[21]</ref>, indicating that our deep framework can encode richer information via high-level representations even with limited data; (2) As the number of training samples increases, the performance saturates quickly for both ours and the other two methods, thus a clear advantage of the proposed method can be observed, suggesting that the deep neural network is most effective when applied on a large training set. In all, the plotting result shows that our method surpasses the state-of-the-art approaches by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS</head><p>Computerized approaches for GI bleeding detection in WCE image videos have recently received much attention with the goal of relieving the workload of physicians. In this paper, we present a new method for automatic GI bleeding detection based on a deep convolutional neural network. To evaluate the performance of the proposed method, we construct a large WCE dataset that contains 10,000 annotated images for bleeding detection. Experimental results on clinical data validate our ideas and show that we improve the state-of-the-art for GI bleeding detection in WCE image videos. In future work, we plan to adapt the proposed method to other lesion detection tasks and further boost the performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of WCE images in our dataset. (a) A normal WCE image. (b) An active bleeding WCE image (c) An inactive bleeding WCE image.</figDesc><graphic coords="1,329.76,157.68,211.68,79.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of the proposed CNN architecture.</figDesc><graphic coords="3,54.00,50.08,504.00,145.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of the 32 learned kernels in the first convolutional layer (C1).</figDesc><graphic coords="3,54.00,236.04,252.01,137.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Some training samples and their #26, #27, #28 feature maps after forward passing the first convolutional layer (C1). (a) Raw data. (b) The #26 feature maps. (c) The #27 feature maps. (d) The #28 feature maps.</figDesc><graphic coords="3,313.20,236.04,252.00,180.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The comparison of bleeding detection performance with variable size of training sets. (a)-(c): The comparison of recall, precision and F 1 score amongst [3], [21] and our method.</figDesc><graphic coords="4,80.87,378.60,194.54,145.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF BLEEDING DETECTION METHODS</figDesc><table><row><cell>Method</cell><cell>Recall</cell><cell cols="2">Precision F 1 score</cell></row><row><cell>Fu et al. [3]</cell><cell>0.9475</cell><cell>0.9831</cell><cell>0.9650</cell></row><row><cell cols="2">Yuan et al. [21] 0.9538</cell><cell>0.9987</cell><cell>0.9757</cell></row><row><cell>Our method</cell><cell>0.9920</cell><cell>0.9990</cell><cell>0.9955</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>• A novel bleeding detection approach based on a deep convolutional neural network.978-1-4577-0220-4/16/$31.00 ©2016 IEEE</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Software for enhanced video capsule endoscopy: challenges for essential progress</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><surname>Koulaouzidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Gastroenterology &amp; Hepatology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The role of endoscopy in the management of obscure gi bleeding</title>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Lee</forename><surname>Krinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasundhara</forename><surname>Appalaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhas</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamir</forename><surname>Ben-Menachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><forename type="middle">D</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Friis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="479" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer-aided bleeding detection in wce video. Biomedical and Health Informatics</title>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max Q-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-level local feature classification for bleeding detection in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">That</forename><surname>Chee Khun Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Mon Htwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo Hwee</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kap</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><forename type="middle">Chun</forename><surname>Luk Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cybernetics and Intelligent Systems (CIS), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="76" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A feature extraction scheme from region of interest of wireless capsule endoscopy images for automatic bleeding detection</title>
		<author>
			<persName><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Bashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sa Fattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><surname>Wahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing and Information Technology (ISSPIT), 2014 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="256" to="000260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transform based bleeding detection technique for endoscopic images</title>
		<author>
			<persName><forename type="first">Mekha</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName><surname>Gopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronics and Communication Systems (ICECS), 2015 2nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1730" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic bleeding detection in wireless capsule endoscopy based on rgb pixel intensity ratio</title>
		<author>
			<persName><forename type="first">Tonmoy</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaikh</forename><surname>Anowarul Fattah</surname></persName>
		</author>
		<author>
			<persName><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Wahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical Engineering and Information &amp; Communication Technology (ICEEICT), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical feature based novel method to detect bleeding in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">Tonmoy</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khairul</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Bashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khan</forename><surname>Shamsul Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaikh</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fattah</forename><surname>Anowarul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Informatics, Electronics &amp; Vision (ICIEV), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An automatic bleeding detection scheme in wireless capsule endoscopy based on statistical features in hue space</title>
		<author>
			<persName><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Bashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sa Fattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><surname>Wahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer and Information Technology (ICCIT), 2014 17th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="354" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bleeding detection in wireless capsule endoscopy based on mst clustering and svm</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dihu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Systems (SiPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic bleeding frame detection in the wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max Q-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1310" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei</title>
		<author>
			<persName><forename type="first">Youyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiying</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2903" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Dan C Cires ¸an</surname></persName>
		</author>
		<author>
			<persName><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34003" to="034003" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleeding frame and region detection in the wireless capsule endoscopy video. Biomedical and Health Informatics</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="624" to="630" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
