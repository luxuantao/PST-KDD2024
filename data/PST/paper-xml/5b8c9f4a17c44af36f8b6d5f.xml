<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A34CF904C775739F0B9A5E24D78CAD4</idno>
					<idno type="DOI">10.1109/TMM.2018.2877886</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2877886, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2877886, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2877886, IEEE Transactions on Multimedia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video-based person re-identification</term>
					<term>Gated recurrent units</term>
					<term>Spatial correlations</term>
					<term>Visual attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-based person re-identification (re-id) is a central application in surveillance systems with significant concern in security. Matching persons across disjoint camera views in their video fragments is inherently challenging due to the large visual variations and uncontrolled frame rates. There are two steps crucial to person re-id, namely discriminative feature learning and metric learning. However, existing approaches consider the two steps independently, and they do not make full use of the temporal and spatial information in videos. In this paper, we propose a Siamese attention architecture that jointly learns spatiotemporal video representations and their similarity metrics. The network extracts local convolutional features from regions of each frame, and enhance their discriminative capability by focusing on distinct regions when measuring the similarity with another pedestrian video. The attention mechanism is embedded into spatial gated recurrent units to selectively propagate relevant features and memorize their spatial dependencies through the network. The model essentially learns which parts (where) from which frames (when) are relevant and distinctive for matching persons and attaches higher importance therein. The proposed Siamese model is end-to-end trainable to jointly learn comparable hidden representations for paired pedestrian videos and their similarity value. Extensive experiments on three benchmark datasets show the effectiveness of each component of the proposed deep network while outperforming state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The person re-identification (re-id) research aims to recognize an individual over disjoint camera views <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. It is a problem of practical importance to multimedia and computer vision communities because of its wide range of potential applications, such as the public security and forensic investigation. Also, person re-id is especially beneficial to multimedia content analysis. Examples include human movement analysis <ref type="bibr" target="#b7">[8]</ref>, clothing retrieval and recommendation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, finegrained object recognition <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, and so on. This is an inherently challenging problem because people appearance are intrinsically limited. The limitations of visual appearance are caused by inevitable visual ambiguities in terms of appearance similarity among different people and visual variations of the same person under viewpoints, illumination, and occlusion.</p><p>The problem of re-identification has been extensively studied for still images by matching spatial appearance features Lin Wu and Xue Li are with The University of Queensland, St Lucia 4072, Australia; Email: lin.wu@uq.edu.au; xueli@itee.uq.edu.au.</p><p>Yang Wang* (Corresponding author) is with Dalian University of Technology, China; Email: yang.wang@dlut.edu.cn.</p><p>Junbin Gao is with Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney, NSW 2006, Australia; Email: junbin.gao@sydney.edu.au. (e.g., color and texture) in correspondence using a pair of static images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>. However, less attention has been paid to video-based approaches. In practice, video-based person re-id provides a more natural solution to person recognition because videos of pedestrians can be easily captured in a surveillance system. Furthermore, videos contain richer information than a single image, and beneficial for identifying a person under complex conditions. Given a bunch of frames in sequences, temporal priors in relation to some person motion can be captured and may assist in disambiguating an impostor across camera views. Also, sequential frames provide more samples for pedestrian appearance, where each sample may contain different poses and viewpoints and allow a reliable appearance based model to be constructed. However, making use of videos brings new challenges to re-id, namely the demand of dealing with time series with variable length and/or different frame rates. Moreover, the problem of seeking discriminative features from the videos with partial or full occlusion <ref type="bibr" target="#b17">[18]</ref> also needs to be tackled.</p><p>In general, video based person re-id methods involve two key steps: feature learning and metric learning. Feature learning algorithms describe persons in videos by ascribing to spatial and temporal cues. The spatial part carries the information about visual appearance in correspondence while the temporal term is complementary to spatial alignment by providing motion across frames. To explore spatiotemporal information, a number of approaches <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> are developed to extract low-level features from manually aligned sequences. They commonly extract frame-wise low-level 3D features (e.g., HOG3D <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, color/gradient features over color channels <ref type="bibr" target="#b19">[20]</ref>) through pre-aligned sequences and aggregate these features via an encoding strategy. While metric learning refers to develop metrics through which the similarity between two matched videos of the same pedestrian is higher than that between videos of different pedestrians <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Recent advances in Recurrent Neural Networks (RNNs) and the variants of Long Short-Term Memory (LSTM) models <ref type="bibr" target="#b25">[26]</ref> provide the insights on how to leverage temporal and spatial information for learning video representations. Inspired by the effectiveness of RNNs in sequence modeling, video-based person re-id gains some progress by using LSTM networks to learn feature representations <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref> or learn metrics <ref type="bibr" target="#b26">[27]</ref>. However, these methods still suffer from certain limitations. First, the recurrence is only applied at the temporal dimension across frames while spatial correlation within each frame is merely revealed by performing convolutional operations. As a result, local features are independently extracted from each region whereas spatially contextual connections between regions are not captured, which are very crucial to matching persons <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Second, the hidden LSTM unit is defined as a fixed-dimensional vector, which does not have the flexibility in choosing relevant contexts in spatial correlations when measuring the similarity between video pairs. Last but not the least, those RNN methods pay attention to learn video features or similarity metrics independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Our Approach</head><p>In this paper, we introduce a novel deep attention based Siamese model to video-based person re-id by jointly learning spatiotemporal video representations and similarity metrics. The network is designed to improve the discriminative capability of local features by leveraging the spatially contextual features within frames into the representation learning. In this sense, the similarity between two corresponding locations in a pair of videos is the integration of spatial context. We achieve the spatial recurrence by proposing convolutional gated recurrent units (GRUs) <ref type="bibr" target="#b29">[30]</ref> to propagate their relevance across temporal dimensions. The self-recurrent connections in the GRUs enable them to learn representations from the inputs that have been "seen". Moreover, the hidden units in our network are modified to be 3D feature cubes, which are flexible in capturing the varied input signals.</p><p>However, not all the spatial dependencies are relevant for the pedestrian matching, and indiscriminately scanning all regions incurs very high computational cost. To this end, we introduce the attention mechanism between the local convolutional activations and hidden states to enable the propagation of certain spatial contexts with discriminations and block the irrelevant ones. As a result, video fragments are not compressed into static hidden representations, but dynamically pooled out to enhance the discriminative capability of local features. Generally, for the metric learning of person re-id, feature vectors of similar pairs from the same subject must be "close" to each other while those from dissimilar pairs should be distant. To jointly learn similarity metrics, the proposed Siamese networks consist of two identical subnetworks which are integrated at the last convolution operations for comparing the input videos <ref type="bibr" target="#b30">[31]</ref>. To optimize the set of network parameters, inputs are given in the form of pairs, and our network can be trained to minimize the cross-entropy loss on the input pair.</p><p>Formally, the Siamese architecture takes a pair of videos</p><formula xml:id="formula_0">X a = {x a 1 , x a 2 , . . . , x a Ta }, X b = {x b 1 , x b 2 , . . . , x b T b }</formula><p>in their RGB values along with a class label y a (y b ) for each video. At each time-step, frames x a t (x b t ) are processed by Convolutional Neural Networks (CNNs) to produce their convolutional features. These local features are then weighted by a soft attention to capture their spatial correlations via a probability distribution on spatial dimensions. The resulting features are fed into convolutional GRUs to propagate the most relevant features over time. This soft attention is deterministic to enable efficient training using back-propagation, and potentially determines which local regions should be focused with higher importance being attached. Finally, an average temporal pooling is applied on hidden states across all time steps to generate the videolevel representations ha ( hb ). The similarity between X a and X b , i.e., s(X a , X b ), is defined as a weighted inner product between their video representations. The network can be trained by minimizing the cross-entropy loss on pairs of similar (y a = y b ) and dissimilar time series (y a = y b ). The overall architecture is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions</head><p>The major contributions of this paper are summarized below.</p><p>• We propose a deep Siamese attention architecture to jointly learn feature representations and similarity metrics for video-based person re-id. The network leverages spatial contexts to enhance discriminative capabilities of local features and propagate critical spatial dependencies in computing the similarity of video pairs. • We introduce attention mechanism to select the most relevant features during the recurrence, which learns to attend at distinct regions in cross-view and help re-identify persons under complex conditions. Our method discloses which elements are important in video-based matching by visualizing "where" and "when" the attention focused on, and how to dynamically pool out the relevant features. • We conduct extensive experiments to demonstrate the state-of-the-art performance achieved by our method for video-based person re-id. The rest of this paper is structured as follows. In Section II, we briefly review related works in terms of video-based person re-id, deep learning and visual attention. The proposed method is presented in Section III, followed by parameter learning and experimental evaluations in Section IV and V, respectively. Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Person Re-identification</head><p>Many approaches to person re-id are image-based, which can be generally categorized into three categories. The first pipeline of these methods is appearance based design that aims to extract features that are discriminative and invariant against dramatic visual changes across views <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The second stream is based on metric learning which works by extracting features for each image first, and then learning a metric with which the training data have strong inter-class differences and intra-class similarities <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Deep neural network approaches to person re-id jointly learn reliable features and corresponding similarity value for a pair or triplet of images <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>. The idea is to directly comparing pairs of images and answer the question of whether the two images depict the same person or not. Another pipeline of methods in person reid are post-ranking based optimization <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> which can exploit the context information from the first ranked positions to detect/remove visual ambiguities to compute an improved new ranking. However, these approaches are developed to work on still image based person re-id, whilst not applicable to the video setting.</p><p>Many realistic scenarios indicate that multiple images can be exploited to improve the matching performance. Multishot approaches in person re-id <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b39">[40]</ref> use multiple images of a person to extract appearance descriptors to model person appearance. For these methods, multiple images from a sequence are used to enhance spatial feature descriptions from local regions <ref type="bibr" target="#b13">[14]</ref>. Some methods attempt to select and match video fragments to maximize the cross-view ranking <ref type="bibr" target="#b18">[19]</ref>. These methods, however, deal with multiple images independently whereas in the video context, a video contains more information than independent images, e.g., dynamics of motion and temporal evolution. Learning temporal dynamics in person re-id with videos is very challenging, and earlier approaches use optical flow, HOG and hand-crafted features to generate descriptors with both appearance and dynamics information encoded. These spatiotemporal appearance models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b40">[41]</ref> treat the video data as a 3D volume and extract local spatiotemporal features. They construct spatiotemporal volume based representations by extending image descriptors such as 3D-SIFT <ref type="bibr" target="#b41">[42]</ref> and HOG3D <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, HOG3D is utilized as a spatiotemporal feature representation for person re-id due to its robustness against cluttered backgrounds and occlusions. Liu et al. <ref type="bibr" target="#b19">[20]</ref> proposed a better representation that encodes both the spatial layout of the body parts and the temporal ordering of the action primitives of a walking pedestrian. However, these methods construct representations from lowlevel descriptors including color, texture and gradients, which are still not discriminate enough to distinguish persons and very sensitive to large intra-person visual variations. Also, the temporal nature of videos is not explicitly modeled in the their approaches. A recent approach proposed by McLaughlin et al. <ref type="bibr" target="#b17">[18]</ref> combines CNNs with LSTMs and performs temporal pooling of features prior to classifying sequences into proper identities. A similar work is presented in <ref type="bibr" target="#b21">[22]</ref> where low-level features are fused under LSTMs.</p><p>In this paper, we deliver an attention based Siamese model to jointly learn spatiotemporal expressive features and corresponding similarity metric for a pair of pedestrian sequences. Our architecture improves the discriminative capability of local features by encoding spatial contexts via attention selection. This also regulates the propagation of the most relevant contexts through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning and Visual Attention</head><p>Deep learning methods in person re-id are effective in combating visual variations by learning a projection into deep embedding space under which the identity information can be preserved <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. And the learned representations can also be transferred to different datasets <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>. However, these deep models are still difficult to be examined to understand the learned capability of patterns in person recognition.</p><p>The principled saliency based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b45">[46]</ref> resemble human gazing capabilities to identify the salient regions of a person appearance to tackle this challenging problem. We are inspired by saliency methods in detecting informative regions whilst we attempt the first efforts in localizing regions both spatially and temporally over a video sequence.</p><p>Recent deep attention models add a dimension of interpretation by capturing where the model is focusing its attention when performing a particular task <ref type="bibr" target="#b46">[47]</ref>. For instance, Xu et al. <ref type="bibr" target="#b47">[48]</ref> used both soft attention and hard attention mechanisms to generate image descriptions. Their models look at the perspective objects when generating their descriptions. While Xu et al. <ref type="bibr" target="#b47">[48]</ref> primarily work on caption generation in static images, in this paper, we focus on using a soft attention mechanism for person re-id in videos. A similar work is from <ref type="bibr" target="#b48">[49]</ref> where an attention model is presented to classify actions in videos. However, pedestrians in the wild exhibit similar walking behaviors without distinctive and semantically categorisable actions unique to different people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP SIAMESE ATTENTION NETWORKS FOR VIDEO-BASED PERSON RE-IDENTIFICATION</head><p>In this section, we introduce attention based deep Siamese networks for video-based person re-id. The goal is to match sequences of same pedestrians obtained from different surveillance systems. The proposed Siamese architecture employs two convolution-recurrent networks that share their parameters in order to extract comparable hidden-unit representations from which a similarity value is computed as output. The network is optimized based on the cross-entropy loss. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the proposed Siamese architecture. At each time step t, frames x a t and x b t of sequence X a and X b passes through the CNNs, the recurrent layers with spatial context encoding, and the temporal pooling layer, to jointly learn spatiotemporal video features and their similarity value s(X a , X b ). A notation table is given in Table <ref type="table" target="#tab_0">I</ref>. Two sequences across view T Total length of a sequence</p><formula xml:id="formula_1">K × K × D Convolutional feature cubes W Input-to-hidden parameter U Hidden-to-hidden parameter k1 × k2</formula><p>The convolutional kernel size z, r</p><p>Update and reset gates h l t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden representations at time-step t of layer-l h</head><p>The final pooled hidden video represenations mt,i Attention weight on location i at time-step t λ</p><p>The attention penalty coefficient</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Layers</head><p>Our architecture employs Convolutional Neural Networks (CNNs) to transfer the input data to a representation where the learning of pedestrian motion dynamics is easy <ref type="bibr" target="#b49">[50]</ref>. In general, CNNs process an image using a series of layers, where each individual layer is composed of convolution, pooling, and non-linear activation function steps. In our approach, we extract the last convolutional layer by passing the video sequence through GoogLeNet model <ref type="bibr" target="#b50">[51]</ref> trained on the Ima-geNet dataset <ref type="bibr" target="#b51">[52]</ref>. Let X = [x 1 , . . . , x T ] be a video sequence of length T , consisting of whole-body images of a person, where x t is the image at time t. The last convolutional layer in GoogLeNet has D convolutional maps and is feature cube of shape K × K × D (7 × 7 × 1024 in our experiments). Thus, at each time-step t, we extract K 2 D-dimensional vectors. We refer to these vectors as feature slices in a feature cube:</p><formula xml:id="formula_2">Xt = [Xt,1, . . . , X t,K 2 ], Xt,i ∈ R D .<label>(1)</label></formula><p>Each of these K 2 vertical slices links to different overlapping regions, and corresponds to portions of local regions in the 2-D frame. Our model tends to selectively focus on certain parts of these K 2 regions to propagate through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deterministic Soft Attention Mechanism</head><p>We use a particular type of RNN model, Gated Recurrent Units (GRUs) <ref type="bibr" target="#b29">[30]</ref> to learn an effective representation of person videos by capturing its long range dependencies. We implement GRUs as the recurrent units due to its superior performance in many tasks such as music/speech signal modeling <ref type="bibr" target="#b52">[53]</ref> and fewer parameters as opposed to the Long Short Term Memory (LSTM) <ref type="bibr" target="#b53">[54]</ref>. The GRU is formulated as follows:</p><formula xml:id="formula_3">zt = σ(Wzxt + Uzht-1), rt = σ(Wrxt + Urht-1), ĥt = tanh(W xt + U (rt ht-1)), ht = (1 -zt)ht-1 + zt ĥt,<label>(2)</label></formula><p>where is an element-wise multiplication. z t is an update gate that determines the degree to which the unit updates its activation. r t is a reset gate, and σ is the sigmoid function. The candidate activation ĥt is computed similarly to that of traditional recurrent unit. When r t is close to zero, the reset gate make the unit act as reading the first symbol of an input sequence and forgets the previously computed state.</p><p>The vector x t is a dynamic representation of the relevant part of the frame input at time t. To compute x t from the feature cube X t,i , i = 1, . . . , K 2 corresponding to features extracted at different image regions, our model defines a mechanism to predict m t+1 , a softmax over K × K locations. For each location i, the mechanism generates a positive weight m t,i which can be interpreted as the relative importance to give to location i in blending the X t,i 's together (deterministic attention mechanism). The weight m t,i of each feature slice X t,i , i.e., location softmax, is computed by an attention model for which we use a multi-layer perception conditioned on the previous hidden state h t-1 . The location softmax is defined as follows:</p><formula xml:id="formula_4">mt,i = p(Lt = i|ht-1) = exp(W T i ht-1) K×K j=1 (W T j ht-1) , i ∈ 1, . . . , K 2 ,<label>(3)</label></formula><p>where W i are weights mapping to the i-th element of the location softmax and M i is a random variable which can take 1 of K 2 values. This softmax can be interpreted as the probability from which our model suggests the corresponding region in the input frame is important <ref type="bibr" target="#b48">[49]</ref>. We emphasize that the hidden states varies as the RNN output advances in its output sequence: "where" the network looks next depends on the sequence of regions that has been already examined.</p><p>Within the probabilities of locations, the deterministic soft attention mechanism computes the expected value of the input at the next time-step x t by taking weighted mask over the feature slices at different regions. A graphical depiction of deterministic soft attention mechanism is shown in Fig. <ref type="figure" target="#fig_1">2 (a)</ref>. Hence, the deterministic attention model by computing a soft attention weighted feature cube is formulated as</p><formula xml:id="formula_5">x t = m t,i X t,i .<label>(4)</label></formula><p>This corresponds to feeding a soft weighted context into the system. Note that an alternative mechanism for attention is stochastic "hard" attention where the location variable L t should be sampled from the softmax distribution of Eq.( <ref type="formula" target="#formula_4">3</ref>) <ref type="bibr" target="#b47">[48]</ref>. However, hard attention models are not differentiable and have to resort to some form of sampling. By contrast, a soft attention model as a whole is smooth and differentiable under the deterministic attention, so that learning end-to-end is trivial by using standard back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Attention based Spatial Correlation Encoding</head><p>Given the attention weighted feature cube x t , we apply a stack of L-layer RNNs on each x t to propagate their attention over time. The stacked RNNs can be parameterized as φ 1 , φ 2 , . . . , φ L , such that h l t = φ l (x t , h l t-1 ), l = 1, . . . , L. The output feature representations h are obtained by average/attention pooling the hidden unit activations over all time steps h L 1 , h L 2 , . . . , h L T , as defined in Eq.( <ref type="formula" target="#formula_8">6</ref>). The recurrent function φ l is implemented by using Gated Recurrent Units as formulated in Eq.(2). However, the formulation of GRU is a fully-connected GRU (FC-GRU) where the input and states are all 1-D vectors. To model the spatiotemporal relationship in video sequence in the form of convolutional activations, we restructure the fully-connected GRUs to be convolutional (ConvGRUs) which perform convolutional operations in both the input-to-hidden and hidden-to-hidden transitions.</p><p>The output convolutional maps of the encoder subordinate to recurrent layers are 3D tensors (spatial dimensions and input channels) for which directly applying a FC-GRU can lead to a massive number of parameters and too much redundancy for these spatial data. Let H 1 and H 2 and C x be the input convolutional map spatial dimension and number of channels. A FC-GRU would require the input-to-hidden parameters W l z , W l r and W l of size H 1 × H 2 × C x × C h where C h is the dimensionality of the GRU hidden representation. Inspired by Convolutional LSTMs <ref type="bibr" target="#b54">[55]</ref>, we employ convolutional GRUs to capture the motion dynamics involved over time series. The ConvGRUs are first introduced by Ballas et al. <ref type="bibr" target="#b55">[56]</ref> to take advantage of the underlying structure of convolutional maps. The rationale of using ConvGRUs is based on two observations: 1) convolutional maps extracted from images represent strong local correlations among responses, and these local correlations may repeat over different spatial locations due to the shared weights of convolutional filters (a.k.a. kernels) across the image spatial domain; 2) videos are found to have strong motion smoothness on temporal variation over time, e.g., motion associated with a particular patch in successive frames will appear and be restricted within a local spatial neighborhood. In this sense, we reduce the computational complexity of the deep RNNs by further sparsifying their hidden unit connection structures. Specifically, our model is designed to characterize the feature locality and motion smoothness, and the fully-connected units in GRUs are replaced with convolutional filters (kernels). Thus, recurrent units are encouraged to have sparse connectivity (hiddento-hidden) and share their parameters across different input spatial locations (input-to-hidden), and we can formulate a ConvGRU as:</p><formula xml:id="formula_6">z l t = σ(W l z * h l-1 t + U l z * h l t-1 ), r l t = σ(W l r * h l-1 t + U l r * h l t-1 ), ĥl t = tanh(W l * h l-1 t + U l * (r l t h l t-1 )), h l t = (1 -z l t )h l t-1 + z l t ĥl t ; l = L, . . . , 1,<label>(5)</label></formula><p>where * denotes a convolution operation and h 0 t = x t . In Eq <ref type="bibr" target="#b4">(5)</ref>, input-to-hidden parameters W l z , W l r , W l , and hidden-tohidden parameters U l z , U l r , U l are 2-D convolutional kernels with size of k 1 × k 2 where k 1 × k 2 is much smaller than the original convolutional map size H 1 × H 2 . The input and hidden representation can be imagined as vectors standing on spatial grid, and the ConvGRU determines the future hidden representation in the grid by the inputs and past hidden units of its local neighbors. The resulting hidden recurrent representation h l t = {h l t (i, j)} can preserve the spatial structure where h l t (i, j) is a feature vector defined at the grid location (i, j). To ensure the spatial size of the hidden representation remains the same over time, zero-padding is needed before applying the convolution operation. The structure of a convolutional GRU is illustrated in Fig. <ref type="figure">3</ref>. In fact, the hidden recurrent representation of moving objects is able to capture faster motion moving with a larger transitional kernel size while one with a smaller kernel can capture slower motion. Recall that FC-GRU models inputs and hidden units as vectors, which can be viewed as a feature map with 1 × 1 spatial dimension. In this sense, FC-GRU becomes a special case of ConvGRU with all features being a single vector. The advantage of adding the deep hidden connections is to enable the model to leverage representations with different resolutions since these low-level spatial resolutions certainly encode informative motion patterns. In particular, the receptive field associated</p><formula xml:id="formula_7">x t h t l h t l -1 h t -1 l -1 h t -1 l x t -1 m t</formula><p>Fig. <ref type="figure">3</ref>: The structure of a convolutional GRU. xt = mt,iXt,i have dimension K×K×D, corresponding to the frame-level CNN features with attention applied. To encode the spatial information into hidden state h l t , the state in the grid is determined by the input (xt) and the past state of its local neighbors (h l t-1 ), as implemented by a convolutional operation.</p><p>with each h l t (i, j) increases in previous hidden states h l t-1 , h l t-2 ,. . . , h l 1 if we go back along the time line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporal Pooling: Attention on Temporal Selection</head><p>A temporal pooling is needed in our architecture because RNN's outputs may be biased towards later time-steps, which could reduce the effectiveness of RNN when used to summarize the relevant information over a full sequence since discriminative regions can appear anywhere in the sequence. To address this limitation, an effective average temporal pooling is introduced on the top of the network to combine the features at all convolutional levels but from selected frames to generate overall feature appearance for the complete sequence. Specifically, we propose a soft-attention based temporal pooling, which is formulated as</p><formula xml:id="formula_8">h = αth L t T t=1 αth L t ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">α = [α 1 , • • • , α T ], ( T t=1 α t = 1)</formula><p>is the selection vector that is able to re-weigh each frame to selectively focus on certain subset of frames. For each frame at time step t, the mechanism generates a positive weight α t , which can be interpreted as the relative importance to give to the frame at time t in blending the h L t 's together. This corresponds to feeding a soft α weighted temporal context into the system. And the whole model is smooth and differentiable under this soft deterministic attention, so learning end-to-end is still trivial by using standard back-propagation. Then, h is flattened to produce a single vector as its video-level representation. This allows for the aggregation of information across all time steps, thus avoiding bias towards later time-steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complexity</head><p>Due to the convolution operation, input-to-hidden parameters W l z , W l r , W l have the size of</p><formula xml:id="formula_10">k 1 × k 2 × C x × C h .</formula><p>The activation gate z l t (i, j), reset gate r l t (i, j), and candidate hidden representation ĥl t (i, j) are defined based on a local neighborhood of size k 1 × k 2 at the location (i, j) on both the input data x t and the previous hidden units h l t-1 . A ConvGRU layer conducts 2D convolutional operation 6 times at each time step i.e., 2 convolutions per GRU gate and another 2 in computing ĥl t . Assume the input-to-hidden and hidden-to-hidden transition parameters have the same kernel size while preserving the input dimensions, ConvGRU requires</p><formula xml:id="formula_11">O(3T H 1 H 2 k 1 k 2 (C x C h + C 2 h )</formula><p>) computations where T is the length of a time sequence. Apparently, ConvGRU saves computations substantially compared with a FC-GRU which requires</p><formula xml:id="formula_12">O(3T H 2 1 H 2 2 (C x C h + C 2 h )) multiplications.</formula><p>In memory, a ConvGRU only needs to store parameters for 6 convolutional kernels, yielding O(3k</p><formula xml:id="formula_13">1 k 2 (C x C h + C 2 h )) parameters.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PARAMETER LEARNING A. Initialization</head><p>We use the following initialization strategy <ref type="bibr" target="#b47">[48]</ref> for the hidden states for faster convergence:</p><formula xml:id="formula_14">h0 = finit   1 T T t=1   1 K 2 K 2 i=1 Xt,i     ,<label>(7)</label></formula><p>where f init is a multi-layer perception. These values are used to calculate the first softmax location m 1 which determines the initial input x 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Function and Attention Penalty</head><p>We use cross-entropy loss coupled with the doubly stochastic penalty <ref type="bibr" target="#b47">[48]</ref>. We impose an additional constraint over the location softmax, such that T t=1 m t,i ≈ 1. This is the attention regularization which forces the model to look at each region of the frame at some point in time. The cross-entropy loss can be computed from the similarity value between two timeseries input. Given two time series</p><formula xml:id="formula_15">X a = {x a 1 , x a 2 , . . . , x a Ta } and X b = {x b 1 , x b 2 , . . . , x b</formula><p>T b }, the hidden unit representations ha and hb computed from the two subnetworks can be combined to compute the prediction for the similarity of the two time series. Thus, we define the similarity of the two sequences as:</p><formula xml:id="formula_16">s(X a , X b ) = 1 1 + e -v T [diag( ha ( hb ) T )]+c ,<label>(8)</label></formula><p>where the element-wise inner product between the hidden representations is computed i.e., ha ( hb ) T , and the output is a weighted sum of the resulting product (v is the weight vector). c is a constant value. The similarity between two time series is defined as a weighted inner product between the representations ha and hb . Our approach learns a vectorial representation for each time series in such a way that similar/dissimilar time series are modeled by similar/dissimilar representations. Thus, time series modeling and metric learning can be combined and studied through the Siamese recurrent networks, which can be optimized to minimize the cross-entropy loss on pairs to learn a good similarity measure between time series.</p><p>Let Z denote a training set containing two sets of pairs of time series: a set with similar pairs Sim and a set with dissimilar pairs of Dis. We aim to learn all parameters Θ = {W l z , W l r , W l , U l z , U l r , U l , v, c} in our network jointly by minimizing the binary cross-entropy prediction loss coupled with the penalty on location softmax. This is equivalent to maximizing the conditional log-likelihood of the training data:</p><formula xml:id="formula_17">L(Θ; Z) = λ K 2 i=1 (1 - T t=1 mt,i) 2 -   (a,b)∈Sim log s(X a , X b ) + (a,b)∈Dis log(1 -s(X a , X b ))   ,<label>(9)</label></formula><p>where a and b denote the index of two time sequences in training pairs, and λ is the attention penalty coefficient.</p><p>The loss function is maximized and back-propagated through stacked recurrent networks (the weights of two subnetworks are shared) using a variant of the back-propagation through time algorithm with gradient clipping between -5 and 5 <ref type="bibr" target="#b56">[57]</ref>. The sets Sim and Dis are defined using class labels for each time sequence: Sim = {(a, b) : y a = y b } and Dis = {(a, b) : y a = y b } where y a and y b are class labels for time sequences X a and X b . In the case of person re-id, each person can be regarded as a class, and thus training class label can be assigned on each sequence accordingly. In contrast to existing classification deep models in person re-identification <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, our loss function allows our architecture to be applied on objects from unknown classes. Our network can be trained one dataset (domain) and applied on a different test dataset (out-of-domain) to verify the new fragment sequences that were not present in training set. Thus, this loss function is more suitable to person re-id where the underlying assumption is that inter-person variations have been modeled well. The experimental analysis on cross-domain testing is given in Section V-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we present empirical evaluations on the proposed model. Experiments are conducted on three benchmark video sequences for person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We experimented on three image sequence datasets (Fig. <ref type="figure" target="#fig_2">4</ref>): iLIDS-VID <ref type="bibr" target="#b18">[19]</ref>, PRID 2011 <ref type="bibr" target="#b57">[58]</ref>, and MARS <ref type="bibr" target="#b58">[59]</ref>.</p><p>• The iLIDS-VID dataset consists of 600 image sequences for 300 randomly sampled people, which was created based on two non-overlapping camera views from the i-LIDS multiple camera tracking scenario. The sequences are of varying length, ranging from 23 to 192 images, with an average of 73. This dataset is very challenging due to variations in lighting and viewpoint caused by cross-camera views, similar appearances among people, and cluttered backgrounds. • The PRID 2011 dataset includes 400 image sequences for 200 persons from two adjacent camera views. Each sequence is between 5 and 675 frames, with an average of 100. Compared with iLIDS-VID, this dataset was captured in uncrowded outdoor scenes with rare occlusions and clean background. However, the dataset has obvious color changes and shadows in one of the views. • The MARS dataset contains 1,261 pedestrians, and 20,000 video sequences, making it the largest video re-id dataset. Each sequence is automatically obtained by the Deformable Part Model <ref type="bibr" target="#b59">[60]</ref> detector and the GMMCP <ref type="bibr" target="#b60">[61]</ref> tracker. These sequences are captured by six cameras at most and two cameras at least, from which each identity has 13.2 sequences on average. This dataset is evenly divided into train and test sets, containing 625 and 636 identities, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Training</head><p>We implemented our architecture in Python with Theano <ref type="bibr" target="#b61">[62]</ref>. All experiments are run on a single PC with a single NVIDIA GTX 980 GPU with 12GB memory. For the iLIDS-VID and PRID 2011 datasets, the whole dataset is randomly split into 50% (50%) of persons for training (testing). For the MARS dataset, we comply with the fixed training and testing sets, containing 631 and 630 identities, respectively.</p><p>Since our network is Siamese alike, we randomly select positive and negative sequence pairs in on-line manner. Nonetheless, each positive/negative sequence pair consists of an arbitrary length in their full sequences to depict the same person or different persons under disjoint cameras. Thus, to ensure the fairness of experiments, we follow the same sequence length setting in <ref type="bibr" target="#b17">[18]</ref>. During training, sub-sequences of T =20 consecutive frames are used for computational purpose, where a different subset of 20 frames are randomly selected from the whole video sequence at each epoch. The network is trained with 1,000 epochs on the iLIDS-VID and PRID 2011 datasets while 2,000 epochs on the MARS dataset (The convergence evaluations are shown in Fig. <ref type="figure" target="#fig_3">5 (b)</ref>. It takes 0.06s for a pair of frames on a single NVIDIA GeForce GTX 980 GPU with 12GB memory.). In testing, we regard the first camera as the probe while the second camera as the gallery.</p><p>We did self data augmentation in training by cropping and mirroring all frames for a given sequence. In addition, we artificially augment the data by performing random 2D translation <ref type="bibr" target="#b17">[18]</ref>. For a frame of size A × B, the same sized frames around the image center are sampled with translation drawn from a uniform distribution in the range of [-0.05A, 0.05A]× [-0.05B, 0.05B]. In testing, data augmentation is also applied into the probe and the gallery sequences in the case that the length of each person sequence is less than 128, and the similarity scores between sequences are computed and averaged over all augmentation conditions <ref type="bibr" target="#b62">[63]</ref>.</p><p>In our deep GRU, all input-to-hidden and hidden-to-hidden kernels are of size 5×5, i.e., k 1 = k 2 = 5. The structure setting is optimized by extensive self-study on architecture (Please refer to Table <ref type="table" target="#tab_1">II</ref>). We apply zero-padded 5 × 5 convolutions on each ConvGRU to preserve the spatial dimension. The stacked GRU consists of 3 layers with 128, 256, 256 channels, respectively. Max-pooling is applied on hidden-representations between the recurrent layers for the compatibility of the spatial dimensions. The hidden representations at the top of GRU across frames are average pooled to be the overall representation for the complete sequence. In our training, the convolutional activations with parameters regarding GoogLeNet are pre-trained on ImageNet, while the parameters of ConvGRUs are initialized by sampling them from an uniform distribution . A dropout ratio of 0.7 is applied on the hidden-unit activations. We define similar video sequence to be those from the same person and dissimilar ones to be those from different persons. For the penalty coefficient, we study the effect with varied values (see Fig. <ref type="figure" target="#fig_3">5 (a)</ref>) and finally we experimented with λ = 1 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines and Evaluation Metric</head><p>We consider a variety of baselines in our experiments:</p><p>• (1) VGG 16 + FC-GRU: A deep architecture with FC-GRUs, which are based on VGG 16 model <ref type="bibr" target="#b64">[65]</ref>, pretrained on ImageNet <ref type="bibr" target="#b51">[52]</ref>, and fine-tune it on the MARS dataset. We extract features from fully-connected layer f c7 (This can be viewed as a feature map with 1 × 1 spatial dimension), which are given as the inputs to the proposed deep network. This baseline has three layers of GRUs with 128, 256, and 256 channels, respectively. And the kernel sizes across three layers are set to be k 1 = k 2 = 1 throughout. • (2) GoogLeNet + FC-GRU: The architecture structure is the same as VGG16 + FC-GRU except that the features are extracted from the last fully connected layers of Inception module. • (3) Ours G (9 × 9, 1 × 1): In all variants of our model, the input-to-state kernel size in the first layer GRU is set to be 3 × 3 (i.e., x t = h 0 t 3×3 --→ h 1 t ) while the state-to-state kernel sizes across the second and third layers are varied to determine the variants of each model. We modify the network with the state-to-state kernels of size 9 × 9 and 1×1 for the second and third layers of GRU, respectively. The features are extracted from the last convolutional layer of GoogLeNet. Table <ref type="table" target="#tab_1">II</ref> shows the configurations of all variants of our model. • (4) Ours G (1 × 1, 9 × 9): This variant has the same configuration as Ours G (9 × 9, 1 × 1) while the sizes of state-to-state kernels for the second and third layers of GRU are set to be 1 × 1 and 9 × 9, respectively. • (5) Ours G (5 × 5, 5 × 5): The configuration is the same as above while the sizes of state-to-state kernels for the second and third layers of GRU are set to be both 5 × 5.  The performance is evaluated by the average Cumulative Matching Characteristics (CMC) curves after 10 trials with different train/test split. Specifically, in the testing phase, the Euclidean distances between probe sequence features and those of gallery sequences are computed. Then, for each probe person, a rank order of all the candidates in the gallery is sorted from the one with the smallest distance to the largest distance. Finally, the percentage of true matches sorted out among the first R ranked persons is computed and denoted as rank@R.</p><formula xml:id="formula_18">xt = h 0 t 3×3 ---→ h 1 t xt = h 0 t 3×3 ---→ h 1 t xt = h 0 t 3×3 ---→ h 1 t [h 1 t , h 2 t-1 ] 9×9 ---→ h 2 t [h 1 t , h 2 t-1 ] 1×1 ---→ h 2 t [h 1 t , h 2 t-1 ] 5×5 ---→ h 2 t [h 2 t , h 3 t-1 ] 1×1 ---→ h 3 t [h 2 t , h 3 t-1 ] 9×9 ---→ h 3 t [h 2 t , h 3 t-1 ] 5×5 ---→ h 3 t •<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Study on Architecture Variations:</head><p>In this experiment, we study the properties of our model by comparing with variants in terms of varied convolutional kernel sizes: (9 × 9, 1 × 1), (1 × 1, 9 × 9), (5 × 5, 5 × 5), and VGG 16 /GoogLeNet + FC-GRU. Evaluation results are presented in Table <ref type="table" target="#tab_2">III</ref>. Our experiments show that the variants of deep attention with ConvGRUs perform consistently better than VGG-16 /GoogLeNet + FC-GRU. This is mainly because lowlevel percepts from convolution layers provide more spatial information in relation to a person's moving patterns whereas FC-GRUs ignore these spatial variations among time steps. More specific, the performances of Ours * (9 × 9, 1 × 1) are inferior to those with a larger state-to-state kernel size i.e., Ours * (1 × 1, 9 × 9) and Ours * (5 × 5, 5 × 5). This provides evidence that larger state-to-state kernels are more suitable for capturing spatiotemporal correlations. Finally, additional contribution can be seen from the performance improvement caused by the attention mechanism where the rank-1 value drops when we remove attention in Ours G (5 × 5, 5 × 5).</p><p>2) Study on Temporal Pooling with Attention: To achieve a video-level representation where the local descriptor relies on recurrence at frame-level, one needs to apply normalization on frame descriptors and then average pooling of the normalized descriptors over time. In our approach, the average/attention pooling over frames is defined in Eq.( <ref type="formula" target="#formula_8">6</ref>). An alternative is max pooling which can select the maximum activation of each element from frame-level features: h video = max(h 1 , . . . , h T ). Fisher vector <ref type="bibr" target="#b65">[66]</ref> is a leading pooling technique which can provide state-of-the-art results in many different applications.</p><p>In Fisher vector encoding, a Gaussian Mixture Model (GMM) with C components can be denoted as</p><formula xml:id="formula_19">∆ = {(µ k , σ k , π k ), k = 1, 2, .</formula><p>. . , C} where µ k , σ k and π k are the mean, variance and prior parameters of c-th component learned from the deep recurrent architecture in the frame level, respectively. Given H = [h 1 , . . . , h T ] of deep descriptors extracted from a video by a network, the mean and covariance deviation vectors for the c-th component can be computed as:</p><formula xml:id="formula_20">u k = 1 T √ π k T t=1 q kt ht -µ k σ k , v k = 1 T √ 2π k T t=1 q kt ht -µ k σ k 2 -1 ,<label>(10)</label></formula><p>where q kt is the posterior probability. The Fisher vector is formed by concatenating u k and v k of all the C components. Thus, the dimension of the Fisher vector is 2DC where D is the dimension of the hidden representation h t . We compare the performance using different pooling methods. Results are given in Table <ref type="table" target="#tab_4">IV</ref>. It can be seen that our average/attention pooling is superior to the max pooling. One reason is the average/attention pooling considers all the time steps equally important while subject to attention selection with re-weighting on each frame, whilst max pooling only employs the feature value in the temporal step with the largest activation. The illustration on temporal selection over informative frames is shown in Fig. <ref type="figure" target="#fig_4">6</ref>, which demonstrates that important frames are attached with higher weights while less important frames are eliminated. In Table <ref type="table" target="#tab_4">IV</ref>, Fisher vector encoding performs slightly better than our averaging/attention pooling. This is mainly because Fisher vectors use higher order statistics to model feature elements in a generative process. However, our average/attenion is advantageous in terms of end-to-end trainable and efficient in pooling whereas Fisher vector has high computational cost in updating parameters.</p><p>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.     3) Study on Varied Sequence Lengths: In this experiment, we investigate how the performance varies against length of the probe and gallery sequences in test stage. Evaluations are conducted on two datasets, and the lengths of the probe and gallery sequences are varied between 1 and 128 frames in step of power of two. Results are shown in Fig. <ref type="figure">7</ref> where a bar matrix shows the rank-1 re-identification matching rate as a function of the probe and gallery sequence lengths. We can see that increasing the length in either probe or gallery can increase the matching accuracy. Also longer gallery sequences can bring about more benefits than longer probe sequences.</p><formula xml:id="formula_21">@ R R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R =</formula><formula xml:id="formula_22">@ R R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R =</formula><p>4) Attention over Time: Fig. <ref type="figure" target="#fig_6">8</ref> shows test examples of where our model attends on the iLIDS-VID and PRID2011 datasets. We can see that the model is able to focus on distinctive parts of the person and staying on them over time. For instance, on the cases of two persons from the iLIDS-VID dataset where the cluttered backgrounds render the recognition very challenging, our method is seen by effectively localizing the spatial regions, such as the jackets and the bags, which can help recognize the person. It can also be observed that the model is able to continuously attend to important parts in videos by focusing on some noticeable part e.g., the large black bag (as shown in the last row). This can demonstrate the improved discriminative capability of local features produced by our deep attention Siamese networks. Hence, it shows that our attention model is capable of focusing on the distinct parts across time steps and capture the dynamic appearance of different people both spatially and temporally even in the context of varied background clutters and occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with Other Representations</head><p>In this experiment, we compare the learned hidden representations with four competing representations for person re-id.</p><p>• FV2D is a multi-shot approach <ref type="bibr" target="#b66">[67]</ref> which treats the video sequence as multiple independent images and uses Fisher vectors as features. • HOG3D 3D HOG features <ref type="bibr" target="#b22">[23]</ref> from volumes of video data <ref type="bibr" target="#b18">[19]</ref>. Specifically, after extracting a walk cycle by computing local maxima/minima of the FEP signal, video fragments are further divided into 2 × 5 (spatial) × 2 (temporal) cells with 50% overlap. A spatiotemporal gradient histogram is computed for each cell which is concatenated to form the HOG3D descriptor. • FV3D is similar to HOG3D where a local histogram of gradients is extracted from divided regular grids on the volume. However, we encode these local HOG features with Fisher vectors instead of simply concatenating them. • STFV3D is a low-level feature-based Fisher vector learning and extraction method which is applied to spatially and temporally aligned video fragments <ref type="bibr" target="#b19">[20]</ref>. STFV3D proceeds as follows: 1) temporal segments are obtained separately by extracting walk cycles <ref type="bibr" target="#b18">[19]</ref>, and spatial alignment is implemented by detecting spatial bounding boxes corresponding to six human body parts; 2) Fisher vectors are constructed from low-level feature descriptors on those body-action parts. Experimental results are shown in Table <ref type="table" target="#tab_7">V</ref>. From the results, we can observe that our deep representation outperforms consistently over other representations. More specifically, HOG3D is inferior to Fisher vectors based features since Fisher vectors encode local descriptors in a higher order and suitable for person re-identification problem. It is not a surprise to see our features are superior to STFV3D because our deep networks work well in reconstructing spatiotemporal patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with Multi-shot Methods</head><p>To evaluate the effectiveness of our model in producing deep spatiotemporal feature for person re-id, we compare the proposed method with a variety of multi-shot methods where multiple images of a sequence are exploited to construct more reliable descriptors to capture appearance variability. The competitors include   Part Appearance Mixture by modeling a person's appearance as a multi-channel appearance mixture, where each channel corresponds to a particular region of the body. • S-LSTM <ref type="bibr" target="#b27">[28]</ref>: For each fragment, the LSTM model processes image regions sequentially to obtain its framelevel representations, and then all frame-level features are aggregated into the video-level by using average pooling. Table <ref type="table" target="#tab_9">VI</ref> shows the comparison results. It is evident that deeply learned space-time features produced by our deep attention model are more effective in matching person in multishot setting compared with low-level feature combination in SDALF <ref type="bibr" target="#b13">[14]</ref>, eSDC <ref type="bibr" target="#b2">[3]</ref>, RankSVM <ref type="bibr" target="#b67">[68]</ref>, and PaMM <ref type="bibr" target="#b39">[40]</ref>. This can be explained by the stacked convolution operations that extract local features robustly. Moreover, convolutional GRUs with attention selection can leverage spatial contexts as well as capture input signal variation in temporal dimension. Compared with the S-LSTM <ref type="bibr" target="#b27">[28]</ref> that applies LSTM unit to learn deep yet context-aware features within each frame, our method outperform S-LSTM <ref type="bibr" target="#b27">[28]</ref> in rank-1 accuracy by 8.8, 10.9, and 12.1 on three datasets, respectively.</p><formula xml:id="formula_23">@ R R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R = 20</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison with State-of-the-art Approaches</head><p>In this section, we compare our method with state-of-the-art approaches. Recent works <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> show that spatiotemporal features from sequences can be combined with distance metric learning algorithms to achieve better performance. Thus, we combine the learned deep features with two supervised metric learning methods: Local Fisher Discriminant Analysis (LFDA <ref type="bibr" target="#b3">[4]</ref>) and KISSME <ref type="bibr" target="#b32">[33]</ref>. The two methods are applied on top of deep features. Specifically, in the two methods, PCA is first performed to reduce the dimensionality of the original representation, and the reduced dimension is empirically chosen to be 150, as suggested by <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this experiment, we consider the competitors as follows: RCN <ref type="bibr" target="#b17">[18]</ref>, HOG3D+Discriminative Video Ranking (DVR) <ref type="bibr" target="#b18">[19]</ref>, TDL <ref type="bibr" target="#b24">[25]</ref>, RFA-net <ref type="bibr" target="#b21">[22]</ref>, SI 2 DL <ref type="bibr" target="#b23">[24]</ref>, STFV3D <ref type="bibr" target="#b19">[20]</ref>+ LFDA <ref type="bibr" target="#b3">[4]</ref>, and STFV3D <ref type="bibr" target="#b19">[20]</ref>+ KISSME <ref type="bibr" target="#b32">[33]</ref>. Table VII  and Fig. <ref type="figure" target="#fig_7">9</ref> show the comparison results. It is seen that distance metric learning can further improve the performance of our method. Notably our method combined with KISSME achieves rank-1 accuracy of 61.9%, 77.0% and 73.5% on the iLIDS-VID, PRID2011 and MARS datasets, outperforming benchmark methods. In particular, comparing with TDL <ref type="bibr" target="#b24">[25]</ref> that optimizes a distance learning method based on the learning to rank principle, our model can jointly learn video features and its similarity metric. RFA-net <ref type="bibr" target="#b21">[22]</ref> also employs LSTM to capture long-range dependencies to fuse framelevel features into video-level representations, our deep model is advantageous by leveraging spatial priors to improve the discriminative capability of local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Failure Examples</head><p>In this experiment, we discuss the limitations of our approach, and show some failure examples on the iLIDS-VID dataset in Fig. <ref type="figure" target="#fig_8">10</ref>. For each pair of video segments, the top row displays the query sequence and the bottom row shows the gallery sequence. It shows that our method is degenerated in the case when some different pedestrians exhibit very similar appearance in their upper bodies and walking gait patterns. For example, in the first matching case, the lower body parts of query are occluded by a notice sign. Even though our approach is more likely focusing on visual patterns on upper parts, as demonstrated in Fig. <ref type="figure" target="#fig_6">8</ref>, the visual similarity of upper parts between the query person and the retrieved wrong person    causes trouble for our method to identify the correct candidate from the gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Cross-Dataset Testing</head><p>In this experiment, we perform cross-dataset testing to understand how well our approach generalizes across different training and testing datasets. This is a real-world problem due to the dataset bias <ref type="bibr" target="#b62">[63]</ref>, which is a form of over-fitting: the performance of a machine-learning based system, trained on that a particular dataset, is much worse when evaluated on a different dataset. Specifically, our deep model is trained on the largest and diverse MARS dataset and transferred to the target domains on 50% of the iLIDS-VID and PRID 2011 datasets. The results of this experiment are reported in Table <ref type="table" target="#tab_10">VIII</ref>. In the cross-dataset scenario, recognition results are worse, as expected and probably due to the dataset bias. However, our framework's performance is not much below, and is well above other deep learning systems. We consider to improve the generalization performance of our re-identification system in future with some strategy in cross-domain learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we present a deep attention based Siamese model to jointly learn spatiotemporal expressive video representations and similarity metrics for video-based person re-identification. Our approach embeds visual attention into convolutional activations from local regions to dynamically encode spatial context priors and capture the relevant patterns for the propagation through the network. As a consequence, local features are augmented to be highly discriminative to recognize persons in a challenging video scenario. Experimental results are conducted over three benchmark datasets to demonstrate the state-of-the-art performance of our method for video-based person re-id. In particular, we carefully study the structure and configuration of our network model by extensive self-evaluations on varied CNN models and spatial kernel sizes in stacked GRU layers. Also, some attention visualizations are present to show the effectiveness of our approach in combating against cluttered backgrounds and occlusions. Thus, our method is demonstrated to be effective in aligning the dynamic appearance of different people both spatially and temporally.</p><p>There are some interesting directions for further improvement of our framework. From the spatial alignment perspective, the arbitrary change of viewpoints still cause problems in spatial alignment. Thus, we are interested in investigating robust body part models to address the pose/viewpoint issue. The other direction is to explore a spatiotemporal primitive unit as the attention element, which can improve the generalization ability of our framework. We are also interested in developing an end-to-end trainable network that considers Fisher vector encoding as a pooling strategy to select discriminative frames amongst only a few video frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Given a pair of pedestrian videos with variable length (X a and X b ), the proposed deep Siamese attention model incorporates CNNs, gated recurrent units and temporal pooling over time to jointly learn spatiotemporal video features and their corresponding similarity value (s(X a , X b )). Horizontal red arrows indicate that two subnetworks share the same parameterizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: (a) The CNN encoder takes the video frame as its input, and produces a feature cube Xt ∈ R K×K×D , which is averaged and weighted on the location softmax mt to compute the input xt for the subsequent recurrent layer. (b) At each time step t, the feature cube xt is fed into three layers of GRU to propagate and predict the next location probabilities mt+1 and hidden-representations ht.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Image sequences of the same pedestrian (in row) in different camera views from the three datasets.</figDesc><graphic coords="7,227.05,57.76,60.46,53.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: (a) The loss is computed from the cross-validation (90% of training split as training against 10% of training split as validation set) w.r.t the varied value of λ on the MARS dataset. (b) The convergence study on three datasets with increased training epoches.within an interval [-1, 1]. The whole set of parameters are trained/updated by using a RMSProp<ref type="bibr" target="#b63">[64]</ref> stochastic gradient descent procedure with mini-batches of 10 pairs of time series. To prevent the gradients from exploding, we clip all gradients to lie in the interval [-5, 5]<ref type="bibr" target="#b56">[57]</ref>. A dropout ratio of 0.7 is applied on the hidden-unit activations. We define similar video sequence to be those from the same person and dissimilar ones to be those from different persons. For the penalty coefficient, we study the effect with varied values (see Fig.5 (a)) and finally we experimented with λ = 1 in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 )</head><label>6</label><figDesc>Ours V (9 × 9, 1 × 1): This variant has the same configuration as Ours G (9 × 9, 1 × 1) while the features are extracted from the last convolution of VGG-16. • (7) Ours V (1 × 1, 9 × 9): This variant has the same configuration as Ours G (1 × 1, 9 × 9) while the features are extracted from the last convolution of VGG-16. • (8) Ours V (5 × 5, 5 × 5): This variant has the same configuration as Ours G (5 × 5, 5 × 5) while the features are extracted from the last convolution of VGG-16. • (9) Ours G (5 × 5, 5 × 5): This variant has the same configuration as Ours G (5 × 5, 5 × 5) whereas the attention effect is muted by fixing all attention weights to an equal value of 1/K 2 = 1/49.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: When to focus: Temporal selection with soft attention over frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Attention over time on different persons from iLIDS-VID (the upper two rows) and PRID2011 (the last row). Each row shows six original frames, a high-response feature map, and corresponding attention maps. Our method embeds attention into spatial local correlation to focus on distinct parts that help recognize the person in dramatic visual changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: CMC curves on the iLIDS-VID and PRID2011 datasets. Rank-1 matching rate is marked after the name of each approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: The showcase of failure examples from the iLIDS-VID dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Notations and definitions.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>X a , X b</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Details on architecture variants.</figDesc><table /><note><p><p><p>* denotes G (GoogLeNet) or V (VGG 16).</p>Ours * (9 × 9, 1 × 1)</p>Ours * (1 × 1, 9 × 9) Ours * (5 × 5, 5 × 5)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Citation information: DOI 10.1109/TMM.2018.2877886, IEEE Transactions on Multimedia 9 Evaluation on architecture variants.</figDesc><table><row><cell>Dataset</cell><cell>iLIDS-VID</cell><cell>PRID2011</cell><cell>MARS</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Evaluation on different feature pooling schemes on top of convolutional GRUs.</figDesc><table><row><cell>Dataset</cell><cell>iLIDS-VID</cell><cell>PRID2011</cell><cell>MARS</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2877886, IEEE Transactions on Multimedia</figDesc><table><row><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Comparison with different feature representations.</figDesc><table><row><cell>Dataset</cell><cell>iLIDS-VID</cell><cell>PRID2011</cell><cell>MARS</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>RankSVM<ref type="bibr" target="#b67">[68]</ref>+(Color,LBP): Color and Local Binary Pattern features are combined and averaged over each frame, then RankSVM is employed as distance metric.• PaMM<ref type="bibr" target="#b39">[40]</ref>: This Pose-aware Multi-shot Matching robustly estimates poses and conducts multi-shot matching. • RFA-net<ref type="bibr" target="#b21">[22]</ref>: A progressive fusion framework based on LSTM aggregates the frame-wise human region representation and yields a sequence level feature representation.</figDesc><table><row><cell>HOG3D [23]</cell><cell>8.3</cell><cell>28.7</cell><cell>38.3</cell><cell>60.7</cell><cell>20.7</cell><cell>44.5</cell><cell>57.1</cell><cell>76.8</cell><cell>2.6</cell><cell>9.9</cell><cell>12.4</cell><cell>20.4</cell></row><row><cell>FV2D [67]</cell><cell>18.2</cell><cell>35.6</cell><cell>49.2</cell><cell>63.8</cell><cell>33.6</cell><cell>64.0</cell><cell>76.3</cell><cell>86.0</cell><cell>9.7</cell><cell>19.8</cell><cell>33.5</cell><cell>43.5</cell></row><row><cell>FV3D [20]</cell><cell>25.3</cell><cell>54.0</cell><cell>68.3</cell><cell>87.3</cell><cell>38.7</cell><cell>71.0</cell><cell>80.6</cell><cell>90.3</cell><cell>13.2</cell><cell>24.3</cell><cell>38.8</cell><cell>52.4</cell></row><row><cell>STFV3D [20]</cell><cell>37.0</cell><cell>64.3</cell><cell>77.0</cell><cell>86.9</cell><cell>42.1</cell><cell>71.9</cell><cell>84.4</cell><cell>91.6</cell><cell>15.4</cell><cell>28.0</cell><cell>40.3</cell><cell>55.0</cell></row><row><cell>Ours</cell><cell>61.2</cell><cell>80.7</cell><cell>90.3</cell><cell>97.3</cell><cell>74.8</cell><cell>92.6</cell><cell>97.7</cell><cell>98.6</cell><cell>69.7</cell><cell>83.4</cell><cell>88.3</cell><cell>96.6</cell></row><row><cell cols="6">• SDALF [14]: For each subject, local features includ-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ing HSV histograms and colors are accumulated and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">weighted to form comparable features.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">• eSDC [3]: Patch-level dense distinctive features.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>• • PAM [69]: It produces a signature representation, namely</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison with multi-shot methods.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">iLIDS-VID</cell><cell></cell><cell></cell><cell cols="2">PRID2011</cell><cell></cell><cell></cell><cell cols="2">MARS</cell><cell></cell></row><row><cell>Rank @ R</cell><cell>R = 1</cell><cell>R = 5</cell><cell>R = 10</cell><cell>R = 20</cell><cell>R = 1</cell><cell>R = 5</cell><cell>R = 10</cell><cell>R = 20</cell><cell>R = 1</cell><cell>R = 5</cell><cell>R = 10</cell><cell>R = 20</cell></row><row><cell>SDALF [14]</cell><cell>6.3</cell><cell>18.8</cell><cell>27.1</cell><cell>37.3</cell><cell>5.2</cell><cell>20.7</cell><cell>32.0</cell><cell>47.9</cell><cell>8.9</cell><cell>32.1</cell><cell>40.3</cell><cell>56.7</cell></row><row><cell>eSDC [3]</cell><cell>10.2</cell><cell>24.8</cell><cell>35.5</cell><cell>52.9</cell><cell>25.8</cell><cell>43.6</cell><cell>52.6</cell><cell>62.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RankSVM [68]</cell><cell>23.2</cell><cell>44.2</cell><cell>54.1</cell><cell>68.8</cell><cell>34.3</cell><cell>56.0</cell><cell>65.5</cell><cell>77.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RFA-net [22]</cell><cell>49.3</cell><cell>76.8</cell><cell>85.3</cell><cell>90.0</cell><cell>58.2</cell><cell>85.8</cell><cell>93.4</cell><cell>97.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PaMM [40]</cell><cell>30.3</cell><cell>56.3</cell><cell>70.3</cell><cell>82.7</cell><cell>56.5</cell><cell>85.7</cell><cell>96.3</cell><cell>97.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAM [69]</cell><cell>33.3</cell><cell>57.8</cell><cell>68.5</cell><cell>80.5</cell><cell>70.6</cell><cell>90.2</cell><cell>94.6</cell><cell>97.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>S-LSTM [28]</cell><cell>52.4</cell><cell>74.2</cell><cell>86.5</cell><cell>92.1</cell><cell>63.9</cell><cell>87.8</cell><cell>94.8</cell><cell>97.4</cell><cell>57.6</cell><cell>79.3</cell><cell>84.1</cell><cell>93.7</cell></row><row><cell>Ours</cell><cell>61.2</cell><cell>80.7</cell><cell>90.3</cell><cell>97.3</cell><cell>74.8</cell><cell>92.6</cell><cell>97.7</cell><cell>98.6</cell><cell>69.7</cell><cell>83.4</cell><cell>88.3</cell><cell>96.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison with state-of-the-art methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>VIII: Cross-dataset testing on iLIDS-VID and PRID2011.Training is on the MARS dataset.</figDesc><table><row><cell></cell><cell>19]</cell><cell></cell><cell>23.3</cell><cell>42.4</cell><cell>55.3</cell><cell>68.4</cell><cell>28.9</cell><cell>55.3</cell><cell>65.5</cell><cell>82.8</cell><cell>12.4</cell><cell>33.2</cell><cell>54.7</cell><cell>71.8</cell></row><row><cell cols="3">STFV3D [20] + LFDA [4]</cell><cell>38.3</cell><cell>70.1</cell><cell>83.4</cell><cell>90.2</cell><cell>48.1</cell><cell>81.2</cell><cell>85.7</cell><cell>90.1</cell><cell>20.1</cell><cell>31.2</cell><cell>44.1</cell><cell>57.7</cell></row><row><cell cols="3">STFV3D [20] + KISSME [33]</cell><cell>44.3</cell><cell>71.7</cell><cell>83.7</cell><cell>91.7</cell><cell>64.1</cell><cell>87.3</cell><cell>89.9</cell><cell>92.0</cell><cell>22.0</cell><cell>33.4</cell><cell>44.8</cell><cell>59.0</cell></row><row><cell>TDL [25]</cell><cell></cell><cell></cell><cell>56.3</cell><cell>87.6</cell><cell>95.6</cell><cell>98.3</cell><cell>56.7</cell><cell>80.0</cell><cell>87.6</cell><cell>93.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RFA-net [22]</cell><cell></cell><cell>49.3</cell><cell>76.8</cell><cell>85.3</cell><cell>90.0</cell><cell>58.2</cell><cell>85.8</cell><cell>93.4</cell><cell>97.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SI 2 DL [24]</cell><cell></cell><cell>48.7</cell><cell>81.1</cell><cell>89.2</cell><cell>97.3</cell><cell>76.7</cell><cell>95.6</cell><cell>96.7</cell><cell>98.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PaMM [40]</cell><cell></cell><cell></cell><cell>30.3</cell><cell>56.3</cell><cell>70.3</cell><cell>82.7</cell><cell>56.5</cell><cell>85.7</cell><cell>96.3</cell><cell>97.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TS-DTW [21]</cell><cell></cell><cell>31.5</cell><cell>62.1</cell><cell>72.8</cell><cell>82.4</cell><cell>41.7</cell><cell>67.1</cell><cell>79.4</cell><cell>90.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RCN [18]</cell><cell></cell><cell></cell><cell>58.0</cell><cell>84.0</cell><cell>91.0</cell><cell>96.0</cell><cell>70.0</cell><cell>90.0</cell><cell>95.0</cell><cell>97.0</cell><cell>54.7</cell><cell>79.1</cell><cell>83.6</cell><cell>88.4</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>61.2</cell><cell>80.7</cell><cell>90.3</cell><cell>97.3</cell><cell>74.8</cell><cell>92.6</cell><cell>97.7</cell><cell>98.6</cell><cell>69.7</cell><cell>83.4</cell><cell>88.3</cell><cell>96.6</cell></row><row><cell cols="2">Ours + LFDA [4]</cell><cell></cell><cell>61.4</cell><cell>83.4</cell><cell>91.0</cell><cell>97.9</cell><cell>75.7</cell><cell>94.8</cell><cell>98.3</cell><cell>98.8</cell><cell>71.3</cell><cell>85.1</cell><cell>89.5</cell><cell>96.9</cell></row><row><cell cols="2">Ours + KISSME [33]</cell><cell></cell><cell>61.9</cell><cell>86.8</cell><cell>94.7</cell><cell>98.6</cell><cell>77.0</cell><cell>96.4</cell><cell>99.2</cell><cell>99.4</cell><cell>73.5</cell><cell>85.0</cell><cell>89.5</cell><cell>97.5</cell></row><row><cell>Dataset</cell><cell cols="2">iLIDS-VID</cell><cell></cell><cell></cell><cell>PRID2011</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank @ R</cell><cell>R = 1</cell><cell>R = 10</cell><cell>R = 20</cell><cell>R = 1</cell><cell>R = 10</cell><cell>R = 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>49.2</cell><cell>87.6</cell><cell>92.1</cell><cell>61.4</cell><cell>91.2</cell><cell>94.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCN [18]</cell><cell>38.1</cell><cell>84.7</cell><cell>90.9</cell><cell>42.0</cell><cell>84.6</cell><cell>86.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S-LSTM [28]</cell><cell>30.6</cell><cell>74.8</cell><cell>83.3</cell><cell>37.0</cell><cell>81.4</cell><cell>84.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGE</head><p>This work is partially supported by ARC DP 160104075. Junbin Gao's research is partially supported by Australian Research Council Discovery Projects funding scheme (Project No. DP140102270) and the University of Sydney Business School ARC Bridging Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identfiation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep adaptive feature embedding with local sample distributions for person reidentification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="275" to="288" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What-and-where to match: Deep spatially multiplicative integration networks for person reidentification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="727" to="738" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video-based human movement analysis and its application to surveillance systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="384" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clothes co-parsing via joint image segmentation and labeling with application to clothing retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1175" to="1186" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personal clothing retrieval on photo collections by color and attributes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Tretter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2035" to="2045" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective multi-query expansions:collaborative deep networks for robust landmark retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1393" to="1404" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Rincon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A spatiotemporal appearance representation for video-based person reidentification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised video matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification via recurrent feature aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video-based person reidentification by simultaneously learning intra-video and intervideo distance metrics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Top-push video-based person re-identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representation using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01609</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep attention-based spatially recursive networks for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep ranking for reidentification via joint representation learning</title>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2353" to="2367" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pop: Person reidentification post-rank optimisation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminant context information analysis for post-ranking person re-identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1650" to="1665" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving person re-identification via pose-aware multi-shot matching</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person re-identification via unsupervised transfer of learned visual representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Distributed Smart Cameras</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernelized saliency-based person re-identification through multiple metric learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5645" to="5658" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1421.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person reidentification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gmmcp tracker: globally optimal generalized maximum multi clique problem for multiple object tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Modiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Theano: New features and speed improvements</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning and unsupervised learning Advances in Neural Information Processing Systems workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Data augmentation for reducing dataset bias in person re-identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Rincon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coursera Course: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vector for person reidentification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Person reidentification by support vector ranking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-shot person re-identification using part appearance mixture</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
