<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition from Arbitrary Views using 3D Exemplars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Weinland</surname></persName>
							<email>weinland@inrialpes.fr</email>
						</author>
						<author>
							<persName><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
							<email>eboyer@inrialpes.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK -INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LJK -INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Remi Ronfard Artificialife Inc</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition from Arbitrary Views using 3D Exemplars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9AC6687543ED3F4DDE177505F9CA654</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of learning compact, view-independent, realistic 3D models of human actions recorded with multiple cameras, for the purpose of recognizing those same actions from a single or few cameras, without prior knowledge about the relative orientations between the cameras and the subjects. To this aim, we propose a new framework where we model actions using three dimensional occupancy grids, built from multiple viewpoints, in an exemplar-based HMM. The novelty is, that a 3D reconstruction is not required during the recognition phase, instead learned 3D exemplars are used to produce 2D image information that is compared to the observations. Parameters that describe image projections are added as latent variables in the recognition process. In addition, the temporal Markov dependency applied to view parameters allows them to evolve during recognition as with a smoothly moving camera. The effectiveness of the framework is demonstrated with experiments on real datasets and with challenging recognition scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the problem of recognizing actions using a priori unknown camera configurations. Action recognition has received considerable attention over the past decades, as a result of the growing interest for automatic and advanced scene interpretations shown in several applications domains, e.g. video-surveillance or human machine interactions. In this field, two main directions have been followed. Model based approaches, e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> assume a known parametric model, typically a kinematic model, and represent actions in a joint or parameter space. Unfortunately, recovering the parameters, e.g. the pose, of the model appears to be a difficult intermediate task without the help of landmarks.</p><p>In contrast, template based or holistic approaches, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>, do not use such an intermediate representation and directly model actions using image information, silhouettes or optical flow for instance. Action templates are then spatio-temporal shapes either in a three-dimensional space, when a single camera is considered, or in a four dimensional space when multiple calibrated cameras are considered. In both cases, action recognition is achieved by comparing a motion template, built from observations, with learned models of the same type. This limits recognition to situations where observed and learned models are obtained using similar camera configurations.</p><p>In this work, we propose an approach that takes advantage of the template based methods but that does not constrain camera configurations during recognition. Instead, actions can be observed with any camera configuration, from single to multiple cameras, and from any viewpoint. Our main motivation is to be able to cope with unknown recognition scenarios without learning multiple and specific databases. This has particularly clear applications in videosurveillance where actions are often observed from a single and arbitrary viewpoint.</p><p>To this purpose, we propose an exemplar-based hidden Markov model (HMM) inspired by the works of Frey and Jojic <ref type="bibr" target="#b8">[9]</ref> and Toyama and Blake <ref type="bibr" target="#b17">[18]</ref>. This model accounts for dependencies between three dimensional exemplars, i.e. representative pose instances, and image cues, this over time sequences. Inference is then used to identify the action sequence that best explains the image observations. In particular, a nice feature is that observations from any calibrated view can be incorporated. In addition, explicitly modeling the transformation between exemplars and image cues allows such transformation to change over time during recognition.</p><p>The paper proceeds as follows. In Section 2 we review the state of the art in view-independent action recognition. In Section 3 we present an overview of the proposed approach. Details on the exemplar-based HMM design are given in Section 4. In Section 5 the exemplar selection and the model learning are explained. Section 6 details recogni-978-1-4244-1631-8/07/$25.00 ©2007 IEEE tion. Experiments using a challenging dataset of 11 actions are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In order to allow actions to be learned and recognized using different camera configurations, action descriptions must exhibit some view invariance. Campbell <ref type="bibr" target="#b4">[5]</ref> describes 3D hand and head trajectories using view invariant coordinate representations. Fundamental matrices can also be used to compare 2D action representations from different views, as joint trajectories in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref> or silhouettes in <ref type="bibr" target="#b16">[17]</ref>. To achieve similar comparisons, Parameswaran and Chellappa <ref type="bibr" target="#b13">[14]</ref> use projective invariants of coplanar landmark points on a human body. In a previous work <ref type="bibr" target="#b18">[19]</ref> we compare 3D action representations based on visual hulls and propose invariant Fourier-descriptors that are computed from multiple-view reconstructions. These approaches have focused on representations in which view dependent information is removed, often at the cost of an impoverished action model and without adding full flexibility in camera configurations. This motivates the search for another solution.</p><p>In a different context, Frey and Jojic <ref type="bibr" target="#b8">[9]</ref> show how to account for view transformations in a dynamic probabilistic model. In the same spirit, Toyama and Blake <ref type="bibr" target="#b17">[18]</ref> extend the idea for tracking with powerful image distances, and Elgammal et al. <ref type="bibr" target="#b7">[8]</ref> propose a nonparametric mixture extension that, however, applies to view-dependent action recognition. Our approach builds on a similar model and incorporates geometric transformations into the probabilistic modeling of an action.</p><p>It is worth to mention also the work of <ref type="bibr">Brand[4]</ref> that uses HMMs and a direct mapping between a three dimensional joint space and silhouette observations for pose estimation. It shares some similarities with our approach since we also use HMMs to model temporal sequences of exemplars.</p><p>A very recent and interesting work is that of Lv and Nevatia <ref type="bibr" target="#b11">[12]</ref>. Developed in parallel to our method, it shares the idea of projecting a set of learned 3D exemplars/keyposes into 2D to infer actions from arbitrary view. However we use a probabilistic model instead of the deterministic linked action graph introduced in <ref type="bibr" target="#b11">[12]</ref>, allowing therefore to naturally handle uncertainties inherent to actions performed by different people and with different styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>We model an action as a sequence over a set of keyposes, the exemplars. Figure <ref type="figure" target="#fig_0">1</ref> shows two examples of observation sequences and the corresponding best matching exemplar sequences computed with our model.</p><p>Exemplars are represented in 3D as visual hulls that have been computed using a system of 5 calibrated cameras. The model does thus not rely on motion capture data, which is generally difficult to obtain.</p><p>The observation sequence comes in this example from a single camera and is represented trough silhouettes obtained from background subtraction. To match observation and exemplars, the visual hulls are projected into 2D and a match between the resulting silhouettes is computed. The recognition phase thus generates 2D from 3D and never has to infer 3D from a single view observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling actions and views</head><p>The matching between model and observation is represented in a probabilistic framework (Section 4). Consequently, and crucially, that neither the best matching exemplar sequence, nor the exact projection parameters need to be known. Instead a probability of all potential exemplar sequence and projection is computed. Using the classical HMM algorithms <ref type="bibr" target="#b14">[15]</ref>, such a probability can be efficiently computed under the following conditions: First, we use a small set of exemplars that is shared by all models. As we show in Section 5.1, a small set of exemplars is sufficient to describe a large variety of actions, if the exemplars are discriminative with respect to these actions. Second, we make a few reasonable assumptions on the parameters of the projective transformation, i.e. the camera calibration and position of a person can be robustly observed during recognition and only the orientation of a person around the vertical axis is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplar selection and model learning Learning an action model consists of two steps:</head><p>A set of exemplars is selected and shared by all actions models (Section 5.1); probabilities over these exemplars are learned individually for each action (Section 5.2).</p><p>When selecting the exemplars, we are interested in finding the subset of poses from the training sequences, that bests discriminates actions. To this purpose, we present in Section 5.1 a novel solution based on a method for feature subset selection, a wrapper <ref type="bibr" target="#b10">[11]</ref>.</p><p>Given a set of exemplars, the action specific probabilities are estimated using standard probability estimation techniques for HMMs, as described in Section 5.2. Interestingly, the learning of dynamics over a set of selected 3D exemplars can be performed either on 3D sequences of aligned visually hulls (Section 5.2.1), thus under ideal conditions, or simply from single view observations (Section 5.2.2). Hence 3D information is not mandatory for that step.</p><p>Classification Classification is performed using standard HMM algorithms, as described in Section 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic Model of Actions and Views</head><p>Our representation for human action is a product of two independent random processes, one for the orientation of the subject relative to the camera, and the other for the viewindependent, body-centered poses taken by the performer during the various stages of the action. The two processes are modeled in an exemplar based Markov model, shown in Figure <ref type="figure">2</ref>, in the spirit of <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b17">[18]</ref>.</p><p>Hidden Motion States Dynamics in exemplar space are represented by a discrete N -state latent variable q that follows a first order Markov chain over time. Thus: p(q t |q t-1 , . . . , q 1 ) = p(q t |q t-1 ), with t ∈ [1 . . . T ], and with the prior p(q 1 ) at time t = 1. Though generally hidden, q can intuitively be interpreted as a quantization of the joint motion space into action-characteristic configurations.</p><p>Exemplars At each time t, a three dimensional body template x t is drawn from p(x t |q t ). A crucial remark here is that these templates do not result from body models and joint configurations but are instead represented by a set of M exemplars: X = {x i∈[1...M ] }, learned from three dimensional training sequences.</p><p>Note here that p(x t = x i |q t ) models the nondeterministic dependencies between motion states and body configuration. Thus motion states q are not deterministically linked to exemplars as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>, allowing therefore a single motion state q to be represented with different exemplars, to account for different body proportions, style, or clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>View Transformation and Observation</head><p>To ensure independence with respect to the view projection onto the image plane: P ll (x) = P [R θ , u]x, we condition observations y on parameters that represent this transformation. We differentiate view transformation parameters { lt } that can be robustly observed (i.e. the camera matrix P and position u), and body pose parameters { lt } that are latent (i.e. the orientation around the vertical axis θ).</p><p>The resulting density p(y t |x t , lt , lt ) is represented in form of a kernel function centered on the transformed exemplars P ll (x i ):</p><formula xml:id="formula_0">p(y t |x t = x i , lt , lt ) ∝ 1 Z exp -d(y t , P ll (x i ))/σ 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where d is a distance function between between the resulting silhouettes, e.g. the Euclidean distance (i.e. the number of pixels which are different), or a more specialized distance such as the chamfer distance <ref type="bibr" target="#b9">[10]</ref>. (Note that both were giving similar results in our experiments.)</p><p>The temporal evolution of the latent transformation variables is modeled as a Markov process with transitions probabilities p( lt | lt-1 ), and a prior p( l1 ). This is equivalent to a temporal filtering of the transformation parameters where, interestingly, various assumptions could be made on the dynamic of these parameters: a static model or an autoregres-sive model, or even a model taking into account dependencies between an action and view changes.</p><p>In our implementation all variables { l, l} are discretized. For instance, the orientation θ is discretized into L equally spaced angles within [0, 2π] and u is discretized into a set of discrete positions. The temporal evolution of θ is modeled using a von Mises distribution: p(θ t |θ t-1 ) ∝ exp(κ cos(θ t -θ t-1 )), that can be seen as the circular equivalent of a normal distribution, and a uniform prior p(θ 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning</head><p>We learn separate action models λ c for each action class c ∈ {1, . . . , C}. A sequence of observations Y = {y 1 , . . . , y T } is then classified with respect to the maximum a posteriori (MAP) estimate:</p><formula xml:id="formula_2">g(Y ) = arg max c p(Y |λ c )p(λ c ).<label>(2)</label></formula><p>The set λ c is composed of the probability transition matrices p(q t |q t-1 , c), p(q 1 |c) and p(x t |q t , c), which are specific to the action c, as they represent the action's dynamics. In contrast, the observation probabilities p(y t |x t , lt , lt ) are tied between classes, meaning that all actions {c = 1..C} share a common exemplar set, i.e. X c = X, and a unique variance</p><formula xml:id="formula_3">σ 2 c = σ 2 .</formula><p>In the context of HMMs, such an architecture is known as a tied-mixture or semi-continuous HMM <ref type="bibr" target="#b0">[1]</ref>. This architecture is particularly well adapted to action recognition since different actions naturally share similar poses. For example, many actions share a neutral rest position and some actions only differ by the sequential order of poses that composed them. In addition, sharing parameters dramatically reduces complexity during recognition, when every exemplar must be projected with respect to numerous latent orientations.</p><p>Learning consists then in two main operations: selecting the exemplar set that is shared by all models; learning the action specific probabilities. As we will see in the following, the two operations are tightly coupled. Selection uses learning to evaluate the discriminant quality of an candidate exemplar set, and learning probabilities relies on a selected set of exemplars. Both operations are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Exemplar Selection</head><p>Identifying discriminative exemplars is an essential step of the learning process. Previous works use motion energy minima and maxima <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, or k-means clustering (adapted to return exemplars) <ref type="bibr" target="#b17">[18]</ref> to this end. However, there is no apparent relationship between such criteria and the action discriminant quality of the selected exemplars. In particular for the adapted k-means clustering <ref type="bibr" target="#b17">[18]</ref> we observed experimentally, that clusters tend to consist of different poses performed by similar actors rather than similar poses performed by different actors. Consequently, selecting exemplars as poses with minimum within-cluster distance often leads to neutral and therefore non-discriminative poses.</p><p>In light of this, we propose a novel approach for exemplar selection, to better link the discriminant quality of exemplars and the selection. We therefore use a wrapper <ref type="bibr" target="#b10">[11]</ref>, a technique for discriminant feature subset selection. The idea behind a wrapper is to use the trained classifier (2) itself to evaluate how discriminative a candidate set of exemplars is. Thus a wrapper performs a greedy search over the full set of exemplars, where in each iteration classifiers are learned and evaluated for each possible subset considered.</p><p>The wrapper method we use is called "forward selection" <ref type="bibr" target="#b10">[11]</ref>, and proceeds as follows: Let Y denote a set of 3D visual hulls. Assume training sequences and test sequences for all actions c ∈ {1, . . . , C} are given.</p><p>1. Set X = ∅.</p><p>2. Find y * ∈ {Y \ X}, where a classifier g (trained on all actions) using exemplar set {X ∪ y * } has best recognition performance on the test-set. Add y * to X.</p><p>3. Repeat step 2 until M visual hulls from Y have been added to X.</p><p>Note that the above procedure can only work when the exemplar set is shared by all action models. The selection thus starts by training a classifier for each singleton exemplar. The exemplar for which the classifier has best evaluation performance is selected, and the procedure is repeated for couples of exemplars, triples, etc., until M exemplars have been selected. Note that training and evaluation of the classifier can be performed in 3D or 2D, as detailed in Section 5.2. In case that the training sequences are 3D, Y can simply be the training-set.</p><p>The approach is illustrated in Figures <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> where exemplars and the associated classification rates are shown. Figure <ref type="figure" target="#fig_2">3</ref> shows that the selected poses naturally represent key-frames or characteristic frames of an action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning Dynamics</head><p>Given a set of exemplars, the action parameters λ c∈{1,...,C} :</p><p>probabilities p(q t |q t-1 , c), p(q 1 |c) and p(x t |q t , c), can be learned. Various strategies can be considered for that purpose. In the following, we sketch 2 of them: learning from 3D observations (sequences of visual hulls), and learning from 2D observations (image sequences). Note that in both cases, motion is learned in 3D over the set of 3D exemplars, obtained as described in section 5.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Learning from 3D Observations</head><p>In this training scenario, several calibrated viewpoints are available, leading therefore to 3D visual hull sequences, and all actions are performed with the same orientation. In that case, motion dynamics are learned independently from any viewing transformation, thus p(y t |x t , lt , lt ) = p(y t |x t ) with y being 3D. Transformation parameters appear later during the recognition phase where both dynamics and viewing process are joined into a single model.</p><p>Each model λ c is learned through a forward-backward algorithm that is similar to the standard algorithm for Gaussian mixture HMMs <ref type="bibr" target="#b14">[15]</ref>, except that the kernel parameters, that correspond to mean and variance of the Gaussians (i.e. X and σ), are not updated. Note that a similar forwardbackward algorithm was already proposed in the context of exemplar based HMMs <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Learning from 2D Observations</head><p>In this scenario, dynamics in the exemplar 3D space are learned using 2D cues only. In that case, the situation is similar when either learning or recognizing. A nice feature here is that only a valid set of 3D exemplars is required, but no additional 3D reconstruction. This is particularly useful when large amounts of 2D observations are available but no 3D inference capabilities (e.g. 3D exemplars can be synthesized using a modeling software; the dynamics over these exemplars are learned form real observations).</p><p>View observations are not aligned and so the orientation variable l is latent. Nevertheless, the number of latent states remains in practice small, (i.e. L×N , with L being the number of discrete orientations l and N the number of states q). The model can be learned by introducing a new variable q = (q, l) of size L × N that encodes both state and orientation. Probabilities of this extended states are then simply defined as Cartesian products of the transition probabilities for q and l. Loops in the model are thus eliminated, and learning can be performed via the forward-backward algorithm introduced in 5.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Action Recognition from 2D Cues</head><p>A sequence of observations Y is classified using the MAP estimate <ref type="bibr" target="#b1">(2)</ref>. Such a probability can now be computed using the classical forward variable α(q t |λ c ) = p(y 1 , . . . , y t , qt |λ c ) as explained in <ref type="bibr" target="#b14">[15]</ref>, where q = (q, l) is a variable encoding state and orientation as explained in Section 5.2.2 Arbitrary viewpoints do not share similar parameters; in particular scales and metrics can be different. However, the kernel parameter σ 2 is uniquely defined, with the consequence that distances computed in equation ( <ref type="formula" target="#formula_0">1</ref>) can be inconsistent when changing the viewpoint. To adjust σ 2 with respect to changes in these parameters, we introduce σ 2 l = s lσ 2 . Ideally, σ 2 l should be estimated using test data. In practice, the following simple approximation of σ 2 l appears to give satisfactory results with the distance functions we are considering:</p><formula xml:id="formula_4">s l = 1 M M i=1 1 L L l=1 ||P ll (x i )|| 2 ||x i || 2 . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Another remark is that observations from multiple calibrated cameras can easily be incorporated. Assuming multiple view observations {y </p><p>Figure <ref type="figure">5</ref>. Camera setup and extracted silhouettes: (Top) the action "watch clock" from the 5 different camera views. (Middle and bottom) sample actions: "cross arms", "scratch head", "sit down", "get up", "turn", "walk", "wave", "punch", "kick", and "pick up". Volumetric exemplars are mapped onto the estimated interest regions indicated by blue box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>Experiments were conducted on our publicly available dataset 1 , the IXMAS dataset. We choose 11 actions, performed by 10 actors, each 3 times, and viewed by 5 calibrated cameras (see Figure <ref type="figure">5</ref>). In this dataset, actor orientations are arbitrary since no specific instruction was given during the acquisition. The 3D sequences are segmented into elementary segments using our approach proposed in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Note, that the same dataset was used in <ref type="bibr" target="#b11">[12]</ref> in a similar context. However, results are reported only for a single sequence (out of three) per actor. This sequence has been selected to give best results, thus making a direct comparison difficult.</p><p>Our experimental scheme is as follows: 9 of the actors are used for exemplar selection and model learning, the remaining actor is then used for testing. We repeat this procedure by permuting the test-actor and compute the average recognition rate. Examplar selection is performed on subsampled sequences (i.e. 2.5 frames/s) to save computational costs. Example results for exemplars are shown in Figure <ref type="figure" target="#fig_2">3</ref>. The number M of examplars was empirically set to 52 . Parameter learning and testing is performed using all frames in the database. Action are modeled with 2 states, which appears to be adequate since most segmented actions cover short time periods. Voxel grids are of size: 64 × 64 × 64 and image ROIs: 64 × 64. The rotation around the vertical axis is discretized into 64 equally spaced values. Consequently, each frame is matched to 52×64 exemplar projections. The ground plane is clustered into 4 positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Learning in 3D</head><p>In these experiments, learning is performed in 3D (as explained in 5.2.1). Recognition is then performed on 2D views with arbitrary actor orientations. Recognition rates 1 The data-set is available on the Perception website http://perception.inrialpes.fr in the "Data" section.  per camera are given in Figure <ref type="figure" target="#fig_5">6</ref>(a), the corresponding views are shown in Figure <ref type="figure">5</ref>. Unsurprisingly, the best recognition rates are obtained with fronto-parallel views (cameras 2 and 4). The top camera (camera 5) scores worst. For this camera, we observe that: the silhouette information is not discriminative; the perspective distortion results in strong bias in distances; estimating the position of the actor is difficult. All these having a strong impact on the recognition performance.</p><p>In the next experiment, several views were used in conjunction to test camera combinations. First, 2 view combinations were experimented. Camera 2 and 4 give the best recognition rate at 81.27%. Those 2 cameras are both approximately fronto-parallel and perpendicular one another. Figure <ref type="figure" target="#fig_5">6</ref>(b) shows the resulting confusion matrix for this specific setup. Adding further cameras did not improve results. We also try other camera combinations (Table <ref type="table" target="#tab_1">1</ref>). For instance, combining the two cameras with the worst recognition results (camera 3 and 5) raises the recognition rate to 61.59%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Learning from single views</head><p>In this experiment, learning is performed using single cameras (as explained in Section 5.2.2). Observations during learning and recognition are thus not aligned. The exemplars considered are the same than in the previous section. Learning from a single view is obviously prone to ambiguities, especially when the number of training samples is limited. We thus restricted the experiments to the 3 best cameras with respect to the previous experiments. .86 .00 .00 .00 .00 .07 .03 .03 .00 .00 .00</p><p>.13 .73 .00 .00 .00 .03 .00 .03 .07 .00 .00</p><p>.00 .09 .68 .00 .00 .00 .00 .09 .09 .05 .00</p><p>.00 .00 .00 .93 .07 .00 .00 .00 .00 .00 .00</p><p>.00 .00 .00 .00 .93 .07 .00 .00 .00 .00 .00</p><p>.00 .00 .00 .00 .00 .97 .00 .03 .00 .00 .00</p><p>.00 .00 .00 .00 .00 .33 .67 .00 .00 .00 .00</p><p>.04 .04 .27 .04 .04 .00 .00 .50 .08 .00 .00</p><p>.00 .00 .00 .04 .00 .04 .00 .00 .82 .00 .11</p><p>.00 .00 .00 .00 .00 .07 .00 .00 .00 .90 .03</p><p>.00 .00 .00 .10 .00 .00 .00 .00 .03 .00 .87 and per camera. Compared to the previous scenario, recognition rates drop drastically, as a consequence of learning from non-aligned data and single view observations. Surprisingly, some of the actions, e.g. "cross arms", "kick" still get very acceptable recognition rates, as well as "sit down" and "pick up" that would normally be confused. The average rate for camera 1 is 55.24%, 63.49% for camera 2 and 60.00% for camera 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper presented a new framework for view independent action recognition. The main contribution is a probabilistic 3D exemplar model that can generate arbitrary 2D view observations. It results in a versatile recognition method that adapts to various camera configurations. The approach was evaluated on a dataset of 11 actions and with different challenging scenarios. The best results where obtained with a pair of fronto-parallel perpendicular cameras, validating the fact that actions can be recognized from view arbitrary viewpoints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. 2D observation sequences yt ("walk in cycle" and "punch"), observed from different viewpoints and with unknown orientation of the persons, are explained trough 3D action models. The best matching exemplar sequence xt and the best matching 2D projection P ll (xi), as generated by the models, are displayed. Both models share a small set of exemplars (labeled on top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>q t- 1 x t- 1 yFigure 2 .</head><label>112</label><figDesc>Figure 2. Probabilistic dependencies of actions: an action is modeled as a hidden state sequence Q, e.g. a motion sequence in a pose space. At each time step t, a 3D exemplar xt, i.e. a visual hull, is drawn from the motion sequence Q. Observations yt, i.e. silhouettes, result then from a geometric transformation of exemplars that is defined by 2 sets of parameters l and l. l are observed parameters, e.g. camera parameters determined in a preliminary step, and l are latent parameters, e.g. body orientation determined during recognition. Shaded nodes in the graph correspond to observed variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Selected exemplars: first 24 discriminative exemplars as returned by the forward selection. The dataset is composed of 11 actions performed by 10 actors. Recognition rates are shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Recognition rate vs. number of selected exemplars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>c) shows the recognition results per action class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) Recognition rates when learning in 3D and recognizing in 2D. The average rates per camera are {65.4, 70.0, 54.3, 66.0, 33.6}. (b) Confusion matrix for recognition using cameras 2 and 4.Note that actions performed with the hand are confused, e.g. "wave" and "scratch head" as well as "walk" and "turn". (c) Recognition rates when learning and recognizing in 2D.</figDesc><graphic coords="7,267.15,83.35,90.92,90.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Recognition rates with camera combinations. For comparisons, a full 3D recognition considering 3D manually aligned models as observations, instead of 2D silhouettes, yields 91.11%.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is supported by a grant from the European Community under the EST Marie-Curie Project Visitor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tied mixture continuous parameter modeling for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASSP</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2033" to="2045" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time recognition of activity using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shadow puppetry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1237" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Invariant features for 3-d gesture recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of human body motion using phase space constraints</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="624" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning dynamics for exemplar-based gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="571" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning graphical models of images, videos and their spatial transformations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Real-time object detection for smart vehicles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Philomin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Single view human action recognition using key pose matching and viterbi path searching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">View-invariant identification of pose sequences for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karapurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guerra-Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<editor>VACE</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">View invariance for human action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">View-invariant representation and recognition of actions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing action events from multiple viewpoints</title>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EventVideo01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic tracking in a metric space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing human actions in videos acquired by uncalibrated moving cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
