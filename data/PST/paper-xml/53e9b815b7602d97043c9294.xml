<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B636688D3EA8CF224A6B59100D0C8DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Reactive and Planning Rules in a Motivationally Autonomous Animat</head><p>Jean-Yves Donnart and Jean-Arcady Meyer</p><p>Abstract-This work describes a control architecture based on a hierarchical classifier system. This system, which learns both reactive and planning rules, implements a motivationally autonomous animat that chooses the actions it performs according to its perception of the external environment, to its physiological or internal state, to the consequences of its current behavior, and to the expected consequences of its future behavior. The adaptive faculties of this architecture are illustrated within the context of a navigation task, through various experiments with a simulated and a real robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE work presented in this paper fits into the so-called T animat approach, which aims at designing animats, i.e., simulated animals or real robots whose rules of behavior are inspired by those of animals. The proximate goal of this approach is to discover architectures or working principles that allow an animal or a robot to exhibit an adaptive behavior and, thus, to survive or fulfill its mission even in a changing environment ( <ref type="bibr" target="#b2">[13]</ref>, [35], <ref type="bibr">[36]</ref>). The ultimate goal of this approach is to embed human intelligence within an evolutionary perspective and to seek how the highest cognitive abilities of man can be related to the simplest adaptive behaviors of animals ([37], <ref type="bibr">[40]</ref>, <ref type="bibr" target="#b40">[54]</ref>).</p><p>An animat is usually equipped with sensors, with actuators, and with a control architecture that allow it to react or to respond to variations in its external or internal environment, notably to those that might impair its chances of survival. This paper describes the control architecture of a motivationally autonomous animat <ref type="bibr">([17]</ref>, <ref type="bibr">[33]</ref>, <ref type="bibr">[34]</ref>) that enhances its chances of survival by learning to do the right thing at the right time. Action selection <ref type="bibr">[31]</ref> in this animat depends upon its perception of the external environment, upon its physiological or internal state, upon the consequences of its current behavior, and upon the expected consequences of its future behavior. In other words, like an animal, such an animat is endowed with a motivational system that, according to Toates and Jensen, "selects [at every moment] a goal to be pursued and organizes [the animat's] commerce with it" <ref type="bibr" target="#b34">[48]</ref>. This system is said to be autonomous because it is very unlikely to be completely controllable and observable by an external agent.</p><p>Such a control architecture relies upon reactive and planning rules whose expected utilities in solving a given task can be Manuscript received July 1, 1994; revised March 17, 1995, April 24 1995, and <ref type="bibr">May 23, 1995.</ref> The authors are with the AnimatLab, Ecole Normale SupBrieure, 75230 Paris Cedex 05, France (e-mails: donna&amp; wotan.ens.fr, meyer@wotan.ens.fr).</p><p>Publisher Item Identifier S 1083-4419(96)03237-2.</p><p>submitted to a reinforcement learning procedure, thus allowing the animat to improve over time the way it selects its actions. An originality of this architecture is that its capability of analyzing the behavior it generates allows it to learn situated plans ([l], <ref type="bibr" target="#b31">[45]</ref>). Thus, although the animat's moment-tomoment decisions can be taken at a certain level of abstraction and depend upon a global view of the survival problem to be solved, they are also reactive to the actual state of the environment that, in turn, can modify the expected utilities of the memorized plans. Such adaptive capacities should prove to be very general and to be exhibited in a variety of future applications. They will be illustrated here within $e context of a simple navigation task through various simulations. The operational value of these capacities will then be demonstrated through equivalent experiments with a robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE CONTROL ARCHITECTURE</head><p>In order to survive in a complex environment, an animat may rely on various reinforcement signals that indicate whether or not a given action, or sequence of actions, enhances its chances of survival in given circumstances. However, although various algorithms-like Holland's bucket brigade <ref type="bibr">[24]</ref>, Sutton' s temporal difference learning <ref type="bibr" target="#b32">[46]</ref> or Watkins's Q-learning [5O]--can be used to let the animat learn how to choose a favorable action in preference to an unfavorable one in each possible circumstance, such a learning process turns out to be very slow. This is due to the fact that reinforcement signals are generally provided in a few particular environmental states and that the step by step propagation of the corresponding information to every other state through a flat organization takes a long time. Among the solutions that have been suggested to improve learning speed-like generalization procedures <ref type="bibr" target="#b1">[12]</ref> or the use of an action model <ref type="bibr" target="#b33">[47]</ref>-the most promising seem to be those that advocate a hierarchical organization of state transitions,' within which the propagation of reinforcement signals is expedited.</p><p>The hierarchical control architecture presented here is called MonaLysa2 and relies upon the same principles. However, the hierarchy it implements, instead of being fixed by the designer (1181, [27], [29], <ref type="bibr" target="#b28">[42]</ref>, <ref type="bibr" target="#b39">[53]</ref>), can be dynamically reconfigured-thus enhancing the animat' s adaptive capacities. In particular, the possibility of introducing or suppressing tasks within the current hierarchy is afforded by a mechanism 'The operators of such transitions, which will be referred to as tasks hereafter, are called behavioral modules <ref type="bibr" target="#b39">[53]</ref>, skills <ref type="bibr">[</ref> that allows the survival value of these tasks to be monitored, according to a heuristic criterion of internal satisfaction.</p><p>In the present application, the MonaLysa architecture enables a simulated robot to explore a two-dimensional environment that may contain various assorted materials, in particular some obstacles, and to navigate from a given initial place to a given goal place despite such obstacles. The robot is equipped with proximate sensors that keep it informed of the presence or absence of any obstacle in front of it, 90" to its right, or 90" to its left. It is also able to estimate the spatial coordinates of the position it is located in and the direction of a goal to be reached in each of the eight sectors of the space surrounding it. Lastly, it is capable of moving straight ahead, 90" to its right, or 90" to its left. The size of each such move is equal to the length of the robot. This architecture, which relies upon a hierarchical classi-$er system [24], is organized into five modules-a reactive module, a planning module, a context manager, an autoanalysis module and an internal reinforcement module (Fig. <ref type="figure" target="#fig_0">1</ref>). It allows the robot to reactively escape from any obstacle it gets trapped into by skirting around it, and to analyze the corresponding skirting path in order to plan a trajectory that will later allow it to avoid the obstacle from a distance.</p><p>The role of the reactive module is to decide what action to perform next. The corresponding decisions depend not only upon the robot's sensory detections, but also upon the direction of the robot's current goal. The latter is specified by the robot's current task that is posted on the context manager module either by the planning or the auto-analysis modules. Thus, the context manager contains the pile of all the tasks the animat has to perform. The role of the planning module is to use its planning rules to decompose a given task into simpler tasks. The auto-analysis module either generates a skirting task that is posted on the context manager module when an obstacle is detected, or generates avoidance tasks that are converted into planning rules sent to the planning module. The internal reinforcement module is used to monitor the internal satisfaction of the robot and to provide a reinforcement signal to the reactive and planning modules.</p><p>A. The Reactive Module robot performs at every mo rules--or classijiers-that allow the robot to sensory information from the environment, internal context specified by current configuration, the inte the current goal. The reactive rules take the form:</p><p>If (sensory infomation} and (direction of current g (action).</p><p>the robot becomes aware of the presence of a material This module is responsible for c For example, rule R: 1001001 ==+ 01 can be activated when g o d ) correspond, at any moment, three rules capable of actuated, each of which is associated with one of possible actions. The choice of which rule is actually is effected probabilistically on the basis of the strength-that will be discussed later (see Section 11-E)-of each of the three rules involved. In the case 8 * 8 * 3 = 192 possible created when the system is reactive module, which does not change in size as long as the system is in operation. The strength of each rule is initialized to a given value and is sub other experiments, yet unp has been used successfully general rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Planning Module</head><p>The planning rules it contains take the (sensory infomation) and (curre Sensory information is provided from the environment. It consists by the proximate sensors and of the robot's coordinates and the context manager and, when a planning rule is triggered, the corresponding (subtask) is put above the (current task) on the context manager's pile. Such a procedure will be ca task decomposition hereafter. of coordinates, which respectively define an initial position The (current task) and the (subtask) are each CO and a final position-i.e., a goal-to be reached. Thus, rule P: 5,1)001)000)3,0; 3,5 3 5 , l ; 5 , 2 can be activated at position 5, 1 if the robot is headed in a north-easterly direction (coded by "OOl"), if it perceives no material element ahead or on either side of it (coded by "OOO"), and if its current task is to reach the position with coordinates 3, 5 when starting at position 3, 0. If this rule is activated, the subtask that involves reaching position 5 , 2 from position 5, 1 will be placed on top of the context manager pile.</p><p>With each rule of the planning module are associated two strengths-one local, the other global-the evaluation of which will be explained later (see Section 1I.E). The local strength is used to determine the probability of triggering a rule whose condition part matches the current situation. When the system is initialized, the planning module is empty. During operation, this module can dynamically acquire rules-generated by the auto-analysis modu1e-a loose rules-according to how the global strengths of these rules evolve. The size of the planning module thus varies over time, though it cannot exceed a preestablished upper limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Context Manager</head><p>The context manager provides an internal context to the other modules and influences their inner working. This internal context is the direction of the robot's current goal in the case of the reactive module; it is the robot's current task in the case of all other modules. The context manager consists essentially of a pile of tasks, at the top of which is the system's current task. New tasks are added to the pile either by the auto-analysis module, when an obstacle is detected, or by the planning module, as just mentioned. The former allow the robot to escape from an obstacle by skirting around it, while trying to cross specific lines that characterize the obstacle. The latter allow the robot to avoid an obstacle from a distance, while passing through specific positions characterized along the obstacle.</p><p>The context manager includes an algorithm that transforms the current task into a goal, then supplies the direction of this goal to the reactive module. In the case of a task posted by the planning module, the corresponding goal is simply described by the coordinates of the final position to be reached. In the case of a task posted by the auto-analysis module, the corresponding goal is described by the coordinates of the projection of the robot's current location on the line that must be crossed to skirt around the obstacle. This projection, and accordingly the corresponding direction information, varies whenever the robot moves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Auto-Analysis Module</head><p>The role of the auto-analysis module is to analyze the current behavior of the robot in order to alter its current task dynamically and to create new tasks that will enhance its behavior in the future. In other words, the auto-analysis module is responsible for detecting obstacles, for triggering skirting behaviors and for characterizing salient states in the environment, through which it will be useful to travel in the future in order to avoid the obstacles from a distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Obstacle Detection: Within the present application, an</head><p>obstacle is any material element that prevents the execution of the best action the robot can perform in order to move in the direction of its current goal. To escape from such an obstacle, the robot must skirt around it. However, it may happen that some material elements-like a wall at the periphery of the area that is explored-can be detected in the vicinity of the robot by its proximate sensors without impeding movement in the direction of the current goal. In such a case, no specific skirting behavior needs to be triggered. Therefore, the detection of an obstacle and the triggering of the relevant skirting behavior depend upon the following procedure. When the proximate sensors detect a material element in a given position, the robot first searches which reactive rule has the greatest strength among the three rules that could be actuated in the context of the current goal's direction if no material element were actually detected. If the move that this rule would trigger cannot be executed because of the presence of the material element, this element is recognized as an obstacle at the position where that move would lead, and a subsequent skirting subtask is sent to the context manager. Recursively, if the move that corresponds to the second best rule cannot be executed, a second obstacle is recognized at the corresponding position, and so on. Whether the material element has been recognized as an obstacle or not, the robot then probabilistically chooses a rule that matches both the (sensory information) in the presence of the material element and the (direction of current goal). If the material element doesn't prevent the corresponding action from being executed, the action is executed and the strength of the rule is changed as explained in Section E. If the action cannot be executed, the strength of the rule is set to zero and other reactive rules matching the (direction of current goal) in the presence of a material element are probabilistically selected in turn, until one is found that allows motion.</p><p>The reason why obstacle detection involves two rule matching procedures, one that takes the actual (sensory information) into account and one that operates as if this information were set to "000," is that the strength of rules triggered according to the former procedure can be equal to zero-thus indicating that these rules would lead the robot in the direction of the obstacle, an action that is not suitable. A zero strength prevents the corresponding rules from being probabilistically selected for action and, if such rules cannot be executed, they cannot be used to recognize the obstacle, nor to skirt around it. Hence, the necessity of relying on the second rule matching procedure for these purposes.</p><p>2) Skirting Behavior: When an obstacle is detected, the auto-analysis module generates a subtask that specifies that the robot must cross the line that lies parallel to the direction of the move performed, and that passes through the position where the obstacle has been detected. This subtask is coded by the pair (coordinates of the place) (direction vector of the straight line to be crossed) and is registered at the top of the pile of the context manager. An emergent functionality <ref type="bibr" target="#b30">[44]</ref> of such subtasks is to enable the robot to skirt around the obstacles it encounters. For example, in the case of Fig. <ref type="figure">2</ref>(a), an obstacle is detected in front of the robot when it arrives at position 1. If the robot probabilistically chooses to turn left to try to avoid the obstacle, the task of having to cross the straight line 61 is placed on top of the current task-which corresponds to the goal direction d0-and the current goal, becomes the position marked with a "?" on the figure, in direction d l . After its move to the left, the robot arrives at position 2. The cusrent goal, with which direction d2 is associated, becomes that in Fig. <ref type="figure">2(b</ref>), but the pile of tasks doesn't change. If, at position 2, the robot chooses to move forward, it arrives at position 3, and the current goal becomes that of Fig. <ref type="figure">2(c</ref>). In this position, if the robot chooses to turn right, it reaches the current goal and the task associated with 61 is erased. Then, the robot can resume pursuing its initial goal, in direction d4 (Fig. <ref type="figure">2(d)</ref>). Following this reasoning, had the robot encountered an obstacle perpendicular to the preceding, preventing it, for example, from reaching position 3, the task of having to cross the line 62 would have been placed on top of the task associated with 61. The robot would thus have been directed successively through positions 3 and 4 (Fig. <ref type="figure">3</ref>). In position 4, it would have been able to turn right and reach position 5, where the task linked with 62 would have been erased.</p><p>Likewise, the task linked with 61 would in turn have been erased at position 9.</p><p>In some situations, it can happen that the robot, seeking to attain the current goal, erases a task placed farther down than the current task in the pile. In this case, all the tasks situated above the erased task are also erased, as they were generated for the sole purpose of ex any justification. For instance, if in the situation shown Fig. <ref type="figure">3</ref>, the robot chooses to move through it will erase the task of having to cross line 61 at pos As the task of having to cross line 62 will no more be by the necessity of crossing 61, this task will also be e from the pile of the context manager.</p><p>the robot to skirt around obstacles and to extricate itself dead-ends with arbitrarily complicated shapes.</p><p>3) Detec$on of Salient States: Another analysis modrrle is to analyze the trajecto robot in order to detect recursively the salient states the environment. To accomplish this, the module evaluates the internal satisfaction of the robot after each completed action-that is, the success with which this rule brought the robot closer to, or took it farther from, the current goal-and calculates the variation of this satisfaction between two sives actions. At positions where the corresponding gra ting this task and no longer ha It will be demonstrated later on that such manager pile contains a skirti</p><p>The recursive process execu then to the successive fictitious from the satisfactio process is thus an ill planning as a series of satisfaction states, the recursion satisfaction states discovered are rec is computed. State a can th memorizes three salient states, i, f and b, each characterized by its coordinates, by the sensory information robot at this state, and by the corresponding o robot.</p><p>The salient states are then used to genera Thus, at the conclusion of the path described 31n a purely navigational task, one could instead We prefer to refer to "salient states, more general tasks. rules P1 and P 2 that decompose path a-f into i-b a d b-f P1: 1,1)000~000~1,1; 1 , 7 1 , l ; 4,2 P2: 4,2~010~000~1,1; 1, 7==+ 4,2; 1 , 7 are created and input to the planning module. The advantage of the redundancy in the description of these rules is to make it possible to recognize the salient states, even when there are imprecisions in the coordinates, or in the sensory information, or in the orientation of the robot, or when the robot reaches one of these states with a new orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Internal Reinforcement Module</head><p>The internal reinforcement module works through a process of reinforcement which causes the strengths of the rules of the reactive and planning modules to change. Within the reactive module, this internal reinforcement takes place each time a rule is used and depends on the satisfaction of the robot, that is, on an estimation of the success with which this rule brought the robot closer to, or took it farther from, the current goal</p><formula xml:id="formula_0">S(R),+I = (1 -a) * S(R), + a * satisf where dist-goal(u) -dist-goal(u + 1) + max-dist max -dist * 2 satis f = S ( R ) , strength of rule R after U triggers.</formula><p>S(R)o, a, max-dist parameters of simulation (set respectively to 0.1, 0.1 and to the robot's length in the experiments described herein). dist-goal(u) estimated distance to the current goal. If max-dist evaluates the maximum distance by which any move can bring the robot closer or farther from its current goal, the satisfaction brought by a given move varies between 0 and 1. The strength of a given reactive rule is an evaluation of the average satisfaction brought by the move it generates and this evaluation is performed by an incremental learning procedure, which is in line with the arguments of biological already mentioned, the strength of a rule that is selected for action and that would move the robot through a material element is set to zero. Differently from the reactive module in which rules have a single associated strength, the rules of the planning module are characterized by two strengths: a local. strength and a global strength. The local strength evaluates the usefulness of decomposing a task into a subtask proposed by the rule.</p><p>The global strength, on the other hand, is used to detect and suppress the rules which are unlikely to be used by the system.</p><p>To calculate the local strength of a rule P1 that decomposes, for example, a task T-such as that of going from i to f in Fig. <ref type="figure" target="#fig_3">5</ref>-into a subtask 7'1-such as that of going from i to j-the reinforcement module evaluates the average cost of all the paths tested by the robot which enable it to reach f from i without resorting to any decomposition. In the present version of the system, this cost, denoted by AC(T), is an average of the lengths of the paths expressed in terms of the number of elementary moves they necessitate. AC(T) is evaluated incrementally .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AC(T),+l</head><formula xml:id="formula_1">= (1 -Q) * AC(T), + Q * C</formula><p>where U number of times that task T was achieved without C cost (number of moves) of the U + lth path a parameter (set to 0.1 in the experiments described</p><p>The reinforcement module also evaluates the average eost of all the paths joining i to f and using rule P1, that is, that reach j from i using any planning rule herein). The shorter the paths joining i to f and passing through j have turned out to be than the paths joining i to f by way of other paths, the stronger this strength is. For example, while following paths 1 and 2 under the control of task T in Fig. <ref type="figure" target="#fig_3">5</ref>, the robot has characterized six salient states-i, j , 5, I , m and f-and created six planning rules-P1,P2, P3, P4, P 5 and P6-detailed in Fig. <ref type="figure" target="#fig_6">6</ref>. When task T has been decomposed into the subtasks associated with these planning rules, paths 3, 4 and 5 have been followed by the robot. Tl?e lengths of paths 1 to 5 being respectively of 29, 39, 23, 27 and 23 moves, it follows that AC(T) = (29 + 39)/2 = 34: because AC(T) is the average cost of all the paths-like path 1 and path 2-that lead the robot from z to f without using any planning rule. AC(P1) is the average cost of all the paths-like path 3 and path 5-that lead the robot from z to f via j , under the control of the planning rule P1. Therefore, AC(P1) = (23+23)/2 = 23. Likewise, P 4 being a rule that decomposes T into the subtask of going from i to I , AC(P4) is the cost of path 4</p><p>and equals 27. It then follows that LS(P1) = 34/23 =1.478 and that LS(P4) = 34/27 = 1.259. Because LS(P1) turns out to be greater than LS (P4), the robot in position i is more likely to choose moving in the direction of 3 than in the directioh of 1. In other words, after having been able to reactively escape from the obstacle via paths 1 and 2 and after having detected the salient states j and 1 on these paths, the robot is now capable of directing itself toward each of these states from state 2 and, thus, of avoiding the obstacle from a distance. Moreover, it is capable of choosing the most advantageous among these avoidance trajectories. Each time a task generated by the planning module is at the top of the context manager pile, the planning module can decide whether or not to decompos(e this task and to trigger one of various planning rules P I , P 2 , , . . it contains according to current sensory information. The decision to decompose is made on the basis of a probabilistic choice depending on the local strength of each rule (a local strength of 1 being assigned to the non-decomposition' option), weighted by an exploration-exploitation coefficient. The role of t h s coefficient is to modify, in the course of operation, the probability that the units) is provided in position f. In future implementations, the robot will discover by itself the positions where reinforcements are provided and will autonomously assess the values of these reinforcements. Whatever the case, the above equations allow the global strength of the principal task to be propagated down the hierarchy of tasks and subtasks by means of the corresponding planning rules and, if a given task T happens to be generated only rarely, its global strength will be weak, as will those of all the rules that decompose T . the global strengths of rules P4, P5, and P6 are smaller than those of rules P1, P2, and P3, the former rules have a higher likelihood of being eliminated from the planning module than the latter, in the event of memory overflow. However if, in this situation, a new obstacle is introduced in the environment, as in Fig. <ref type="figure" target="#fig_5">7</ref>, the robot is committed to skirting around the new obstacle. Having followed paths 6 and 7 under the control of task i-j, it characterizes four additional salient states-n, 0, p , and q-and creates six new planning rules-P7 to P12-of which only the first three are detailed in Fig. <ref type="figure" target="#fig_7">8</ref>. When task i -j has been decomposed into the subtasks associated with these planning rules, paths 8 and 9 have been followed by the robot. The lengths of paths 6 to 9 being respectively of 31, 31, 29, and 29 moves, it follows that AC(P1) is now equal to the mean of these four values, i.e., to 30. Therefore, LS(P1) being now equal to 34/30 = 1.133, its value turns out to be lower than that of LS(P4), i.e., 1.259. As a consequence, there are now more chances that the robot at position i will try to avoid the obstacle in Fig. <ref type="figure" target="#fig_5">7</ref> by turning in the direction of state 1 than by turning in the direction of state j . In other words, in the presence of the new obstacle, the robot will now tend to avoid the initial obstacle by turning to the right instead of turning to the left.</p><p>Figs. 6 and 8 also illustrate how task decomposition proceeds within the MonaLysa architecture. In particular, they show that such decomposition is modular, contrary to what happens with classical planning procedures. In the latter case, indeed, the decomposition of a task like i-f into the sequence z-j-k-f is made once and for all during the planning phase, and it is up to the execution algorithm to figure out a means for successively reaching positions j , k , and f from i . Once the execution of such a sequence has started, there is no opportunity to decide, for instance, to go to position other than IC when arriving at j . In contrast, the procedure used here allows for several different decompositions of a task like i -f . Thus it becomes possible to decompose i -f into i -j according to rule P1 and, when arriving at j , to opportunistically choose to use other rules than P 2 and P3-these rules not being shown on the figures-that will lead the robot to f via another position than IC. Such opportunities for reactive planning are at the core of the adaptive capacities of the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">SIMULATED RESULTS</head><p>This section describes the results obtained in various experiments performed in a square environment, each side of which being 40 times as long as the robot's length. The robot's main task was to reach-within given approximation limits-a goal position situated near the middle of one side, from a starting position situated near the middle of the opposite side. One or several obstacles might be placed in the environment, which forced the robot to deviate from the direct trajectory to the goal.</p><p>To illustrate the learning abilities of the robot, each experiment consisted in placing the robot in its starting position and letting it use its rules to choose where to move. When the robot succeeded in reaching the approximate goal position, the number of moves since the starting position was recorded, the robot was brought back in its starting position, and the whole process was iterated again, for a given number of loops called iterations hereafter. Because all the reactive rules withm the control architecture had the same strengths at initialization, the first moves of the robot were chosen at random. However, because the strengths of the reactive rules that tend to bring the robot closer to its current goal were increased and because the strengths of the reactive rules that tend to bring the robot farther from the current goal were decreased, the former soon acquiied a higher likelihood of being activated than the latter. Thus, according to such a learning scheme, the number of moves leading to the goal position or to any subgoal decreased from iteration to iteration and, eventually, an optimal reactive trajectory was followed by the robot.</p><p>Each experiment was run in two phases. During the first one, although planning rules were learned since the beginning, they were not allowed to modify the goal of the robot, which accordingly chose its action in a purely reactive mode. During the second phase, the planning rules were allowed to generate a succession of appropriate subgoals, which the robot, now acting in planning mode, tried to reach in turn. Figs. 9 to 11 illustrate the optimal trajectories that were followed by the robot when acting in reactive trajectories were followed in three enviro when the robot had to learn to skirt around dead-end obstacle, and a double-spiral ob Fig. <ref type="figure" target="#fig_0">12</ref> shows the corresponding learning curves, where f experiments, all starting from scmtch, have been performed during 200 iterations each. The first experiment was performed in an environment with no obstacle. The three others were performed in environments each containing one of the obstacles shown in Figs.'9 to 11. Results obtained show in each of the four environments, the robot quickly le select the reactive rules that are efficient for skirting around the obstacles. Differences&amp; the sizes and numbers of the leaps that characterize these curves indicate that, the more complex the obstacle, the greater the likelihood of probabilistic triggering suboptimal rules, and the greater the likelihood such rules will lengthen the trajectory to the goal.</p><p>Likewise, Fig. <ref type="figure" target="#fig_0">13</ref> illustrates the results obtained when the four experiments were performed consecutively, the environment being changed every 50 iterations, but with no versus number of iterations (abscissa). reinitialization of the strengths of the reactive rules. Thus the robot had to capitalize on the rules learned in the previous environment to adapt its behavior to a new environment. These results show that the reactive rules that are found to be efficient in a given environment are general enough to be useful in another environment. Therefore, adaptation from one environment to another is very quick. Fig. <ref type="figure" target="#fig_2">14</ref> shows three planning rules discovered in the environment containing a double spiral. These rules were created while the robot was moving along the reactive trajectory of Fig. <ref type="figure" target="#fig_0">11,</ref><ref type="figure">i</ref>.e., in a single iteration.</p><p>Fig. <ref type="figure" target="#fig_3">15</ref> illustrates the actual trajectory followed by the robot in the environment with a double-spiral when acting in planning mode. From its starting position, the robot successively reached two subgoals that allowed it to avoid entering the obstacle and to finally reach its initial goal.</p><p>Figs. <ref type="bibr" target="#b5">16</ref> to 18 illustrate the fact that the robot retains several plans in its memory and that it is continually updating the local and global strengths of its planning rules according to the procedures described in Section I1.E above. Therefore, the robot can switch rapidly from one plan to another, or create new plans, and thus adapt its behavior to new obstacles appearing in its environment. Thus, after 15 iterations in the environment depicted in Fig. <ref type="figure" target="#fig_6">16</ref>, the robot has memorized two plans for avoiding the dead-end. The best plan is shown in Fig. <ref type="figure" target="#fig_6">16</ref>, while the less effective one is shown in Fig. <ref type="figure" target="#fig_11">17</ref>. If, at iteration 16, a new obstacle is added to the environment along the robot's optimum path, the robot skirts around this obstacle, and the corresponding plan is modified accordingly. However, as the cost of this modified plan exceeds the cost of the second plan stored in memory, this second plan is the one most likely to govern the robot's path from the 22nd iteration on (Fig. <ref type="figure" target="#fig_11">17</ref>).</p><p>Likewise, introducing a new obstacle into the environment at the 30th iteration gives the advantage to the modified version of the first plan (Fig. <ref type="figure" target="#fig_7">18</ref>). It is thereby seen that the robot is capable of altering its plans as a reaction to modifications in its environment.</p><p>It should be noted that the plan presented in Fig. <ref type="figure" target="#fig_7">18</ref> is a hierarchical plan with two levels. Indeed, the first task of the plan shown in Fig. <ref type="figure" target="#fig_6">16</ref> has been decomposed into a threetask sub-plan. Another exemple of this faculty to generate hierarchical plans can be found in <ref type="bibr">[17]</ref>.</p><p>Finally, results shown in Fig. <ref type="figure" target="#fig_0">19</ref>, as well as results published elsewhere <ref type="bibr">[17]</ref>, demonstrate that the decomposition of goals into subgoals can be efficient enough to devise a way of escaping from rather complex mazes. The best plan in an environment with a dead-end and two obstacles</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. REAL ROBOT IMPLEMENTATION</head><p>To reproduce the simulated experiments and results of the previous section, the MonaLysa architecture has been used with a Khepera robot ([28], [38]), shown in Fig. <ref type="figure">20</ref>. The size of each side of the square environment used for such a purpose was set at 70 cm. The Khepera robot is equipped with six IR frontal sensors (with two additional sensors in the back) that can detect an obstacle within a range of approximately 5 cm, and with two wheels that can turn forward or backward. For the present application, the signals generated by the frontal sensors have been thresholded to mimick the functioning of the proximate sensors of the previous simulated robot, whose number has thus been raised from three to six. The functionality of obstacle detection has been assigned to the four sensors nearer the frontal axis of the robot, in such a way that, when the signals of two neighboring sensors were at their maximum value, an obstacle preventing any move in its direction was detected. Likewise, because the sensors of JShepera are not evenly distributed over the periphery of the robot, it sometimes happens that they are unable to detect an obstacle before the robot has turned 90" to its right or 90" to its left. Therefore, the JXhepera robot was given three / \ Fig. <ref type="figure" target="#fig_0">19</ref>. Overall plan discovered in a maze elementary actions: move a step forward, t and move a step forward, or turn 45" to t step forward. In principle, flhe size of each step forward has , been set at 5.5 cm, i.e., to a value that is approximately equal to the size of the robot. Howeve move one or more sensors did environment (i.e., one or more switched from 0 to 1 or from 1 to 0), the robot stopped until a new reactive rule was selected and actuated. Thus, the actual 0" or 45". Furthermore, an odometric device each wheel allowed the robot's position and the reactive module of Khepera was 64 * 8 * 3 = 1536. Fig. <ref type="figure" target="#fig_0">21</ref> shows the learning curves obtained when three experiments of 50 iterations each were performed consecutively. The first experiment used an environment without any obstacle, the second and third experiments used an environment with, respectively, a barrier and a dead-end obstacle. obtained in the presence of are qualitatively similar to those obtained by simulation (Fig. <ref type="figure" target="#fig_0">13</ref>). The robot is able to use reactive rules already  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>The transfer of the MonaLysa architecture from a simulated robot to a real robot has been almost straightforward. Moreover, results obtained in both cases have been qualitatively very similar, as they were in the work of Jakobi, Husbands, and Harvey [26] that also involved a Khepera robot. Therefore the common argument (e.g., [8], [9], [lo], <ref type="bibr" target="#b29">[43]</ref>) according to which no simulation will ever replace actual robot experimentation is certainly not universal, although it probably gets stronger support when a sophisticated robot is used instead of a primitive one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>barrier.</head><p>Khepera's optimal reactive trajectory in an environment with a</p><p>The MonaLysa architecture has been conceived in order to generate very general adaptive behaviors, and current work aims at demonstrating its applicability to, for example, traditional block-world planning <ref type="bibr">[39]</ref>. Likewise, this architecture should prove useful for managing more numerous and varied motivations than those studied here. In particular, it will be used to control the behavioral sequences of a simulated animal facing realistic survival problems, like those described by Tyrrel <ref type="bibr" target="#b35">[49]</ref>.</p><p>Whatever the case, results shown here demonstrate that the MonaLysa architecture allows expedient learning of reactive and planning rules within the context of simple navigation tasks. In particular, a robot equipped with such a control architecture is not only capable of generating and memorizing plans that help to avoid obstacles, but also of altering these plans in reaction to modifications in its environment. In the experiments described here, such modifications were caused by the addition of one or more obstacles in the environment. It turns out that the removal of obstacles leads to the same kind of results as those obtained by Sutton with the DYNA Three planning rules discovered by JShepera in an environment with architecture <ref type="bibr" target="#b33">[47]</ref>. In particular, MonaLysa allows the animat to discover new shortcuts leading to the goal, but the speed of such a discovery depends upon the current value of the exploration-exploitation coefficient. For instance if, in the situation depicted on Fig. <ref type="figure" target="#fig_11">17</ref>, the obstacle that has been previously added were removed, less than 100 iterations would allow the animat to favor again the plan on Fig. <ref type="figure" target="#fig_6">16</ref>, when a very low exploration-exploitation coefficient (0.1) is used. In future implementations, it will be easy to dynamically control such a coefficient and to take advantage of +he fact that each salient state is characterized by an associated (seasoly information). Thus, the animat would notice that the value of this information shifted from "0 10"-indicating that the obstacle was present to the left of the robot in this state-to "000"-indicating that the obstacle has been removed. This event would automatically reset the exploration-exploitation to 1, thus increasing the likelihood of shortcut discovery.</p><p>As far as the results shown here are concerned, it should also be noted that, although the external contours of the obstacles were always piecewise linear for convenience, it has been shown elsewhere [ 171 that the same control architecture allows navigation in the presence of more complex obstacles. Likewise, it should also be noted that the MonaLysa architecture is iterations (abscissa) capable of discovering quite intricate optimal simulated results like those of Fig. <ref type="figure" target="#fig_0">19</ref>   the number of moves necessary to reach a goal reactively, and Figs. <ref type="bibr" target="#b5">16</ref> to 18 show that about the same number of iterations allows the robot to switch adaptively from one plan to another. This efficiency is due to the fact that the classical problem of temporal credit assignment <ref type="bibr" target="#b33">[47]</ref> is avoided in the case of the reactive rules because the strength of each such rule is updated after each utilization, thanks to the management of an internal reward based on the satisfaction criterion. Thus, the internal reinforcement module plays here the role of an adaptive critic element <ref type="bibr" target="#b32">[46]</ref>. Learning efficiency is also due to the fact that the temporal credit assignment problem is minimized in the case of the planning rules because of the hierarchy of tasks and subtasks they implement. Indeed, as soon as a given task is achieved, the corresponding reinforcement is immediately forwarded to each rule that contributed to this result, and applied either to the task itself or to all its corresponding subtasks. Thus, the strenghts of many rules can be updated at reinforcement time, a logic more efficient than that of the classical bucket brigade algorithm</p><p>[24], as demonstrated by Wilson <ref type="bibr" target="#b39">[53]</ref>. However, although Wilson's description involved a hierarchical bucket brigade algorithm, the solution implemented here can be qualified as a hierarchical projit sharing algorithm.</p><p>Like MonaLysa, the control architectures described by Wilson [53], <ref type="bibr">Shu and Schaeffer [41]</ref> and by Dorigo and Colombetti [ 181 also rely on hierarchical classifier systems, but only the first-which is a theoretical construction and has not given rise to any concrete application-might implement a planning process. Nevertheless, Wilson does not specify how the corresponding tasks and subtasks could be identified by the system. Likewise, though the architecture proposed by Colombetti and Dorigo provides for a classifier system to coordinate the actions proposed by other classifier systems, the hierarchical relationships are predetermined by the programmer. On the contrary, in the present work, the hierarchical relationships among tasks are dynamic, because they are generated internally on the basis of the experience gained by the animat.</p><p>As to planning, the MonaLysa architecture does not call on any predefined operators for decomposing problems into subproblems, for the purpose of generating a plan which would A trajectory followed when using the best plan of Fig. <ref type="figure" target="#fig_17">27</ref>. Iteration then be executed [39]. Such a practice, which implies that planning precedes acting, has shown itself to be singularly ineffective [9]. Conversely, here, acting precedes planning, and the latter does not depend on predefined operators, but rather is abstracted from the paths actually travelled. The plans thus elaborated are initially high level plans and are based on a small number of rules. However, these plans are refined as needed. They are not executed mechanically by the animat, but instead are used as one resource among others to decide which action to perform ([l], <ref type="bibr" target="#b31">[45]</ref>). The organization of these plans thus appears as an emergent property, arising from the interactions between the animat and its environment and elicited by the animat's needs. Lastly, the value of these plans is continually reevaluated, which confers considerable adaptive faculties to the system. As already stressed in Section 11-E above, these incremental changes contrast with the way other robotic realizations that involve planning solve the problem of reacting as quickly as possible to modifications in the environment. For example, within the AURA architecture ([4], [5]), any environmental alteration changes the reactive motor schemas that are used to generate action, but does not explicitly modify the corresponding overall plan. If this plan does need to be changed, because the challenge of the environment is too great to be dealt with by a mere alteration of motor schemas, then the plan must be rebuilt entirely from scratch.</p><p>Albus [2], too, described a hierarchical architecture able to decompose a complex task into a series of subtasks, then into a series of elemental moves, then into a series of motor drive signals which actuate observable behavior in a robot. However, although Albus describes how such an architecture relates to a general theory of intelligence [3], he doesn't state how the corresponding hierarchy might be dynamically generated, nor how it could be modified according to the robot's needs and to the environmental conditions encountered.</p><p>In comparison with the literature on animal behavior, it must be stressed that the MonaLysa architecture is not dedicated to navigation tasks only and that it has been conceived for solving general survival problems. In fact this architecture implements a motivationally autonomous agent ([33], [34]) that acts in particular ways in order to achieve certain ends. To do so, the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART d CYBERNETICS, V in the former category, belong to the latter.</p><p>Finally, it is interesting to note that the a sequences that are triggered by its motiv not random, which could be demonstrate nized according to the i n [ 14]-but rather tends to maximize the utili react opporhmistically to the surpr agent must decide what action to perform next, according to its physiological or internal state, to the cue state arising from its perception of the external world, to the consequences of its current behavior and to the expected consequences of its future behavior. The latter point requires knowledge of the probable consequences-or expected utility-of each possible action. In other words, a motivationally autonomous agent must have some memory of the past consequences of similar activities, y d it must be capable of planning-it., it must use some form of cognition. Furthermore, as Dennett <ref type="bibr" target="#b5">[16]</ref> pointed out, it must want something, it must have goals?</p><p>An animat endowed with the MonaLysa arcLitecture displays all these characteristics. Indeed, the action it performs at any time depends both on sensors and on what was called here the "internal context." This context actually takes into account the goals that the animat has selected and that it seeks to achieve. There is nothing to prevent this context from subsequently including other information about the internal state of the animat like, for instance, its energy level. The animat's goals are generated by an explicit planning process, and the strengths of the rules memorize the consequences of the various choices that the animat has made in the past. Ln the navigation task, these consequences were evaluated in terms of their aptitude in bringing the animat nearer to its goal; they may later depend on an appropriate utility function and help the animat decide, for instance, whether it should seek food, seek water, or try to escape from a predator.</p><p>In comparison with other approaches m e d at including a motivational system in the architecture of an animat ([6],</p><p>[7], [Ill, <ref type="bibr" target="#b8">[19]</ref>, <ref type="bibr" target="#b12">[23]</ref>, [30], <ref type="bibr" target="#b37">[51]</ref>), this approach is the only one that incorporates a planning process that, as seen previously, substantially enhances the adaptive faculties of the animat. It' would accordingly seem that, in the continuum described by <ref type="bibr">McFarland and Bosser [34]</ref>, which distinguishes motivated automata-that choose the action to perform next without taking into account its expected consequences-from motivationally autonomous agents, these other approaches tend to be situated 5Jn other words, the agent must be goal-achieving and goal-seekzg Whether its behavior is goddirected or intentional is another issue ( <ref type="bibr" target="#b5">[16]</ref>, WI). about the motivational systems of architecture is based upon particular, it has been shown that the ani equipped with very rudimentary sensors learn to escape from obstacles it may get even to learn plans that will allow it to a in the future. Moreover, if the environment chan least some of the adaptiye behaviors t advanced animals to survive, even in quite U threatening environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>suggestions.</p><p>[ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The MonaLysa architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 12 IFig. 3 .</head><label>2123</label><figDesc>Fig 2 Different stages in a skrting behavior involving one skirting task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Detection of satisfaction and salient states. Numerical values indicate the satisfaction brought by each action. &gt;, &lt; and = symbols indicate the sign of the satisfaction gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Several trajectories in a specific environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>plausibility put forth by Wilson [52] and Holland [25]. As AC(P1),+1 = (1a) * AC(Pl), + Q * C where U number of times that task T was achieved using C cost (number of moves) of the U + lth path cr parameter (set to 0.1 in the experiments described The cost C of the path covered is used as an internal reinforcement and is dispatched to all the rules P1, P2, . . . that decomposed T . This s ccession of rules is thus memorized, average costs AC(Pl),AC(PZ), . . . Under these conditions, the local strength of P1, LS(Pl), is updated according to the equation planning rule P1 herein). and a profit shar&amp; 2 gorithm [21] is called on to change their</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Several trajectories in the environment of Fig. 5 with a new obstacle added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>describes the tasks and planning rules that are associated with Fig.5. Tasks are characterized by their initial and final positions; planning rules are arrows decomposing tasks into subtasks. Fig.6also gives the local and global strengths of the planning rules, the global strengths of the tasks, and the associated costs of both rules and tasks. For example, because LS(P1) = 1.478 and GS(PT) = G S ( i -f ) = 1000, it follows that GS(P1) = 1.478 * 1000 = 1478. Then, because</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Tasks and planning rules associated with Fig. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>IFig 9 IFig 10 .</head><label>910</label><figDesc>Fig 9 Optimal reactive trajectory in an enwonment with a barrierii-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>I ! 1 Fig. 11 .Fig. 12 .</head><label>11112</label><figDesc>Fig. 11. Ophmal reactive traject ublespiral.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .Fig. 15 .Fig. 14 .</head><label>131514</label><figDesc>Fig. 13. Adamation to new environments. Number of moves (ordinate) Fig. 15. Actual trajectory followed by the robot in the environment with a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. added. Iteration 22.The best plan in an environment with a dead-end and one obstacle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figs. 22</head><label>22</label><figDesc>Figs. 22 ana 23 shdw</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 20 .Fig. 21 .</head><label>2021</label><figDesc>Fig. 20. The robot Khepera and the experimental setting. 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig 23 .</head><label>23</label><figDesc>Fig 23. dead-end.Khepera's optimal reactive trajectory in an environment with a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>which re1 position and orientatlon estimates, robot with the possibility of relocat T h i s has been accomplished by mean tion rules in an extension of the M proved to be effective by simulation. Su state among others, wi Results shown here als</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 27 .</head><label>27</label><figDesc>Fig. 27. The best plan (on the right) and an alternative (on the left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig</head><label></label><figDesc>Fig. 28. 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig. 29 Iteration 25A trajectory followed when using the alternative plan of Fig.27.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>lJ P E Agre and D. Chapm tonornous Agents. Theory an Back, P Maes, Ed Camb [Z] J. S. Albus, Bmzns, Behavzo [7] L B. Booker, "Classifier Machine Learning, vol. 3, [8] R A Brooks, "Intelligenc</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Jean-Yves Donnart was born in Quimper, France, in 1966. He has received the Master degree in computer science in 1991 from the University Paris XI, France. He is currently pursuing the Ph.D. degree in bio-mathematics at the Ecole Normale Superieure, Paris, France.</p><p>He works in the AnimatLab of the Ecole Normale Superieure. His research interests are in the field of machine learning, planning, situated activity, autonomous agents and adaptive behavior.</p><p>Jean-Arcady Meyer was born in Le Grand Lemps, France, in 1941. He received the engineer degree from the Ecole Nationale Superieure de Chimie de Paris in 1964, the graduate degree in human psychology in 1966, the graduate degree in animal psychology in 1969 and the French Ph.D. degree in biology in 1974.</p><p>He is currently Research Director, CNRS, and heads the AnimatLab, Ecole Normale Superieure. His main scientific interests are the interactions of learning, development and evolution in adaptive </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural networks with motivational units</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">F</forename><surname>Cecconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>2nd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Input generalization in delayed reinforcement learning: An algorithm and performance comparisons</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI-91</title>
		<meeting>IJCAI-91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="726" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From Animals to Animats 3</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Husband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>3rd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training agents to perform sequential behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adapt. Beh</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="275" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Nature of Explanation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J W</forename><surname>Craik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943">1943</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intentional systems in cognitive ethology: The &apos;Panglossian paradigm&apos; defended</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="343" to="390" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hierarchical classifier system implementing a motivationally autonomous animat</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Donnart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>3rd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="144" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robot shaping: Developing autonomous agents through learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art&amp; Intell</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="370" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Should I stay or should I go: Coordinating biological needs with continuously-updated assessments of the environment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gabora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>2nd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="156" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Genetic Algorithms zn Search, Optimization, and Machine Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Credit assignment in rule discovery systems based on genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="225" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revue g6n6rale des methodes d&apos;ttude des sequences comportementales</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guillot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Etudes et Analyzes Comportementales</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine motivation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R P</forename><surname>Haloenn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation ojddaptive Behavior</title>
		<imprint>
			<date type="published" when="1986">1986. 1991</date>
			<biblScope unit="page" from="213" to="221" />
		</imprint>
	</monogr>
	<note>Proc. 1st Int</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: An Artificial Intelligence Approach</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Michalski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">.</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="593" to="623" />
			<date type="published" when="1986">1986</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Nisbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Thagard</surname></persName>
		</author>
		<title level="m">Induction: Processes of Inference, Learning and Discovery</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT PressBradford Books</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noise and the reality gap: The use of simulation in evolutionary robotics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jakobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Europ. Con$ on Artijicial Life</title>
		<meeting>Third Europ. Con$ on Artijicial Life</meeting>
		<imprint>
			<publisher>in Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical learning in stochastic dorhains: Preliminary results</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth Int. Con$ on Machine Learning</title>
		<meeting>Ninth Int. Con$ on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical learning of robot skills by reinforcement</title>
		<author>
			<persName><forename type="first">K-Team ; L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con$ on Neural Networks-93</title>
		<meeting>IEEE Int. Con$ on Neural Networks-93<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Khepera Users Manual</publisher>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A bottom-up mechanism for behavior selection in an artificial creature</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Int. Con5 on Simulation of Adaptive Behavior</title>
		<meeting>the 1st Int. Con5 on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="238" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling adaptive autonomous agents</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Defining motivation and cognition in animals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfarland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Goals, No Goals and Own Goals, Montefiore and Noble</title>
		<editor>
			<persName><surname>London: Uuwin-Hyman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1989">1989. 1991</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="170" />
		</imprint>
	</monogr>
	<note>The teleological imperative</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bosser</surname></persName>
		</author>
		<title level="m">Intelligent Behavior in Animals and Robots</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT PressBradford Books</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From Animals to Animats</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. ConJ on Simulation of Adaptive Behavior</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</editor>
		<meeting>1st Int. ConJ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The animat approach to cognitive science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roitblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 2. Proc. 2nd Int. Con$ on Simulation of Adaptive Behavior</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roitblat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT PressBradford Books</publisher>
			<date type="published" when="1993">1993. 1995</date>
		</imprint>
	</monogr>
	<note>Comparative Approaches to Cognitive Science</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mobile robot miniaturization: A tool for investigation in control algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mondada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Franzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd lnt. Symp. on Experimental Robotics</title>
		<meeting>3rd lnt. Symp. on Experimental Robotics</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Principles of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Tioga</publisher>
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to comparative cognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roitblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative Approaches to Cognitive Science</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roitblat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="167" to="173" />
			<date type="published" when="1995">1995</date>
			<publisher>The MIT PressBradford Books</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HCS: Adding hierarchies to classifier systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ProC4th Int. Con$ on Genetic Algorithms</title>
		<imprint>
			<biblScope unit="page" from="339" to="345" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning with a hierarchy of abstract models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On why better robots make it harder</title>
		<author>
			<persName><forename type="first">T</forename><surname>Smithers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>3rd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward a theory of emergent functionality</title>
		<author>
			<persName><forename type="first">L</forename><surname>Steels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Con$ on Simulation ofAdaptive Behavior</title>
		<meeting>1st Int. Con$ on Simulation ofAdaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">451461</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Plans and Situated Actions: The Problem of Human-Machine Communication</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Suchman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Temporal credit assignment in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note>Doctoral Dissertation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforcement learning architectures for animats</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Con5 on Simulation of Adaptive Behavior</title>
		<meeting>1st Int. Con5 on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ethological and psychological models of motivation: Toward a synthesis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Toates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>1st Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The use of hierarchies for action selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tyrrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adapt. Beh</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="420" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning with delayed rewards</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">5</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using second order neural connections for motivation of behavioral choices</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Con$ on Simulation of Adaptive Behavior</title>
		<meeting>3rd Int. Con$ on Simulation of Adaptive Behavior</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classifier systems and the animat problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="199" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Hierarchical credit allocation in a classifier system</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>New York Pitman</pubPlace>
		</imprint>
	</monogr>
	<note>Genetic Algorithms and Simulated Annealing</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The animat path to AI</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Con$ on Simulation 104-1 15. of Adaotive Behavior</title>
		<meeting>1st Int. Con$ on Simulation 104-1 15. of Adaotive Behavior</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
