<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ozan</forename><forename type="middle">Oktay</forename><surname>Oktay</surname></persName>
							<email>o.oktay13@imperial.ac.uk</email>
							<idno type="ORCID">0000-0003-2976-0874</idno>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enzo</forename><surname>Ferrante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MRC Clinical Sciences Centre</orgName>
								<address>
									<postCode>W12 0NN</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>De Marvao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MRC Clinical Sciences Centre</orgName>
								<address>
									<postCode>W12 0NN</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Dawes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MRC Clinical Sciences Centre</orgName>
								<address>
									<postCode>W12 0NN</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Declan</forename><forename type="middle">P</forename><surname>O'regan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MRC Clinical Sciences Centre</orgName>
								<address>
									<postCode>W12 0NN</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analy-sis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<postCode>23538</postCode>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42FD9DC926BA76AA2046CD3FE61B8C70</idno>
					<idno type="DOI">10.1109/TMI.2017.2743464</idno>
					<note type="submission">received July 26, 2017; accepted August 14, 2017. Date of publication September 26, 2017; date of current version February 1, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Shape prior</term>
					<term>convolutional neural network</term>
					<term>medical image segmentation</term>
					<term>image super-resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Incorporation of prior knowledge about organ shape and location is key to improve performance of image analysis approaches. In particular, priors can be useful in cases where images are corrupted and contain artefacts due to limitations in image acquisition. The highly constrained nature of anatomical objects can be well captured with learning-based techniques. However, in most recent and promising techniques such as CNN-based segmentation it is not obvious how to incorporate such prior knowledge. State-of-the-art methods operate as pixel-wise classifiers where the training objectives do not incorporate the structure and inter-dependencies of the output. To overcome this limitation, we propose a generic training strategy that incorporates anatomical prior knowledge into CNNs through a new regularisation model, which is trained endto-end. The new framework encourages models to follow the global anatomical properties of the underlying anatomy (e.g. shape, label structure) via learnt non-linear representations of the shape. We show that the proposed approach can be easily adapted to different analysis tasks (e.g. image enhancement, segmentation) and improve the prediction accuracy of the state-of-the-art models. The applicability of our approach is shown on multi-modal cardiac data sets and public benchmarks. In addition, we demonstrate how the learnt deep models of 3-D shapes can be interpreted and used as biomarkers for classification of cardiac pathologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>underlying data and a prior on the solution space, where the latter is useful in cases where the images are corrupted or contain artefacts due to limitations in the image acquisition. For example, bias fields, shadowing, signal drop-out, respiratory motion, and low-resolution acquisitions are the few common limitations in ultrasound (US) and magnetic resonance (MR) imaging.</p><p>Incorporating prior knowledge into image segmentation algorithms has proven useful in order to obtain more accurate and plausible results as summarised in the recent survey <ref type="bibr" target="#b31">[32]</ref>. Prior information can take many forms: boundaries and edge polarity <ref type="bibr" target="#b9">[10]</ref>; shape models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>; topology specification; distance prior between regions; atlas models <ref type="bibr" target="#b4">[5]</ref>, which were commonly used as a regularisation term in energy optimisation based traditional segmentation methods (e.g. region growing). In particular, atlas priors are well suited for medical imaging applications since they enforce both location and shape priors through a set of annotated anatomical atlases. Similarly, autocontext models <ref type="bibr" target="#b44">[45]</ref> have made use of label and image priors in segmentation, which require a cascade of models.</p><p>In the context of neural networks (NNs), early work on shape analysis has focused on learning generative models through deep Boltzmann Machines (DBMs), namely ShapeBM <ref type="bibr" target="#b17">[18]</ref> that uses a form of DBM with sparse pixel connectivity. Follow-up work in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b16">[17]</ref> has demonstrated the application of DBMs to binary segmentation problems in natural images containing vehicles and other types of objects. However, fully connected DBM for images require a large number of parameters and consequently model training may become intractable depending on the size of images. For this reason, convolutional deep belief nets <ref type="bibr" target="#b47">[48]</ref> were recently proposed for encoding shape prior information. Besides variational models, cascaded convolutional architectures <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b36">[37]</ref> have been shown to discover priors on shape and structure in label space without any a priori specification. However, this comes at the cost of increased model complexity and computational needs.</p><p>In the context of medical imaging and neural networks, anatomical priors have not been studied in much depth, particularly in the current state-of-the-art segmentation techniques <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Recent work has shown simple use cases of priors through adjacency <ref type="bibr" target="#b6">[7]</ref> and boundary <ref type="bibr" target="#b9">[10]</ref> conditions. Inclusion of priors in medical imaging could potentially have much more impact compared to their use in natural image analysis since anatomical objects in medical images are naturally more constrained in terms of their shape and location.</p><p>As explained in a recent NN survey paper <ref type="bibr" target="#b27">[28]</ref>, the majority of the classification and regression models utilise a pixellevel loss function (e.g. cross-entropy or mean square error) which does not fully take into account the underlying semantic information and dependencies in the output space (e.g. class labels). In this paper, we present a novel and generic way to incorporate global shape/label information into NNs. The proposed approach, namely anatomically constrained neural networks (ACNN), is mainly motivated by the early work on shape priors and image segmentation, in particular PCA based statistical <ref type="bibr" target="#b12">[13]</ref> and active shape models <ref type="bibr" target="#b13">[14]</ref>. Our framework learns a non-linear compact representation of the underlying anatomy through a stacked convolutional autoencoder <ref type="bibr" target="#b30">[31]</ref> and enforces network predictions to follow the learnt statistical shape/label distributions. In other words, it favours predictions that lie on the extracted low dimensional data manifold. More importantly, our approach is independent of the particular NN architecture or application; it can be combined with any of the state-of-the-art segmentation or super-resolution (SR) NN models and potentially improve its prediction accuracy and robustness without introducing any memory or computational complexity at inference time. Lastly, ACNN models, trained with the proposed prior term which acts as a regulariser, remove the need for post-processing steps such as conditional random fields <ref type="bibr" target="#b23">[24]</ref> which are often based on heuristics parameter tuning. In ACNN, the regularisation is part of the endto-end learning which can be a great advantage.</p><p>The proposed global training objective in SR corresponds to a prior on the space of feasible high-resolution (HR) solutions, which is experimentally shown to be useful since SR is an illposed problem. Similar modifications of the objective function during training have been introduced to enhance the quality of natural images, such as perceptual <ref type="bibr" target="#b20">[21]</ref> and adversarial <ref type="bibr" target="#b25">[26]</ref> loss terms, which were used to synthesise more realistic images in terms of texture and object boundaries. In the context of medical imaging, our priors enforce the synthesised HR images to be anatomically meaningful while minimising a traditional image reconstruction loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Clinical Motivation</head><p>Cardiac imaging has an important role in diagnosis, preoperative planning, and post-operative management of patients with heart disease. Imaging modalities such as US and cardiac MR (CMR) are widely used to provide detailed assessment of cardiac function and morphology. Each modality is suitable for particular clinical use cases; for instance, 2D-US is still the first line of choice due to its low cost and wide availability, whereas, CMR is a more comprehensive modality with excellent contrast for both anatomical and functional evaluation of the heart <ref type="bibr" target="#b22">[23]</ref>. Similarly, 3D-US is recommended over the use of 2D-US since it has been demonstrated to provide more accurate and reproducible volumetric measurements <ref type="bibr" target="#b24">[25]</ref>.</p><p>Some of the standard clinical acquisition protocols in 3D-US and CMR still have limitations in visualising the underlying anatomy due to imaging artefacts (e.g. cardiac motion, low slice resolution, lack of slice coverage <ref type="bibr" target="#b34">[35]</ref>) or operator-dependent errors (e.g. shadows, signal drop-outs). In the clinical routine, these challenges are usually tackled through multiple acquisitions of the same anatomy and repeated patient breath-holds leading to long examination times. Similar problems have been reported in large cohort studies such as the UK Biobank <ref type="bibr" target="#b34">[35]</ref>, which leads to inaccurate quantitative measurements or even the discarding of acquired images. As can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>, the existing state-ofthe-art convolutional neural network (CNN) approaches for segmentation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b43">[44]</ref> and image enhancement <ref type="bibr" target="#b33">[34]</ref> tasks perform poorly when the input data is not self-consistent for the analysis. For this reason, incorporation of prior knowledge into cardiac image analysis could provide more accurate and reliable assessment of the anatomy, which is shown in the third column of the same figure. Most importantly, the proposed ACNN model allows us to perform HR analysis via subpixel feature maps generated from low resolution (LR) input data even in the presence of motion artefacts. Using the proposed approach we can perform full 3D segmentation without explicit motion correction and do not have to rely on LR slice-by-slice 2D segmentation.</p><p>We demonstrate the applicability of the proposed approach for cine stacks of 2D MR and 3D-US datasets composed of 1200 and 45 cardiac image sequences respectively. We show that the proposed segmentation and SR models become more robust against imaging artefacts mentioned earlier which is underlined by our state-of-the-art results on the MICCAI'14 CETUS public benchmark <ref type="bibr" target="#b7">[8]</ref>. We also demonstrate that the lower dimensional representations learnt by the proposed ACNN can be useful for classification of pathologies such as dilated and hypertrophic cardiomyopathy, and it does not require point-wise correspondence search between subjects as in <ref type="bibr" target="#b38">[39]</ref>. For the evaluation, the MICCAI'17 AC/DC Fig. <ref type="figure">2</ref>. Block diagram of the baseline segmentation (Seg) and super-resolution (SR) models which are combined with the proposed T-L regularisation block (shown in Fig. <ref type="figure" target="#fig_1">3</ref>) to build the ACNN-Seg/SR frameworks. In SR, the illustrated model extracts SR features in low-resolution (LR) space, which increases computational efficiency. In segmentation, the model achieves sub-pixel accuracy for given LR input image. The skip connections between the layers are shown in red.</p><p>classification benchmark was used. In that regard, the proposed method is not only useful for image enhancement and segmentation but also for the study of anatomical shape variations in population studies and their associations with cardiac related pathologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions</head><p>In this study, we propose a generic and novel technique to incorporate priors on shape and label structure into NNs for medical image analysis tasks. In this way, we can constrain the NN training process and guide the NN to make anatomically more meaningful predictions, in particular in cases where the input image data is not informative or consistent enough (e.g. missing object boundaries). More importantly, to the best of our knowledge, this is one of the earliest studies demonstrating the use of convolutional autoencoder networks to learn anatomical shape variations from medical images.</p><p>The proposed ACNN model is evaluated on multi-modal cardiac datasets from MR and US. Our evaluation shows: (I) A sub-pixel cardiac MR image segmentation approach that, in contrast to previous CNN approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b43">[44]</ref>, is robust against slice misalignment and coverage problems; (II) An implicit statistical parametrisation of the left ventricular shape via NNs for pathology classification; (III) An image SR technique that extends previous work <ref type="bibr" target="#b33">[34]</ref> and that is robust against slice misalignments; our approach is computationally more efficient than the state-of-the-art SR-CNN model <ref type="bibr" target="#b33">[34]</ref> as the feature extraction is performed in the low-dimensional image space. (IV) Last, we demonstrate state-of-the-art 3D-US cardiac segmentation results on the CETUS'14 Benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In the next section, we briefly summarise the state-of-theart methodology for image segmentation (SEG) and superresolution (SR), which is based on convolutional neural networks (CNNs). We then present a novel methodology that extends these CNN models with a global training objective to constrain the output space by imposing anatomical shape priors. For this, we propose a new regularisation network that is based on the T-L architecture which was used in computer graphics <ref type="bibr" target="#b18">[19]</ref> to 3D render objects from natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Medical Image Segmentation With CNN Models</head><p>Let y s = {y i } i∈S be an image of class labels representing different tissue types with</p><formula xml:id="formula_0">y i ∈ L = {1, 2, . . . C}. Furthermore let x = {x i ∈ R, i</formula><p>∈ S} be the observed intensity image. The aim of image segmentation is to estimate y s having observed x. In CNN based segmentation models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, this task is performed by learning a discriminative function that models the underlying conditional probability distribution P( y s |x).</p><p>The estimation of class densities P( y s |x) consists in assigning to each x i the probability of belonging to each of the C classes, yielding C sets of class feature maps f c that are extracted through learnt non-linear functions. The final decision for class labels is then made by applying softmax to the extracted class feature maps, in the case of cross-</p><formula xml:id="formula_1">entropy L x = -C c=1 i∈S log e f (c,i) j e f ( j,i)</formula><p>these feature maps correspond to log likelihood values.</p><p>As in the U-Net <ref type="bibr" target="#b37">[38]</ref> and DeepMedic <ref type="bibr" target="#b21">[22]</ref> models, we learn the mapping between intensities and labels φ (x) : X → L by optimising the average cross-entropy loss of each class L x = C c=1 L (x,c) using stochastic gradient descent. As shown in Fig. <ref type="figure">2</ref>, the mapping function φ is computed by passing the input image through a series of convolution layers and rectified linear units across different image scales to enlarge the model's receptive field. The presented model is composed of two parts: feature extraction (analysis) similar to a VGG-Net <ref type="bibr" target="#b41">[42]</ref> and reconstruction (synthesis) as in the case of a 3D U-Net <ref type="bibr" target="#b37">[38]</ref>. However, in contrast to existing approaches, we aim for sub-pixel segmentation accuracy by training up-sampling layers with high-resolution ground-truth maps. This enables 3D analysis of the underlying anatomy in case of thick slice 2D image stack acquisitions such as cine cardiac MR imaging. In this way, it is possible to perform analysis on the high-resolution image grid without any preceding upsampling operation with a SR model <ref type="bibr" target="#b33">[34]</ref>.</p><p>Similar segmentation frameworks (cf. <ref type="bibr" target="#b27">[28]</ref>) have been studied in medical imaging. However, in most of the existing methods, the models are supervised purely through a local loss function at pixel level (e.g. cross-entropy, Dice) without exploiting the global dependencies and structure in the output space. For this reason, the global description of predictions is usually not adhering to shape, label, or atlas priors. In contrast to this we propose a model that can incorporate the aforementioned priors in segmentation models. The proposed framework relies on autoencoder and T-L network models to obtain a non-linear compact representation of the underlying anatomy, which are used as priors in segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Autoencoder Model and ACNN-Seg</head><p>An autoencoder (AE) <ref type="bibr" target="#b45">[46]</ref> is a neural network that aims to learn an intermediate representation from which the original input can be reconstructed. Internally, it has a hidden layer h whose activations represent the input image, often referred as codes. To avoid the AE to directly copy its output, the AE are often designed to be undercomplete so that the size of the code is less than the input dimension as shown in Fig. <ref type="figure" target="#fig_1">3</ref>. Learning an AE forces the network to capture the most salient features of the training data. The learning procedure minimises a loss function L x ( y s , g( f ( y s ))), where L x is penalising g( f ( y s )) being dissimilar from y s . The functions g and f are defined as the decoder and encoder components of the AE.</p><p>In the proposed method, the AE is integrated into the standard segmentation network, described in Sec. II-A, as a regularisation model to constrain class label predictions y towards anatomically meaningful and accurate outputs. The crossentropy loss function operates on individual pixel level class predictions, which does not guarantee global consistency and plausible anatomical shapes even though the segmentation network has a receptive field larger than the size of structures to be segmented. This is due to the fact that back-propagated gradients are parametrised only by pixel-wise individual probability divergence terms and thus provide little global context. To overcome this limitation, class prediction label maps are passed through the AE to obtain a lower dimensional (e.g. 64 dimensions) parametrisation of the segmentation and its underlying structure <ref type="bibr" target="#b39">[40]</ref>. By performing AE-based nonlinear lower dimensional projections on both predictions and ground-truth labels, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>, we can build our ACNN-Seg training objective function though a linear combination of cross-entropy (L x ), shape regularisation loss (L h e ), and weight decay terms as follows:</p><formula xml:id="formula_2">L h e = f (φ(x); θ f ) -f ( y; θ f ) 2 2 min θ s L x (φ(x; θ s ), y) + λ 1 • L h e + λ 2 2 ||w|| 2 2 (1)</formula><p>Here w corresponds to weights of the convolution filters, and θ s denotes all trainable parameters of the segmentation model and only these parameters are updated during training. The coupling parameters λ 1 and λ 2 determine the weights of shape regularisation loss and weight decay terms used in the training. In this equation, the second term L h e ensures that the generated segmentations are in a similar low dimensional space (e.g. shape manifold) as the ground-truth labels. In addition to imposing shape regularisation, this parametrisation encourages label consistency in model predictions, and reduces false-positive detections as they can influence the predicted codes in the hidden layer. The third term corresponds to weight decay to limit the number of free parameters in the model to avoid over-fitting. The proposed AE model is composed of convolutional layers and a fully connected layer in the middle as shown in Fig. <ref type="figure" target="#fig_1">3</ref>, which is similar to the stacked convolutional autoencoder model proposed in <ref type="bibr" target="#b30">[31]</ref>.</p><p>The AE model details (e.g. layer configuration, parameter choices) are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Medical Image Super-Resolution (SR) With CNNs</head><p>Super-resolution (SR) image generation is an inverse problem where the goal is to recover spatial frequency information that is outside the spatial bandwidth of the low resolution (LR) observation x ∈ R N to predict a high resolution (HR) image y r ∈ R M (N M), as illustrated in the top row of Fig. <ref type="figure" target="#fig_0">1</ref>. Since the high frequency components are missing in the observation space, usually training examples are used to predict the most likely P( y r |x) HR output. Image SR is an ill-posed problem as there are an infinite number of solutions for a given input sample but only a few would be anatomically meaningful and accurate. As for the case of image segmentation, learnt shape representations can be used to regularise image SR, constraining the model to make only anatomically meaningful predictions.</p><p>Similar to the SR framework described in <ref type="bibr" target="#b33">[34]</ref>, our proposed SR model learns a mapping function : X → Y to estimate a high-resolution image ŷr = (x ; θ r ) where θ r denotes the model parameters such as convolution kernels and batchnormalisation statistics. The parameters are optimised by minimising the smooth 1 loss, also known as Huber loss, between the ground-truth high resolution image and the corresponding prediction. The smooth 1 norm is defined as</p><formula xml:id="formula_3">1 (k) = {0.5 k 2 if |k| &lt; 1 , |k| -0.5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>otherwise} and the SR training objective becomes min</head><formula xml:id="formula_4">θ r i∈S 1 (x i ; θ r ) -y i</formula><p>In the proposed SR framework, we used the same model as shown in Fig. <ref type="figure">2</ref>. It provides two main advantages over the state-of-the-art medical image SR model proposed in <ref type="bibr" target="#b33">[34]</ref>: (I) the network generates image features in the LR image grid rather than early upsampling of the features, which reduces memory and computation requirements significantly. As highlighted in <ref type="bibr" target="#b40">[41]</ref>, early upsampling introduces redundant computations in the HR space since no additional information is added into the model by performing transposed convolutions <ref type="bibr" target="#b48">[49]</ref> at an early stage. (II) The second advantage is the use of a larger receptive field to learn the underlying anatomy, which was not the case in earlier SR methods used in medical imaging <ref type="bibr" target="#b33">[34]</ref> and natural image analysis <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b40">[41]</ref> because these models usually operate on local patch level. Capturing large context indeed helps our model to better understand the underlying anatomy and this enables us to enforce global shape constraints. This is achieved by generating SR feature-maps in multiple scales using multi strides in the in-plane direction.</p><p>Similar to the ACNN-Seg model, it is possible to regularise SR models to synthesise anatomically more meaningful HR images. To achieve this goal, we extend the standard AE model to the T-L model which enables us to obtain shape representation codes directly from the intensity space. The idea is motivated by the recent work <ref type="bibr" target="#b18">[19]</ref> on 3D shape analysis in natural images. In the next section we will explain the training strategy and the use of the T-L model as a regulariser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. T-L Network Model and SR-ACNN</head><p>Shape encoding AE models operate only on the segmentation masks and this limits its application to SR problem where the model output is an intensity image. To circumvent this problem, we extend the standard denoising AE to the T-L regularisation model by combining the AE with a predictor network (Fig. <ref type="figure" target="#fig_1">3</ref>) p(x) : X → H. The predictor can map an input image into a low dimensional non-parametric representation of the underlying anatomy (e.g. shape and class label information), which is learnt by the AE. In other words, it enables us to learn a hidden representation space that can be reached by non-linear mappings from both image label space Y and image intensity space X . In this way, SR models can be regularised as well with respect to learnt anatomical priors.</p><p>This network architecture is useful in image analysis applications for two main reasons: (I) It enables us to build a regularisation network that could be used in applications different than image segmentation such as image SR. We propose to use this new regularisation network at training time of SR to enforce the models to learn global information about the images besides the standard pixel-wise ( 1 distance) image reconstruction loss. In this way, the regressor SR model is guided by the additional segmentation information, and it becomes robust against imaging artefacts and missing information. (II) The second important feature of the T-L model is the generalisation of the learnt representations. Joint training of the AE and predictor enables us to learn representations that could be extracted from both intensity and label space. The learnt codes will encode the variations that could be interpreted from both manual annotations and intensity images. Since a perfect mapping between the intensity and label spaces is practically not achievable, the T-L learnt codes are expected to be more representative due to the inclusion of additional information.</p><p>The T-L model is trained in two stages: In the first stage, the AE is trained separately with ground-truth segmentation masks and cross-entropy loss L x . Later, the predictor model is trained to match the learnt latent space h by minimising the Euclidean distance L h between the codes predicted by the AE and predictor as shown in Fig. <ref type="figure" target="#fig_1">3</ref>. Once the loss functions for both the AE and the predictor converge, the two models are trained jointly in the second stage. The encoder f is updated using two separate back-propagated gradients ( ∂ L x ∂θ f , ∂ L h ∂θ f ) and the two loss functions are scaled to match their range. The first gradient encourages the encoder to generate codes that could be easily extracted by the predictor while the second gradient making sure that a good segmentation-reconstruction can be obtained at the output of the decoder. Training details are further discussed in Section III-B. It is important to note that the T-L regulariser model is used only at training time but not during inference; in other words, the fully convolutional (FCN) segmentation and super-resolution models can still be used for applications using different image sizes. In this paper, the proposed SR model is referred to as ACNN-SR and its training scheme is shown in the bottom part of Fig. <ref type="figure" target="#fig_2">4</ref>.</p><formula xml:id="formula_5">L h p = p ((x); θ p ) -p ( y r ; θ p ) 2 2 min θ r 1 (x ; θ r ) -y r + λ 1 • L h p + λ 2 2 ||w|| 2 2</formula><p>(2) The training objective shown above is composed of weight decay, pixel-wise and global loss terms. Here λ 1 and λ 2 determine the weight of shape priors and weight decay terms while the smooth 1 norm loss function quantifies the reconstruction error. The global loss L h p is defined as the Euclidean distance between the codes generated from the synthesised and ground-truth HR images. The T-L model is used only in the network training phase as a regularisation term, similar to VGG features <ref type="bibr" target="#b41">[42]</ref> that were used for representing a perceptual loss function <ref type="bibr" target="#b20">[21]</ref>. However, we are not interested in expanding the output space to a larger feature-map space, but instead obtain a compact representation of the underlying anatomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Learnt Hidden Representations</head><p>The learnt low dimensional representation h is used to constrain NN models. Low dimensional encoding enables us to train models with global characteristics but also yields better generalisation power for the underlying anatomy as shown in earlier work <ref type="bibr" target="#b42">[43]</ref>. However, since we update our segmentation and SR model parameters with the gradients back-propagated from the global loss layer using the Euclidean distance of these representations, it is essential to analyse the distribution of the extracted codes. In Fig. <ref type="figure" target="#fig_3">5</ref>, due to space limitations, we show the histogram of 16 randomly chosen codes (out of 64) of a T-L model trained with cardiac MR segmentations. Note that each histogram is constructed using the corresponding code for every sample in the full dataset. It is observed that the learnt latent representations in general follow a normal distribution and they are not separated in multi-clusters (e.g. mixture of Gaussians). A smooth distribution of the codes ensures better supervision for the main NN model (SR, Seg) since the global gradients are back-propagated by computing the Euclidean distance between the obtained distributions.</p><p>This observation can be explained by the fact that the proposed T-L network is trained with small Gaussian input noise as in the case of denoising autoencoders. In <ref type="bibr" target="#b0">[1]</ref>, Alain and Bengio showed that the denoising reconstruction error is equivalent to contractive penalty, which forces the feature extraction (encoder) function f resist perturbations of the input and contracts these input samples to similar low dimensional codes. The penalty is defined as</p><formula xml:id="formula_6">(h) = λ ∂ f (x) ∂ x 2 F</formula><p>, where F denotes the Frobenius norm (sum of squared elements), and h = f (x) represents the codes. The given penalty function promotes the network to learn the underlying low-dimensional data manifold and capture its local smooth structure. In addition to the smoothness of the latent distributions, the extracted codes are expected to be correlated since the decoder merges some of the codes along the three spatial dimensions to construct input feature maps for the transposed convolutions, but this characteristic is not a limitation in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPLICATIONS AND EXPERIMENTS</head><p>In this section, we present three different applications of the proposed ACNN model: 3D-US and cardiac MR image segmentation, as well as cardiac MR image SR. The experiments focus on demonstrating the importance of shape and label priors for image analysis. Additionally, we analyse the salient information stored in the learnt hidden representations and correlate them with clinical indices, showing their potential use as biomarkers for pathology classification. The next subsection describes the clinical datasets used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Clinical Datasets 1) UK Digital Heart Project Dataset:</head><p>This dataset<ref type="foot" target="#foot_0">1</ref> is composed of 1200 pairs of cine 2D stack short-axis (SAX) and cine 3D high resolution (HR) cardiac MR images. Each image pair is acquired from a healthy subject using a standard imaging protocol <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In more detail, the 2D stacks are acquired in different breath-holds and therefore may contain motion artefacts. Similarly, 3D imaging is not always feasible in the standard clinical setting due to the requirements for long image acquisition. The voxel resolution of the images are fixed to 1.25 × 1.25 × 10.00 mm and 1.25 × 1.25 × 2.00 mm for 2D stack low resolution (LR) and HR images respectively. Dense segmentation annotations for HR images are obtained by manually correcting initial segmentations generated with a semi-automatic multi-atlas segmentation method <ref type="bibr" target="#b4">[5]</ref>, and all the annotations are performed on the HR images to minimise errors introduced due to LR in through plane direction. Since the ground-truth information is obtained from the HR motionfree images, the experimental results are expected to reflect the performance of the method with respect to an appropriate reference. The annotations consist of pixel-wise labelling of endocardium and myocardium classes. Additionally, the residual spatial misalignment between the 2D LR stacks and HR volumes is corrected using a rigid transformation estimated by an intensity based image registration algorithm.</p><p>2) CETUS'14 Challenge Dataset: CETUS'14 segmentation challenge <ref type="bibr" target="#b7">[8]</ref> is a publicly available platform 2 to benchmark cardiac 3D ultrasound (US) left-ventricle (LV) segmentation methods. The challenge dataset is composed of 3D+time US image sequences acquired from 15 healthy subjects and 30 patients diagnosed with myocardial infarction or dilated cardiomyopathy. The images were acquired from apical windows and LV chamber was the main focus of analysis. Resolution of the images was fixed to 1 mm isotropic voxel size through linear interpolation. The associated manual contours of the LV boundary were drawn by three different expert cardiologists, and the annotations were performed only on the frames corresponding to end-diastole (ED) and endsystole (ES) phases. Method evaluation is performed in a blinded fashion on the testing set (30 out of 45) using the MIDAS web platform. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details of the Proposed Model</head><p>In this section, we discuss the details of data augmentation used in training, and also the optimisation scheme of the T-L model training. To improve the model's generalisation capability, the input training samples are artificially augmented using affine transformations, which is used in both the segmentation and T-L models. For the SR models, on the other hand, respiratory motion artefacts between the adjacent slices are simulated via in-plane rigid transformations that are defined for each slice independently. The corresponding ground-truth HR images are not spatially transformed; in this way, the models learn to output anatomically correct results when the input slices are motion corrupted. Additionally, additive Gaussian noise is applied to input intensity images to make the segmentation and super-resolution models more robust against image noise. For the AE, the tissue class labels are randomly swapped with the probability of 0.1 to encourage the model to map slightly different segmentation masks to neighbouring points in the lower dimensional latent space. It ensures the smoothness of the learnt low-dimensional manifold space as explained in Section II-E.</p><p>In the joint training of the T-L network, parameters of the encoder model ( f ) are updated by the gradients originating from both the cross-entropy loss (L x ) and Euclidean distance terms (L h ). Instead of applying these two gradient descent updates sequentially in an iterative fashion, we perform a joint update training scheme and experimentally observed better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cardiac Cine-MR Image Segmentation</head><p>In this experiment, NN models are used to segment cardiac cine MR images in the dataset described in Sec. III-A1. As an input to the models, only the 2D stack LR images are used, 3 https://www.creatis.insa-lyon.fr/Challenge/acdc/ Fig. <ref type="figure">6</ref>.</p><p>Segmentation results on two different 2D stack cardiac MR images. The proposed ACNN model is insensitive to slice misalignments as it is anatomically constrained and it makes less errors in basal and apical slices compared to the 2D-FCN approach. The results generated from low resolution image is better correlated with the HR ground-truth annotations (green).</p><p>which is a commonly used acquisition protocol for cardiac imaging, and the segmentation is performed only on the ED phase of the sequences. The corresponding ground-truth label maps, however, are projected from the HR image space, which are annotated in the HR image grid. The dataset (1200 LR images &amp; HR labels) is randomly partitioned into three subsets: training (900), validation (100), and testing (200). All the images are linearly intensity normalised and cropped based on the automatically detected six anatomical landmark locations <ref type="bibr" target="#b32">[33]</ref>.</p><p>The proposed ACNN-Seg method is compared against: the current state-of-the-art cine MR 2D slice by slice segmentation method (2D-FCN) <ref type="bibr" target="#b43">[44]</ref>, 3D-UNet model <ref type="bibr" target="#b11">[12]</ref>, cascaded 3D-UNet and convolutional AE model (AE-Seg) <ref type="bibr" target="#b36">[37]</ref>, subpixel 3D-CNN segmentation model (3D-Seg) proposed in Sec. II-A, and the same model trained with various types of motion augmentation (3D-Seg-MAug). As the models have a different layout, the number of trainable parameters (pars) used in each model is kept fixed to avoid any bias. For the cascaded AE-Seg model, however, additional convolutional kernels are used in the AE as suggested in <ref type="bibr" target="#b36">[37]</ref>. To observe the influence of the AE model's capacity on the AE-Seg model's performance, we performed experiments using different number of AE pars, and the largest capacity case is denoted by AE-Seg-M.</p><p>The results of the experiments are provided in Table I together with the capacity of each model. Statistical significance of the results is verified by performing the Wilcoxon signed-rank test between the top two performing methods for each evaluation metric. Based on these results we can draw three main conclusions: (I) Slice by slice analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b43">[44]</ref> significantly under-performs compared to the proposed subpixel and ACNN-Seg segmentation methods. In particular, the dice score metrics are observed to be lower since 2D analysis can yield poor performance in basal and apical parts of the heart as shown in Fig. <ref type="figure">6</ref>. Previous slice by slice segmentation approaches validated their methods on LR annotations; however, we see that the produced label maps are far off from the true underlying ventricular geometry and it can be a limiting factor for the analysis of ventricle morphology. Similar results were obtained in clinical studies <ref type="bibr" target="#b14">[15]</ref>, which however required HR image acquisition techniques. (II) The results also show that introduction of shape priors in segmentation models can be useful to tackle false-positive detections and motion-artefacts. As can be seen in the bottom row of Fig. <ref type="figure">6</ref>, without the learnt shape priors, label map predictions are more prone to imaging artefacts. Indeed, it is the main reason why we observe such a large difference in terms of Hausdorff distance. For endocardium labels, on the other hand, the difference in dice score metric is observed to be less due to the larger size of the LV blood pool compared to the myocardium. Lastly (III), we observe a performance difference between the cascaded AE based segmentation (AE-Seg <ref type="bibr" target="#b36">[37]</ref>) and the proposed ACNN-Seg models: the segmentations generated with the former model are strongly regularised due to the second stage AE. It results in reduced Hausdorff distance with marginal statistical significance, but the model overlooks fine details of the myocardium surface since the segmentations are generated only from the coarse level feature-maps. More importantly, cascaded approaches add additional computational complexity due to the increased number of filters, which could be redundant given that the standard segmentation model is able to capture shape properties of the organs as long as it has a large receptive field and is optimised with shape constraints. In other words, shape constraints can be learnt and utilised in standard segmentation models, as shown in ACNN-Seg, without a need for additional model parameters and computational complexity. We also analysed the performance change in AE-Seg with respect to the number of parameters, which shows that the small capacity AE-Seg model (8 × 10 4 pars) is not suitable for cardiac image segmentation as the second stage in the cascaded model does not improve the performance significantly.</p><p>We performed additional segmentation experiments using only the T-L network. In detail, the input LR image is passed first through the predictor network and then the extracted codes are fed to the decoder network shown in Fig. <ref type="figure" target="#fig_1">3</ref>. Label map predictions are collected at the output of the decoder and they are compared with the same ground-truth annotations described previously, which was similar to the AE based segmentation method proposed in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>. We observed that reconstruction of label-maps from low dimensional representations was limited since the ventricle boundaries were not delineated properly but rather a rough segmentation was generated (DSC: .734). We believe that this is probably the main reason why Avendi et al. <ref type="bibr" target="#b1">[2]</ref> proposed the use of a separate deformable model at the output of a NN. Nevertheless, the proposed ACNN-Seg does not require an additional postprocessing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cardiac 3D Ultrasound Image Segmentation</head><p>In the second experiment, the proposed model is evaluated on 3D cardiac ultrasound data which is described in Sec III-A2. Segmentation models are used to delineate endocardial boundaries and the segmentations obtained on ED and ES frames are later used to measure volumetric indices such as ejection fraction (EF). The models are compared also in terms of surface to surface distance errors of their corresponding endocardium segmentations. As a baseline CNN method, we utilised the fully convolutional network model suggested by <ref type="bibr" target="#b10">[11]</ref> for multi-view 3D-US image segmentation problem. It is also observed to be more memory efficient compared to the standard 3D-UNet architecture <ref type="bibr" target="#b11">[12]</ref>. Additionally, we compare our proposed model against the CETUS'14 challenge winner approach (BEAS) <ref type="bibr" target="#b5">[6]</ref> that utilised deformable models to segment the left ventricular cavity. The challenge results can be found in <ref type="bibr" target="#b7">[8]</ref>. The experimental results, given in Table <ref type="table" target="#tab_1">II</ref>, show that neural network models outperforms previous state-of-the-art approaches in this public benchmark dataset although the training data size was limited to 15 image sequences. The experimental results were evaluated in a blinded fashion by uploading the generated segmentations from separate 30 sequences into the CETUS web platform.</p><p>The main contribution of ACNN model over the standard FCN approaches is the improved shape delineation of the LV, as it can be seen in terms of the distance error metrics. In particular, Hausdorff distances were reduced significantly as global regularisation reduces the amount of spurious false positive predictions and enforces abnormal LV segmentation shapes to fit into the learnt representation model. This situation is illustrated in Fig. <ref type="figure" target="#fig_5">7</ref>. Similarly, we observed an improvement in terms of normalised Dice score, which was quantitatively not significant due to large volumetric size of the LV cavity. Lastly, we compared the extracted ejection fraction results to understand both the accuracy of segmentations and also the consistency of these predictions on both ED and ES phases. It is observed that the ACNN approach ensures better consistency between frames although none of the methods have used temporal information.</p><p>The reported results could be further improved by segmenting both ED and ES frames simultaneously or by extracting the temporal content from the sequences. For instance, propagation of ED masks to ES frames through optical flow has been shown to be a promising way to achieve this goal. However, this study mainly focuses on demonstrating the advantages of using priors in neural network models, and achieving the best possible segmentation accuracy was not our main focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cardiac MR Image Enhancement</head><p>The proposed ACNN model is also applied to the image SR problem and compared against the state-of-the-art CNN model used in medical imaging <ref type="bibr" target="#b33">[34]</ref>. The cardiac MR dataset, described in Sec. III-A1, was split into two disjoint subsets:  <ref type="bibr" target="#b19">[20]</ref>. More details about the acquisition model can be found in <ref type="bibr" target="#b33">[34]</ref>.</p><p>The quality of the upsampled images is evaluated in terms of SSIM metric <ref type="bibr" target="#b46">[47]</ref> between the clinical HR image data and reconstructed HR images. SSIM measure assesses the correlation of local structures and is less sensitive to image noise than PSNR which is not used in our experiments since small misalignments between LR-HR image pairs could introduce large errors in the evaluation due to pixel by pixel comparisons. More importantly, intensity statistics of the images are observed to be different for this reason PSNR measurements would not be accurate. In addition to the SSIM metric, we used the mean opinion score (MOS) testing <ref type="bibr" target="#b25">[26]</ref> to quantify the quality and similarity of the synthesised and real HR cardiac images. Two expert cardiologists were asked to rate the upsampled images from 1 (very poor) to 5 (excellent) based on the accuracy of the reconstructed LV boundary and geometry. To serve as a reference, the corresponding clinical LR and HR images are displayed together with the upsampled images that are anonymised for a fair comparison.</p><p>In Table <ref type="table" target="#tab_1">III</ref>, SSIM and MOS scores for the standard interpolation techniques, SR-CNN, and the proposed ACNN-SR models are provided. In addition to the increased image quality, the ACNN-SR model is computationally more efficient in terms of run-time in comparison to the SR-CNN model <ref type="bibr" target="#b33">[34]</ref> by a factor of 5. This is due to the fact that ACNN-SR performs feature extraction in the low dimensional image space. Furthermore, we investigated the contribution of shape regularisation term in the application of SR, which is visualised in Fig. <ref type="figure" target="#fig_6">8</ref>.</p><p>Moreover, we investigated the use of SR as a pre-processing technique for subsequent analysis such as image segmentation, similar to the experiments reported in <ref type="bibr" target="#b33">[34]</ref>. In that regard, the proposed SR model and U-Net segmentation models are concatenated to obtain HR segmentation results. However, we observed that the proposed baseline sub-pixel segmentation model (3D-Seg), which merges both SR and segmentation tasks, performs better than the concatenated models. The 3D-Seg approach uses the convolution kernels more efficiently without requiring the model to output a high-dimensional intensity image. For this reason, SR models should be trained by taking into account the final goal and in some cases it's not required to reconstruct a HR intensity image for HR analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Learnt Latent Representations and Pathology Classification</head><p>The jointly trained T-L model and its latent representations are analysed and evaluated in the experiment of image pathology classification. This experiment focuses on understanding the information stored in the latent space and also investigates whether they can be used to distinguish healthy subjects from dilated and hypertrophic cardiomyopathy patients. For this, we collected 64 dimensional codes from segmentation images of the cardiac MR dataset explained in Sec. III-A3. Similarly, principal component analysis (PCA) was applied to the same segmentation images (containing LV blood-pool and myocardium labels) to generate 64 dimensional linear projection of the labels, which requires additional spatialnormalisation prior to linear mapping. The generated codes were then used as features to train an ensemble of decision trees to categorise each image. We used 10-fold crossvalidation on 60 CMR sequences and obtained 76.6% vs 83.3% accuracy using PCA and T-L codes extracted from ED phase. By including the codes from ES phase, the classification accuracies were improved to 86.6% vs 91.6%. This result shows that although the AE and T-L models are not trained with the classification objective, they can still capture anatomical shape variations that are linked to cardiac related pathologies. In particular, we observed that some latent dimensions are more commonly used than others in tree node splits. By sampling codes from the latent space across these dimensions, we observed that the network captures the variation in wall thickness and blood pool size as shown in Fig. <ref type="figure" target="#fig_7">9</ref>. Since we obtain a regular and smooth latent representation, it is possible to transverse along the latent space and generate LV shapes by interpolating between data points. It is important to note that classification accuracies can be further improved by training the AE and T-L models with a classification objective. Our main goal in this experiment was to understand whether the enforced prior distributions contain anatomical information or they are abstract representations only meaningful to the decoder of the AE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONCLUSION</head><p>In this work, we presented a new image analysis framework that utilises autoencoder (AE) and T-L networks as regularisers to train neural network (NN) models. With this new training objective, at testing time NNs make predictions that are in agreement with the learnt shape models of the underlying anatomy, which are referred as image priors. The experimental results show that the state-of-the-art NN models can benefit from the learnt priors in cases where the images are corrupted and contain artefacts. The proposed regulariser model can be seen as an application-specific training objective. In that regard, our model differentiates from the VGG-Net <ref type="bibr" target="#b41">[42]</ref> feature based training objectives <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>. VGG features tend to be more general purpose representations that are learnt from ImageNet dataset containing natural images of a large variety of objects. In contrast to this, our AE model is trained solely on cardiac segmentation masks and features are customised to identify anatomical variations observed in the heart chambers. For this reason, we would expect the AE features of the segmentations to be more distinctive and informative.</p><p>As an alternative to the proposed framework, label space dependencies could be exploited also through adversarial loss (AL) objective functions. Such approaches have been used successfully in natural image super-resolution (SR) <ref type="bibr" target="#b25">[26]</ref> and segmentation <ref type="bibr" target="#b29">[30]</ref> tasks. In SR application, AL enables the SR network to hallucinate fine texture detail, and the synthesized HR images appear qualitatively more realistic. However, at the same time the PSNR and SSIM scores are usually worse. For this reason, the authors of <ref type="bibr" target="#b25">[26]</ref> have pointed out that adversarial training may not be suitable for medical applications, where the accuracy and fidelity of the visual content more important than the qualitative appearance of the HR images. Moreover, we believe that adversarial training comes at the expense of less interpretability of the regularisation term and unstable model training behaviour, which still remains an open research problem.</p><p>Additionally, in the experiments we demonstrated that the learnt codes can be used as biomarkers for classification of cardiac related pathologies and we analysed the distribution of the learnt latent space. This latent space can be further constrained to be Gaussian distributed by replacing the proposed regularisation model with a variational autoencoder. However, this design choice was not considered in our ACNN framework due to two main reasons: (I) the additional K-L divergence term (constraint) would reduce the representation power of the AE; thus, the local anatomical variations would not be captured in detail. (II) A generative AE model is not essential for the regularisation of the proposed segmentation and SR models. A variational architecture would be useful if it was required to sample random instances from the latent space and reconstruct anatomically meaningful segmentation masks; however, in our framework we are only interested in the anatomy specific AE features for model regularisation.</p><p>The presented ACNN framework is not only limited to the medical image segmentation and SR tasks but can be extended to other image analysis tasks where prior knowledge can provide model guidance and robustness. In that regard, future research will focus on the application of ACNN to the problems such as human pose estimation, anatomical and facial landmark localisation on partially occluded image data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Results for cardiac MR super-resolution (SR) (top), MR segmentation (middle), and ultrasound (US) segmentation (bottom). From left to right, we show the input image, a state-of-the-art competing method, the proposed result, and the ground-truth. (a) Stack of 2D MR images with respiratory motion artefacts, (b) SR based on CNNs [34], (c) the proposed ACNN-SR, (d) ground-truth high-resolution (HR) image, (e) low resolution MR image, (f) 2D segmentation resulting in blocky contours<ref type="bibr" target="#b43">[44]</ref>, (g) 3D sub-pixel segmentation from stack of 2D MR images using ACNN, (h) manual segmentation from HR image, (i) input 3D-US image, (j) FCN based segmentation<ref type="bibr" target="#b10">[11]</ref>, (k) ACNN, and (l) manual segmentation.</figDesc><graphic coords="2,312.23,121.61,62.97,63.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Block diagram of the stacked convolutional autoencoder (AE) network (in grey), which is trained with segmentation labels. The AE model is coupled with a predictor network (in blue) to obtain a compact nonlinear representation that can be extracted from both intensity and segmentation images. The whole model is named as T-L network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Training scheme of the proposed anatomically constrained convolutional neural network (ACNN) for image segmentation and superresolution tasks. The proposed T-L network is used as a regularisation model to enforce the model predictions to follow the distribution of the learnt low dimensional representations or priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Histogram of the learnt low-dimensional latent representations (randomly selected 16 components are shown). The codes in general follow a smooth and normal distribution which is important for the training of ACNN models.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 )</head><label>3</label><figDesc>ACDC MICCAI'17 Challenge Dataset: The aim of the ACDC'17 challenge 3 is to compare the performance of automatic methods for the classification of MR image examinations in terms of healthy and pathological cases: infarction, dilated cardiomyopathy, and hypertrophic cardiomyopathy. The publicly available dataset consists of 20 (per class) cine stacks of 2D MR image sequences which are annotated at ED and ES phases by a clinical expert. In the experiments, latent representations (codes) extracted with the proposed T-L network are used to classify these images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Cavity noise limits accurate delineation of the LV cavity in apical areas. (b) The segmentation model can be guided through learnt shape priors to output anatomically correct delineations. (c) Similarly, it can make accurate predictions even when the ventricle boundaries are occluded.</figDesc><graphic coords="8,310.91,303.53,253.69,77.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Image super-resolution (SR) results. From left to right, input low resolution MR image, baseline SR approach [34] (no global loss), the proposed anatomically constrained SR model, and the ground-truth high resolution acquisition.</figDesc><graphic coords="10,231.83,59.33,57.38,55.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Anatomical variations captured by the latent representations in T-L network (swipe from μ -2σ to μ + 2σ). Based on our observation, the first and second dimensions capture the variation in the wall thickness of the myocardium (x-axis) and lateral wall of the ventricle (y-axis).</figDesc><graphic coords="10,355.67,59.21,176.18,174.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STACKS</head><label>I</label><figDesc>OF 2D CARDIAC MR IMAGES (200) ARE SEGMENTED INTO LV ENDOCARDIUM AND MYOCARDIUM, AND THE SEGMENTATION ACCURACY IS EVALUATED IN TERMS OF DICE METRIC AND SURFACE TO SURFACE DISTANCES. THE GROUND-TRUTH LABELS ARE OBTAINED FROM HIGH RESOLUTION 3D IMAGES ACQUIRED FROM SAME SUBJECTS, WHICH DO NOT CONTAIN MOTION AND BLOCKY ARTEFACTS. THE PROPOSED APPROACH (ACNN-SEG) IS COMPARED AGAINST STATE-OF-THE-ART SLICE BY SLICE SEGMENTATION (2D-FCN<ref type="bibr" target="#b43">[44]</ref>) METHOD, 3D-UNET MODEL<ref type="bibr" target="#b11">[12]</ref>, CASCADED 3D-UNET AND CONVOLUTIONAL AE MODEL (AE-SEG)<ref type="bibr" target="#b36">[37]</ref>, PROPOSED SUB-PIXEL SEGMENTATION MODEL (3D-SEG) AND THE SAME MODEL WITH MOTION AUGMENTATION USED IN TRAINING (3D-SEG-MAUG)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II 3D</head><label>II</label><figDesc>-US CARDIAC IMAGE SEQUENCES (IN TOTAL 30) ARE SEGMENTED INTO LV CAVITY AND BACKGROUND. SEGMENTATION ACCURACY IS EVALUATED IN TERMS OF DICE SCORE (DSC), SURFACE-TO-SURFACE DISTANCES. THE CONSISTENCY OF DELINEATIONS ON BOTH ED AND ES PHASES ARE MEASURED IN TERMS COMPUTED EJECTION FRACTION (EF) VALUES. THE PROPOSED ACNN-SEG METHOD IS COMPARED AGAINST STATE-OF-THE ART DEFORMABLE SHAPE FITTING [6] AND FULLY-CONVOLUTIONAL 3D SEGMENTATION [11] METHODS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://digital-heart.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.creatis.insa-lyon.fr/Challenge/CETUS/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The views expressed are those of the authors and not necessarily those of the NHS, the EPSRC, the NIHR or the Department of Health.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by an EPSRC Program under Grant EP/P001009/1, in part by the British Heart Foundation, U.K., under Grant PG/12/27/29489, in part by the National Institute for Health Research (NIHR) Biomedical Research Centre-based at Imperial College Healthcare NHS Trust.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What regularized auto-encoders learn from the data-generating distribution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3563" to="3593" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A combined deeplearning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Avendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheradvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jafarkhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA Image Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic segmentation of the right ventricle from cardiac MRI using a learning-based approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Avendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheradvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jafarkhani</surname></persName>
		</author>
		<idno type="DOI">10.1002/mrm.26631</idno>
	</analytic>
	<monogr>
		<title level="m">Magnetic Resonance in Medicine</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A bi-ventricular cardiac atlas built from 1000+ high resolution MR images of healthy subjects and an analysis of shape and motion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA Image Anal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="145" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A probabilistic patch-based label fusion model for multiatlas segmentation with registration refinement: Application to cardiac MR images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1302" to="1315" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and fully automatic 3-D echocardiographic segmentation using B-spline explicit active surfaces: Feasibility study and validation in a clinical setting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="101" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topology aware fully convolutional networks for histology gland segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bentaieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. MICCAI</title>
		<meeting>Int. Conf. MICCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Standardized evaluation system for left ventricular segmentation algorithms in 3D echocardiography</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning shape priors for object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1870" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DCAN: Deep contour-aware networks for object instance segmentation from histology images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. MICCAI</title>
		<meeting>Int. Conf. MICCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D U-Net: Learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining point distribution models with shape models based on finite element analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="403" to="409" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical active shape models, using the wavelet transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="414" to="423" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Population-based studies of myocardial hypertrophy: High resolution cardiovascular magnetic resonance atlases improve statistical power</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Marvao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cardiovascular Magn. Reson</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generative model for parts-based object segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The shape Boltzmann machine: A strong model of object shape</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolution in medical imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of cardiovascular magnetic resonance imaging in heart failure</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Karamitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Myerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Selvanayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neubauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. College Cardiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1407" to="1424" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EAE/ASE recommendations for image acquisition and display using three-dimensional echocardiography</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Echocardiogr</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.04802" />
		<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1702.05747" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08408</idno>
		<ptr target="https://arXivpreprint" />
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Neural Netw</title>
		<meeting>Int. Conf. Artif. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Incorporating prior knowledge in medical image segmentation: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nosrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.01092" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stratified decision forests for accurate anatomical landmark localization in cardiac images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="332" to="342" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-input cardiac image super-resolution using convolutional neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. MICCAI</title>
		<meeting>Int. Conf. MICCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UK Biobank&apos;s cardiovascular magnetic resonance protocol</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cardiovascular Magn. Reson</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint deep learning of foreground, background and shape for robust contextual segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiruvenkadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPMI</title>
		<meeting>IPMI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="622" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning and incorporating shape models for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiruvenkadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sudhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ResearchGate</title>
		<meeting>ResearchGate</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. MICCAI, 2015</title>
		<meeting>Int. Conf. MICCAI, 2015</meeting>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep spectralbased shape features for Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SASHIMI</title>
		<meeting>SASHIMI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">VConv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A fully convolutional neural network for cardiac segmentation in short-axis MRI</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1604.00494" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks and 3D brain image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
