<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraRep: Learning Graph Representations with Global Structural Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research -China Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GraRep: Learning Graph Representations with Global Structural Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2806416.2806512</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.6 [Artificial Intelligence]: Learning; H.2.8 [Database Management]: Database Applications -Data Mining Algorithms</term>
					<term>Experimentation Graph Representation</term>
					<term>Matrix Factorization</term>
					<term>Feature Learning</term>
					<term>Dimension Reduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present GraRep, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the Deep-Walk model of Perozzi et al. [20]  as well as the skip-gram model with negative sampling of Mikolov et al. <ref type="bibr" target="#b17">[18]</ref> We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In many real-world problems, information is often organized using graphs. For example, in social network research, classification of users into meaningful social groups based on social graphs can lead to many useful practical applications such as user search, targeted advertising and recommendations. Therefore, it is essential to accurately learn useful information from the graphs. One strategy is to learn the graph representations of a graph: each vertex of the graph is represented with a low-dimensional vector in which meaningful semantic, relational and structural information conveyed by the graph can be accurately captured.</p><p>Recently, there has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type="bibr" target="#b19">[20]</ref>, one recent model, transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling (which is also called truncated random walk). The skip-gram model <ref type="bibr" target="#b17">[18]</ref>, originally designed for learning word representations from linear sequences, can also be used to learn the representations of vertices from such samples. Although this method is empirically effective, it is not well understood what is the exact loss function defined over the graph involved in their learning process.</p><p>In this work, we first present an explicit loss function of the skip-gram model defined over the graph. We show that essentially we can use the skip-gram model to capture the k-step (k = 1, 2, 3, . . . ) relationship between each vertex and its k-step neighbors in the graph with different values of k. One limitation of the skip-gram model is it projects all such k-step relational information into a common subspace. We argue that such a simple treatment can lead to potential issues. The above limitation is overcome in our proposed model through the preservation of different k-step relational information in distinct subspaces.</p><p>Another recently proposed work is LINE <ref type="bibr" target="#b24">[25]</ref>, which has a loss function to capture both 1-step and 2-step local relational information. To capture certain complex relations in such local information, they also learn non-linear transformations from such data. While their model can not be easily extended to capture k-step (with k &gt; 2) relational information for learning their graph representation, one important strategy used to enhance the effectiveness of their model is to consider higher-order neighbors for vertices with small degrees. This strategy implicitly captures certain kstep information into their model to some extent. We believe k-step relational information between different vertices, with different values of k, reveals the useful global structural information associated with the graph, and it is essential to explicitly take full advantage of this when learning a good graph representation.</p><p>In this paper, we propose GraRep, a novel model for learning graph representations for knowledge management. The model captures the different k-step relational information with different values of k amongst vertices from the graph directly by manipulating different global transition matrices defined over the graph, without involving slow and complex sampling processes. Unlike existing work, our model defines different loss functions for capturing the different k-step local relational information (i.e., a different k). We optimize each model with matrix factorization techniques, and construct the global representations for each vertex by combining different representations learned from different models. Such learned global representations can be used as features for further processing.</p><p>We give a formal treatment of this model, showing the connections between our model and several previous models. We also demonstrate the empirical effectiveness of the learned representations in solving several real-world problems. Specifically, we conducted experiments on a language network clustering task, a social network multi-label classification task, as well as a citation network visualization task. In all such tasks, GraRep outperforms other graph representation methods, and is trivially parallelizable.</p><p>Our contributions are as follows:</p><p>• We introduce a novel model to learn latent representations of vertices on graphs, which can capture global structural information associated with the graph.</p><p>• We provide from a probabilistic prospective an understanding of the uniform sampling method used in DeepWalk for learning graph representations, which translates a graph structure into linear sequences. Furthermore, we explicitly define their loss function over graphs and extend it to support weighted graphs.</p><p>• We formally analyze the deficiency associated with the skip-gram model with negative sampling. Our model defines a more accurate loss function that allows nonlinear combinations of different local relational information to be integrated.</p><p>The organization of this paper is described below. Section 2 discusses related work. Section 3 proposes our loss function and states the optimization method using matrix factorization. Section 4 presents the overall algorithm. Section 5 gives a mathematical explanation to elucidate the rationality of the proposed work and shows its connection to previous work. Section 6 discusses the evaluation data and introduces baseline algorithms. Section 7 presents experiments as well as analysis on the parameter sensitivity. Finally, we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linear Sequence Representation Methods</head><p>Natural language corpora, consisting of streams of words, can be regarded as special graph structures, that is, linear chains. Currently, there are two mainstream methods for learning word representations: neural embedding methods and matrix factorization based approaches.</p><p>Neural embedding methods employ a fixed slide window capturing context words of current word. Models like skipgram <ref type="bibr" target="#b17">[18]</ref> are proposed, which provide an efficient approach to learning word representations. While these methods may yield good performances on some tasks, they can poorly capture useful information since they use separate local context windows, instead of global co-occurrence counts <ref type="bibr" target="#b18">[19]</ref>. On the other hand, the family of matrix factorization methods can utilize global statistics <ref type="bibr" target="#b4">[5]</ref>. Previous work include Latent Semantic Analysis (LSA) <ref type="bibr" target="#b14">[15]</ref>, which decomposes termdocument matrix and yields latent semantic representations. Lund et al. <ref type="bibr" target="#b16">[17]</ref> put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to generate word representations. Levy et al. <ref type="bibr" target="#b3">[4]</ref> presented matrix factorization over shifted positive Pointwise Mutual Information (PMI) matrix for learning word representations and showed that the Skip-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Representation Approaches</head><p>There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) <ref type="bibr" target="#b7">[8]</ref>, IsoMap <ref type="bibr" target="#b27">[28]</ref>, LLE <ref type="bibr" target="#b20">[21]</ref>, and Laplacian Eigenmaps <ref type="bibr" target="#b2">[3]</ref>. Recently, Tang et al. <ref type="bibr" target="#b26">[27]</ref> presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification. Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type="bibr" target="#b19">[20]</ref> presented an approach, which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip-gram model. This is considered as an equally weighted linear combination of k-step information. Tang et al. <ref type="bibr" target="#b24">[25]</ref> later proposed a large-scale information network embedding, which optimizes a loss function where both 1-step and 2-step relational information can be captured in the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GRAREP MODEL</head><p>In this section, we define our task and present our loss function for our task, which is then optimized with the matrix factorization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graphs and Their Representations</head><p>Definition 1. (Graph) A graph is defined as G = (V, E). V = {v1, v2, . . . , vn} is the set of vertices with each V indicating one object while E = {ei,j} is the set of edges with each E indicating the relationship between two vertices. A path is a sequence of edges which connect a sequence of vertices.</p><p>We first define adjacency matrix S for a graph. For an unweighted graph, we have Si,j = 1 if and only if there exists an edge from vi to vj, and Si,j = 0 otherwise. For a weighted graph, Si,j is a real number called the weight of the edge ei,j, which indicates the significance of edge. Although weights can be negative, we only consider non-negative weights in this paper. For notational convenience, we also use w and c to denote vertices throughout this paper.</p><p>The following diagonal matrix D is known as the degree matrix for a graph with adjacency matrix S:</p><formula xml:id="formula_0">Dij = p Sip, if i = j 0, if i = j</formula><p>Assume we would like to capture the transitions from one vertex to another, and assume in the adjacency matrix Si,j is proportional to the transition probability from vi to vj, we can define the following (1-step) probability transition matrix A:</p><formula xml:id="formula_1">A = D −1 S</formula><p>where Ai,j is the probability of a transition from vi to vertex vj within one step. It can be observed that the A matrix can be regarded as a re-scaled S matrix whose rows are normalized. In this paper, global structural information serves two functions: 1) the capture of long distance relationship between two different vertices and 2) the consideration of distinct connections in terms of different transitional steps. This would be further illustrated later.</p><p>As we have discussed earlier, we believe the k-step (with varying k) relational information from the graph needs to be captured when constructing such global graph representations. To validate this point, Figure <ref type="figure" target="#fig_0">1</ref> gives some illustrative examples showing the importance of k-step (for k=1,2,3,4) relational information that needs to be captured between two vertices A1 and A2. In the figure, a thick line indicates a strong relation between two vertices, while a thin line indicates a weaker relation. Here, (a) and (e) show the importance of capturing the simple 1-step information between the two vertices which are directly connected to each other, where one has a stronger relation and the other has a weaker relation. In (b) and (f), 2-step information is shown, where in (b) both vertices share many common neighbors, and in (f) only one neighbor is shared between them. Clearly, 2step information is important in capturing how strong the connection between the two vertices is -the more common neighbors they share, the stronger the relation between them is. In (c) and (g), the importance of 3-step information is illustrated. Specifically, in (g), despite the strong relation between A1 and B, the relation between A1 and A2 can be weakened due to the two weaker edges connecting B and C, as well as C and A2. In contrast, in (c), the relation between A1 and A2 remains strong because of the large number of common neighbors between B and A2 which strengthened their relation. Clearly such 3-step information is essential to be captured when learning a good graph representation with global structural information. Similarly, the 4-step information can also be crucial in revealing the global structural properties of the graph, as illustrated in (d) and (h). Here, in (d), the relation between A1 and A2 is clearly strong, while in (h) the two vertices are unrelated since there does not exist a path from one vertex to the other. In the absence of 4-step relational information, such important distinctions can not be properly captured. We also additionally argue that it is essential to treat different k-step information differently when learning graph representations. We present one simple graph in (a) of Figure <ref type="figure" target="#fig_1">2</ref>. Let us focus on learning the representation for the vertex A in the graph. We can see that A receives two types of information when learning its representation: 1-step information from B, as well as 2-step information from all C vertices. We note that if we do not distinguish these two different types of information, we can construct an alternative graph as shown in (b) of Figure <ref type="figure" target="#fig_1">2</ref>, where A receives exactly the same information as (a), but has a completely different structure.</p><p>In this paper, we propose a novel framework for learning accurate graph representations, integrating various k-step information which together captures the global structural information associated with the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function On Graph</head><p>We discuss our loss function used for learning the graph representations with global structural information in this section. Assume we are now given a graph with a collection of vertices and edges. Consider a vertex w together with another vertex c. In order for us to learn global representations to capture their relations, we need to understand how strongly these two vertices are connected to each other.</p><p>Let us begin with a few questions. Does there exist a path from w to c? If so, if we randomly sample a path starting with w, how likely is it for us to reach c (possibly within a fixed number of steps)? To answer such questions, we first use the term p k (c|w) to denote the probability for a transition from w to c in exactly k steps. We already know what are the 1-step transition probabilities, to compute the k-step transition probabilities we introduce the following kstep probability transition matrix</p><formula xml:id="formula_2">A k = A • • • A k</formula><p>We can observe that A k i,j exactly refers to the transition probability from vertex i to vertex j where the transition consists of exactly k step(s). This directly leads to:</p><formula xml:id="formula_3">p k (c|w) = A k w,c</formula><p>where A k w,c is the element from w-th row and c-th column of the matrix A k .</p><p>Let us consider a particular k first. Given a graph G, consider the collection of all paths consisting of k steps that can be sampled from the graph which start with w and end with c (here we call w "current vertex", and c "context vertex"). Our objective aims to maximize: 1) the probability that these pairs come from the graph, and 2) the probability that all other pairs do not come from the graph.</p><p>Motivated by the skip-gram model by Mikolov et al. <ref type="bibr" target="#b17">[18]</ref>, we employ noise contrastive estimation (NCE), which is proposed by Gutmann et al. <ref type="bibr" target="#b10">[11]</ref>, to define our objective function. Following a similar discussion presented in <ref type="bibr" target="#b15">[16]</ref>, we first introduce our k-step loss function defined over the complete graph as follows:</p><formula xml:id="formula_4">L k = w∈V L k (w)</formula><p>where</p><formula xml:id="formula_5">L k (w) = c∈V p k (c|w) log σ( w • c) +λE c ∼p k (V ) [log σ(− w• c )]</formula><p>Here p k (c|w) describes the k-step relationship between w and c (the k-step transition probability from w to c), σ(•) is sigmoid function defined as σ(x) = (1 + e −x ) −1 , λ is a hyper-parameter indicating the number of negative samples, and p k (V ) is the distribution over the vertices in the graph. The term E c ∼p k (V ) <ref type="bibr">[•]</ref> is the expectation when c follows the distribution p k (V ), where c is an instance obtained from negative sampling. This term can be explicitly expressed as:</p><formula xml:id="formula_6">E c ∼p k (V ) [log σ(− w • c )] = p k (c) • log σ(− w • c) + c ∈V \{c} p k (c ) • log σ(− w • c )</formula><p>This leads to a local loss defined over a specific (w,c):</p><formula xml:id="formula_7">L k (w, c) = p k (c|w) • log σ( w • c) + λ • p k (c) • log σ(− w • c)</formula><p>In this work, we set a maximal length for each path we consider. In other words, we assume 1 ≤ k ≤ K. In fact, when k is large enough, the transition probabilities converge to certain fixed values. The distribution p k (c) can be computed as follows:</p><formula xml:id="formula_8">p k (c) = w q(w )p k (c|w ) = 1 N w A k w ,c</formula><p>Note that here N is the number of vertices in graph G, and q(w ) is the probability of selecting w as the first vertex in the path, which we assume follow a uniform distribution, i.e., q(w ) = 1/N . This leads to:</p><formula xml:id="formula_9">L k (w, c) = A k w,c • log σ( w • c) + λ N w A k w ,c • log σ(− w • c)</formula><p>Following <ref type="bibr" target="#b15">[16]</ref>, we define e = w • c, and setting ∂L k ∂e = 0. This yields the following:</p><formula xml:id="formula_10">w • c = log A k w,c w A k w ,c − log(β)</formula><p>where β = λ/N . This concludes that we essentially need to factorize the matrix Y into two matrices W and C, where each row of W and each row of C consists of a vector representation for the vertex w and c respectively, and the entries of Y are:</p><formula xml:id="formula_11">Y k i,j = W k i • C k j = log A k i,j t A k t,j − log(β)</formula><p>Now we have defined our loss function and showed that optimizing the proposed loss essentially involves a matrix factorization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization with Matrix Factorization</head><p>Following the work of Levy et al. <ref type="bibr" target="#b15">[16]</ref>, to reduce noise, we replace all negative entries in Y k with 0. This gives us a positive k-step log probabilistic matrix X k , where</p><formula xml:id="formula_12">X k i,j = max(Y k i,j<label>,</label></formula><p>0) While various techniques for matrix factorization exist, in this work we focus on the popular singular value decomposition (SVD) method due to its simplicity. SVD has been shown successful in several matrix factorization tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, and is regarded as one of the important methods that can be used for dimensionality reduction. It was also used in <ref type="bibr" target="#b15">[16]</ref>.</p><p>For the matrix X k , SVD factorizes it as:</p><formula xml:id="formula_13">X k = U k Σ k (V k ) T</formula><p>where U and V are orthonormal matrices and Σ is a diagonal matrix consisting of an ordered list of singular values. We can approximate the original matrix This way, we can factorize our matrix X k as:</p><formula xml:id="formula_14">X k with X k d : X k ≈ X k d = U k d Σ k d (V k d ) T where Σ k d is</formula><formula xml:id="formula_15">X k ≈ X k d = W k C k where W k = U k d (Σ k d ) 1 2 , C k = (Σ k d ) 1 2 V k d T</formula><p>The resulting W k gives representations of current vertices as its column vectors, and C k gives the representations of context vertices as its column vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>. The final matrix W k is returned from the algorithm as the low-d representations of the vertices which capture k-step global structural information in the graph.</p><p>In our algorithm we consider all k-step transitions with all k = 1, 2, . . . , K, where K is a pre-selected constant. In our algorithm, we integrate all such k-step information when learning our graph representation, as to be discussed next.</p><p>Note that here we are essentially finding a projection from the row space of X k to the row space of W k with a lower rank. Thus alternative approaches other than the popular SVD can also be exploited. Examples include incremental SVD <ref type="bibr" target="#b21">[22]</ref>, independent component analysis (ICA) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, and deep neural networks <ref type="bibr" target="#b11">[12]</ref>. Our focus in this work is on the novel model for learning graph representations, so we do not pursue any alternative methods. In fact, if alternative method such as sparse auto-encoder is used at this step, it becomes relatively harder to justify whether the empirical effectiveness of our representations is due to our novel model, or comes from any non-linearity introduced in this dimensionality reduction step. To maintain the consistency with Levy et al. <ref type="bibr" target="#b15">[16]</ref>, we only employed SVD in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHM</head><p>We detail our learning algorithm in this section. In general, graph representations are extracted for other applications as features, such as classification and clustering. An effective way to encode k-step representation in practice is to concatenate the k-step representation as a global feature for each vertex, since each different step representation reflects different local information. Table <ref type="table" target="#tab_0">1</ref> shows the overall algorithm, and we explain the essential steps of our algorithm here.</p><p>Step 1. Get k-step transition probability matrix A k for each k = 1, 2, . . . , K.</p><p>As shown in Section 3, given a graph G, we can calculate the k-step transition probability matrix A k through the product of inverse of degree matrix D and adjacent matrix S. For a weighted graph, S is a real matrix, while for an unweighted graph, S is a binary matrix. Our algorithm is applicable to both cases.</p><p>Step 2. Get each k-step representation We get k-step log probability matrix X k , then subtract each entry by log (β), and replace the negative entries by zeros. After that, we construct the representational vectors as rows of W k , where we introduce a solution to factorize the positive log probability matrix X k using SVD. Finally, we get all k-step representations for each vertex on the graph.</p><p>Step 3. Concatenate all k-step representations We concatenate all k-step representations to form a global representation, which can be used in other tasks as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SKIP-GRAM MODEL AS A SPECIAL CASE OF GRAREP</head><p>GraRep aims to learn representations for graphs where we optimize the loss function based on matrix factorization. On the other hand, SGNS has been shown to be successful in handling linear structures such as natural language sentences. Is there any intrinsic relationship between them? In this section, we provide a view of SGNS as a special case of the GraRep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Explicit Loss of Skip-gram Model on Graph</head><p>SGNS aims at representing words in linear sequences, so we need to translate a graph structure into linear structures. Deepwalk reveals an effective way with uniform sampling </p><formula xml:id="formula_16">k Compute A = D −1 S Calculate A 1 , A 2 , . . . , A K , respectively 2. Get each k-step representations For k = 1 to K 2.1 Get positive log probability matrix calculate Γ k 1 , Γ k 2 , . . . , Γ k N (Γ k j = p A k p,j ) respectively calculate {X k i,j } X k i,j = log A k i,j Γ k j − log(β) assign negative entries of X k to 0 2.2 Construct the representation vector W k [U k , Σ k , (V k ) T ] = SV D(X k ) W k = U k d (Σ k d ) 1 2</formula><p>End for 3. Concatenate all the k-step representations</p><formula xml:id="formula_17">W = [W 1 , W 2 , . . . , W K ] Output</formula><p>Matrix of the graph representation W (truncated random walk). This method first samples uniformly a random vertex from the graph, then walks randomly to one of its neighbors and repeats this process. If the length of the vertex sequence reaches a certain preset value, then stop and start generating a new sequence. This procedure can be used to produce a large number of sequences from the graph. Essentially, for an unweighted graph, this strategy of uniform sampling works, while for a weighted graph, a probabilistic sampling method based on the weights of the edges is needed, which is not employed in DeepWalk. In this paper, we propose an Enhanced SGNS (E-SGNS) method suitable for weighted graphs. We also note that DeepWalk optimizes an alternative loss function (hierarchical softmax) that is different from negative sampling.</p><p>First, we consider total K-step loss L on whole graph</p><formula xml:id="formula_18">L = f (L1, L2, . . . , LK )</formula><p>where f (•) is a linear combination of its arguments defined as follows:</p><formula xml:id="formula_19">f (ϕ1, ϕ2, • • • , ϕK ) = ϕ1 + ϕ2 + • • • + ϕK</formula><p>We focus on the loss of a specific pair (w,c) which are ith and j-th vertex in the graph. Similar to Section 3.2, we assign partial derivative to 0, and get</p><formula xml:id="formula_20">Y E−SGN S i,j = log Mi,j t Mt,j − log(β)</formula><p>where M is transition probability matrix within K step(s), and Mi,j refers to transition probability from vertex i to vertex j. Y E−SGN S i,j is a factorized matrix for E-SGNS, and</p><formula xml:id="formula_21">M = A 1 + A 2 + • • • + A K</formula><p>The difference between E-SGNS and GraRep model is on the definition of f (•). E-SGNS can be considered as a linear combination of K-step loss, and each loss has an equal weight. Our GraRep model does not make such a strong assumption, but allows their (potentially non-linear) relationship to be learned from data in practice. Intuitively, different k-step transition probabilities should have different weights, and linear combination of these may not achieve desirable results for heterogeneous network data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Intrinsic Relation Between Sampling and Transition Probabilities</head><p>In our approach, we used transition probabilities to measure relationship between vertices. Is this reasonable? In this subsection, we articulate the intrinsic relation between sampling and transition probabilities.</p><p>Among the sequences generated by random walk, we assume vertex w occurs total times:</p><formula xml:id="formula_22">#(w) = αwγK</formula><p>where γ is the total length of sequences and αw is the probability of observing the vertex w in such all sequences. We regard vertex w as the current vertex, then the expected number of times that we see c1 as its direct neighbor (1-step away from w) is:</p><formula xml:id="formula_23">#(w, c1) = αwγK • p1(c|w)</formula><p>This holds for both uniform sampling for unweighted graphs or our proposed probabilistic sampling for weighted graphs.</p><p>Further, we analyze the expected number of times of cooccurrence for w and c of context window size 2,</p><formula xml:id="formula_24">#(w, c2) = αwγK c p(c2|c ) • p(c |w) = αwγK • p2(c|w)</formula><p>where c can be any vertex bridging w and c2. That is, c is shared neighbor between w and c2. Similarly, we can derive the equations for k = 3, 4, • • • , K:</p><formula xml:id="formula_25">#(w, c3) = αwγK • p3(c|w) . . . #(w, cK ) = αwγK • pK (c|w)</formula><p>then we add them up and divide both sides by K, leading to:</p><formula xml:id="formula_26">#(w, c) = αwγ K k=1 p k (c|w)</formula><p>where #(w, c) is the expected co-occurrence count between w and c within K step(s).</p><p>According to definition of Mw,c, we can get</p><formula xml:id="formula_27">#(w, c) = αwγMw,c</formula><p>Now, we can also compute the expected number of times we see c as the context vertex, #(c):</p><formula xml:id="formula_28">#(c) = w αwγMw,c</formula><p>where we consider the transitions from all possible vertices to c. To find the relationship with SGNS model, we consider a special case for αw here. If we assume αw follows an uniform distribution, we have αw =<ref type="foot" target="#foot_0">1</ref> N . We plug these expected counts into the equation of Y E−SGN S i,j , and we arrive at:</p><formula xml:id="formula_29">Y E−SGN S w,c = log #(w, c) • |D| #(w) • #(c) − log(λ)</formula><p>where D is the collection of all observed pairs in sequences, that is, |D| = γK. This matrix Y E−SGN S becomes exactly the same as that of SGNS as described in <ref type="bibr" target="#b15">[16]</ref>. This shows SGNS is essentially a special version of our GraRep model that deals with linear sequences which can be sampled from graphs. Our approach has several advantages over the slow and expensive sampling process, which typically involves several parameters to tune, such as maximum length of linear sequence, sampling frequency for each vertex, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL DESIGN</head><p>In this section, we assess the effectiveness of our GraRep model through experiments. We conduct experiments on several real-word datasets for several different tasks, and make comparisons with baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Tasks</head><p>In order to demonstrate the performance of GraRep, we conducted the experiments across three different types of graphs -a social network, a language network and a citation network, which include both weighted and unweighted graphs. We conducted experiments across three different types of tasks, including clustering, classification, and visualization. As we mentioned in earlier sections, this work focuses on proposing a novel framework for learning good representation of graph with global structural information, and aims to validate the effectiveness of our proposed model. Thus we do not employ alternative more efficient matrix factorization methods apart from SVD, and focused on the following three real-world datasets in this section.</p><p>1. 20-Newsgroup 1 is a language network, which has approximately 20,000 newsgroup documents and is partitioned by 20 different groups. In this network, each document is represented by a vector with tf-idf scores of each word, and the cosine similarity is used to calculate the similarity between two documents. Based on these similarity scores of each pair of documents, a language network is built. Following <ref type="bibr" target="#b28">[29]</ref>, in order to show the robustness of our model, we also construct the following 3 graphs built from 3, 6 and 9 different newsgroups respectively (note that NG refers to "Newsgroups"):</p><p>3-NG:comp.graphics, comp.graphics and talk.politics.guns; 6-NG: alt.atheism, comp.sys.mac.hardware, rec.motorcycles, rec.sport.hockey, soc.religion.christian and talk.religion.misc; 9-NG: talk.politics.mideast, talk.politics.misc, comp.os.mswindows.misc, sci.crypt, sci.med, sci.space, sci.electronics, misc.forsale, and comp.sys.ibm.pc.hardware</p><p>Besides randomly sampling 200 documents from a topic as described in <ref type="bibr" target="#b28">[29]</ref>, we also conduct experiment on all documents as comparison. The topic label on each document is considered to be true.</p><p>This language network is a fully connected and weighted graph, and we will demonstrate the results of clustering, using graph representation as features.</p><p>2. Blogcatalog<ref type="foot" target="#foot_1">2</ref> is a social network, where each vertex indicates one blogger author, and each edge corresponds to the relationship between authors. 39 different types of topic categories are presented by authors as labels.</p><p>Blogcatalog is an unweighted graphs, and we test the performance of the learned representations on the multi-label classification task, where we classify each author vertex into a set of labels. The graph representations generated from our model and each baseline algorithm are considered as features.</p><p>3. DBLP Network<ref type="foot" target="#foot_2">3</ref> is a citation network. We extract author citation network from DBLP, where each vertex indicates one author and the number of references from one author to the other is recorded by the weight of edge between these two authors. Following <ref type="bibr" target="#b25">[26]</ref>, we totally select 6 different popular conferences and assign them into 3 groups, where WWW and KDD are grouped as data mining, NIPS and ICML as machine learning, and CVPR and ICCV as computer vision. We visualize the learned representations from all systems using a visualisation tool t-SNE <ref type="bibr" target="#b30">[31]</ref>, which provides both qualitative and quantitative results for the learned representations. We give more details in Section 6.4.3.</p><p>To summarize, in this paper we conduct experiments on both weighted and unweighted graphs, and both sparse and dense graphs, where three different types of learning tasks are carried out. More details of the graph we used are shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baseline Algorithms</head><p>We use the following methods of graph representation as baseline algorithms.</p><p>1. LINE <ref type="bibr" target="#b24">[25]</ref>. LINE is a recently proposed method for learning graph representations on large-scale information networks. LINE defines a loss function based on 1-step and 2-step relational information between vertices. One strategy to improve the performance of vertices with small degrees is to make graph denser by expanding their neighbors. LINE will get the best performance, if concatenating the representation of 1step and 2-step relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type="bibr" target="#b19">[20]</ref>. DeepWalk is a method that learns the representation of social networks. The original model only works for unweighted graph. For each vertex, truncated random walk is used to translate graph structure into linear sequences. The skip-gram model with hierarchical softmax is used as the loss function.</p><p>3. E-SGNS. Skip-gram is an efficient model that learns the representation of each word in large corpus <ref type="bibr" target="#b17">[18]</ref>. For this enhanced version, we first utilize uniform sampling for unweighted graph and probabilistic sampling proportional to weight of edges for weighted graph, to generate linear vertex sequences, and then introduce SGNS to optimize. This method can be regarded as a special case of our model, where different representational vector of each k-step information is averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Parameter Settings</head><p>As suggested in <ref type="bibr" target="#b24">[25]</ref>, for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.025, the number of negative samples as 5, and the total number of samples as 10 billion. We also concatenate both 1-step and 2-step relational information to form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type="bibr" target="#b19">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. According to <ref type="bibr" target="#b24">[25]</ref>, LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E-SGNS can achieve optimal performance without normalization. For GraRep, we found the L2 normalization yielded better results. We reported all the results based on these findings for each system accordingly. For a fair comparison, the dimension d of representations is set as 128 for Blogcatalog network and DBLP network as used in <ref type="bibr" target="#b24">[25]</ref> and is set as 64 for 20-NewsGroup network as used in <ref type="bibr" target="#b28">[29]</ref>. For GraRep, we set β = 1 N and maximum matrix transition step K=6 for Blogcatalog network and DBLP network and K=3 for 20-NewsGroup network. To demonstrate the advantage of our model which captures global information of the graph, we also conducted experiments under various parameter settings for each baseline systems, which are discussed in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Results</head><p>In this section, we present empirical justifications that our GraRep model can integrate different k-step local relational information into a global graph representation for different types of graphs, which can then be effectively used for different tasks. We make the source code of GraRep available at http://shelson.top/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">20-Newsgroup Network</head><p>We first conduct an experiment on a language network through a clustering task by employing the learned representations in a k-means algorithm <ref type="bibr" target="#b1">[2]</ref>.</p><p>To assess the quality of the results, we report the averaged Normalized Mutual Information (NMI) score <ref type="bibr" target="#b23">[24]</ref> over 10 different runs for each system. To understand the effect of different dimensionality d in the end results, we also show the results when dimension d is set to 192 for DeepWalk, E-SGNS and Spectral Clustering. For LINE, we employ the reconstruction strategy proposed by their work by adding neighbors of neighbors as additional neighbors to improve performance. We set k-max=0, 200, 500, 1000 for experiments, where k-max is a parameter that is used to control how the higher-order neighbors are added to each vertex in the graph.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, the highest results are highlighted in bold for each column. We can see that GraRep consistently outperforms other baseline methods for this task. For DeepWalk, E-SGNS and Spectral Clustering, increasing   the dimension d of representations does not appear to be effective in improving the performance. We believe this is because a higher dimension does not provide different complementary information to the representations. For LINE, the reconstruction strategy does help, since it can capture additional structural information of the graph beyond 1-step and 2-step local information.</p><p>It is worth mentioning that GraRep and LINE can achieve good performance with a small graph. We believe this is because both approaches can capture rich local relational information even when the graph is small. Besides, for tasks with more labels, such as 9NG, GraRep and LINE can provide better performances than other methods, with GraRep providing the best. An interesting finding is that Spectral Clustering arrives at its best performance when setting dimension d to 16. However, for all other algorithms, their best results are obtained when d is set to 64. We show these results in Figure <ref type="figure" target="#fig_7">5</ref> when we assess the sensitivity of parameters.</p><p>Due to limited space, we do not report the detailed results with d=128 and d=256 for all baseline methods, as well as k-max=500, 1000 and without reconstruction strategy for LINE, since these results are no better than the results presented in Table <ref type="table" target="#tab_2">3</ref>. In Section 6.5, we will further discuss the issue of parameter sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Blogcatalog Network</head><p>In this experiment, we focus on a supervised task on a social network. We evaluate the effectiveness of different graph representations through a multi-label classification task by regarding the learned representations as features.</p><p>Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, we use the LibLinear package <ref type="bibr" target="#b9">[10]</ref> to train one-vs-rest logistic regression classifiers, and we run this process for 10 rounds and report the averaged Micro-F1 and Macro-F1 measures. For each round, we randomly sample 10% to 90% of the vertices and use these samples for training, and use the remaining vertices for evaluation. As suggested in <ref type="bibr" target="#b24">[25]</ref>, we set k-max as 0, 200, 500 and 1000, respectively, and we report the best performance with k-max=500. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">DBLP Network</head><p>In this experiment, we focus on visualizing the learned representations by examining a real citation network -DBLP. We feed the learned graph representations into the standard t-SNE tool <ref type="bibr" target="#b30">[31]</ref> to lay out the graph, where the authors from the same research area (one of data mining, machine learning or computer vision, see Section 6.1) share the same color. The graphs are visualized on a 2-dimensional space and the Kullback-Leibler divergence is reported <ref type="bibr" target="#b30">[31]</ref>, which captures the errors between the input pairwise similarities and their projections in the 2-dimensional map as displayed (a lower KL divergence score indicates a better performance).</p><p>Under the help of t-SNE toolkit, we set the same parameter configuration and import representations generated by each algorithm. From Figure <ref type="figure" target="#fig_5">3</ref>, we can see that the layout using Spectral Clustering is not very informative, since vertices of different colors are mixed with each other. For DeepWalk and E-SGNS, results look much better, as most vertices of the same color appear to form groups. However, vertices still do not appear in clearly separable regions with clear boundaries. For LINE and GraRep, the boundaries of each group become much clearer, with vertices of different colors appearing in clearly distinguishable regions. The results for GraRep appear to be better, with clearer boundaries for each regions as compared to LINE.</p><p>Table <ref type="table" target="#tab_5">5</ref> reports the KL divergence at the end of iterations. Under the same parameter setting, a lower KL divergence indicates a better graph representation. From this result, we also can see that GraRep yields better representations than all other baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Parameter Sensitivity</head><p>We discuss the parameter sensitivity in this section. Specifically, we assess the how the different choices of the maximal k-step size K, as well as the dimension d for our representations can affect our results.  Figure <ref type="figure" target="#fig_6">4</ref> shows the Micro-F1 and Macro-F1 scores different choices of K on the Blogcatalog data. We can observe that the setting K=2 has a significant improvement over the setting K=1, and K=3 further outperforms K=2. This confirms that different k-step can learn complementary local information. In addition, as mentioned in Section 3, when k is large enough, learned k-step relational information becomes weak and shifts towards a steady distribution.</p><p>Empirically we can observe that the performance of K=7 is no better than K=6. In this figure, for readability and clarity we only present the results of K=1,2,3,6 and 7. We found the performance of K = 4 is slightly better than K=3, while the results for K=5 is comparable to K=4. Figure <ref type="figure" target="#fig_7">5</ref> shows the NMI scores of each algorithm over different settings of the dimension d on 3NG and 9NG data. We can observe that GraRep consistently outperforms other baseline algorithms which learn representations with the same dimension. This set of experiments serves as an additional supplement to Table <ref type="table" target="#tab_2">3</ref>. Interestingly, all algorithms can obtain the optimal performance with d = 64. As we increase d from 64 to larger values, it appears that the performances of all the algorithms start to drop. Nevertheless, our GraRep algorithm is still superior to the baseline systems across different d values.  Figure <ref type="figure" target="#fig_9">6</ref> shows the running time over different dimensions as well as over different graph sizes. In Figure <ref type="figure" target="#fig_9">6</ref>(a), we set K from 1 to 7 where each complementary feature vector has a dimension of 128. This set of experiments is conducted on the Blogcatalog dataset which contains around 10,000 vertices. It can be seen that the running time increases approximately linearly as the dimension increases. In Figure <ref type="figure" target="#fig_9">6</ref>(b), we analyze running time with respect to graph sizes. This set of experiment is conducted on different size of the 20-NewsGroup dataset. The result shows a significant increase of running time as graph size increases. The reason resulting in the large increase in running time is mainly due to high time complexity involved in the computation of the power of a matrix and the SVD procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, GraRep, a novel model for learning better graph representations is proposed. Our model, with our kstep loss functions defined on graphs which integrate rich local structural information associated with the graph, captures the global structural properties of the graph. We also provide mathematical derivations justifying the model and establish the connections to previous research efforts. Empirically, the learned representations can be effectively used as features in other learning problems such as clustering and classification. This model comes with one limitation: the expensive computation of the power of a matrix and SVD involved in the learning process. Future work would include the investigation of efficient and online methods to approximate matrix algebraic manipulations, as well as investigation of alternative methods by employing deep architectures for learning low-dimensional representations in place of SVD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The importance of capturing different k-step information in the graph representations. Here we give examples for k = 1, 2, 3, 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 .</head><label>2</label><figDesc>(Graph Representations with Global Structural Information) Given a graph G, the task of Learning Graph Representations with Global Structural Information aims to learn a global representation matrix W ∈ R |V |×d for the complete graph, whose i-th row Wi is a d-dimensional vector representing the vertex vi in the graph G where the global structural information of the graph can be captured in such vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The importance of maintaining different k-step information separately in the graph representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the matrix composed by the top d singular values, and U k d and V k d are first d columns of U k and V k , respectively (which are the first d eigenvector of XX T and X T X respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of author citation network. Each point indicates one author. Green: Data Mining, magenta: computer vision and blue: Machine Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance over steps, K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance over dimensions, d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Running time as a function of a) dimension and b) size of graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall Algorithm GraRep Algorithm Input Adjacency matrix S on graph Maximum transition step K Log shifted factor β Dimension of representation vector d 1. Get k-step transition probability matrix A</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the real-world graphs</figDesc><table><row><cell></cell><cell cols="2">Language Network</cell><cell cols="2">Social Network Citation Network</cell></row><row><cell>Name</cell><cell>20-NewsGroup (200 samples)</cell><cell>20-NewsGroup (all data)</cell><cell>Blogcatalog</cell><cell>DBLP (author citation)</cell></row><row><cell>Type</cell><cell>weighted</cell><cell>weighted</cell><cell>unweighted</cell><cell>weighted</cell></row><row><cell>#(V)</cell><cell cols="2">600, 1200 and 1800 1,720, 3,224 and 5,141</cell><cell>10,312</cell><cell>7,314</cell></row><row><cell>#(E)</cell><cell>Fully connected</cell><cell>Fully connected</cell><cell>333,983</cell><cell>72,927</cell></row><row><cell>Avg. degree</cell><cell>-</cell><cell>-</cell><cell>64.78</cell><cell>19.94</cell></row><row><cell>#Labels</cell><cell>3, 6 and 9</cell><cell>3, 6 and 9</cell><cell>39</cell><cell>3</cell></row><row><cell>Task</cell><cell>Clustering</cell><cell>Clustering</cell><cell>Classification</cell><cell>Visualization</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on 20-NewsGroup</figDesc><table><row><cell></cell><cell></cell><cell>200 samples</cell><cell></cell><cell></cell><cell>all data</cell><cell></cell></row><row><cell>Algorithm</cell><cell cols="6">3NG(200) 6NG(200) 9NG(200) 3NG(all) 6NG(all) 9NG(all)</cell></row><row><cell>GraRep</cell><cell>81.12</cell><cell>67.53</cell><cell>59.43</cell><cell>81.44</cell><cell>71.54</cell><cell>60.38</cell></row><row><cell>LINE (k-max=0)</cell><cell>80.36</cell><cell>64.88</cell><cell>51.58</cell><cell>80.58</cell><cell>68.35</cell><cell>52.30</cell></row><row><cell>LINE (k-max=200)</cell><cell>78.69</cell><cell>66.06</cell><cell>54.14</cell><cell>80.68</cell><cell>68.83</cell><cell>53.53</cell></row><row><cell>DeepWalk</cell><cell>65.58</cell><cell>63.66</cell><cell>48.86</cell><cell>65.67</cell><cell>68.38</cell><cell>49.19</cell></row><row><cell>DeepWalk (192dim)</cell><cell>60.89</cell><cell>59.89</cell><cell>47.16</cell><cell>59.93</cell><cell>65.68</cell><cell>48.61</cell></row><row><cell>E-SGNS</cell><cell>69.98</cell><cell>65.06</cell><cell>48.47</cell><cell>69.04</cell><cell>67.65</cell><cell>50.59</cell></row><row><cell>E-SGNS (192dim)</cell><cell>63.55</cell><cell>64.85</cell><cell>48.65</cell><cell>66.64</cell><cell>66.57</cell><cell>49.78</cell></row><row><cell>Spectral Clustering</cell><cell>49.04</cell><cell>51.02</cell><cell>46.92</cell><cell>62.41</cell><cell>59.32</cell><cell>51.91</cell></row><row><cell>Spectral Clustering (192dim)</cell><cell>28.44</cell><cell>27.80</cell><cell>36.05</cell><cell>44.47</cell><cell>36.98</cell><cell>47.36</cell></row><row><cell>Spectral Clustering (16dim)</cell><cell>69.91</cell><cell>60.54</cell><cell>47.39</cell><cell>78.12</cell><cell>68.78</cell><cell>57.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on Blogcatalog</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell>GraRep</cell><cell cols="9">38.24 40.31 41.34 41.87 42.60 43.02 43.43 43.55 44.24</cell></row><row><cell>Micro-F1</cell><cell>LINE DeepWalk</cell><cell>37.19 35.93</cell><cell>39.82 38.38</cell><cell>40.88 39.50</cell><cell>41.47 40.39</cell><cell>42.19 40.79</cell><cell>42.72 41.28</cell><cell>43.15 41.60</cell><cell>43.36 41.93</cell><cell>43.88 42.17</cell></row><row><cell></cell><cell>E-SGNS</cell><cell>35.71</cell><cell>38.34</cell><cell>39.64</cell><cell>40.39</cell><cell>41.23</cell><cell>41.66</cell><cell>42.01</cell><cell>42.16</cell><cell>42.25</cell></row><row><cell></cell><cell cols="2">Spectral Clustering 37.16</cell><cell>39.45</cell><cell>40.22</cell><cell>40.87</cell><cell>41.27</cell><cell>41.50</cell><cell>41.48</cell><cell>41.62</cell><cell>42.12</cell></row><row><cell></cell><cell>GraRep</cell><cell cols="9">23.20 25.55 26.69 27.53 28.35 28.78 29.67 29.96 30.93</cell></row><row><cell>Macro-F1</cell><cell>LINE DeepWalk</cell><cell>19.63 21.02</cell><cell>23.04 23.81</cell><cell>24.52 25.39</cell><cell>25.70 26.27</cell><cell>26.65 26.85</cell><cell>27.26 27.36</cell><cell>27.94 27.67</cell><cell>28.68 27.96</cell><cell>29.38 28.41</cell></row><row><cell></cell><cell>E-SGNS</cell><cell>21.01</cell><cell>24.09</cell><cell>25.61</cell><cell>26.59</cell><cell>27.64</cell><cell>28.08</cell><cell>28.33</cell><cell>28.34</cell><cell>29.26</cell></row><row><cell></cell><cell cols="2">Spectral Clustering 19.26</cell><cell>22.24</cell><cell>23.51</cell><cell>24.33</cell><cell>24.83</cell><cell>25.19</cell><cell>25.36</cell><cell>25.52</cell><cell>26.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>reports the results of GraRep and each baseline algorithm. The highest performance is highlighted in bold for each column which corresponds to one particular train/test split. Overall, GraRep significantly outperforms other methods, especially under the setting where only 10% of the vertices are used for training. This indicates different types of rich local structural information learned by the GraRep model can be used to complement each other to capture the global structural properties of the graph, which serves</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Final KL divergence for the DBLP dataset</figDesc><table><row><cell>Algorithm</cell><cell cols="4">GraRep LINE DeepWalk E-SGNS</cell></row><row><cell>KL divergence</cell><cell>1.0070</cell><cell>1.0816</cell><cell>1.1115</cell><cell>1.1009</cell></row><row><cell cols="5">as a distinctive advantage over existing baseline approaches,</cell></row><row><cell cols="4">especially when the data is relatively scarce.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">qwone.com/˜jason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">leitang.net/code/social-dimension/data/blogcatalog.mat</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">aminer.org/billboard/citation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">. Spectral Clustering<ref type="bibr" target="#b22">[23]</ref>. Spectral clustering is a reasonable baseline algorithm, which aims at minimizing Normalized Cut (NCut). Like our method, Spectral clustering also factorize a matrix, but it focuses on a different matrix of the graphs -the Laplacian Matrix. Essentially, the difference between spectral clustering and E-SGNS lies on their different loss function.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. The authors would also like to thank Fei Tian, Qingbiao Miao and Jie Liu for their suggestions or help with this work. This paper was done when the first author was an intern at SUTD. This work was supported by SUTD grant ISTD 2013 064 and Temasek Lab project IGDST1403013.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
				<imprint>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BRM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">BRM</biblScope>
			<biblScope unit="page" from="890" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Experiments with lsa scoring: Optimal rank and basis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIR</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="157" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multidimensional scaling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The singular value decomposition: Its computation and some applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Klema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Laub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatic Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="176" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical co-occurrence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BRMIC</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental singular value decomposition algorithms for highly scalable recommender systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIS</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Impact of similarity measures on web-page clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Artificial Intelligence for Web Search (AAAI 2000)</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>WWW. ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Domain and function: A dual-space model of semantic relations and compositions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>JAIR</publisher>
			<biblScope unit="page" from="533" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
