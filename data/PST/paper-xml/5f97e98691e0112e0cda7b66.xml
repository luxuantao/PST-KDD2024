<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Style Transfer: A Review and Experiment Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-24">24 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
							<email>zhiqianghu@std.uestc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ibm</forename><forename type="middle">T J</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Research</forename><surname>Center</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China ROY KA-WEI LEE</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>4E 2nd Section, 1st Ring Rd, Jianshe Road</addrLine>
									<postCode>611731</postCode>
									<settlement>Chenghua, Chengdu, Sichuan, Roy Ka-Wei Lee</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>05-301, 487372</postCode>
									<settlement>Singapore, Charu C. Aggarwal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights, New York</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Style Transfer: A Review and Experiment Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-24">24 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2010.12742v1[cs.CL]</idno>
					<note type="submission">Manuscript submitted to ACM Manuscript submitted to ACM</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Style Transfer</term>
					<term>Natural Language Processing</term>
					<term>Text Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The stylistic properties of text have intrigued computational linguistics researchers in recent years. Specifically, researchers have investigated the Text Style Transfer (TST) task, which aims to change the stylistic properties of the text while retaining its styleindependent content. Over the last few years, many novel TST algorithms have been developed, while the industry has leveraged these algorithms to enable exciting TST applications. The field of TST research has burgeoned because of this symbiosis. This article aims to provide a comprehensive review of recent research efforts on text style transfer. More concretely, we create a taxonomy to organize the TST models, and provide a comprehensive summary of the state of the art. We review the existing evaluation methodologies for TST tasks, and conduct a large-scale reproducibility study where we experimentally benchmark 19 state-of-the-art TST algorithms on two publicly available datasets. Finally, we expand on current trends and provide new perspectives on the new and exciting developments in the TST field.</p><p>CCS Concepts: â€¢ Computing methodologies â†’ Natural language generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The stylistic properties of text have intrigued linguistic researchers for a long time. Enkvist <ref type="bibr" target="#b13">[14]</ref> opined that text style is a "concept that is as common as it is elusive" and suggested that style may be described as a linguistic variation while preserving the conceptual content of the text. To give a practical example, the formality of text will vary across settings for similar content; examples include a conversation with friends such as "let's hang out on Sunday afternoon!", or a professional email such as "We will arrange a meeting on Sunday afternoon. "</p><p>In recent years, the studies on text style have attracted not only the attention of the linguist but also many computer science researchers. Specifically, computer science researchers are investigating the Text Style Transfer (TST) task, which is an increasingly popular branch of natural language generation (NLG) <ref type="bibr" target="#b15">[16]</ref> that aims to change the stylistic properties of the text while retaining its style-independent content. Earlier TST studies have mainly attempted to perform TST with parallel corpus <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84]</ref>. For instance, Xu et al. <ref type="bibr" target="#b83">[84]</ref> were one of the first works to apply a phrase-based machine translation (PBMT) model to perform TST. They generated a parallel corpus of 30K sentence pairs by scraping the modern translations of Shakespearean plays and training a PBMT system to translate from modern English to Shakespearean English. However, parallel data are scarce in many real-world TST applications, such as dialogue generation of different styles. The scarcity of parallel data motivated a new breed of TST algorithms that attempt to transfer text style without parallel data <ref type="bibr">[9, 11, 15, 18, 20, 22, 24, 29, 40, 41, 44, 46-48, 54, 60, 67, 70, 72, 76, 78, 81, 82, 85, 86, 88, 90, 92, 93]</ref>.</p><p>The goal of this survey is to review the literature on the advances in TST thoroughly and to provide experimental comparisons of various algorithms. It provides a panorama through which readers can quickly understand and step into the field of TST. It is noteworthy that the literature in the field is rather disparate, and a unified comparison is critical to aid in understanding the strengths and weaknesses of various methods. This survey lays the foundations of future innovations in the area of TST and taps into the richness of this research area. To summarize, the key contributions of this survey are three-fold: (i) We investigate, classify, and summarise recent advances in the field of TST. (ii) We present several evaluation methods and experimentally compare different TST algorithms. (iii) we discuss the challenges in this field and propose possible directions on how to address them in future works.</p><p>The organization of this paper is as follows. We start our discussion on the related research areas that inspire the commonly used TST techniques in Section 2. Next, we explore and demonstrate some of the commercial applications of TST in Section 3. In Section 4, we categorize and explain the existing TST algorithms. The methodologies for evaluating TST algorithms are presented in Section 5. In Section 6, we present experiments on publicly available datasets to benchmark the existing TST algorithms. In Section 7, we outline the open issues in TST research and offer possible future TST research directions. Finally, we conclude our survey paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED RESEARCH AREAS</head><p>Text style transfer (TST) is a relatively new research area. Many of the earlier TST works are heavily influenced by two related research areas: neural style transfer, i.e., transferring styles in images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> and neural machine translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b70">71]</ref>. We found that a substantial number of TST techniques were adapted from the common methods used in neural style transfer and neural machine translation. In addition, some of the evaluation metrics used in TST are also "inherited" from the neural machine translation task. In this section, we will introduce the two related research areas and highlight some of the common techniques and evaluation metrics that are transferred or adapted for the TST task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Style Transfer</head><p>Gatys et al. <ref type="bibr" target="#b16">[17]</ref> first explored the use of a convolutional neural network (CNN) to extract content and style features from images separately. Their experimental results demonstrated that CNN is capable of extracting content information from an arbitrary photograph and is also capable of extracting style information from a well-known artwork. Based on this finding, Gatys et al. <ref type="bibr" target="#b16">[17]</ref> further experimented with the idea of exploiting CNN feature activation to recombine the content of a given photo with the style of famous artwork. The underlying idea in their proposed algorithm is to minimize the loss between the synthesized image's CNN latent representation and the desired CNN feature distributions, which is the combination of the photo's content feature representation and artwork's style feature representation.</p><p>Interestingly, the algorithm also does not have any explicit restriction on the type of style images and does not require ground truth for training. The seminal work of Gatys et al. opened the new field of Neural Style Transfer (NST), which is the process of using neural networks to render content images in different styles <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manuscript submitted to ACM</head><p>The burgeoning research in the emerging field of NST has attracted wide attention from both academia and industry.</p><p>In particular, natural language processing(NLP) researchers are motivated to adopt similar strategies to implicitly disentangle the content and style features in text, and transfer the learned style features on another textual content <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref>. For example, Fu et al. <ref type="bibr" target="#b14">[15]</ref> proposed two TST models, which adopted an adversarial learning approach to implicitly disentangle the content and style in text. The first method used multiple decoders for each type of style to generate text of different styles from a common content embedding. In the second approach, style embeddings are learned and augmented to a content embedding, and one decoder is used to generate output in different styles.</p><p>While the goal in this line of TST works is similar to the objective of NST, disentangling content and style in text has proven to be much harder than in the case of images <ref type="bibr" target="#b40">[41]</ref>. Firstly, the styles in images are distinctive; it is easier to visualize and differentiate styles in two images in terms of patterns that can be modeled easily by a neural network.</p><p>However, text styles are somewhat more subtle, which makes it challenging to differentiate and define styles in two given pieces of text. Secondly, unlike the image's content and style, which is easily separated in the different CNN layers, the content and styles in the text are tightly coupled and not easily separated even with the style labels. Hence, some of the recent TST works have proposed a new direction to transfer text style transfer without disentanglement of text's content and style <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b87">88]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Machine Translation</head><p>Neural machine translation (NMT), which is a deep learning-based approach for machine translation, is a well-studied research area <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b70">71]</ref>. Unlike the traditional statistical machine translation techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref>, NMT can perform end-to-end training of a machine translation model without the need to deal with word alignments, translation rules, and complicated decoding algorithms. Both TST and NMT are branches of natural language generation (NLG) <ref type="bibr" target="#b15">[16]</ref>. Naturally, the two research areas share a few similarities. Firstly, two tasks are quite similar; NMT aims to change the language of a text while preserving the content, TST aims to modify the stylistic properties of a text while preserving the content.</p><p>Secondly, most of the TST models have "borrowed" the most commonly used NMT technique: the sequence-to-sequence encoder-decoder model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b70">71]</ref>. Other TST studies have also adopted the back-translation technique originally proposed for NMT <ref type="bibr" target="#b64">[65]</ref> to transfer styles in text <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b61">62]</ref>. For instance, Prabhumoye et al. <ref type="bibr" target="#b61">[62]</ref> used a back-translation model to extract the content features in text and subsequently generate text in different styles using the extracted content features and multiple decoders. Thirdly, TST works have inherited some of the automatic quantitative evaluation metrics that were originally proposed to evaluate the NMT task. For example, the bilingual evaluation understudy (BLEU) metric <ref type="bibr" target="#b58">[59]</ref>, which is used to evaluate the quality of machine-translated text by computing the modified n-gram precision between the generated and reference text, is also widely used in the TST task.</p><p>Despite the close similarities between TST and NMT, it is also worth noting their subtle differences. In NMT, the languages are definitive, and most of the NMT models only evaluate if the content of the text is preserved during translation while assuming that the language itself is correctly translated. However, in TST, the text style is abstract, and most TST models will need to evaluate if the text's content is preserved and the style is effectively modified. These differences motivated TST researchers to explore other techniques as such controllable generation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b87">88]</ref> to ensure that the style in a text is modified during TST. The need to evaluate if the style is effectively transferred also creates new evaluation metrics to evaluate the TST models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPLICATION</head><p>The research on TST algorithms has many industrial applications and could lead to many commercial benefits. In this section, we summarize these applications and present some potential usages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Writing tools</head><p>One of the industrial applications of TST algorithms is the design of writing tools. Academics across various domains have widely researched Computer-aided writing tools, and the industry has developed many writing tool applications <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. The TST methods can be applied as new useful features in existing writing tool applications.</p><p>The utility of writing style has been widely studied by linguistic and literacy education scholars <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b86">87]</ref>.</p><p>The TST algorithms enable writing tool applications to apply the insights from existing linguistics studies to improve the writings of users. For instance, applying TST algorithms enable writing tool users to switch between writing styles for different audiences while preserving the content in their writing. The style evaluation methods developed to evaluate TST algorithms can also be applied to analyze the writing style of users <ref type="bibr" target="#b60">[61]</ref>. For instance, the writing tool could analyze the style of a user's business email draft to be too informal and recommend the users to modify his or her writing to make the writing style more formal. Cao et al. <ref type="bibr" target="#b5">[6]</ref> developed an interesting real-world TST application, where the text is transferred between expert and layman styles. The underlying intuition is that expertise style transfer aims at improving the readability of a text by reducing the expertise level, such as explaining the complex terminology with simple phrases. On the other hand, it also aims to improve the expertise level based on context, so that laymen's expressions can be more accurate and professional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Persuasion and Marketing</head><p>Studies have explored utilizing persuasive text to influence the attitude or behaviors of people <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref>, and the insights gained from these studies have also been applied in improve marketing and advertising in the industry. The style of text has an impact on its persuasiveness <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b52">53]</ref>, and the TST algorithms can be used to convert a text into a more persuasive style. Recent studies have also explored personalizing persuasive strategies according to the user's profile <ref type="bibr" target="#b32">[33]</ref>. Similarly, TST algorithms could also be used to structure the text in different persuasive text styles that best appeal to the user profiles. For instance, TST algorithms can be applied to modify a marketing message into an authoritative style for users who appeal to authority. Jin et al. <ref type="bibr" target="#b25">[26]</ref> proposed a compelling use-case to utilize TST methods to make news headlines more attractive. Specifically, a TST algorithm is used to transfer news headlines between humor, romance, and clickbaity style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chatbot Dialogue</head><p>The research and development of chatbots, i.e., intelligent dialogue systems that are able to engage in conversations with humans, has been one of the longest-running goals in artificial intelligence <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b94">95]</ref>. Kim et al. <ref type="bibr" target="#b33">[34]</ref> conducted a study on the impact of chatbot's conversational style on users and found that when a causal conversational style is used, experiment participants are less likely to persuade a user to perform an action compared to participants who conversed with formal conversational style chatbot. The encouraging results from the study suggest that a user may be influenced by chatbot's conversational styles, and TST algorithms could be exploited to enhance the chatbots' flexibility in conversational styles.  TST algorithms can be applied to equip chatbots with the ability to switch between conversational styles, and this makes the chatbots more appealing and engaging to the users. For instance, a chatbot recommending products to customers may adopt a more persuasive conversational style while the same chatbot may switch to a formal conversational style when addressing the customer's complaint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manuscript submitted to ACM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A TAXONOMY OF TEXT STYLE TRANSFER METHODS</head><p>In this section, we first propose a taxonomy to organize the most notable and promising advances in TST research in recent years. Subsequently, we discuss each category of TST models in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Categories of Text Style Transfer Models</head><p>To provide a bird-eye view of this field, we classify the existing TST models based on the types of (1) data setting, (2) strategy, and (3) technique used. Fig. <ref type="figure" target="#fig_0">1</ref> summarizes the taxonomy for text style transfer. â€¢ Parallel Data. In this data setting, the TST models are trained with known pairs of text with different styles.</p><p>Commonly, NMT methods such as sequence-to-sequence (Seq2Seq) models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b83">84]</ref> are applied to transfer the style of text. For example, Jhamtani et al. <ref type="bibr" target="#b24">[25]</ref> trained a Seq2Seq model with a pointer network on a parallel corpus and applied the model to translate modern English phrases to Shakespearean English. Details of the techniques applied on parallel datasets will be discussed in Section 4.2 â€¢ Non-Parallel Data. TST models in the non-parallel data setting aim to transfer the style of text without any knowledge of matching text pairs in different styles. Most of the existing TST studies fall into this category as parallel datasets are scarce in many real-world TST applications.</p><p>Manuscript submitted to ACM 4.1.2 Strategy. In order to perform TST in non-parallel data setting, existing studies have proposed to disentangle the style and content in text, a strategy commonly used in NST <ref type="bibr" target="#b16">[17]</ref>. On the whole, there are three types of strategies adopted by existing TST studies:</p><p>â€¢ Explicit Style-Content Disentanglement. In this strategy, the TST models adopted an explicit text replacement approach to generate text of a target style. For instance, Li et al. <ref type="bibr" target="#b43">[44]</ref> first explicitly identify parts of the text that is associated with the original style and then replace them with new phrases associated with the target style. The text with the replaced new phrases is then inputted into a Seq2Seq model to generate a fluent text in the target style. Details of the techniques applied to disentangle content and style will be explicitly discussed in Section 4.3 â€¢ Implicit Style-Content Disentanglement. To disentangle style and content in text implicitly, TST models aim first to learn the content and style latent representations of a given text. Subsequently, the original text's content latent representation is combined with the latent representation of the target style to generate a new text in the target style. Multiple techniques such as back-translation, adversarial learning, and controllable generation have been proposed to disentangle the content and style latent representations.</p><p>â€¢ Without Style-Content Disentanglement. Recent studies have suggested that it is difficult to judge the quality of text style and content disentanglement and the disentanglement is also unnecessary for TST <ref type="bibr" target="#b40">[41]</ref>.</p><p>Therefore, newer TST studies explored performing TST without disentangling the text's style and content.</p><p>Techniques such as adversarial learning, controllable generation, reinforcement learning, probabilistic modeling, and pseudo-parallel corpus have been applied to perform TST without disentanglement of the text's content and style.</p><p>4.1.3 Techniques. . Table <ref type="table" target="#tab_1">1</ref> lists the types of techniques that are commonly used to perform TST. We organize them following the previously mentioned taxonomy and will review them in detail in subsequent sections. Additionally, we also summarize the list of relevant publications that proposed variant models using these techniques.  <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82]</ref> Reinforcement Learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref> Probabilistic Modeling <ref type="bibr" target="#b19">[20]</ref> 4.2 Sequence-to-Sequence Model with Parallel Data</p><p>The Sequence-to-Sequence (Seq2Seq) model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b83">84]</ref> is core to many natural language generation tasks <ref type="bibr" target="#b15">[16]</ref>, and TST is no exception. Generally, a Seq2Seq model is trained on a parallel corpus, where the text of the original style Manuscript submitted to ACM is input into an encoder, and the decoder outputs the corresponding text of the target style. Variants of the general approach were proposed in TST models that trained on parallel datasets. Jhamtani et al. <ref type="bibr" target="#b24">[25]</ref> extended the work in Xu et al. <ref type="bibr" target="#b83">[84]</ref> by adding a pointer network <ref type="bibr" target="#b74">[75]</ref> to the Seq2Seq model to selectively copy word tokens from the input text directly to generate the text in a target style. Carlson <ref type="bibr" target="#b6">[7]</ref> added attention mechanism <ref type="bibr" target="#b72">[73]</ref> to the Seq2Seq model to evaluate their proposed parallel Bible prose style corpus. Other studies have also attempted to use the parallel dataset to train a seq2seq a semi-supervised fashion <ref type="bibr" target="#b65">[66]</ref>, as well as fine-tuning pre-trained models to perform TST <ref type="bibr" target="#b76">[77]</ref>.</p><p>Another interesting approach is to generate pseudo-parallel datasets and apply seq2seq models to perform TST. Jin et al. <ref type="bibr" target="#b26">[27]</ref> first constructed a pseudo-parallel corpus by matching text sentences in a source style corpus X with text sentences in target style corpus Y using cosine similarity. Subsequently, a seq2seq model is trained using the constructed pseudo-parallel corpus to perform TST. Nikolov and Hahnloser <ref type="bibr" target="#b54">[55]</ref> improve the pseudo-parallel corpus generation with a hierarchical method that computes similarity scores at document and sentence levels to find parallel text pairs across different style corpus. Liao et al. <ref type="bibr" target="#b44">[45]</ref> first generated a pseudo-parallel dataset and then applied a dual-encoder seq2seq framework to disentangle the content from style for text style transfer. Zhang et al. <ref type="bibr" target="#b88">[89]</ref> proposed and experimented with a few pseudo-parallel dataset generation methods. Specifically, the researchers explored simultaneous training, pre-training, and fine-tuning data augmentation methods to generate pseudo-parallel data for TST tasks.</p><p>A major drawback of conventional Seq2Seq models is that training requires large parallel corpora, which are scarce in the TST domain. Xu et al. <ref type="bibr" target="#b82">[83]</ref> recognized this limitation and proposed a joint training approach that combined the information gain from the Seq2Seq model trained on parallel corpus and class-labeled annotation learned from training a classifier to predict the text's style. However, most of the TST studies have moved on and experimented with performing TST without parallel datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Explicit Style Keyword Replacement</head><p>A common approach to explicitly disentangle content and style in text is to perform the replacement of keywords that are attributed to a certain style. Li et al. <ref type="bibr" target="#b43">[44]</ref> first proposed a Delete-Retrieve-Generate framework that became the signature approach that utilized explicit style attributed work replacement for TST. Fig. <ref type="figure" target="#fig_2">2</ref> shows the overview of the framework. In the proposed framework, statistics of word frequency is first used to identify and delete style attributed words such as "good, bad" from the original text. Next, a text that is most similar to the original text is retrieved from the corpus of the target style. The style attributed words from the retrieved text are combined with the content words from the original text to generate the text using a rule-based fashion or with a neural sequence-to-sequence model. Zhang et al. <ref type="bibr" target="#b89">[90]</ref> adopted a similar explicit style keyword replacement frame to perform sentiment transfer in text. Sudhakar et al. <ref type="bibr" target="#b69">[70]</ref> extended the Delete-Retrieve-Generate framework and improve the delete step by exploiting a Transformer <ref type="bibr" target="#b72">[73]</ref> to identify style attributed keywords. Wu et al. <ref type="bibr" target="#b78">[79]</ref> proposed the two-step "Mask and Infill" approach. In the mask step, style attributed words are masked. In the infill step, a pre-trained masked language model is used to infill the masked positions by predicting words or phrases attributed to the target style. Leeftink and Spanakis <ref type="bibr" target="#b41">[42]</ref> applied a classifier with an attention mechanism to highlight keywords and phrases that determine the style of a sentence. Then, these identified phrases are changed to phrases of target style using a seq2seq approach. Recent works have also combined explicit style keyword replacement with cycled reinforcement learning to iteratively replace style attributed keywords while maintaining the content in text <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b80">81]</ref>. .</p><formula xml:id="formula_0">-----------------neg -----------------pos -----------------pos -----------------pos -----------------neg -----------------neg -----------------pos ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adversarial Learning</head><p>A common technique used to perform implicit content-style disentanglement is adversarial learning. Shen et al. <ref type="bibr" target="#b66">[67]</ref> leverage an adversarial training scheme where a classifier is used to evaluate if an encoder is able to generate a latent representation devoid of style. Zhao et al. <ref type="bibr" target="#b91">[92]</ref>, inspired by the earlier TST study, proposed a generic natural language generation technique, named Adversarially Regularized Autoencoders (ARAE), where similar adversarial learning technique is used to generate natural textual output by removing specific attributes from an encoder latent output through adversarial learning and use the manipulated latent output of encoder to induce changes in the natural language output of the decoder. Among the earlier adversarial learning TST works, Fu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a framework that is definitive of works that utilized adversarial learning to disentangle content and style in text implicitly. Fig. <ref type="figure" target="#fig_4">4</ref> illustrates the two models included in their proposed framework. </p><formula xml:id="formula_1">Manuscript submitted to ACM ğ¿ ğ‘ğ‘‘ğ‘£1 (Î˜ ğ¶ ) = âˆ’ ğ‘€ âˆ‘ï¸ ğ‘–=1 ğ‘™ğ‘œğ‘”ğ‘ (ğ‘™ ğ‘– |ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (x ğ‘– ; Î˜ ğ¸ ); Î˜ ğ¶ )<label>(1)</label></formula><p>where Î˜ ğ¶ and Î˜ ğ¸ are the parameters of the classifier and encoder, respectively. ğ‘€ denotes the size of the training data, and ğ‘™ ğ‘– refers to the style label. The second part of the adversarial network aims at making the classifier unable to identify the style of input x by maximizing the entropy (i.e., minimizing the negative entropy) of the predicted style labels:</p><formula xml:id="formula_2">ğ¿ ğ‘ğ‘‘ğ‘£2 (Î˜ ğ¸ ) = âˆ’ ğ‘€ âˆ‘ï¸ ğ‘–=1 ğ‘ âˆ‘ï¸ ğ‘—=1 ğ» (ğ‘ ( ğ‘— |ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (x ğ‘– ; Î˜ ğ¸ ); Î˜ ğ¶ )) (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ğ‘ is the number of styles. Note that the two parts of the adversarial network update different sets of parameters, they work together to make sure that the output of ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (x ğ‘– ; Î˜ ğ¸ ) do not contain style information. and concatenating it to the content representation to output in target style using a decoder.</p><p>There are, however, limitations to the above adversarial learning TST framework. The style label alone may not be able to guide the generation of fluent sentences in target style, and some content information may still be lost during its latent representation generation process. Improved variants of the adversarial learning TST framework have also been proposed to address these limitations. For instance, reconstruction loss and cycle-consistency loss are introduced to improve content preservation during TST <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b92">93]</ref>. As part of the decoder model, a reconstruction loss is introduced to enforce that the decoder, which takes the content representation ğ‘ and original style embedding ğ‘  ğ‘œ as input, can reconstruct the input sentence x itself. We require the sentence transferred by the decoder to preserve the content of its original sentence. Thus it should have the capability to recover the original sentence in a cyclic manner.</p><p>Manuscript submitted to ACM Therefore, the transferred style sentence ğ‘¥ â€² is input into the TST model to transfer the sentence back to its original style, and a cycle-consistency loss is used to enforce the generated sentence in original style is similar to the input sentence.</p><p>Other types of losses were also added to improve the adversarial learning TST models. Chen et al. <ref type="bibr" target="#b8">[9]</ref> proposed FM-GAN, which enhances the cycle-consistency loss by minimizing the feature-movers distance between the latent representation of the input and generated sentence in the original style. John et al. <ref type="bibr" target="#b28">[29]</ref> added a style-oriented loss to ensure the style information is contained in style embedding ğ‘ , i.e., a loss which ensures that ğ‘  is discriminative for the style. Zhao et al. <ref type="bibr" target="#b92">[93]</ref> adopted a different approach and added a style discrepancy loss to ensure that the style representation accurately encodes the style information by maximizing the discrepancy between the input and target sentences' styles. Park et al. <ref type="bibr" target="#b59">[60]</ref> proposed relational losses to distinguish the semantics, syntactic, and lexical features between the input and generated transferred style sentences.</p><p>Besides adding loss functions, other studies have proposed auxiliary components to enhance the adversarial learning process. Yin et al. <ref type="bibr" target="#b85">[86]</ref> presented two partial comparators to guide adversarial learning, a content comparator that judges whether two the input sentence and generated sentence share the same content, and a style comparator that judge if they have different styles. Lai et al. <ref type="bibr" target="#b39">[40]</ref> combine the adversarial learning framework with a word-level conditional mechanism to preserve content information by retaining style-unrelated words while modifying the other style-related words. Yang et al. <ref type="bibr" target="#b84">[85]</ref> replaced the binary classifiers with a target domain language model as the discriminator to provide richer and more stable token-level feedback during the adversarial learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Back-Translation</head><p>Another technique that had been explored to disentangle content and style in text is back-translation. Fig. <ref type="figure">5</ref> shows a back translation framework adopted by Prabhumoye et al. <ref type="bibr" target="#b61">[62]</ref> to perform TST. In this approach, Prabhumoye et al.</p><p>attempted to use NMT models to rephrase the sentence and remove the stylistic properties in text. In the proposed model, an English text is first translated to French using an NMT model, and the translated French text is subsequently translated back to English using another NMT model. The latent representation ğ‘§ learned using the NMT model is assumed to contain only content information, devoid of any stylistic properties. Finally, the latent representation ğ‘§ is used to generate text in a different style using the multi-decoder approach. Zhang et al. <ref type="bibr" target="#b90">[91]</ref> adopted an iterative back-translation pipeline to perform TST. The pipeline first learns a cross-domain word embedding in order to build an initial phrase- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Attribute Control Generation</head><p>Existing TST works have also explored learning a style attribute to control text generation in different styles. Fig. <ref type="figure">6</ref> shows the proposed attribute controlled generation model by Hu et al. <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Classifier</head><p>Independency constraint for z Fig. <ref type="figure">6</ref>. Attribute controlled generation proposed in by Hu et al. <ref type="bibr" target="#b21">[22]</ref> Unlike an autoencoder, which learns a compressed representation for an input data, the Variational Autoencoder (VAE) <ref type="bibr" target="#b34">[35]</ref> learns the parameters of a probability distribution representing the data. The learned distribution can also be sampled to generate new data samples. Therefore, the generative nature of VAE makes it widely explored and utilized in many natural language generation tasks <ref type="bibr" target="#b15">[16]</ref>. Hu et al. <ref type="bibr" target="#b21">[22]</ref> proposed a TST model that utilized VAE to learn a sentence's latent representation ğ‘§ and leverage a style classifier to learn a style attribute vector ğ‘ . The probabilistic encoder of VAE also functions as an additional discriminator to capture variations of implicitly modeled aspects, and guide the generator to avoid entanglement during attribute code manipulation. Finally, ğ‘§ and ğ‘  are input into a decoder to generate a sentence in the specific style. Specifically, the VAE loss function is shown as follows:</p><formula xml:id="formula_4">ğ¿ ğ‘‰ ğ´ğ¸ (Î˜ ğ· , Î˜ ğ¸ ; ğ‘¥) = ğ¾ğ¿(ğ‘ ğ¸ (ğ‘§|ğ‘¥)||ğ‘ (ğ‘§)) âˆ’ E ğ‘ ğ¸ (ğ‘§ |ğ‘¥)ğ‘ ğ¶ (ğ‘  |ğ‘¥ [ğ‘™ğ‘œğ‘”ğ‘ ğ· (ğ‘¥ |ğ‘§, ğ‘ )]<label>(3)</label></formula><p>where ğ¾ğ¿(â€¢||â€¢) is the KL-divergence. Î˜ ğ· and Î˜ ğ¸ denote the parameters of the decoder and encoder, respectively.</p><p>ğ‘ ğ¸ (ğ‘§|ğ‘¥) is the conditional probabilistic encoder ğ¸ to infer the latent representation ğ‘§ given input sentence ğ‘¥, and ğ‘ ğ¶ (ğ‘  |ğ‘¥) is the conditional distribution defined by the classifier ğ¶ for each structured variable in ğ‘ . To ensure that ğ‘§ retain the style-independent content information, a independency constraint is proposed to ensure that the latent representation ğ‘§ of the input sentence ğ‘¥ and transferred style sentence ğ‘¥ â€² remains close. Tian et al. <ref type="bibr" target="#b71">[72]</ref> further extended this approach by proposing to add more constraints to preserve the style-independent content by using POS information preservation and a content conditional language model. Lample et al. <ref type="bibr" target="#b40">[41]</ref> proposed an attribute-controlled text generation approach without style and content disentanglement. They argued that it is difficult to judge if the style is indeed disentangled from the content representation, and it is also unnecessary to perform style and content disentanglement for TST. Fig. <ref type="figure">7</ref> illustrates the proposed model by Lample et al. <ref type="bibr" target="#b40">[41]</ref>.</p><p>The model employed Denoising Auto-encoders <ref type="bibr" target="#b73">[74]</ref> and back-translation <ref type="bibr" target="#b64">[65]</ref> to build a translation style between different styles. The intuition for this model is that the noise function may corrupt words in input sentence ğ‘¥ that convey its original style. The corrupted input sentence is then fed into an encoder to generate the latent representation ğ‘§. ğ‘§ is subsequently combined with a trainable target style attribute ğ‘  ğ‘¡ and input into a decoder to generate a sentence in target style ğ‘¥ â€² . Finally, a back-translation process is initiated to have the generated sentence is feed into the sample encode-decoder process to reconstruct the original sentence using the latent representation ğ‘§, and original style attribute Noise Function Noise Function Fig. <ref type="figure">7</ref>. Attribute controlled generation with back translation proposed in by Lample et al. <ref type="bibr" target="#b40">[41]</ref> There are other variants of attribute-controlled approaches that perform TST without content and style disentanglement. For instance, Dai et al. <ref type="bibr" target="#b10">[11]</ref> adopted a transformer-based autoencoder <ref type="bibr" target="#b72">[73]</ref> to perform TST with a trainable style attribute. The model's goal is to harvest the power of attention mechanism in the Transformer to achieve better style transfer and better content preservation. Zhang et al. <ref type="bibr" target="#b87">[88]</ref> proposed a Shared-private encoder-decoder (SHAPED) framework that learns the style attributes to transfer the text style. Li et al. <ref type="bibr" target="#b42">[43]</ref> extended the attribute-controlled TST works and proposed a domain adaptive TST which enable style transfer to be performed in a domain-aware manner. Specifically, besides the latent style attributes, the proposed model also learned domain vectors of the text in the source and target domain. The domain vectors are subsequently used with the style attributes and sentence's latent representations to perform TST across domains. Jain et al. <ref type="bibr" target="#b23">[24]</ref> proposed an unsupervised TST method using the attribute-controlled technique. The approach is similar to the work proposed in <ref type="bibr" target="#b21">[22]</ref>. However, unlike most of the TST methods, which require a style classifier, the proposed model assumes that the style label is unknown. Instead, the model proposed a scoring mechanism that provided scores on the sentences' semantic relatedness, fluency, and readability grade, to guide the learning of the style attribute for TST. Similarly, Zhou et al. <ref type="bibr" target="#b93">[94]</ref> proposed an unsupervised method to perform fine-grained attribute-control to perform TST. The proposed model utilized an attentional seq2seq model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Included in the model is a carefully-designed objective function that fine-tuned the model's style transfer, style relevance consistency, content preservation, and fluency modeling loss terms.</p><formula xml:id="formula_5">ğ‘  ğ‘œ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Entangled Latent Representation Edition</head><p>Another line of work, which also attempted to perform TST without any content and style disentanglement, is directly editing the latent representation learned using the autoencoder-based models. Fig. <ref type="figure">8</ref> shows a common framework adopted by work that edits latent representations for TST. Typically, the latent representation ğ‘§ learned using an autoencoder is manipulated using various methods. The manipulated latent representation ğ‘§ â€² is then input into the decoder to generate text of the target style.</p><p>In earlier work, Mueller et al. <ref type="bibr" target="#b53">[54]</ref> explored manipulating the hidden representation learned using VAE to generate sentences that contain a certain style measured using a corresponding classifier. However, it is interesting to note that there was no quantitative evaluation of the effectiveness of text style transfer in this earlier work. Liu et al. <ref type="bibr" target="#b45">[46]</ref> adopt a gradient-based optimization in the continuous space to manipulate the latent representation learned using VAE and style classifiers to achieve text style transfer. Moreover, the proposed method naturally has the ability to simultaneously control multiple fine-grained attributes, such as sentence length and the presence of specific words, when performing text style transfer tasks. Wang et al. <ref type="bibr" target="#b75">[76]</ref> adopted a similar approach and perform a Fast-Gradient-Iterative-Modification algorithm to edit the latent representation learned using transformer-based autoencoder until the generated text conforms to the target style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Reinforcement Learning</head><p>Reinforcement learning has also been applied to perform TST. For instance, Luo et al. <ref type="bibr" target="#b47">[48]</ref> proposed to learn two seq2seq models between two styles via reinforcement learning, without disentangling style and content. Fig. <ref type="figure">9</ref> illustrates the proposed dual reinforcement learning framework. The authors considered the learning of source-to-target style and target-to-source style as a dual-task. The style classifier reward, ğ‘… ğ‘  , and reconstruction reward, ğ‘… ğ‘ , are designed to encourage style transfer accuracy and content preservation. The overall reward is the harmonic mean of the two rewards, and it is used as the feedback signal to guide learning in the dual-task structure. As such, the model can be trained via reinforcement learning without any use of parallel data or content-style disentanglement. Fig. <ref type="figure">9</ref>. Dual reinforcement learning model for TST proposed in <ref type="bibr" target="#b47">[48]</ref> Manuscript submitted to ACM Gong et al. <ref type="bibr" target="#b17">[18]</ref> proposed a reinforcement learning-based generator-evaluator framework to perform TST. Similar to previous TST works, the proposed model employs an attention-based encoder-decoder mode to transfer and generate target style sentences. However, unlike the previous models that utilize a style classifier to guide the generation process, the proposed model employed a style classifier, semantic model, and a language model to provide style, semantic, and fluency rewards respectively to guide the text generation. The authors' intuition is that the transfer of text style should not only ensure the transfer of style and content preservation but also generate fluent sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Probabilistic Model</head><p>He et al. <ref type="bibr" target="#b19">[20]</ref> proposed a probabilistic deep generative model to infer the latent representations of sentences for TST.</p><p>The proposed model hypothesizes a parallel latent sequence that generates each observed sequence, and the model learns to transform sequences from one domain to another in a completely unsupervised fashion. Specifically, the model combines a recurrent language model prior with an encoder-decoder transducer to infer the latent representations of the sentence in assumed parallel style corpus. The inferred latent representation is then used to generate the sentence of a specific style via a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION METHODOLOGY</head><p>As TST is a relatively new research area, new methods will need to be designed to evaluate the TST algorithms. In this section, we first summarize the downstream tasks and existing datasets used to evaluate the TST models. Next, we discuss the automated and human evaluation methods used to assess the quality of TST algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks and Datasets</head><p>Datasets. Table <ref type="table" target="#tab_7">2</ref> summarises the datasets used to existing studies to evaluate the performance of TST algorithms.</p><p>These corpora often contain texts in labeled with two or more attributes. For example, the Yelp dataset contains review text records labeled with binary sentiment class (i.e., positive or negative), and the Caption dataset contain caption-text records labeled with romantic, humorous, and factual classes. Most of these datasets are non-parallel datasets, i.e., there are no matching text pairs in the different attribute classes, except for Shakespeare-Modern and GYAFC. It is also interesting to note that while the GYAFC corpus is a parallel dataset, most of the existing TST studies assume a non-parallel setting when training the TST models with this dataset.</p><p>Many downstream tasks have been proposed to leverage these datasets to evaluate TST models. In the rest of this section, we will review these downstream tasks in greater detail. the dataset size is small, and the approach is limited to transfer to only one author's style. An interesting future work may be to collect text written by various authors and transfer the style of text among multiple authors.</p><p>Sentiment transfer. Sentiment transfer is a very popular evaluation task adopted in many TST studies. The task involves modifying the sentiment of a sentence while preserving its original contextual content. Table <ref type="table" target="#tab_8">3</ref> shows an example of the sentiment transfer task. Given the input sentence with positive sentiment, "Everything is fresh and so delicious!", the goal of the TST model is to covert the sentence into negative sentiment while preserving the contextual content information. In this example, the word "Everything" represents the content information and is preserved during the style transfer operation. The examples also reveal an interesting aspect of sentiment transfer task: while the sentence's style is transferred, and the contextual content is preserved, the semantic of the sentence has also changed.</p><p>For instance, a sentence supporting a particular political party may semantically change to a negative opinion during the sentiment transfer process. Nevertheless, this task is widely-used to evaluate TST models, and three popular datasets have been proposed for this task:</p><p>â€¢ Yelp<ref type="foot" target="#foot_3">3</ref> is a corpus of restaurant reviews from Yelp collected in <ref type="bibr" target="#b66">[67]</ref>. The original Yelp reviews are on a 5 points rating scale. As part of data preprocessing, reviews with 3 points and above ratings are labeled as positive, while  those below 3 points are labeled as negative. The reviews with an exact 3 points rating considered neutral and are excluded in this dataset.</p><p>â€¢ Amazon<ref type="foot" target="#foot_4">4</ref> is product review dataset from amazon collected in <ref type="bibr" target="#b20">[21]</ref>. It is preprocessing using the same method in the Yelp dataset.</p><p>â€¢ IMDb<ref type="foot" target="#foot_5">5</ref> is a movie reviews dataset. Dai et al. <ref type="bibr" target="#b10">[11]</ref> constructed this dataset by performing similar data preprocessing methods on a publicly available and popular movie reviews dataset <ref type="bibr" target="#b48">[49]</ref>.</p><p>Formality transfer. The formality transfer task involves modifying the formality of a given sentence. Typically, an informal sentence to its formal form and vice versa. Formality transfer is presumably more complex than sentiment transfer, as multiple attributes may affect the formality of text. For instance, the modification on the sentence structure, the length of text, punctuation, capitalization, etc., influences the formality of text. Table <ref type="table" target="#tab_9">4</ref> shows an example of transferring an informal sentence to its formal form. We illustrate that simple keyword replacement methods cannot achieve the formality transfer of a sentence from the four transferred formal sentences. After the sentence is transferred to its formal form, the length of the sentence (as shown in Ref-0) and punctuation may be changed (e.g., replacement of ellipsis with a full stop). Furthermore, unlike sentiments, the formality of a sentence is highly subjective; individuals may perceive a sentence's degree of formality differently.</p><p>GYAFC <ref type="foot" target="#foot_6">6</ref> is the largest human-labeled parallel dataset proposed for the formality transfer task <ref type="bibr" target="#b62">[63]</ref>. The authors extracted informal sentences from Entertainment&amp;Music (E&amp;M) and Family&amp;Relationship (F&amp;R) domains of the Yahoo Answers L6 corpus <ref type="foot" target="#foot_7">7</ref> . The collected dataset was further preprocessed to remove sentences that are too short or long.</p><p>Finally, the authors crowd-sourced workers to manually re-write the informal sentences to each formal sentences, resulting in a parallel formality dataset. This dataset was also widely used to evaluate the recent TST models.</p><p>Paper-news title transfer. Paper-news title transfer was the task of transferring a title to a different type while preserving the content. Fu et al. <ref type="bibr" target="#b14">[15]</ref> collected the PNTD<ref type="foot" target="#foot_8">8</ref> dataset, which consisted of paper titles retrieved from academic Republican defund them all, especially when it comes to the illegal immigrants. Republican thank u james, praying for all the work u do . Democracy on behalf of the hard-working nh public school teachers-thank you! Democracy we need more strong voices like yours fighting for gun control .</p><p>publication archive websites such ACM Digital Library, Arxiv, Sprint, ScienceDirect and Nature, and news titles from the UC Irvine Machine Learning Repository.</p><p>Captions style transfer. Li et al. 2018 <ref type="bibr" target="#b43">[44]</ref> proposed the task of transferring factual formal captions into romantic and humorous styles. The researchers collected the caption dataset 4 , where each sentence was labeled as factual, romantic, or humorous. This is also the smallest TST dataset.</p><p>Gender style transfer. The differences between male and female writing styles is a widely studied research topic in sociolinguistics. Prabhumoye et al. <ref type="bibr" target="#b61">[62]</ref> extended the sociolinguistics studies to perform TST between text written by the different gender, i.e., transfer a text written by a male to female writing style and vice versa. The researchers constructed the gender dataset<ref type="foot" target="#foot_9">9</ref> by first preprocessing a Yelp review dataset annotated with the gender of the reviewers <ref type="bibr" target="#b63">[64]</ref> by split the reviews into sentences and preserve the gender label for each sentence. The sentences that are deemed to be gender-neutral are removed from the dataset.</p><p>Political slant transfer. Political slant transfer is the task of modifying the writer's political affiliation writing style while preserving the content. Prabhumoye et al. <ref type="bibr" target="#b61">[62]</ref> collected comments of Facebook posts from 412 members of the United States Senate and House who have public Facebook pages. The comments are annotated with the congressperson's political part affiliation: democracy or republican. Table <ref type="table" target="#tab_10">5</ref> shows examples of comments collected in the dataset 9 .</p><p>Offensive language correction. The use of offensive and abusive languages is a growing problem in online social media. The offensive language correction task aims to transfer offensive sentences into non-offensive ones. Santos et al. <ref type="bibr" target="#b12">[13]</ref> collected posts from Twitter and Reddit. The posts were subsequently classified into "offensive" and "non-offensive" classes using a classifier pre-trained on an annotated offensive language dataset. Multiple-attribute style transfer. Thus far, the tasks we have discussed involved transferring text between two style attributes. Lai et al. <ref type="bibr" target="#b39">[40]</ref> proposed a multiple-attribute style transfer task and collected multi-style attribute datasets based on Yelp and Amazon review datasets. Table <ref type="table" target="#tab_11">6</ref> summarizes the statistics of three datasets collected in their studies.</p><p>The goal is to transfer a text with specific multiple style attributes such as a sentiment, gender of the author, etc.</p><p>The specification of multiple attributes makes the TST task more complex and realistic as the text style should be multi-faceted. For instance, the gender and sentiment of the author could both affect the style of the text. We postulate that Multiple-attribute style transfer may be one of the TST research's future directions, and we will discuss this further in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automated Evaluation</head><p>Several automated evaluation metrics have been proposed to measure the effectiveness of TST models <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref>.</p><p>Broadly, these metrics evaluate the TST algorithms in three criteria:</p><p>(1) The ability to transfer the text style. (2) The amount of original content preserved after the TST operation.</p><p>(3) The fluency of the transferred style sentence.</p><p>A TST algorithm underperforming in any of these three criteria is considered ineffective in performing the TST task.</p><p>For example, a TST algorithm transfers a negative sentiment sentence, "the pasta taste bad!", to a positive one, "the movie is great!". While the algorithm can transfer the style of the input text, i.e., from negative to positive sentiment, it fails to preserve the original statement's content, i.e., describing the pasta. Like many other natural language generation tasks, the transferred sentence will also have to achieve a certain level of fluency for the TST algorithm to be useful in real-world applications. Therefore, an effective TST algorithm will have to perform well in all three criteria of the evaluation.</p><p>Transfer strength. A TST model's transfer strength or its ability to transfer text style is commonly measured using Style Transfer Accuracy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b66">67]</ref>. Typically, a binary style classifier TextCNN <ref type="bibr" target="#b51">[52]</ref> is first pre-trained separately to predict the style label of the input sentence. The style classifier is then used to approximate the style transfer accuracy of the transferred style sentences by considering the target style as the ground truth. It is also important to note that the style classifier is not perfect. For instance, when pre-trained on the Yelp and GYAFC datasets and applied to classify tweets on their respective validation dataset, the style classifier is only able to achieve accuracies of 97.2% and 83.4% respectively. Nevertheless, the style transfer accuracy is thus far the only known automated quantitative approach to evaluate the transfer strength of the TST algorithms.</p><p>Content preservation. To quantitatively measure the amount of original content preserved after the style transfer operation, TST studies have borrowed three automated evaluation metrics that are commonly used in other natural language generation tasks:</p><p>â€¢ self-BLEU : The BLEU score <ref type="bibr" target="#b58">[59]</ref> was originally designed to evaluate the quality of a machine-translated text.</p><p>The BLEU score was one of the first metrics to claim a high correlation with human judgment on the translated text quality. To compute the BLEU score, the machine-translated text is compared with a set of good quality reference translations. However, most of the TST task assumes a non-parallel setting, and matching references of style transferred sentences are not always available. Therefore, a self -BLEU is adopted by comparing the style transferred sentence with its original sentence. The intuition is that the content is assumed to be preserved when the style transferred sentence shared many similar n-grams with the original sentence.</p><p>â€¢ Cosine Similarity): Fu et al. <ref type="bibr" target="#b14">[15]</ref> calculated the cosine similarity between original sentence embedding and transferred sentence embedding. The intuition is that the embeddings of the two sentences should be close to preserve the semantics of the transferred sentences.</p><p>â€¢ Word Overlap: Vineet et al. <ref type="bibr" target="#b28">[29]</ref> argued that the cosine similarity is not a sensitive metric as the original and transferred sentences may have high cosine similarity scores even the content of the sentences are different. Thus, Manuscript submitted to ACM they employed a simple metric that counts the unigram word overlap rate of the original and style transferred sentences. Noted that stop words and style-attributed words (e.g., sentiment words) are excluded in the word overlap calculation.</p><p>Fluency. Generating fluent sentences is a common goal for almost all natural language generation models. A common approach to measuring the fluency of a sentence is using a trigram Kneser-Ney language model <ref type="bibr" target="#b37">[38]</ref>. The Kneser-Ney language model is pre-trained to estimate the empirical distribution of trigrams in a training corpus. Subsequently, the perplexity score of a generated sentence is calculated by comparing the sentence's trigram and the estimated trigram distribution. The intuition is that a generated sentence with a lower perplexity score is considered more "aligned" to the training corpus, and therefore considered as more fluent. In TST tasks, the language model is similarly trained on the TST datasets, and the perplexity scores of the style transferred sentences are computed to evaluate the sentences' fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Evaluation</head><p>Few TST studies have performed human evaluations on their proposed TST algorithms <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b66">67]</ref> as such evaluations are often expensive and laborious. In a typical human evaluation setting, human workers are crowd-sourced to rate how the style transferred sentence fair on the three evaluation criteria using a range scale. For example, given a pair of original and transferred sentences, a human worker is asked to rate how well the content is preserved in the transferred sentence on the scale of 1 to 5 points, with 5 points being "very well preserved". Multiple human workers are asked to evaluate a given pair of original and transferred sentences, and the average scores are reported to reduce individual bias. Although researchers have put in great effort to ensure the quality of the human evaluation on TST tasks, the evaluation approach has proven to be very challenging as the interpretation of the text style is subjective and may vary across individuals <ref type="bibr">[51, 56?</ref> ]. Nevertheless, human evaluations still offer valuable insights into how well TST algorithms are able to transfer style and generate sentences that are acceptable by human standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">REPRODUCIBILITY STUDY</head><p>Although most of the existing TST methods were evaluated in the original works using the downstream tasks discussed in Section 5, the experiments were often carried out with no or few baselines. Thus, we conduct a reproducibility study <ref type="foot" target="#foot_10">10</ref> and benchmark 19 TST models on two popular corpora: Yelp reviews and GYAFC, representing the sentiment transfer tasks and formality transfer tasks, respectively. To the best of our knowledge, this is the first time where so many TST models are evaluated on the same datasets. Specifically, the experimental results from this study provide new insights into how each TST algorithm fair against each other in terms of transfer strength, content preservation, and fluency. This section is organized as follows: We first describe the experimental setup of our reproducibility study. Next we discuss the experimental results on sentiment transfer and formality transfer tasks. Finally, we perform the trade-off analyses, where we investigate how the relationships between multiple evaluation criteria influence the TST model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Environment Settings. The experiments were performed on an Ubuntu 18.04.4 LTS system with 24 cores, 128 GB RAM, and a clock speed of 2.9 GHz. The GPU used for deep neural network-based models was Nvidia GTX 2080Ti. We followed the environmental requirements and hyperparameter settings of the released code implementations of the TST models to reproduce the experimental results. For TST models that that did not experiment on our datasets in their original publications, we will experiment and optimally set the hyperparameters of TST models. Table <ref type="table" target="#tab_12">7</ref> shows the training, validation, and test splits of the Yelp and GYAFC datasets used in our experiments.</p><p>Evaluation Metrics. We adopt the evaluation metrics discussed in Section 5 to measure the performance of the TST models. Specifically, we apply the Style Transfer Accuracy (ACC) to measure transfer strength. For measuring content preservation, we adopt self-BLEU, Cosine Similarity (CS), and Word Overlap (WO). Specifically, for the experiments on the GYAFC dataset, the human style transferred sentences of the test set are available. Therefore, we also compute the BLEU score as between the TST model's transferred sentences and the human style transferred sentences. We compute the perplexity score (PPL) to quantify the fluency of the transferred sentences. Finally, we compute two average metrics that consider all evaluation aspects:</p><p>â€¢ Geometric Mean (G-Score): We compute the geometric mean of ACC (transfer strength), self-BLEU (content preservation), WO (content preservation), and 1/PPL (fluency). We excluded the CS measure in mean computation due to its insensitivity, and we take calculated the inverse of the perplexity score because a smaller PPL score is designed. â€¢ Harmonic Mean (H-Score): Different averaging methods reflect different priorities. Thus, we also compute the harmonic mean of ACC, self-BLEU, WO, and 1/PPL. Reproduced Models. We limit our reproducibility study to these 19 TST models as they had published their implementation codes. We hope to encourage fellow researchers to publish their codes and datasets as it can promote this field's development. Specifically, we reproduced and implemented the following TST models:</p><p>â€¢ DeleteOnly; Template; Del&amp;Retri <ref type="bibr" target="#b43">[44]</ref>: A style keyword replacement method, which disentangles the style and content of sentence explicitly by keyword replacement. The authors proposed three variants of their mode: DeleteOnly, which first remove the removed style attributed keywords from the source sentence. Subsequently, the latent representation of the source sentence is combined with the target style attribute and input into a Manuscript submitted to ACM</p><p>â€¢ UST <ref type="bibr" target="#b80">[81]</ref>: A style keyword replacement method, which utilized cycled reinforcement learning to iteratively replace style attributed keywords while maintaining the content in text for text style transfer. This model was originally implemented for the sentiment transfer task.</p><p>â€¢ PTO <ref type="bibr" target="#b77">[78]</ref>: A style keyword replacement TST model that applied reinforcement-learning to hierarchically reinforced a Point-Then-Operate (PTO) sequence operation. The PTO operation has two agents: a high-level agent iteratively proposes operation positions, and a low-level agent alters the sentence based on the high-level proposals. Using this reinforcement framework, the style-attributed keywords are replaced explicitly to perform TST.</p><p>â€¢ SMAE <ref type="bibr" target="#b89">[90]</ref>: A style keyword replacement model to perform TST by disentangling style and content explicitly.</p><p>The model was originally designed for sentiment transfer. The Sentiment attribute words are first detected, and a sentiment-memory based auto-encoder model is subsequently used to perform sentiment modification without parallel data.</p><p>â€¢ ARAE <ref type="bibr" target="#b91">[92]</ref>: A generic natural language generation technique that utilizes adversarial learning to modify the specific attributes in text. TST is one of the model's applications proposed in its original paper.</p><p>â€¢ CAAE <ref type="bibr" target="#b66">[67]</ref>: An adversarial learning TST model that implicitly disentangle the text's style. Specifically, the model assumes a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform TST. â€¢ Multi-Dec; Style-Emb <ref type="bibr" target="#b14">[15]</ref>: A adversarial learning TST model that utilized a style classifier to perform disentanglement of style and content representation for style transfer task. Two variants of the model were proposed:</p><p>The multi-decoder (Multi-Dec) model that uses different decoders to generate text with different styles. The style embedding (Style-Emb) model that concatenates the style embedding vector with content representation to generate different style text with one decoder.</p><p>â€¢ BST <ref type="bibr" target="#b61">[62]</ref>: A back-translation based TST model that employed a pre-trained back translation model to rephrase a sentence while reducing its stylistic characteristics. Subsequently, separate style-specific decoders were used for style transfer.</p><p>â€¢ DRLST <ref type="bibr" target="#b28">[29]</ref>: An adversarial learning TST model that incorporates auxiliary multi-task and adversarial objectives for style prediction and bag-of-word prediction respectively to perform text style transfer.</p><p>â€¢ Ctrl-Gen <ref type="bibr" target="#b21">[22]</ref>: An attribute-controlled TST model that utilized variational auto-encoders and style classifier to guide the learning of a style attribute to control the generation of text in different styles.</p><p>â€¢ DAST; DAST-C <ref type="bibr" target="#b42">[43]</ref>: An attribute-controlled TST model that performs TST in a domain-aware manner. Two variants are proposed: The Domain Adaptation Style (DAST) model and DAST with generic content information (DAST-C). In these models, latent style attributes and domain vectors are learned to perform TST across domains.</p><p>â€¢ DualRL <ref type="bibr" target="#b47">[48]</ref>: A reinforcement-learning based-TST model that utilized two seq2seq models to transfer between two text styles. Specifically, this model considers the learning of source-to-target style and target-to-source style as a dual-task that mutually reinforce each other to perform TST without disentangling style and content.</p><p>â€¢ PFST <ref type="bibr" target="#b19">[20]</ref>: A probabilistic deep generative TST model. The model combines a language model prior and an encoder-decoder transducer to infer the latent representations of the sentence in an assumed parallel style corpus. The inferred latent representations are subsequently used to generate the sentence of a specific style via a decoder.</p><p>Manuscript submitted to ACM </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sentiment Transfer</head><p>Table <ref type="table" target="#tab_13">8</ref> shows the performance of various TST models on the sentiment transfer task. While there are no TST models that achieved the best performance in all evaluation metrics, DualRL <ref type="bibr" target="#b47">[48]</ref>, PTO <ref type="bibr" target="#b77">[78]</ref>, B-GST <ref type="bibr" target="#b69">[70]</ref>, and PFST <ref type="bibr" target="#b19">[20]</ref> have achieved a well-balanced trade-off among between text fluency, content preservation, and the style transfer accuracy.</p><p>We also note that the different average methods, i.e., G-Score and H-Score, weighted the different evaluation metrics differently. For instance, H-score gives a higher weightage to perplexity scores of generated sentences. Thus, DRLST <ref type="bibr" target="#b28">[29]</ref>, which has the lowest PPL score, also has the highest H-score. Conversely, the Template model <ref type="bibr" target="#b43">[44]</ref> has the highest PPL score and lowest H-score.</p><p>More interestingly, we observed that the style keyword replacement methods such as DeleteOnly <ref type="bibr" target="#b43">[44]</ref>, Template <ref type="bibr" target="#b43">[44]</ref>, Del&amp;Retri <ref type="bibr" target="#b43">[44]</ref>, B-GST <ref type="bibr" target="#b69">[70]</ref>, G-GST <ref type="bibr" target="#b69">[70]</ref>, UST <ref type="bibr" target="#b80">[81]</ref>, PTO <ref type="bibr" target="#b77">[78]</ref>, and SMAE <ref type="bibr" target="#b89">[90]</ref>, have achieved good performance in the sentiment transfer tasks. The methods have achieved a high transfer accuracy score while preserving the content information, i.e., high self-BLEU, CS, and WO scores. A possible reason for the style keyword replacement methods good performance might be due to the nature of the task; the sentiment of a sentence can be easily modified by replacing keywords related to the source sentiment. For example, replacing "fresh" with "rotten" would transform the sentence from positive to negative sentiment. However, it is interesting to note that the Template method <ref type="bibr" target="#b43">[44]</ref>, which is an algorithm that simply replaces the sentiment-related keywords, has a high perplexity score, which indicates bad performance in sentence fluency. This motivates more complex generative approaches that can prevent the generation of implausible sentences by simple keyword replacement.</p><p>Manuscript submitted to ACM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Formality Transfer</head><p>Table <ref type="table" target="#tab_14">9</ref> shows the performance of various TST models on the formality transfer task. Similar to the observation in sentiment transfer, none of the TST models is able to score well on all evaluation metrics. We noted that the average style transfer accuracy in GYAFC is 52.9%, which is significantly lower than Yelp's average score of 84.4%. This highlights the difficulty of the formality transfer task. We also observe that most models performed worse in this task compared to the sentiment transfer task. It is also unsurprising that the style keyword replacement methods did not perform well in the formality transfer task; most of these models achieved a low style transfer accuracy. Some of the adversarial learning-based TST models, such as CAAE <ref type="bibr" target="#b66">[67]</ref> and DRLST <ref type="bibr" target="#b28">[29]</ref>, had achieved high style transfer accuracy but very low content preservation as these models lack the mechanism to control content preservation during the generative process.</p><p>Interestingly, we observe that the attribute-controlled TST methods, i.e., Ctrl-Gen <ref type="bibr" target="#b21">[22]</ref>, DAST <ref type="bibr" target="#b42">[43]</ref>, and DAST-C <ref type="bibr" target="#b42">[43]</ref> have achieved good performance both style transfer accuracy and content preservation.</p><p>The GYAFC dataset also provided the performances of four human references performing the formality transfer task on the test dataset (as shown in the bottom of Table <ref type="table" target="#tab_14">9</ref>). On average, the human references had achieved 78.1% style transfer accuracy. This is considered a reasonable performance, given that the pre-trained binary classifier only managed to achieved 83.4% accuracy on the test set. Furthermore, formality in text is subjective, and the four human references may have different opinions on the degree of formality in text.</p><p>As the GYAFC dataset is a parallel dataset, i.e., there are matching sentences in source and target styles, we are able to compute the BLEU score between the transferred style sentence and the matching sentence in the target style.</p><p>Unsurprisingly, the human references have achieved the highest BLEU score, suggesting that the sentences generated by the human references are quite similar to the matching sentences in the target style. In comparison, the TST models fair poorly in the BLEU scores. We also observe that the TST models' average content preservation metrics scores in the formality transfer task are lower than the scores in the sentiment transfer task. For instance, the WO scores in the sentiment transfer task are higher because only a few keywords need to be replaced to perform the style transfer.</p><p>However, in the formality transfer case, a more drastic and complex modification of the text has to be performed for the style transfer. As such, there will be less word overlap between the original sentence and the transferred sentence, resulting in a lower WO score. The limitation of existing metrics in measuring content preservation in formality transfer highlights the need to search for better evaluation methods for this challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation Metrics Trade-off Analysis</head><p>Besides evaluating the TST models on the two style transfer tasks, we also reproduced the evaluation metrics trade-off analysis proposed in <ref type="bibr" target="#b50">[51]</ref>. The goal of this analysis is to investigate the relationships between the evaluation metrics.</p><p>Specifically, we create variants of TST models by varying their hyperparameters and study the trade-off effects between pairs of evaluation metrics. Similar to the study in <ref type="bibr" target="#b50">[51]</ref>, we select ARAE, DualRL, and CAAR models for our analysis.  The observations made in our trade-off analysis suggest some form competing relationships between transfer strength and content preservation, i.e., when transfer strength scores increase, content preservation scores decrease, and vice versa. A potential reason for observation might be due to the entanglement between semantic and stylistic properties in natural language; it is hard to separate the two properties, and changing one affects the other. Therefore, when optimizing to transfer the style in text, it is hard to maintain the sentence's semantic, i.e., the content information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE RESEARCH DIRECTION AND OPEN ISSUES</head><p>TST is a relatively new research area. While existing works have established a foundation for TST research, this section outlines several promising prospective research directions. We also discuss the open issues that we believe are critical to the present state of the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Deeper Dive into Style Representation</head><p>Although there have been studies that suggested that existing techniques are not able to disentangle a text's style from its content <ref type="bibr" target="#b40">[41]</ref> effectively, we believe that style could still be a standalone element in text. For example, a Shakespearean scholar will be able to recognize a text written in Shakespearean style regardless of its content. Similarly, a Star War fan will also be able to recognize scripts from Yoda's Scenes regardless of its content because the speech style of the Star War character is distinctive. The human's ability to discern the styles in text suggests the possibility to learn a Manuscript submitted to ACM More studies could also be done on the style embeddings learned by the existing techniques. Currently, we have little understanding on the style representations learned using existing techniques; besides knowing the style representation supposedly has some correlation with the style labels, we do not know much about the information that are preserved in the style representations. For instance, in learning the style representations for the formality transfer tasks, little is known about the preservation of the sentence structure in the representations, and the sentence structure may have an impact on the formality of the text.</p><p>To this end, a potential future research direction would be to conduct a deeper analysis of the style representations learned for the different tasks using existing techniques. We believe that this will provide new insights, which can guide the development of future TST techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Unsupervised Text Style Transfer</head><p>While most of the exiting TST methods are developed for the non-parallel dataset setting, these techniques continue to require a large amount of style labeled data to guide the transferrence of text styles. A promising research direction would be to explore unsupervised methods to perform TST with little or no labeled data. For instance, recent studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> have explored guiding the transfer of text style by scoring the sentences' semantic relatedness, fluency, and readability grade instead of the style labels. We postulate that more aspects of the text, such as tone, brevity, sentence structures, etc., can be explored to train the future TST models and reduce the dependence on the style labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Going Beyond Transferring Between Two Styles</head><p>Currently, most of the existing TST methods focus on transferring the text between two styles. We believe that TST studies should go beyond performing a binary style transfer and explore richer and more dynamic tasks. For example, Lai et al. <ref type="bibr" target="#b39">[40]</ref> proposed a multiple-attribute style transfer task where a text is transferred by specifying multiple style attributes such as a sentiment, gender of the author, etc. Domain-aware TST method has also been explored where we consider the domain of the text (e.g., food or movie reviews) when transferring the text styles (e.g., from positive to negative sentiment). We believe that more dynamic TST tasks with better real-life applications will be a promising future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Automatic Evaluation for Text Style Transfer</head><p>Our experimental evaluation in section 6 has illustrated the challenges of evaluating the effectiveness of TST models.</p><p>The existing evaluation methods have a few limitations. Firstly, the evaluation of text style transfer based on transfer accuracy is limited by the performance of style classifier. Secondly, similar to previous studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b57">58]</ref>, we notice that the transfer strength is inversely proportional to the content preservation, suggesting that these metrics may be Manuscript submitted to ACM complementary and challenging to optimize simultaneously. The limitations of existing evaluation metrics see the demand to explore novel automatic evaluation metrics to evaluate TST models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND CONCLUSION</head><p>Although TST is a relatively new branch of the natural language processing field, a considerable amount of TST research has been conducted in recent years. The explosive growth of TST research has generated many novel and interesting TST models. This survey aims to organize these novel TST models using a taxonomy (cf. Fig. <ref type="figure" target="#fig_0">1</ref>). We also summarize the common techniques used by modern TST models to transfer text styles. We also emphasize the important TST research trends, such as the shift from TST models attempting to disentangle text style from content to aiming to perform TST without any style-content disentanglement. While we postulate that the trend on performing TST without any style-content disentanglement will continue, we believe that the study on style representation remains an interesting research direction that deserves further exploration.</p><p>Besides discussing the common TST techniques, we also conducted a large-scale reproducibility study where we replicated and benchmarked 19 state-of-the-art TST algorithms on two publicly available datasets. To the best of our knowledge, this is the first large-scale reproducibility study on TST methods. The results of our study show that none of the TST methods could dominate on all evaluation metrics. This suggests the complexity of the TST task, wherein different methods may have advantages in different aspects, and there is no simple way to declare a winner. The evaluation analysis in our reproducibility study also advocated the need to search for better TST evaluation metrics.</p><p>We believe that research on TST will continue to flourish, and the industry will continue to find more exciting applications for the existing TST methods. We hope that this survey can provide readers with a comprehensive understanding of the critical aspects of this field, clarify the important types of TST methods, and shed some light on future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Text Style Transfer Taxonomy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1 . 1</head><label>11</label><figDesc>Types of Data Setting. We broadly classify the existing TST studies into two categories based on the types of data settings used for model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. An overview of the Delete-Retreive-Generate framework proposed by Li et al.<ref type="bibr" target="#b43">[44]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Two common adversarial learning based TST models: multi-decoder (left) and style-embedding (right) proposed by Fu et al.<ref type="bibr" target="#b14">[15]</ref>. The content representation c is the output of the of the encoder. The style classifier aims at distinguishing the style of the input. An adversarial network is used to make sure content c does not have style representation. In style-embedding, content representation c and style embedding s are concatenated and fed into decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Reconstruction loss ğ¿ ğ‘Ÿğ‘’ğ‘ and cycle-consistency loss ğ¿ ğ‘ğ‘¦ğ‘ used in text style transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>sequence model to generate the sentence in the target style. The Template model is simply replacing the deleted style attributed keywords with keywords of target style. The Del&amp;Retri model first performed the same keyword removal as DeleteOnly method. Next, it retrieves a new sentence associate with the target attribute. Lastly, the keyword-removed source sentence and the retrieved sentence are input into a sequence model to generate the transferred style sentence. â€¢ B-GST; G-GST [70]: A TST model that extended the work in [44] and proposed the Generative Style Transformer (GST) to perform text style transfer. There are two variants of GST model: the Blind Generative Style Transformer (B-GST) and the Guided Generative Style Transformer (G-GST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 show</head><label>10</label><figDesc>Fig.10show the trade-off analysis results on sentiment transfer task. Specifically, Fig.10A,B and Cshow the trade-off relationships between style transfer strength and content preservation metrics, while Fig.10Dshows the trade-off relationship between style transfer strength and fluency metric. Similar to the observations made in<ref type="bibr" target="#b50">[51]</ref>, we notice that as the transfer strength of increases, content preservation metrics decrease across the three models. However, the trade-off relationship between the transfer strength and sentence fluency is less obvious as we notice that ARAE is able to achieve lower PPL when ACC increases. Similar observations are made for the formality transfer task in Fig. Fig.11. Manuscript submitted to ACM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Metrics trade-off analysis for sentiment transfer on Yelp review dataset</figDesc><graphic url="image-3.png" coords="25,73.44,239.04,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Metrics trade-off analysis for formality transfer on GYAFC review dataset</figDesc><graphic url="image-7.png" coords="26,110.16,239.04,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Publications Based on Different Text Style Transfer Techniques</figDesc><table><row><cell>Data Setting</cell><cell>Strategy (Content-Style Disentanglement)</cell><cell>Technique</cell><cell>Publication</cell></row><row><cell>Parallel</cell><cell>-</cell><cell>Sequence-to-Sequence</cell><cell>[7, 25, 27, 45, 55, 66, 77, 83,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>89]</cell></row><row><cell>Non-Parallel</cell><cell>Explicit</cell><cell>Explicit Style Keyword Replacement</cell><cell>[44, 70, 78, 81, 90]</cell></row><row><cell></cell><cell>Implicit</cell><cell>Back-Translation</cell><cell>[62, 91]</cell></row><row><cell></cell><cell></cell><cell>Adversarial Learning</cell><cell>[9, 15, 29, 40, 47, 60, 67, 85,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>86, 92, 93]</cell></row><row><cell></cell><cell></cell><cell>Attribute Control Generation</cell><cell>[22, 72]</cell></row><row><cell></cell><cell>Without</cell><cell>Attribute Control Generation</cell><cell>[11, 24, 41, 88, 94]</cell></row><row><cell></cell><cell></cell><cell>Entangled Latent Representation Edition</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 5. Back Translation framework for Text Style Transfer proposed by Prabhumoye et al.<ref type="bibr" target="#b61">[62]</ref> </figDesc><table><row><cell>Input sentence x in</cell><cell cols="2">NMT (English-to-French)</cell><cell>Output sentence x in</cell><cell>NMT (French-to-English)</cell><cell>representation Latent</cell><cell>Decoder Style A</cell></row><row><cell>English</cell><cell>Encoder</cell><cell>Decoder</cell><cell>French</cell><cell>Encoder</cell><cell>z</cell><cell>Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Style B</cell></row></table><note>table. The phrase-table is then used to bootstrap an iterative back-translation model, which jointly trained two NMT systems to transfer text style.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Note that in this model, the authors did not claim or constrain the latent representation of ğ‘§ only to contain content information.</figDesc><table><row><cell></cell><cell></cell><cell>Target</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>style attribute</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>s t</cell><cell></cell><cell></cell></row><row><cell>Input sentence x</cell><cell>Encoder</cell><cell>Latent representation z</cell><cell>Decoder</cell><cell>Transferred style sentence x'</cell></row><row><cell>Back Translation Loss Lbt</cell><cell>Decoder</cell><cell>Latent representation</cell><cell>Encoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell>z</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>style attribute</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>s o</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Fig. 8. Common framework for editing text's latent representation for TSTXu et al.<ref type="bibr" target="#b81">[82]</ref> conducted extensive experiments to investigate the latent vacancy in unsupervised learning of controllable representation when modeling text with VAEs. Similar to the study in<ref type="bibr" target="#b53">[54]</ref>, Xu et al. studied the impact on text style when manipulating the factors in latent representation and found that if a manipulation fails at decoding accurate sentences, it is because the manipulation results in representation areas that the decoder never seen during training. To handle this issue, they proposed to constrain the posterior mean to a learned probability simplex and only performed manipulation within the probability simplex.</figDesc><table><row><cell></cell><cell></cell><cell>Latent Space</cell><cell></cell></row><row><cell>Input sentence x</cell><cell>Encoder</cell><cell>z</cell><cell></cell></row><row><cell>great food, awesome staff and very polite workers!</cell><cell></cell><cell>z'</cell><cell>Decoder</cell><cell>Transferred style</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sentence x'</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>great food but horrible staff</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and very rude workers!</cell></row><row><cell cols="2">Manuscript submitted to ACM</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Dataset statistics for text style transfer.</figDesc><table><row><cell>Dataset</cell><cell>Subset</cell><cell>Attributes</cell><cell>#Text records</cell></row><row><cell>Shakespeare-Modern [84]</cell><cell>-</cell><cell>Shakespeare Modern</cell><cell>21,076 21,076</cell></row><row><cell>Yelp [67]</cell><cell>-</cell><cell>Positive Negative</cell><cell>381,911 252,343</cell></row><row><cell>IMDb [11]</cell><cell>-</cell><cell>Positive Negative</cell><cell>181,869 190,597</cell></row><row><cell>Amazon [21]</cell><cell>-</cell><cell>Positive Negative</cell><cell>278,713 279,284</cell></row><row><cell>GYAFC [63]</cell><cell>F&amp;R E&amp;M</cell><cell>Informal Formal Informal Formal</cell><cell>56,087 55,233 56,888 56,033</cell></row><row><cell>PNTD [15]</cell><cell>-</cell><cell>Paper News</cell><cell>107,538 108,503</cell></row><row><cell></cell><cell></cell><cell>Romantic</cell><cell>6,300</cell></row><row><cell>Caption [44]</cell><cell>-</cell><cell>Humorous</cell><cell>6,300</cell></row><row><cell></cell><cell></cell><cell>Factual</cell><cell>300</cell></row><row><cell>Gender [62]</cell><cell>-</cell><cell>Male Female</cell><cell>1,604,068 1,604,068</cell></row><row><cell>Political [62]</cell><cell>-</cell><cell>Democracy Republican</cell><cell>298,961 298,961</cell></row><row><cell>Offensive [13]</cell><cell>Twitter Reddit</cell><cell>Offensive Non-offensive Offensive Non-offensive</cell><cell>74,218 1,962,224 266,785 7,096,473</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>Sentiment transfer examples.</figDesc><table><row><cell>Positive Sentiment â†â†’ Negative Sentiment</cell></row><row><cell>input Everything is fresh and so delicious!</cell></row><row><cell>Ref-0 Everything was so stale.</cell></row><row><cell>Ref-1 Everything is rotten and not so delicious.</cell></row><row><cell>Ref-2 Everything is stale and horrible.</cell></row><row><cell>Ref-3 Everything is stale and tastes bad.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Formality transfer examples. Informal Formality â†â†’ formal Formality input He loves you, too, girl...Time will tell. Ref-0 He loves you as well, but only time can tell what will happen. Ref-1 He loves you too, lady...time will tell. Ref-2 He loves you, as well. Time will tell. Ref-3 He loves you too and time will tell.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Political slant examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Dataset statistics for multiple-attribute transfer datasets.</figDesc><table><row><cell>Dataset</cell><cell>Sentiment</cell><cell>Gender</cell><cell></cell><cell></cell><cell>Category</cell></row><row><cell>FYelp</cell><cell cols="4">Positive Negative 2,056,132 639,272 1,218,068 1,477,366 904,026 518,370 Male Female American Asian</cell><cell>Bar 595,681 431,225 246,102 Dessert Mexican</cell></row><row><cell>Amazon</cell><cell>Positive Negative 64,251,073 10,944,310</cell><cell>--</cell><cell>--</cell><cell cols="2">Book 26,208,872 14,192,554 25,894,877 4,324,913 4,574,167 Clothing Electronics Movies Music</cell></row><row><cell>Social Media Content</cell><cell cols="4">Relaxed Annoyed 7,682,688 17,823,468 14,501,958 18,463,789 12,628,250 7,629,505 Male Female age:18-24 age:65+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Dataset statistics for Yelp and GYAFC.</figDesc><table><row><cell cols="3">Dataset Subset Attributes</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Yelp</cell><cell>-</cell><cell cols="4">Positive Negative 176,787 25,278 50,278 267,314 38,205 76,392</cell></row><row><cell>GYAFC</cell><cell>F&amp;R</cell><cell>Informal Formal</cell><cell>51,967 51,967</cell><cell>2,788 2,247</cell><cell>1,332 1,019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>The results on Yelp dataset of style transfer models included.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">ACC(%) self-BLEU</cell><cell>CS</cell><cell>WO</cell><cell cols="3">PPL G-Score H-Score</cell></row><row><cell>DualRL</cell><cell>79.0</cell><cell>58.3</cell><cell cols="3">0.97 0.801 134</cell><cell>2.29</cell><cell>0.030</cell></row><row><cell>DRLST</cell><cell>91.2</cell><cell>7.6</cell><cell cols="2">0.904 0.484</cell><cell>86</cell><cell>1.41</cell><cell>0.045</cell></row><row><cell>DeleteOnly</cell><cell>84.2</cell><cell>28.7</cell><cell cols="3">0.893 0.501 115</cell><cell>1.80</cell><cell>0.034</cell></row><row><cell>Template</cell><cell>78.2</cell><cell>48.1</cell><cell cols="3">0.850 0.603 1959</cell><cell>1.04</cell><cell>0.002</cell></row><row><cell>Del&amp;Retri</cell><cell>88.1</cell><cell>30.0</cell><cell cols="3">0.897 0.464 101</cell><cell>1.87</cell><cell>0.039</cell></row><row><cell>Ctrl-Gen</cell><cell>89.6</cell><cell>49.5</cell><cell cols="3">0.953 0.707 384</cell><cell>1.69</cell><cell>0.010</cell></row><row><cell>BST</cell><cell>83.1</cell><cell>2.3</cell><cell cols="3">0.827 0.076 261</cell><cell>0.49</cell><cell>0.015</cell></row><row><cell>CAAE</cell><cell>82.7</cell><cell>11.2</cell><cell cols="3">0.901 0.277 145</cell><cell>1.15</cell><cell>0.027</cell></row><row><cell>PTO</cell><cell>82.3</cell><cell>57.4</cell><cell cols="3">0.982 0.737 245</cell><cell>1.94</cell><cell>0.016</cell></row><row><cell>ARAE</cell><cell>83.2</cell><cell>18.0</cell><cell cols="3">0.874 0.270 138</cell><cell>1.31</cell><cell>0.028</cell></row><row><cell>B-GST</cell><cell>89.2</cell><cell>46.5</cell><cell cols="3">0.959 0.649 216</cell><cell>1.88</cell><cell>0.018</cell></row><row><cell>G-GST</cell><cell>72.7</cell><cell>52.0</cell><cell cols="3">0.967 0.617 407</cell><cell>1.55</cell><cell>0.010</cell></row><row><cell>DAST</cell><cell>90.7</cell><cell>49.7</cell><cell cols="3">0.961 0.705 323</cell><cell>1.77</cell><cell>0.012</cell></row><row><cell>DAST-C</cell><cell>93.6</cell><cell>41.2</cell><cell cols="3">0.933 0.560 450</cell><cell>1.48</cell><cell>0.009</cell></row><row><cell>Multi-Dec</cell><cell>69.6</cell><cell>17.2</cell><cell cols="3">0.887 0.244 299</cell><cell>0.99</cell><cell>0.013</cell></row><row><cell>Style-Emb</cell><cell>47.5</cell><cell>31.4</cell><cell cols="3">0.926 0.433 217</cell><cell>1.31</cell><cell>0.018</cell></row><row><cell>UST</cell><cell>74.0</cell><cell>41.0</cell><cell cols="3">0.929 0.448 394</cell><cell>1.36</cell><cell>0.010</cell></row><row><cell>SMAE</cell><cell>84.4</cell><cell>14.8</cell><cell cols="3">0.907 0.294 210</cell><cell>1.315</cell><cell>0.019</cell></row><row><cell>PFST</cell><cell>85.3</cell><cell>41.7</cell><cell cols="3">0.902 0.527 104</cell><cell>2.06</cell><cell>0.038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>The results on GYAFC dataset of style transfer models included.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>GYAFC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">ACC(%) self-BLEU BLEU</cell><cell>CS</cell><cell>WO</cell><cell cols="3">PPL G-Score H-Score</cell></row><row><cell>DualRL</cell><cell>56.7</cell><cell>61.6</cell><cell>18.8</cell><cell cols="3">0.944 0.447 122</cell><cell>1.89</cell><cell>0.032</cell></row><row><cell>DRLST</cell><cell>71.1</cell><cell>4.2</cell><cell>2.7</cell><cell cols="2">0.909 0.342</cell><cell>86</cell><cell>1.04</cell><cell>0.045</cell></row><row><cell>DeleteOnly</cell><cell>26.0</cell><cell>35.4</cell><cell>16.2</cell><cell cols="2">0.945 0.431</cell><cell>82</cell><cell>1.48</cell><cell>0.047</cell></row><row><cell>Template</cell><cell>51.5</cell><cell>45.1</cell><cell>19.0</cell><cell cols="3">0.943 0.509 111</cell><cell>1.81</cell><cell>0.035</cell></row><row><cell>Del&amp;Retri</cell><cell>50.6</cell><cell>22.1</cell><cell>11.8</cell><cell cols="2">0.934 0.345</cell><cell>94</cell><cell>1.42</cell><cell>0.041</cell></row><row><cell>Ctrl-Gen</cell><cell>73.1</cell><cell>57.0</cell><cell>15.6</cell><cell cols="3">0.943 0.446 168</cell><cell>1.82</cell><cell>0.023</cell></row><row><cell>BST</cell><cell>69.7</cell><cell>0.5</cell><cell>0.5</cell><cell>0.883</cell><cell>0.04</cell><cell>69</cell><cell>0.38</cell><cell>0.042</cell></row><row><cell>CAAE</cell><cell>72.3</cell><cell>1.8</cell><cell>1.5</cell><cell cols="2">0.896 0.028</cell><cell>55</cell><cell>0.51</cell><cell>0.044</cell></row><row><cell>ARAE</cell><cell>76.2</cell><cell>4.8</cell><cell>2.2</cell><cell cols="2">0.903 0.042</cell><cell>77</cell><cell>0.67</cell><cell>0.040</cell></row><row><cell>B-GST</cell><cell>30.3</cell><cell>22.5</cell><cell cols="4">11.6 0.951 0.557 117</cell><cell>1.34</cell><cell>0.034</cell></row><row><cell>G-GST</cell><cell>31.0</cell><cell>20.7</cell><cell>10.2</cell><cell cols="3">0.941 0.556 127</cell><cell>1.29</cell><cell>0.031</cell></row><row><cell>DAST</cell><cell>73.1</cell><cell>50.6</cell><cell>14.3</cell><cell cols="3">0.934 0.350 204</cell><cell>1.59</cell><cell>0.019</cell></row><row><cell>DAST-C</cell><cell>78.2</cell><cell>48.5</cell><cell>13.8</cell><cell cols="3">0.927 0.328 308</cell><cell>1.42</cell><cell>0.013</cell></row><row><cell>Multi-Dec</cell><cell>22.2</cell><cell>13.4</cell><cell>5.9</cell><cell cols="3">0.911 0.168 146</cell><cell>0.76</cell><cell>0.026</cell></row><row><cell>Style-Emb</cell><cell>27.7</cell><cell>8.3</cell><cell>3.6</cell><cell cols="3">0.897 0.102 136</cell><cell>0.64</cell><cell>0.027</cell></row><row><cell>UST</cell><cell>23.6</cell><cell>0.5</cell><cell>0.5</cell><cell cols="3">0.881 0.012 28</cell><cell>0.27</cell><cell>0.035</cell></row><row><cell>SMAE</cell><cell>21.6</cell><cell>6.5</cell><cell>1.2</cell><cell cols="2">0.898 0.079</cell><cell>74</cell><cell>0.62</cell><cell>0.046</cell></row><row><cell>PFST</cell><cell>50.8</cell><cell>55.3</cell><cell>16.5</cell><cell cols="3">0.940 0.466 200</cell><cell>0.51</cell><cell>0.020</cell></row><row><cell>Human0</cell><cell>78.1</cell><cell>20.5</cell><cell>43.5</cell><cell cols="2">0.942 0.393</cell><cell>80</cell><cell>1.67</cell><cell>0.048</cell></row><row><cell>Human1</cell><cell>78.7</cell><cell>18.2</cell><cell>43.2</cell><cell cols="3">0.931 0.342 199</cell><cell>1.25</cell><cell>0.020</cell></row><row><cell>Human2</cell><cell>78.2</cell><cell>18.6</cell><cell>43.4</cell><cell cols="3">0.932 0.354 192</cell><cell>1.28</cell><cell>0.021</cell></row><row><cell>Human3</cell><cell>77.4</cell><cell>18.8</cell><cell>43.5</cell><cell cols="3">0.931 0.354 196</cell><cell>1.27</cell><cell>0.020</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Manuscript submitted to ACM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">www.sparknotes.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">http://tinyurl.com/ycdd3v6h Manuscript submitted to ACM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://github.com/shentianxiao/language-style-transfer Manuscript submitted to ACM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://github.com/lijuncen/Sentiment-and-Style-Transfer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://github.com/fastnlp/nlp-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://github.com/raosudha89/ GYAFC-corpus</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">https://webscope.sandbox.yahoo.com/catalog.php?datatype=l</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">https://github.com/fuzhenxin/textstyletransferdata Manuscript submitted to ACM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">https://github.com/shrimai/Style-Transfer-Through-Back-Translation Manuscript submitted to ACM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10">Code implementation of the reproduced model are compiled in this repository: https://gitlab.com/bottle_shop/style/tst_survey Manuscript submitted to ACM</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Imitation. Author imitation is the task of paraphrasing a sentence to match a specific author's style. To perform this task, Xu et al. <ref type="bibr" target="#b83">[84]</ref> collected a parallel dataset, which captures the line-by-line modern paraphrases for 16 of Shakespeare's 36 plays (Antony &amp; Cleopatra, As You Like It, Comedy of Errors, Hamlet, Henry V, etc.) using the educational site Sparknotes 1 . The goal was to imitate Shakespeare's text style by transferring modern English sentences into Shakespearean style sentences. This dataset is publicly available 2 and was also used in other TST studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The imitation of authors writing style is an exciting TST task. There are many interesting industrial applications, such as transferring the writing styles of famous novel authors into other stories and unifying the writing styles of multiple authors to a single author in a collaborative setting. However, the Shakespeare-Modern dataset is the only known corpus that facilitates author imitation in TST studies. There are also some apparent limitations in the corpus;</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on chatbot design techniques in speech conversation systems</title>
		<author>
			<persName><forename type="first">Abdul-Kader</forename><surname>Sameera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Success with style: Using writing style to predict the success of novels</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Vikas Ganjigunte Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1753" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><surname>Paul S Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Change of writing style with time</title>
		<author>
			<persName><forename type="first">Fazli</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Patton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="61" to="82" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating prose style transfer with the Bible</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Riddell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society open science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">171920</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Do adults change their minds after reading persuasive text? Written Communication</title>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">J</forename><surname>Chambliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Garner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="291" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial text generation via feature-mover&apos;s distance</title>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4666" to="4677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianze</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5997" to="6007" />
		</imprint>
	</monogr>
	<note>Manuscript submitted to ACM</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Persuasive style and its realization through transitivity analysis: A SFL perspective</title>
		<author>
			<persName><forename type="first">Laya</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darani</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia-social and behavioral sciences</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="179" to="186" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><surname>Padhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Short Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linguistic stylistics</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enkvist</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Walter de Gruyter GmbH &amp; Co KG</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Style transfer in text: Exploration and evaluation</title>
		<author>
			<persName><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<ptr target="http://arxiv.org/abs/1508.06576" />
		<title level="m">A Neural Algorithm of Artistic Style. CoRR abs/1508</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">6576</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3168" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic function and literary style: An inquiry into the language of William Golding&apos;s The Inheritors</title>
		<author>
			<persName><surname>Mak Halliday</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in Modern Stylistics</title>
				<imprint>
			<date type="published" when="1981">1981. 1981</date>
			<biblScope unit="page" from="325" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Probabilistic Formulation of Unsupervised Text Style Transfer</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web</title>
				<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Discourses of writing and learning to write. Language and education</title>
		<author>
			<persName><forename type="first">Roz</forename><surname>IvaniÄ</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="220" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised controllable text formalization</title>
		<author>
			<persName><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Prakash Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6554" to="6561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shakespearizing Modern Language Using Copy-Enriched Sequence to Sequence Models</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
				<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hooks in the Headline: Learning to Generate Headlines with Controlled Styles</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Orii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation</title>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3088" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural style transfer: A review</title>
		<author>
			<persName><forename type="first">Yongcheng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zunlei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangled Representation Learning for Non-Parallel Text Style Transfer</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hareesh</forename><surname>Bahuleyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Linguistic strategies and cultural styles for persuasive discourse</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Johnstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stance, style, and the linguistic individual</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stance: sociolinguistic perspectives</title>
		<imprint>
			<biblScope unit="page" from="29" to="52" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive persuasive systems: a study of tailored persuasive text messages to reduce snacking</title>
		<author>
			<persName><forename type="first">Maurits</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><forename type="middle">De</forename><surname>Ruyter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Panos Markopoulos, and Emile Aarts</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
	<note>TiiS)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personalizing persuasive technologies: Explicit and implicit personalization using persuasion profiles</title>
		<author>
			<persName><forename type="first">Maurits</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><forename type="middle">De</forename><surname>Ruyter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emile</forename><surname>Aarts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparing data from chatbot and web surveys: Effects of platform and conversational style on survey response quality</title>
		<author>
			<persName><forename type="first">Soomin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonhwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gahgene</forename><surname>Gweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">AndrÃ©</forename><surname>Klahold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madjid</forename><surname>Fathi</surname></persName>
		</author>
		<title level="m">Computer Aided Writing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Word Processing as Writing Support</title>
		<author>
			<persName><forename type="first">AndrÃ©</forename><surname>Klahold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madjid</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Writing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved backing-off for M-gram language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1995 International Conference on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
				<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training</title>
		<author>
			<persName><forename type="first">Chih-Te</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Te</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-You</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Jen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3570" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiple-Attribute Text Rewriting</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards Controlled Transformation of Sentiment in Sentences</title>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Leeftink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerasimos</forename><surname>Spanakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Agents and Artificial Intelligence, ICAART 2019</title>
				<meeting>the 11th International Conference on Agents and Artificial Intelligence, ICAART 2019<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-19">2019. February 19-21, 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain Adaptive Text Style Transfer</title>
		<author>
			<persName><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3295" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer</title>
		<author>
			<persName><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quase: Sequence editing under quantifiable guidance</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3855" to="3864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Revision in Continuous Space: Fine-Grained Control of Text Style Transfer</title>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Chris Pal, and Jiancheng Lv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Content preserving text generation with attribute controls</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5103" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A dual reinforcement learning framework for unsupervised text style transfer</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5116" to="5122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume</title>
				<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies-volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reflections on research on writing and technology for struggling writers</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Macarthur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning Disabilities Research &amp; Practice</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluating Style Transfer for Text</title>
		<author>
			<persName><forename type="first">Remi</forename><surname>Mir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Obradovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="495" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25">2014. 2014. October 25-29, 2014</date>
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL. ACL</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">If looks could kill: The impact of different rhetorical styles on persuasive geocommunication</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Muehlenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="361" to="375" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sequence to better sequence: continuous revision of combinatorial structures</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Large-scale Hierarchical Alignment for Author Style Transfer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nikola</surname></persName>
		</author>
		<author>
			<persName><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><surname>Hahnloser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08237</idno>
		<ptr target="http://arxiv.org/abs/1810.08237" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Daunting Task of Real-World Textual Style Transfer Auto-Evaluation</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03747</idno>
		<ptr target="http://arxiv.org/abs/1910.03747" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text</title>
				<meeting>the 5th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="444" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
				<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
				<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Paraphrase Diversification Using Counterfactual Debiasing</title>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6883" to="6891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Automated Writing Evaluation Tools in the Improvement of the Writing Skill</title>
		<author>
			<persName><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Instruction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Style Transfer Through Back-Translation</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer</title>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Obfuscating gender in social media writing</title>
		<author>
			<persName><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP and Computational Social Science</title>
				<meeting>the First Workshop on NLP and Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semi-supervised Text Style Transfer: Cross Projection in Latent Space</title>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Manuscript submitted to ACM Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Manuscript submitted to ACM Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4939" to="4948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6830" to="6841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Computational Linguistics: Analysis of The Functional Use of Microsoft Text Word Processor Text Corrector</title>
		<author>
			<persName><forename type="first">Priscilla</forename><surname>Chantal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duarte</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Luiz</forename><surname>Perez Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria Olivia Araujo Vilas</forename><surname>Boas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Linguistics, Literature and Culture</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>LLC</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Writing with word processors: a research overview</title>
		<author>
			<persName><forename type="first">Ilana</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="68" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Transforming&quot; Delete, Retrieve, Generate Approach for Controlled Text Style Transfer</title>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Sudhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Maheswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3260" to="3270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Structured Content Preservation for Unsupervised Text Style Transfer</title>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06526</idno>
		<ptr target="http://arxiv.org/abs/1810.06526" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">2010. Dec (2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11034" to="11044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer</title>
		<author>
			<persName><forename type="first">Yunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3564" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4873" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mask and infill: applying masked language model to sentiment transfer</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5271" to="5277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A new chatbot for customer service on social media</title>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibha</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Akkiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3506" to="3510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">On Variational Learning of Controllable Representations for Text without Supervision</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11975</idno>
		<ptr target="http://arxiv.org/abs/1905.11975" />
		<imprint>
			<date type="published" when="1905">2019. CoRR abs/1905.11975. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Formality Style Transfer with Hybrid Textual Annotations</title>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06353</idno>
		<ptr target="http://arxiv.org/abs/1903.06353" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Paraphrasing for style</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
				<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2899" to="2914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised text style transfer using language models as discriminators</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7287" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Utilizing non-parallel text for style transfer by making partial comparisons</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5379" to="5386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">The technical writer&apos;s handbook: writing with style and clarity</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>University Science Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1528" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Parallel Data Augmentation for Formality Style Transfer</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning Sentiment Memories for Sentiment Modification without Parallel Data</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1103" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Style Transfer as Unsupervised Machine Translation</title>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07894</idno>
		<ptr target="http://arxiv.org/abs/1808.07894" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Adversarially regularized autoencoders</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018. International Machine Learning Society (IMLS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9405" to="9420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Language Style Transfer from Sentences with Arbitrary Unknown Styles</title>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04071</idno>
		<ptr target="http://arxiv.org/abs/1808.04071" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer</title>
		<author>
			<persName><forename type="first">Chulun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted to ACM</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="53" to="93" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
