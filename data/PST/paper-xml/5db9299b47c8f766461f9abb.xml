<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blind Super-Resolution Kernel Estimation using an Internal-GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Blind Super-Resolution Kernel Estimation using an Internal-GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD48F0EC80A9245DD229FB6016F73908</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super resolution (SR) methods typically assume that the low-resolution (LR) image was downscaled from the unknown high-resolution (HR) image by a fixed 'ideal' downscaling kernel (e.g. Bicubic downscaling). However, this is rarely the case in real LR images, in contrast to synthetically generated SR datasets. When the assumed downscaling kernel deviates from the true one, the performance of SR methods significantly deteriorates. This gave rise to Blind-SR -namely, SR when the downscaling kernel ("SR-kernel") is unknown. It was further shown that the true SR-kernel is the one that maximizes the recurrence of patches across scales of the LR image. In this paper we show how this powerful cross-scale recurrence property can be realized using Deep Internal Learning. We introduce "KernelGAN", an image-specific Internal-GAN [29], which trains solely on the LR test image at test time, and learns its internal distribution of patches. Its Generator is trained to produce a downscaled version of the LR test image, such that its Discriminator cannot distinguish between the patch distribution of the downscaled image, and the patch distribution of the original LR image. The Generator, once trained, constitutes the downscaling operation with the correct image-specific SR-kernel. KernelGAN is fully unsupervised, requires no training data other than the input image itself, and leads to state-of-the-art results in Blind-SR when plugged into existing SR algorithms. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic model of SR assumes that the low-resolution input image I LR is the result of down-scaling a high-resolution image I HR by a scaling factor s using some kernel k s (the "SR kernel"), namely:</p><formula xml:id="formula_0">I LR = (I HR * k s ) ↓ s (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The goal is to recover I HR given I LR . This problem is ill-posed even when the SR-Kernel is assumed known (an assumption made by most SR methods -older <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref> and more recent <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b11">12]</ref>). A boost in SR performance was achieved in the past few years introducing Deep-Learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b11">12]</ref>. However, since most SR methods train on synthetically downscaled images, they implicitly rely on the SR-kernel k s being fixed and 'ideal' (usually a Bicubic downscaling kernel with antialiasing-MATLAB's default imresize command). Real LR images, however, rarely obey this assumption. This results in poor SR performance by state-of-the-art (SotA) methods when applied to real or 'non-ideal' LR images (see Fig. <ref type="figure" target="#fig_0">1a</ref>).</p><p>The SR kernel of real LR images is influenced by the sensor optics as well as by tiny camera motion of the hand-held camera, resulting in a different non-ideal SR-kernel for each LR image, even if taken by the same sensor. It was shown in <ref type="bibr" target="#b25">[26]</ref> that the effect of using an incorrect SR-kernel is of greater Since they train on 'ideal' LR images, they perform poorly on real non-ideal LR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic EDSR+ RCAN+ KernelGAN(Ours) Ground</head><p>Truth HR LR Input image interpolation <ref type="bibr" target="#b20">[21]</ref> [38] SR method: <ref type="bibr" target="#b29">[30]</ref> (b) Comparison to SotA Blind-SR methods (SR×4):</p><p>PDN WDSR Kernel of <ref type="bibr" target="#b23">[24]</ref> KernelGAN(Ours) Ground Truth HR LR Input image <ref type="bibr" target="#b33">[34]</ref> [36] SR method: <ref type="bibr" target="#b29">[30]</ref> SR method: <ref type="bibr" target="#b29">[30]</ref> Figure <ref type="figure">1</ref>: SR×4 on real 'non-ideal' LR images (downloaded from the internet, or downscaled by an unknown kernel). Full images and additional examples in supplementary material (please zoom in on screen).</p><p>influence on the SR performance than any choice of an image prior. This observation was later shown to hold also for deep-learning based priors <ref type="bibr" target="#b29">[30]</ref>. The importance of the SR-kernel accuracy was further analyzed empirically in <ref type="bibr" target="#b26">[27]</ref>.</p><p>The problem of SR with an unknown kernel is known as Blind SR. Some Blind-SR methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref> were introduced prior to the deep learning era. Few Deep Blind-SR methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> were recently presented in the NTIRE'2018 SR challenge <ref type="bibr" target="#b30">[31]</ref>. These methods do not explicitly calculate the SR-kernel, but propose SR networks that are robust to variations in the downscaling kernel. A work concurrent to ours, IKC <ref type="bibr" target="#b9">[10]</ref>, performs Blind-SR by alternating between the kernel estimation and the SR image reconstruction. A different family of recent Deep SR methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>, while not explicitly Blind-SR, allow to input a different image-specific (pre-estimated) SR-kernel along with the LR input image at test time. Note that SotA (non-blind) SR methods cannot make any use of an image-specific SR kernel at test time (even if known/provided), since it is different from the fixed downscaling kernel they trained on. These methods thus produce poor SR results in realistic scenarios -see Fig. <ref type="figure" target="#fig_0">1a</ref> (in contrast to their excellent performance on synthetically generated LR images).</p><p>The recurrence of small image patches (5x5, 7x7) across scales of a single image, was shown to be a very strong property of natural images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>. Michaeli &amp; Irani <ref type="bibr" target="#b23">[24]</ref> exploited this recurrence property to estimate the unknown SR-kernel directly from the LR input image. An important observation they made was that the correct SR-kernel is also the downscaling kernel which maximizes the similarity of patches across scales of the LR image. Based on their observation, they proposed a nearest-neighbor patch based method to estimate the kernel. However, their method tends to fail for SR scale factors larger than ×2, and has a very long runtime.</p><p>This internal cross-scale recurrence property is very powerful, since it is image-specific and unsupervised (requires no prior examples). In this paper we show how this property can be combined with the power of Deep-Learning, to obtain the best of both worlds -unsupervised SotA SR-kernel estimation, with SotA Blind-SR results. We build upon the recent success of Deep Internal Learning <ref type="bibr" target="#b29">[30]</ref> (training an image-specific CNN on examples extracted directly from the test image), and in particular on Internal-GAN <ref type="bibr" target="#b28">[29]</ref> -a self-supervised GAN which learns the image-specific distribution of patches.</p><p>More specifically, we introduce "KernelGAN" -an image-specific Internal-GAN, which estimates the SR kernel that best preserves the distribution of patches across scales of the LR image. Its Generator is trained to produce a downscaled version of the LR test image, such that its Discriminator cannot distinguish between the patch distribution of the downscaled image, and the patch distribution of the original LR image. In other words, G trains to fool D to believe that all the patches of the downscaled image were actually taken from the original one. The Generator, once trained, constitutes the downscaling operation with the correct image-specific SR-kernel. KernelGAN is fully unsupervised, requires no training data other than the input image itself, and leads to state-ofthe-art results in Blind-SR when plugged into existing SR algorithms.</p><p>Since downscaling by the SR-kernel is a linear operation applied to the LR image (convolution and subsampling -see Eq. 1), our Generator (as opposed to the Discriminator) is a linear network (without non-linear activations). At first glance, it may seem that a single-strided convolution layer (which stems from Eq. 1) should suffice as a Generator. Interestingly, we found that using a deep linear network is dramatically superior to a single-strided one. This is consistent with recent findings in theoretical deep-learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b10">11]</ref>, where deep linear networks were shown to have optimization advantages over a single layer network for linear regression models. This is further elaborated in Sec. 4.1.</p><p>Our contributions are therefore several-fold:</p><p>• This is the first dataset-invariant deep-learning method to estimate the unknown SR kernel (a critical step for true SR of real LR images). KernelGAN is fully unsupervised and requires no training data other than the input image itself, hence enables true SR in "the wild". • KernelGAN leads to state-of-the-art results in Blind-SR when plugged into existing SR algorithms.</p><p>• To the best of our knowledge, this is the first practical use for deep linear networks (so far used mostly for theoretical analysis), with demonstrated practical advantages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Approach</head><p>Given only the input image, our goal is to find its underlying image-specific SR kernel. We seek the kernel that best preserves the distribution of patches across scales of the LR image. More specifically, we aim to "generate" a downscaled image (e.g. by a factor of 2) such that its patch distribution is as close as possible to that of the LR image.</p><p>By matching distributions rather than individual patches, we can leverage recent advances in distribution modeling using Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref>. GANs can be understood as a tool for distribution matching <ref type="bibr" target="#b8">[9]</ref>. A GAN typically learns a distribution of images in a large image dataset. It maps examples from a source distribution, p x , to examples indistinguisable from a target distribution, p y : G : x → y with x∼p x , and G(x)∼p y . An internal GAN <ref type="bibr" target="#b28">[29]</ref> trains on a single input image and learns its unique internal distribution of patches.</p><p>Inspired by InGAN <ref type="bibr" target="#b28">[29]</ref>, KernelGAN is also an image specific GAN that trains on a single input image. It consists of a downscaling generator (G) and a discriminator (D) as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. Both G and D are fully-convolutional, which implies the network is applied to patches rather than the whole image, as in <ref type="bibr" target="#b15">[16]</ref>. Given an input image I LR , G learns to downscale it, such that, for D, at the patch level, it is indistinguishable from the input image I LR .</p><p>D trains to output a heat map, referred to as D-map (see fig. <ref type="figure" target="#fig_1">2</ref>) indicating for each pixel, how likely is its surrounding patch to be drawn from the original patch-distribution. It alternately trains on real examples (crops from the input image) and fake ones (crops from G's output). The loss is the pixel-wise MSE difference between the output D-map and the label map. The labels for training D are a map of all ones for crops extracted from the original LR image, and a map of all zeros for crops extracted from the downscaled image.</p><p>We adopt the LSGAN <ref type="bibr" target="#b22">[23]</ref> variant, and define the objective of the KernelGAN as:</p><formula xml:id="formula_2">G * (I LR ) = argmin G max D E x∼patches(I LR ) [(D(x) -1) 2 + D(G(x))) 2 ] + R (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where R is the regularization term on the downscaling SR-kernel resulting from the generator G (see more details in Sec. 4.2). Once converged, the generator G * constitutes, implicitly, the ideal SR downscaling function for the specific LR image.</p><p>The GAN trains for 3,000 iterations, alternating single optimization steps of G and D, with the ADAM optimizer (β 1 = 0.5, β 2 = 0.999). Learning rate is 2e -4 , decaying ×0.1 every 750 iters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discriminator</head><p>The goal of the discriminator D is to learn the distribution of patches of the input image I LR and discriminate between real patches belonging to this distribution and fake patches generated by G. D's real examples are crops from I LR , while fake examples are crops currently outputed by G.</p><p>We use a fully-convolutional patch discriminator, as introduced in <ref type="bibr" target="#b15">[16]</ref>, applied to learn the patch distribution of a single image as in <ref type="bibr" target="#b28">[29]</ref>. To discriminate small image patches, we use no pooling or strides, thus achieving a small receptive field of a 7×7 patch. In this setting, D is implicitly applied to each patch separately and produces a heat-map (D-map) where each location in the map corresponds to a patch from the input. The labels for the discriminator are maps (matrices of real/fake, i.e. 1/0 labels, respectively) of the same size as the input to the discriminator. Each pixel in D-map indicates how likely is its surrounding patch to be drawn from the learned patch distribution. See Fig. <ref type="figure" target="#fig_2">3</ref> for architecture details of the discriminator.</p><p>4 Deep Linear Generator = The downscaling SR-Kernel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Linear Generator</head><p>The generator G constitutes the downscaling model. Since downscaling by convolution and subsampling is a linear transform (Fig. <ref type="figure">1</ref>), we use a linear Generator (without any non-linear activations).</p><p>We refer to the model of downscaling from Eq. 1. In principle, the expressiveness of a single strided convolutional layer should cover all possible downscaling methods captured by Eq. 1. However, we empirically found that such architecture does not converge to the correct solution (see 6). We attribute this behavior to the following conjecture: A generator consisting of a single layer has exactly one set of parameters for the correct solution (the set of weights that make the ground-truth kernel). This means that there is only one point on the optimization surface that is acceptable as a solution and it is the global minimum. Achieving such a global minimum is easy when the loss function is convex (as in linear regression). But in our case the overall loss function is a non-linear neural network-D which is highly non-convex. In such case, the probability for getting from some random initial state to the global minimum using gradient based methods is negligible. In contrast to a single layer, standard (deep) neural networks are assumed to converge from a random initialization due to the fact that there are many good local minima and negligibly few bad local minima <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. Hence, for a problem that by definition has one valid solution, optimizing a single layer generator is impossible.</p><p>A non-linear generator would not be suitable either. Without an explicit constraint on G to be linear, it will generate physically unwanted solutions for the optimization objective. Such solutions include generating any image that contains valid patches but has no downscaling relation (Eq. 1). One example is generating a tile of just several patches from the input. This would be a solution that complies with eq. 2 but is unwanted.</p><p>This conjecture motivated us to use deep linear networks. These are networks consisting of a sequence of linear layers with no activations, and are used for theoretical analysis <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b10">11]</ref>. The expressiveness of a deep linear network is exactly as the one of a single linear layer (i.e. Linear regression), however, its optimization has several different aspects. While it can be convex with respect to the input, the loss would never be convex with respect to the weights (assuming more than one layer). In fact, linear networks have infinitely many equally valued global minima. Any choice of network parameters matching to one of these minima would be equivalent to any other minimum pointthey are just different factorizations of the same matrix. Motivated by these observations, we employ a deep linear generator. By that we allow infinitely many valid solutions to our optimization objective, all equivalent to the same linear solution. Furthermore, it was shown by <ref type="bibr" target="#b1">[2]</ref>, that gradient-based optimization is faster for deeper linear networks than shallow ones.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> depicts the architecture of G. We use 5 hidden convolutional layers with 64 channels each. The first 3 filters are 7×7, 5×5, 3×3 and the rest are 1×1. This makes a receptive field of 13 ×13 (allowing for a 13 × 13 SR kernel). This setting of filters takes into account the effective receptive field <ref type="bibr" target="#b21">[22]</ref>; maintaining the same receptive field while having filters bigger than 1×1 following the first layer encourages the center of the kernel to have higher values. To provide a reasonable initial starting point, the generator's output is constrained to be similar to an ideal downscaling (e.g. bicubic) of the input, for the first iterations. Once satisfied, this constraint is discarded. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extracting the explicit kernel</head><p>At any iteration during the training, the generator G constitutes the currently estimated downscaling operation for the specific LR input image. The image-specific kernel is implicitly captured by the trained weights of G. However, there are two reasons to explicitly extract the kernel from G. First, we are interested in having a compact downscaling convolution kernel rather than a downscaling network. This step is trivial; convolving all the filters of G sequentially with sride 1 results in the SR-kernel k (see Fig. <ref type="figure" target="#fig_3">4</ref>). When extracted, the kernel is just a small array that can be supplied to SR algorithms. The second reason for explicitly extracting the kernel is to allow applying explicit and physically-meaningful priors on the kernel. This is the goal of the regularization term R in Eq.2 which decreases the hypotheses space to a sub-set of the plausible kernels, that obey certain restrictions. Such restrictions are that the kernel would sum up to 1 and be centered so it will not shift the image. The regularization also ameliorates faulty tendencies of the optimization process to produce kernels that are too spread out and smooth. However, it is not enough to extract the kernel, this extraction must be differentiable so that the regularization losses can be back-propagated through it. At each iteration, we apply a differentiable action of calculating the kernel (convolving all the filters of G sequentially with stride-1). The regularization loss term R is then applied and included in the optimization objective.</p><p>The regularization term in our objective in eq. 2 is the following:</p><formula xml:id="formula_4">R = αL sum_to_1 + βL boundaries + γL sparse + δL center<label>(3)</label></formula><p>where α = 0.5, β = 0.5, γ = 5, δ = 1, and:</p><p>• L sum_to_1 = 1i,j k i,j encourages k to sum to 1.</p><p>• L boundaries = i,j |k i,j • m i,j | penalizing non-zero values close to the boundaries. m is a constant mask of weights exponentially growing with distance from the center of k.</p><p>• L sparse = i,j |k i,j | 1 /2 encourages sparsity to prevent the network from oversmoothing kernels.</p><p>• L center = (x 0 , y 0 ) -i,j ki,j •(i,j) i,j ki,j 2 encourages k's center of mass to be at the center of the kernel, where (x 0 , y 0 ) denote the center indices. SR kernels are not only image specific, but also depend on the desired scale-factor s. However, there is a simple relation between SR-kernels of different scales. We are thus able to obtain a kernel for SR ×4 from G that was trained to downscale by ×2. This is advantageous for two reasons: First, it allows extracting kernels for various scales by one run of KernelGAN. Second, it prevents downscaling the LR image too much. Small LR images downscaled by a scale factor of 4 may result in tiny images (×16 smaller than the HR image) which may not contain enough data to train on. KernelGAN is trained to estimate a kernel k 2 for a scale-factor of 2. It is easy to show that the kernel for a scale factor of 4, i.e. k 4 , can be analytically calculated by requiring:</p><formula xml:id="formula_5">I LR * k 4 ↓ 4 = (I LR * k 2 ↓ 2 ) * k 2 ↓ 2 . It implies that k 4 = k 2 * k 2_dilated , where k 2_dilated [n 1 , n 2 ] = k 2 n1 2 , n2<label>2</label></formula><p>n 1 , n 2 even 0 else For a mathematical proof of the above derivation, as well as an ablation study of the various kernel constraints -see our project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head><p>We evaluated our method on real LR images, as well as on 'non-ideal' synthetically generated LR images with ground-truth (both ground-truth HR images, as well as the true SR-kernels).  <ref type="bibr" target="#b23">[24]</ref> and KernelGAN (ours). PSNR of ZSSR <ref type="bibr" target="#b29">[30]</ref>, when supplied with those kernels, is noted in the bottom right of each kernel, emphasizing the significance of kernel estimation accuracy. Our method outperforms <ref type="bibr" target="#b23">[24]</ref> in SR performance (as well as in visual similarity to ground truth SR kernel) </p><formula xml:id="formula_6">Method ×2 ×4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Method</head><p>The performance of our method is analyzed in 2 ways: Kernel estimation accuracy and SR performance. The latter is done both visually (see fig. <ref type="figure" target="#fig_0">1a</ref> and supplementary material), and empirically on the synthetic dataset analyzing PSNR and SSIM measurements (Table <ref type="table" target="#tab_0">1</ref>). Evaluation is done using the script provided by <ref type="bibr" target="#b18">[19]</ref> and used by many works, including <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. For evaluation of the kernel estimation we chose two non-blind SR algorithms that accept a SR-kernel as input <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>, provide them with different kernels (bicubic, ground truth SR-kernel, ours and <ref type="bibr" target="#b29">[30]</ref>) and compared their SR performance. We present four types (categories) of algorithms for the analysis:</p><p>• Type 1 includes the non-blind SotA SR methods trained on bicubically downscaled images.</p><p>• Type 2 are the winners of the NTIRE'2018 Blind-SR challenge <ref type="bibr" target="#b30">[31]</ref>.</p><p>• Type 3 consists of combinations of 2 methods for SR-kernel estimation, <ref type="bibr" target="#b23">[24]</ref> and ours, and 2 non-blind SR methods, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>, that regard the estimated kernel as input. This combination is itself, a full Blind-SR algorithm. • Type 4, is again the combination above, only with the use of the ground truth SR-kernel, thus providing an upper bound to Type 3.</p><p>When providing a kernel to ZSSR <ref type="bibr" target="#b29">[30]</ref> (whether ours, <ref type="bibr" target="#b23">[24]</ref>'s, ground-truth kernel), we provide both the ×2 and ×4 kernels, in order to exploit the gradual process of ZSSR (using 2 sequential SR steps).</p><p>We compare our kernel estimation to Michaeli &amp; Irani <ref type="bibr" target="#b23">[24]</ref> which, to the best of our knowledge, is the leading method for SR-kernel estimation from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset for Blind-SR</head><p>There is no adequate dataset for quantitatively evaluating Blind-SR. The data used in the NTIRE'2018 Blind-SR challenge <ref type="bibr" target="#b30">[31]</ref> has an inherent problem; it suffers from sub-pixel misalignments between the LR image and the HR ground truth, resulting in subpixel misalignments between the reconstructed SR images and the HR ground truth images. Such small misalignments are known to prefer (i.e. give lower error to) blurry images over sharp ones. A different benchmark suggested by <ref type="bibr" target="#b3">[4]</ref> is not yet available, and is only restricted to 3 SR blur kernels. As a result, we generated a new Blind-SR benchmark, referred to as DIV2KRK (DIV2K random kernel). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Our method together with ZSSR <ref type="bibr" target="#b29">[30]</ref> outperforms SotA SR results visually and numerically by a large margin of 1dB and 0.47dB for scales ×2 and ×4 respectively. When the kernel deviates from bicubic, as in DIV2KRK and real images, Type 1, SotA SR, tends to produce blurry results (often comparable to naive bicubic interpolation), and highlights undesired artifacts (e.g. JPEG compression), such examples are shown in Fig. <ref type="figure" target="#fig_0">1a</ref>. Type 2, i.e. SotA Blind-SR methods, produce significantly better quantitative results, than Type 1. Although visually (as shown in Fig. <ref type="figure">1b</ref>), tend to over-smooth patterns and over-sharpen edges in an artificial and graphical manner.</p><p>Our SR-kernel estimation, plugged into ZSSR <ref type="bibr" target="#b29">[30]</ref>, shows superior results over both types by a large margin of 1.1dB, 0.47dB for scales ×2, ×4 respectively, and visually recovering small details from the LR image as shown in Fig 1 . Although, our kernel together with <ref type="bibr" target="#b36">[37]</ref> did not perform as well for scale ×4, ranking lower than <ref type="bibr" target="#b33">[34]</ref> in PSNR (but higher in SSIM).</p><p>Regarding SR-kernel estimation, we analyze Type 3. When fixing a SR algorithm, our kernel estimation outperforms <ref type="bibr" target="#b23">[24]</ref> visually, as seen in Fig. <ref type="figure">1b</ref>, and quantitatively by 1dB, 0.73dB when plugged in <ref type="bibr" target="#b29">[30]</ref> and 4dB, 2.3dB when plugged in <ref type="bibr" target="#b36">[37]</ref> for scales ×2, ×4 respectively.</p><p>The superiority of Type 3 and Type 4 (as shown in Table <ref type="table" target="#tab_0">1</ref>) shows empirically the importance of performing SR with regard to the image specific SR-kernel. Moreover, using a SR algorithm with different SR-kernels leads to significantly different results, demonstrating the importance of the SR-kernel accuracy. This is also shown in Fig. <ref type="figure" target="#fig_4">5</ref> with a large difference in PSNR while small (but visible) difference in the estimated kernel. For more visual results+comparisons see project website.</p><p>Run-time: Network training is done during test time. There is no actual inference step since the trained G implicitly contains the resulting SR-kernel. Runtime is 61 or 102 seconds per image on a single Tesla V-100 or Tesla K-80 GPU, respectively. Runtime is independent both of image size and scale factor, since the GAN analyzes patches rather than the whole image. Since the same kernel applies to the entire image, analyzing a fixed number of crops suffices to estimate the kernel. Fixed sized image crops (64×64 to G, and accordingly 32×32 to D) are randomly selected with probability proportional to their gradient content. KernelGAN first estimates the ×2 SR-kernel, and then derives analytically kernels for other scales (e.g., ×4). For comparison, <ref type="bibr" target="#b23">[24]</ref> runs 45 minutes per image on our DIV2KRK and time consumption grows with image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present significant progress towards real-world SR (i.e., Blind-SR), by estimating an imagespecific SR-kernel based on the LR image alone. This is done via an image-specific internal-GAN, which trains solely on the LR input image, and learns its internal distribution of patches without requiring any prior examples. This gives rise to true SR "in the wild". We show both visually and quantitatively that when our kernel estimation is provided to existing off-the-shelf non-blind SR algorithms, it leads to SotA SR results, by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Comparison to SotA SR methods (SR×4):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: KernelGAN: The patch GAN trains on patches of a single input image (real). D tries to distinguish real patches from those generated by G (fake). G learns to downscale X2 the image while fooling D i.e. maintaining the same distribution of patches. Both networks are fully convolutional, which in the case of images implies that each pixel in the output is a result of a specific receptive field (i.e. patch) in the input.</figDesc><graphic coords="4,108.00,72.00,395.98,121.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fully Convolutional PatchDiscriminator: A 7×7 convolution filter followed by six 1×1 convolutions, including Spectral normalization<ref type="bibr" target="#b24">[25]</ref>, Batch normalization<ref type="bibr" target="#b14">[15]</ref>, ReLU and a Sigmoid activations. An input crop of 32×32 results with a 32×32 map∈ [0,1].</figDesc><graphic coords="5,108.00,91.18,237.61,65.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Deep linear G: A 6 layer convolutional network without nonlinear activations. The deep linear network on the left has equal expressiveness power as the single strided convolution downscaling model, on the right.</figDesc><graphic coords="6,108.00,72.00,257.40,119.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SR kernel estimation: We compare ground truth kernel, Michaeli and Irani<ref type="bibr" target="#b23">[24]</ref> and KernelGAN (ours). PSNR of ZSSR<ref type="bibr" target="#b29">[30]</ref>, when supplied with those kernels, is noted in the bottom right of each kernel, emphasizing the significance of kernel estimation accuracy. Our method outperforms<ref type="bibr" target="#b23">[24]</ref> in SR performance (as well as in visual similarity to ground truth SR kernel)</figDesc><graphic coords="7,110.49,72.00,197.99,119.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Depth is of the essence for the linear network: The deep linear generator outperforms the single layer version by 3.8dB and 1.6dB for ×2 and ×4 over DIV2KRK. Kernel examples emphasize superiority of the deep linear network. Using the validation set (100 images) from the widely used DIV2K [1] dataset, we blurred and subsampled each image with a different, randomly generated kernel. Kernels were 11x11 anisotropic gaussians with random lengths λ 1 , λ 2 ∼U(0.6, 5) independently distributed for each axis, rotated by a random angle θ∼U[-π, π]. To deviate from a regular gaussian, we further apply uniform multiplicative noise (up to 25% of each pixel value of the kernel) and normalize it to sum to one. See Figs. 5 and 6 for a few such examples. Data and reproduction code are available on project website.</figDesc><graphic coords="8,108.00,72.00,396.00,86.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>SotA SR performance (PSNR(dB) / SSIM) on 100 images of DIV2KRK (sec. 5.2) . Red indicates the best performance, blue indicates second best.</figDesc><table><row><cell>Type 1:</cell><cell>Bicubic Interpolation</cell><cell cols="2">28.731 / 0.8040 25.330 / 0.6795</cell></row><row><cell>SotA SR algorithms</cell><cell>Bicubic kernel + ZSSR [30]</cell><cell cols="2">29.102 / 0.8215 25.605 / 0.6911</cell></row><row><cell>(trained on bicubically</cell><cell>EDSRplus [21]</cell><cell cols="2">29.172 / 0.8216 25.638 / 0.6928</cell></row><row><cell>downscaled images)</cell><cell>RCANplus [38]</cell><cell cols="2">29.198 / 0.8223 25.659 / 0.6936</cell></row><row><cell>Type 2:</cell><cell>PDN [34] -1st in NTIRE track4</cell><cell>-</cell><cell>26.340 / 0.7190</cell></row><row><cell>Blind-SR</cell><cell>WDSR [36] -1st in NTIRE track2</cell><cell>-</cell><cell>21.546 / 0.6841</cell></row><row><cell>NTIRE'18 [31]</cell><cell>WDSR [36] -1st in NTIRE track3</cell><cell>-</cell><cell>21.539 / 0.7016</cell></row><row><cell>winners</cell><cell>WDSR [36] -2nd in NTIRE track4</cell><cell>-</cell><cell>25.636 / 0.7144</cell></row><row><cell>Type 3:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>7316</cell></row><row><cell>Type 4:</cell><cell>Ground-truth kernel + SRMD [37]</cell><cell cols="2">31.962 / 0.8955 27.375 / 0.7655</cell></row><row><cell>Upper bound</cell><cell>Ground-truth kernel + ZSSR [30]</cell><cell cols="2">32.436 / 0.8992 27.527 / 0.7446</cell></row></table><note><p><p><p><p>Michaeli &amp; Irani [24]  </p>+ SRMD</p><ref type="bibr" target="#b36">[37]</ref> </p>25.511 / 0.8083 23.335 / 0.6530 kernel estimation Michaeli &amp; Irani [24] + ZSSR [30] 29.368 / 0.8370 26.080 / 0.7138 + KernelGAN (Ours) + SRMD [37] 29.565 / 0.8564 25.711 / 0.7265 non Blind-SR algorithm KernelGAN (Ours) + ZSSR [30] 30.363 / 0.8669 26.810 / 0.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Project funded by the European Research Council (ERC) under the Horizon</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2020" xml:id="foot_1"><p>research &amp; innovation program (grant No. 788535)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind super-resolution using a learning-based approach</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Begin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">P</forename><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition, ICPR &apos;04</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Toward real-world single image super-resolution: A new benchmark and a new model</title>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kaiming He Xiaoou Tang Chao Dong, Chen Change Loy. Learning a deep convolutional network for image super-resolution</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Loss Surfaces of Multilayer Networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04231</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for superresolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution using gaussian process regression</title>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A soft map framework for blind super-resolution image reconstruction</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Psf estimation using sharp edge prediction</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2016-06">06 2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR Oral)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate blur models vs. image priors in single image super-resolution</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Apartsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Nadler Anat Levin Netalee Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ingan: Capturing and remapping the &quot;dna&quot; of a natural image</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Patch based blind image super resolution</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<meeting>the Tenth IEEE International Conference on Computer Vision (ICCV&apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
	<note>ICCV &apos;05</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep poly-dense network for image superresolution</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Xintao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Tak-Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTIRE challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fully progressive approach to single-image super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Wide activation for efficient and accurate image super-resolution</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1808.08718</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
