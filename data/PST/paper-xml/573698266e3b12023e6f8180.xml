<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable SIMD-Efficient Graph Processing on GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of California Riverside</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
							<email>gupta@cs.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of California Riverside</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laxmi</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
							<email>bhuyan@cs.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of California Riverside</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable SIMD-Efficient Graph Processing on GPUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B2271B1327B20FF9A823493A9DF7918</idno>
					<idno type="DOI">10.1109/PACT.2015.15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graphs</term>
					<term>Power Law Graphs</term>
					<term>GPU</term>
					<term>Irregular Computations</term>
					<term>Scalability</term>
					<term>Multi-GPU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The vast computing power of GPUs makes them an attractive platform for accelerating large scale data parallel computations such as popular graph processing applications. However, the inherent irregularity and large sizes of realworld power law graphs makes effective use of GPUs a major challenge. In this paper we develop techniques that greatly enhance the performance and scalability of vertexcentric graph processing on GPUs. First, we present Warp Segmentation, a novel method that greatly enhances GPU device utilization by dynamically assigning appropriate number of SIMD threads to process a vertex with irregular-sized neighbors while employing compact CSR representation to maximize the graph size that can be kept inside the GPU global memory. Prior works can either maximize graph sizes (VWC [11]  uses the CSR representation) or device utilization (e.g., CuSha [13] uses the CW representation; however, CW is roughly 2.5x the size of CSR). Second, we further scale graph processing to make use of multiple GPUs while proposing Vertex Refinement to address the challenge of judiciously using the limited bandwidth available for transferring data between GPUs via the PCIe bus. Vertex refinement employs parallel binary prefix sum to dynamically collect only the updated boundary vertices inside GPUs' outbox buffers for dramatically reducing inter-GPU data transfer volume. Whereas existing multi-GPU techniques (Medusa [31], TOTEM <ref type="bibr" target="#b6">[7]</ref>) perform high degree of wasteful vertex transfers. On a single GPU, our framework delivers average speedups of 1.29x to 2.80x over VWC. When scaled to multiple GPUs, our framework achieves up to 2.71x performance improvement compared to inter-GPU vertex communication schemes used by other multi-GPU techniques (i.e., Medusa, TOTEM).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Due to their ability to represent relationships between entities, graphs have become the building blocks of many high performance data analysis algorithms. A wide variety of graph algorithms can be expressed in an iterative formduring each iteration vertices update their state based upon states of neighbors connected by edges using a computation procedure until the graph state becomes stable. The inherent data parallelism in an iterative graph algorithm makes manycore processors with underlying SIMD hardware such as GPUs an attractive platform for accelerating the algorithms. However, efficient mapping of real-world power law graphs with irregularities to symmetric GPU architecture is a challenging task <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper we present techniques that maximize the scalability and performance of vertex-centric graph processing on multi-GPU systems by fully exploiting the available resources as follows:</p><p>SIMD hardware -The irregular nature of power law graphs makes it difficult to balance load across threads leading to underutilization of SIMD resources. We address the device underutilization problem of a GPU by developing Warp Segmentation that dynamically assigns appropriate number of SIMD threads to process a vertex with irregularsized neighbors. Our experiments show that the warp execution efficiency of warp segmentation exceeds 70% while for the well known VWC <ref type="bibr" target="#b10">[11]</ref> technique it is around 40%.</p><p>GPU global memory -For scaling performance to large graphs, they must be held in the global memory of the GPUs. To maximize the graph sizes that can be held in global memories, a compact graph representation must be used. Therefore Warp Segmentation makes use of the compact CSR representation. To tolerate the long latency of noncoalesced memory accesses that arise while accessing the neighbors of a vertex in CSR, warp segmentation keeps the GPU cores busy by scheduling other useful operations that compute the segment size and lane's intra-segment index.</p><p>Inter-GPU communication bandwidth -Since large graphs must be distributed across the global memories of multiple GPUs, processing at each GPU requires values of neighboring vertices that reside on other GPUs. Here we must make judicious use of the limited bandwidth available for transferring data between GPUs via the PCIe bus. We introduce an approach based upon parallel binary prefix sum that dynamically limits the inter-GPU transfers to only include updated vertices. Existing multi-GPU techniques perform high degree of wasteful vertex value transfers.</p><p>Our solution maximizes the graph sizes for which high performance can be achieved by fully utilizing GPU resources of SIMD hardware, memory, and bandwidth.</p><p>Let us briefly consider the related works and see how our approach overcomes their drawbacks. First, we consider the prominent single GPU techniques for vertex-centric graph processing, namely VWC <ref type="bibr" target="#b10">[11]</ref> and CuSha <ref type="bibr" target="#b12">[13]</ref>. Virtual-Warp Centric (VWC) <ref type="bibr" target="#b10">[11]</ref> is the state-of-the-art method that uses the compact CSR representation and is inevitably prone to load imbalance when processing real-world graphs due to high variation in degrees of vertices. When the size of the virtual warp is less than the number of neighbors for a vertex, the virtual warp needs to iterate over the neighbors forcing other virtual warps within the warp that are assigned to vertices with fewer neighbors to stay inactive. When the size of the virtual warp is greater than the the size of the neighbors for a vertex, a great portion of the virtual warp is disabled. Both cases lead to underutilization of SIMD resources and poor warp execution efficiency. In addition, discovering the best virtual warp size for every graph and every expressed graph algorithm requires multiple tries. CuSha <ref type="bibr" target="#b12">[13]</ref> addresses the drawbacks of VWC, namely warp execution inefficiencies and non-coalesced accesses, but at the cost of using G-Shards and CW graph representations which are 2x-2.5x larger than the CSR representation due to vertex replication. In contrast, Warp Segmentation uses the compact CSR representation while delivering high SIMD hardware utilization. In warp segmentation the neighbors of warp-assigned vertices are grouped into segments. Warp lanes then get assigned to these neighbors and recognize their position inside the segment and the segment size by first performing a quick binary search on the fast shared memory content and then comparing their edge index with corresponding neighbor indices. When the segment size and the position in the segment are known for the lanes, user-defined reduction can be efficiently performed between neighbors of a vertex without introducing any intra-warp load imbalance. When processing on a single device, our framework employs asynchronous parallelism paradigm that allows simultaneous program execution and data transfer in an iteration. It also permits visibility of the updated neighbor vertex content, enabling a faster convergence.</p><p>Next let us consider the related works on multi-GPU graph processing <ref type="bibr" target="#b30">[31]</ref>  <ref type="bibr" target="#b6">[7]</ref>. Given a partitioning of a graph across multiple GPUs, these techniques underestimate the importance of efficient inter-device communication and do not effectively utilize the PCIe bandwidth. This is a significant problem because the PCIe bus, as the path to communicate data from one GPU to other GPUs, is tens of times slower than GPU global memory. Previous multi-GPU techniques either copy the whole vertex set belonging to one GPU to other GPUs at every iteration <ref type="bibr" target="#b30">[31]</ref>, or they identify boundary vertices in a pre-processing stage and make GPUs exchange these subsets of vertices in every iteration <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>. In both approaches, a great number of vertices that are exchanged between devices is redundant. In contrast, we propose Vertex Refinement, a new strategy that enables our framework to efficiently scale to multiple GPUs. Vertex Refinement refines and transfers only those vertices that are updated in the previous round and are needed by other devices. It consists of two stages: online and offline. In the offline stage, boundary vertices are recognized and marked during preprocessing. In the online stage, we exploit parallel binary prefix sum to refine updated vertices from not-updated ones on-the-fly. A vertex is transferred to another device only if it is marked and refined by the online stage. Thus, Vertex Refinement eliminates the communication overhead and provides higher multi-GPU performance.</p><p>The key contributions of this work are:</p><p>• We introduce Warp Segmentation (WS), a novel technique for compact graph representations that overcomes SIMD underutilization during graph processing. On average, WS outperforms VWC by 1.29x-2.80x. • We introduce Vertex Refinement that enables effective scaling of the graph processing procedure to multiple GPUs. It efficiently filters updated vertices of a GPU on-the-fly via parallel binary prefix sum and provides exclusive speedup of up to 2.71x over other multi-GPU vertex communication schemes. • We implemented a framework that embeds above items and enables users to easily express desired iterative graph algorithm and execute it on one or multiple CUDA-enabled GPUs. The remainder of the paper is organized as follows. In Section II, we present Warp Segmentation and in Section III we introduce the framework interface. Then in Section IV, we describe efficient scaling of our framework to multiple GPUs via Vertex Refinement. In Section V, we present the experimental evaluation. Sections VI and VII give related work and conclusion respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WARP SEGMENTATION FOR SIMD-EFFICIENCY</head><p>Here we present Warp Segmentation (WS) that eliminates intra-warp load imbalance and enhances execution efficiency for processing a graph in CSR form. CSR is a compact form suitable for representing large and sparse graphs in a minimum space. Due to its space-efficiency, CSR is a good choice to hold large graphs inside the limited GPU memory.</p><p>As Figure <ref type="figure" target="#fig_0">1</ref> shows, CSR consists of 4 arrays:</p><p>• VertexValues holds the content of the i th vertex in its i th element.  EdgeValues holds the edge values corresponding to the neighbors inside NbrVertexIndices. To motivate the need for WS, we first describe the drawbacks of the Virtual-Warp Centric (VWC) <ref type="bibr" target="#b10">[11]</ref> method that also uses the CSR representation.</p><p>Drawbacks of VWC: VWC divides the SIMD group (warp, in CUDA terms) with the physical length of 32 into smaller virtual warps with fixed lengths (2, 4, 8, 16, or 32). Virtual warp size is kept the same throughout the graph processing. Each virtual warp is assigned to process one vertex and its incoming edges. As an enhancement of the original work <ref type="bibr" target="#b10">[11]</ref>, Khorasani et al. proposed a generalized form of VWC in <ref type="bibr" target="#b12">[13]</ref> in which threads of the virtual warp are involved in reduction over the computed values. However, real-world graphs often exhibit power-law degree distribution, i.e. the number of neighbors a vertex owns vary greatly from one vertex to another. Thus, due to fixed number of virtual lanes involved in a reduction, VWC unavoidably suffers from underutilization:</p><formula xml:id="formula_0">V 0 V 1 V 2 V 3 V 4 8 V 0 V 1 V 2 V 3 V 4 0 1 3 3 5 0 1 0 2 0 2 E 5 E 0 E 1 E 2 E 3 E 4 3 1 E 7 E 6 VertexValues NbrIndices NbrVertexIndices EdgeValues E 0 E 1 E 5 E 2 E 3 E 4 E 7 E 6</formula><p>-If the virtual warp is smaller than the vertex's number of neighbors, it will have to iterate over the vertex's connected edges hence dragging with it other virtual warps that have already finished their jobs (see the example in Figure <ref type="figure">2(a)</ref>); and -If the virtual warp has a size that is larger than the number of neighbors for a vertex, a portion of virtual warp's lanes stays idle during the reduction leading to underutilization (see the example in Figure <ref type="figure">2(b)</ref>).</p><p>This motivates the need for a technique that, independent of inner graph structure, takes minimum number of reduction steps in a SIMD environment, i.e. Warp Segmentation. Note that VWC suffers from the SIMD load imbalance in the same way PRAM-style thread assignment <ref type="bibr" target="#b8">[9]</ref> does. In both PRAM-style and VWC, assigning fixed number of SIMD threads to process one vertex and its edges leads to threadidling due to highly irregular vertex degree distribution. This fixed number in the former is exactly one while in the latter it can be a power of 2.</p><p>Boosting SIMD Utilization via Warp Segmentation: To remedy the drawbacks of fixed-sized virtual warps, we propose Warp Segmentation (WS) technique. In WS, a warp is assigned to a group of 32 consecutive vertices and their connected edges. When warp lanes process edges iteratively, those that process edges belonging to one vertex-i.e. having the same destination index-form a segment. By knowing the segment size and the index inside the segment, lanes can participate in the appropriate reduction of segment, minimizing the total number of reduction steps.</p><p>Figure <ref type="figure">2</ref>(c) shows the reduction in WS in an example scenario. In this example, first six lanes belong to one segment and two last lanes belong to another. The minimum number of reduction steps in this case is log 2 6 = 3 which is also the case in WS. As Figure <ref type="figure">2</ref> shows, on-the-fly efficient reduction procedure in WS leads to better utilization of SIMD resources compared to VWC. In addition, WS does not need any pre-processing or trial-and-error for the best configuration determination.  The key feature of WS is its fast determination of the segment a lane belongs to and the index of the lane within the segment. The step-by-step approach shown in Figure <ref type="figure" target="#fig_2">3</ref> illustrates this. Warp lanes perform a binary search over NbrIndices elements for their assigned edge index. Since NbrIndices elements are already fetched to the fast shared memory of the GPU, the binary search is performed quickly. After log 2 (warpSize) steps, the starting position of the resulting search boundary shows the vertex index to which the edge belongs. Knowing the vertex index, the lane's index inside the segment and the segment size is retrieved using NbrIndices array. The distance of the holding edge index from the vertex's corresponding NbrIndices element reveals the position of the vertex in the segment. The difference between the holding edge index and the next vertex's corresponding NbrIndices element, minus one, yields the distance of the lane from the end of the segment. Addition of these two distances plus one represents the segment size.</p><formula xml:id="formula_1">N 0 N 1 N 2 N 3 N 4 N 5 N 6 N 7 R R R F R R F R F R R F C 0 C 1 C 6 C 7 C 2 C 3 R Lane 0 Lane 1 Lane 2 Lane 3 R F R R F R R F C 4 C 5 R R F Time (a) VWC with Virtual Warp size 2. N 0 N 1 N 2 N 3 N 4 N 5 N 6 N 7 R R R R F R R F R R F C 0 C 1 C 2 C 3 C 6 C 7 R R R R R F R F Lane 0</formula><formula xml:id="formula_2">C 0 C 1 C 2 C 3 C 4 C 5 C 6 C 7 R R R R R F R F R R N 0 N 1 N 2 N 3 N 4 N 5 N 6 N 7 R R R R R R R F R F Lane 0</formula><p>WS is based upon the vertex-centric paradigm where in every iteration the shared memory serves as a scratchpad for vertices. The shared memory regions corresponding  to vertices are: initialized by the vertex content within the global memory, modified depending upon the edges connected to the vertex using appropriate reductions, and at the end of the iteration, the updated values are pushed back to the global memory. Two alternatives for the intrawarp reduction in WS are possible. The first one is to use atomics to survive the concurrent modifications of the vertices as in <ref type="bibr" target="#b12">[13]</ref>. However, this alternative imposes heavy use of atomics on shared memory locations on top of CSR's inherent non-coalesced neighbor accesses. The second alternative is processing a group of vertices by one thread block instead of one warp. However, this approach necessitates multiple synchronization primitive across the thread block that degrade the performance. WS assigns a set of vertices to GPU's architectural SIMD grouping (warp) and performs efficient reductions hence it avoids shared memory atomic operations alongside any explicit synchronizations throughout the kernel.</p><formula xml:id="formula_3">8 V 0 V 1 V 2 V 3 V 4 0 1 3 3 5 N 5 N 0 N 1 N 2 N 3 N</formula><p>The reduction in WS can be viewed as a form of intrawarp segmented reduction but without a head flags array, consisting of two main steps. First, warp lanes identify the vertex index via a fast binary search. Second, they discover the intra-segment index and the segment size. Also, note that these two sets of operations are independent from the neighbor vertex value hence can be used to cover the latency of the non-coalesced access. The thread exploits instruction level parallelism by simultaneously executing non-dependent instructions. Thus, GPU cores are kept busy performing operations while neighbor's vertex value is on its way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH PROCESSING FRAMEWORK FOR WS</head><p>Next we describe the framework that uses the graph processing procedure based on WS. Then, we present the interface functions that allow easy expression of graph algorithms by non-expert users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Core Processing Procedure</head><p>Figure <ref type="figure">4</ref> shows the graph processing procedure. The convergence of iterative graph processing is controlled via a variable passed between the host and the device. If no thread updates this variable, it means the algorithm has converged and no more iterations are needed. In the outermost for loop, according to the WS paradigm, each warp is assigned to process a contiguous set of vertices with the size equivalent to the warp size (32 for current CUDA devices). A warp task during one iteration is to process its assigned vertices. This task consists of three major steps.</p><p>First step: In this step (lines 11 to 14 in Figure <ref type="figure">4</ref>) threads of a warp fetch 32 elements of VertexValues and initialize the designated shared memory region for ver-tex values using user-provided initialization function. The threads also put 32+1 corresponding elements of NbrIndices into another shared memory buffer. Using the NbrIndices starting and ending element, warp lanes can recognize the region within EdgeValues and NbrVertexIndices arrays that belongs to the assigned group of vertices.</p><p>Second step: This step involves iteration of warp lanes over the elements of the EdgeValues and NbrVertexIndices arrays region (lines 15 to 25 in Figure <ref type="figure">4</ref>). Warp lanes perform a user-provided compute function with the fetched neighbor vertex value and the connected edge value and save the outcome in a local shared memory buffer (line 21). Besides, every warp lane must discover which of 32 vertices that are assigned to the warp owns the processed edge and neighbor. This involves a log 32 = 5 stepped binary search on fetched edgeIndices in the shared memory (line 18). Using the resulting vertex index, warp lanes can be grouped into segments, each segment corresponding to one vertex. Each lane identifies its position within the segment and the size of the segment it belongs to (lines 19 and 20). Therefore warp lanes can execute user-provided reduction function in parallel (line 22). Finally, the first lane in each segment performs the reduction function over the outcome and associated element in the shared memory region for vertex values (lines 23 and 24). Warp lanes perform these steps iteratively until all the edges for the set of vertices are processed.</p><p>Third step: In this step, the warp lanes compare the content of designated shared memory region for vertex values with the corresponding VertexValues elements using the user-provided function (line 26). If the function returns true, the vertex content inside the global memory will be updated.</p><p>Once all the vertices are processed, the framework executes another iteration of the algorithm on all the graph vertices if any vertex in the current iteration is updated. Graph processing with WS method dynamically determines the proper size for reduction based on the segment size and it is guaranteed that the number of steps for parallel reduction will never exceed five (log warpSize).</p><p>Note that the memory transactions in all the steps are coalesced except for accessing the neighbor vertex value (line 17), which is inherent in the compact graph representation. However by moving "binary search" and "segment realization" functions (lines 18 to 20) before the neighbor computation function, we exploit instruction level parallelism to hide the latency associated with the non-coalesced memory access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Framework Interface</head><p>In addition to trivial input/output handling functions, type definition for the vertex, and the structure definition for the edge, our framework accepts the following user specified functions:  In SSWP, during multiple rounds, the content of a vertex is updated by the maximum bandwidth it observes picked from the minimums between incoming edges and corresponding neighbors. As Figure <ref type="figure" target="#fig_3">5</ref> shows, this algorithm can be easily expressed in our framework via the above processing functions. First, the vertex content inside the shared memory is initialized by the most updated content of the vertex. Second, for each neighbor a local value is computed, which in this case is the minimum between every connecting edge bandwidth and its corresponding source vertex visible bandwidth. Third, these values are reduced two-by-two using the reduction function and the result is saved to the first argument content. For SSWP, reduction function selects the maximum of visible values through neighbors. Also, at the end of the third step of the processing procedure, the reduction function is executed for the initialized vertex and the final reduction result. Finally, in the fourth step, the framework verifies if the vertex should be updated. If the IsUpdated function returns true-which in case of SSWP is observing a greater bandwidth to the source-the content of the vertex inside global memory is replaced with the reduced vertex content in the current iteration. If any vertex is updated, the host executes another iteration. IV. SCALING VIA VERTEX REFINEMENT To handle larger graphs we must scale our method to use multiple GPUs that provide more memory and processing resources. Although graph partitioning strategies between GPUs have been explored, inter-GPU data transfer efficiency has not received adequate attention. Given a partitioning, for scaling of graph processing to be effective, we must make good use of limited PCIe bandwidth. We show the inefficiency of existing techniques and present Vertex Refinement that avoids redundant data exchange between GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inefficiency of Existing Inter-GPU Communication</head><p>Existing multi-GPU generic graph processing schemes divide the graph in two or more partitions and assign each partition to one GPU. Graph vertices completely fall into partitions while there can be edges that pass the partition boundaries. Due to these boundary edges, a GPU needs to be informed of the vertex updates happening in other GPUs. To keep the content of its assigned vertices held inside other GPUs updated, the GPU needs to transfer vertices belonging to its own partition over the PCIe bus. PCIe data transfer rate happens to be tens of times lower than GPU global memory's thus extra care must be taken to transfer only necessary data so as not to waste PCIe precious bandwidth.</p><p>Nonetheless, since implementing a mechanism to efficiently manage queues in GPU's massively multithreaded environment is challenging, previous works choose simple but inefficient approaches. Medusa <ref type="bibr" target="#b30">[31]</ref> copies all the vertices belonging to one device to other devices at every iteration. We refer to this solution as the ALL method. TOTEM <ref type="bibr" target="#b6">[7]</ref> [8] pre-selects the boundary vertices in a preprocessing stage but similar to Medusa copies the boundary vertices after every iteration. We refer to this solution as Maximal Subset (MS) method. Both of these methods suffer from wastage of PCIe bandwidth because usually only a small portion of the vertices are updated during each iteration. Table <ref type="table">I</ref> shows the ratio of useful transferred vertices-vertices that are updated in the last iteration-to all the vertices that are transferred in such schemes. Such low percentages motivate the need for a new solution to utilize limited PCIe bandwidth economically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vertex Refinement: Efficient inter-GPU Communication</head><p>To eliminate the overhead of transferring unnecessary vertices between devices, our framework performs Vertex Refinement in two steps: offline and online. We first describe the required data structures and then present the two-staged refinement procedure.</p><p>Data structures for Vertex Refinement: To process a graph with multiple GPUs, our framework divides the vertices and their associated edges into partitions and assigns each partition to one GPU, so that each GPU processes a continuous set of vertices. Since the processing time is mostly affected by the memory accesses associated with gathering the values of neighbor vertices, determining the boundaries of vertex partitions depends upon the total number of edges that vertices of each subset hold. In our scheme, vertices of each partition will have roughly the same number of edges in order to provide a balanced load between GPUs. Each GPU will hold relevant subset of NbrIndices, NbrVeretxIndices, and EdgeValues but will contain a full version of VertexValues array. This organization allows each device to process vertices belonging to its own partition as long as vertices inside VertexValues that belong to other GPUs are updated during an iteration.</p><p>In addition to CSR representation buffers, each GPU will hold one Outbox buffer that is filled with updated vertex indices and vertex values of the GPU-specific division. As shown in Figure <ref type="figure">6</ref>, we keep the inboxes inside host pinned buffers. In other words, the set of host buffers is similar to a hub that are filled by devices. At the start of an iteration, a device accesses inboxes corresponding to other devices and updates its own VertexValues array. Also at the end of an iteration, the device transfers its own outbox content to device's corresponding inbox. Moreover, we apply double buffering technique by alternating read buffers and write buffers. In an odd (even) iteration, devices read from the odd (even) inbox buffers and copy their outbox to their designated even (odd) inbox buffer. In summary:</p><p>-Inbox and outbox buffers are vital for a fast data transfer between GPUs. Direct peer-device memory access as an alternative will introduce significant performance penalty due to non-coalesced transactions over PCIe bus <ref type="bibr" target="#b27">[28]</ref>. In contrast, inbox and outbox buffers allow the collection of necessary data together and hence accelerate the inter-device communication.</p><p>-Using Host memory as the hub not only reduces memory constraint pressure for GPUs, but is also beneficial when more than two GPUs are processing the graph. A device copies its own outbox to a host buffer only once. In contrast, if there is no intermediate host buffer, the device has to copy the outbox to each of the other GPUs' inboxes causing unnecessary traffic over connected PCIe lanes since the same data are passed over more than once. Our experiments show that using host as the hub is always beneficial in reducing the communication traffic and overall multi-GPU processing time in comparison to using inbox and outbox buffers residing inside the GPUs.</p><p>-Double buffering eliminates the need for additional costly inter-device synchronization barriers between data transfers and kernel executions. For instance, when device A grabs inbox buffer content of the device B during an iteration, since device B is going to fill another inbox buffer in the current iteration, needless of synchronizing with device B we will be sure that device A does not receive corrupted data.</p><p>If there are two GPUs processing the graph, during the runtime our framework queries the available global memory on the GPUs. If there is enough memory to hold the pertained part of the graph plus both the odd and even inboxes belonging to the other device, the framework puts the inboxes inside the GPUs global memory. Otherwise, it chooses host pinned buffers for this purpose.</p><p>Offline Vertex Refinement: In this pre-processing stage, the framework scans NbrVertexIndices elements and identifies the boundary vertices: those that are being accessed by edges of one division while belonging to another division.</p><formula xml:id="formula_4">O[A+0]=V 0 O[A+1]=V 3 O[A+2]=V 7 Is (Updated &amp; Marked) Shuffle A Binary Prefix Sum Operation V 0 3 A 0 V 1 3 A 1 V 2 3 A 1 V 3 3 A 1 V 4 3 A 2 V 5 3 A 2 V 6 3 A 2 V 7 3 A 2 Y N Y Y N N N N Binary Reduction</formula><p>Reserve Outbox Region A = atomicAdd( deviceOutboxMovingIndex, 3 ); Inspired by TOTEM <ref type="bibr" target="#b6">[7]</ref>, for such a vertex we set the most significant bit of its corresponding element inside NbrIndices buffer. During the online refinement, if a vertex is not a boundary vertex, it will be filtered out. Note that this bit will be ignored during other computations that involve NbrIndices buffer. Also during this stage, the framework can determine the maximum size to allocate for inbox and outbox buffers.</p><note type="other">Fill Outbox Region</note><p>Online Vertex Refinement via parallel binary prefix sum: As opposed to Offline Vertex Refinement, Online Vertex Refinement happens on-the-fly inside the GPU kernel. At the last level of graph processing, lanes of a warp examine warp-assigned vertices for updates, each producing a binary predicate associated with one vertex. If this predicate is true and at the same time the vertex is marked during the offline stage, the vertex is required to be transferred to other devices.</p><p>By means of any() intrinsic, we first verify if any of the warp lanes has an eligible vertex to transfer. If yes, warp lanes quickly count the total number of updated vertices inside the warp via intra-warp binary reduction and realize the number of updated vertices in lower lanes via intra-warp binary prefix sum. For a fast computation of binary reduction and inclusive binary prefix sum, our framework utilizes Harris et al. approach <ref type="bibr" target="#b9">[10]</ref> in which popc() and ballot() CUDA intrinsic functions are exploited. Having total number of updated vertices, one lane in the warp atomically adds it to a moving index inside the global memory, which its returned value specifies the starting position in the designated outbox buffer to write the warp's updated vertex indices and values. In other words, a lane reserves a region inside the Outbox for eligible warp lanes. The starting position of the region is shuffled to other lanes in the warp via shfl() intrinsic, and lanes with updated vertex fill up the buffer using this position plus their intra-warp prefix sum. Figure <ref type="figure" target="#fig_4">7</ref> presents an example showing online vertex refinement procedure.</p><p>An alternative to above approach is extending the binary reduction and the binary scan to the CTA; however, we did not find this alternative faster since it required two synchronizations across the thread-block. Whereas in our approach the atomic addition is performed by only one lane in the warp which avoids contention for the atomic variable.</p><p>When processing in an iteration is done, the moving index determines how much of the device outbox buffer has been filled. We significantly reduce the communication time by transferring the content of this buffer to the corresponding inbox buffer only with the length specified by the moving index. At the beginning of the next iteration, in order to have newly updated vertex values from other devices, each device distributes the content of other devices' inboxes only with the length specified by their associated moving indexes over its own VertexValues array.</p><p>In summary, Offline Vertex Refinement identifies boundary vertices and Online Vertex Refinement recognizes the vertices updated in the previous iteration. The combination of two yields the set of updated boundary vertices and maximizes the inter-device communication efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head><p>The system we performed experiments on has 3 NVIDIA GeForce GTX780 GPUs each having 12 Kepler Streaming Multiprocessors and approximately 3 GBytes of global memory. The first GPU is connected to the system with PCIe 3.0 16x while the rest are operating at 4x speed. The single-GPU experiments are reported from the GPU with the highest PCIe bandwidth. We compiled and ran all programs for Compute Capability 3.5 on Ubuntu 14.04 64-bit with CUDA 6.5 and applied the highest optimization level flag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Warp Segmentation Performance Analysis</head><p>In this section, we analyze the performance of Warp Segmentation on a single GPU. We use the graphs shown in Table <ref type="table" target="#tab_8">II</ref> for experiments in this section. In the table, graphs with prefix RM refer to Rmat <ref type="bibr" target="#b2">[3]</ref> graphs created using PaRMAT <ref type="bibr" target="#b13">[14]</ref> with parameters a = 0.45, b = 0.25, and c = 0.15. Rmat graphs are known to imitate the characteristics of real-world graphs such as power-law degree distribution. The graph with prefix ER is a uniformly random (Erdős-Rényi) graph. Other graphs are extracted from real-world origins and are publicly available at SNAP dataset <ref type="bibr" target="#b15">[16]</ref>. Graphs in Table II cover a wide range of sizes with different densities and characteristics.</p><p>Warp Segmentation vs VWC -Performance Comparison: First we compare the performance of WS method against VWC with graphs shown in Table <ref type="table" target="#tab_8">II</ref> for the benchmarks in Table <ref type="table">I</ref>. Table <ref type="table" target="#tab_10">III</ref> presents the raw processing time for the completion of all the benchmarks over all the graphs for both methods. We experimented on VWC with all the possible virtual warp sizes (2, 4, 8, 16, and 32) hence its processing times are specified in ranges. Table <ref type="table" target="#tab_9">IV</ref> shows the average speedup of WS compared to VWC over input graphs and benchmarks. In comparison with VWC, WS shows better performance across all the graphs and all the benchmarks. WS speedup over VWC averaged across all the input graphs and benchmarks ranges from 1.29x to 2.80x.  To further examine the effectiveness of WS against VWC, as the state-of-the-art CSR based generic graph processing method, we profiled both our framework and VWC over different graphs for warp execution efficiency. Figure <ref type="figure" target="#fig_6">8</ref> shows the average warp execution efficiency (predicated and non-predicated combined) over all the iterations of graph processing with SSSP benchmark. It is evident from the figure that for different graphs, best warp execution efficiency for VWC happens in different virtual warp sizes. For example with RoadNetCA, a 2D mesh of intersections and roads, virtual warp size 2 yields the best results due to special structure of the graph; while it leads to the poorest performance for other graphs. On the other hand, WS exhibits a steady warp execution efficiency (71.8% on average) regardless of the graph. WS warp execution efficiency is 1.75x-3.27x better than VWC when averaged across all graphs. This confirms the SIMD efficiency of WS over fixed-width intra-SIMD thread assignment in VWC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Averages Across Input Graphs Averages Across Benchmarks</head><formula xml:id="formula_5">BFS 1.27x-2.60x RM33V335E 1.23x-1.56x CC 1.33x-2.90x ComOrkut 1.15x-1.99x CS 1.43x-3.34x ER25V201E 1.09x-1.69x HS 1.27x-2.66x RM25V201E 1.15x-1.57x NN 1.21x-2.70x RM16V201E 1.16x-1.41x PR 1.22x-2.68x RM16V134E 1.22x-1.69x SSSP 1.31x-2.76x LiveJournal 1.29x-1.99x SSWP 1.28x-2.80x SocPokec 1.27x-1.77x HiggsTwitter 1.34x-4.78x RoadNetCA 1.24x-9.90x WebGoogle 1.79x-2.69x Amazon0312 1.53x-2.68x</formula><p>Warp Segmentation Performance against CW: We present the speedup of WS over CW having large graphs in   therefore, as a straightforward workaround, we kept vertex value and small auxiliary buffers inside the GPU global memory and put shards at mapped pinned buffers inside the host. For large graphs, CW processing time is significantly higher than our method's due to involvement of PCIe bus, limiting the scalability of CW representation. Also for the small graphs, although CW provides fully regular access patterns, it incurs larger memory footprints. In addition, our framework covers the latency of CSR-inherent irregular accesses, therefore we observe near par performance, as shown by averages in Table <ref type="table" target="#tab_11">VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vertex Refinement Performance Analysis</head><p>Next we analyze the performance of our framework when it is scaled to multiple GPUs. First we present the speedup provided by Vertex Refinement compared to existing methods over very large input graphs, and analyze its cost and  Vertex Refinement Performance Comparison: To better realize the importance of data communication strategy and the efficiency of Vertex Refinement, we have implemented two other inter-device communication methods (mentioned in Section IV) in our framework. The first method is the straightforward solution that copies all the vertices belonging to one device to other devices at every iteration. We refer to this solution as ALL. The second one is the maximal subset method where vertices that belong to one device and can be accessed by another device are identified in a pre-processing stage. During the iterative execution, only these vertices are communicated to other devices. We refer to this method as MS. We compare these methods with Vertex Refinement -VR. Note that to better realize the benefits of VR, for all the inter-device communication methods, we keep intra-device processing style intact. In other words, underlying graph processing method is WS for all experiments in this section.</p><p>Table <ref type="table" target="#tab_13">VIII</ref> shows the speedup of our framework when VR is employed over ALL and MS, for all the graphs and benchmarks. In all cases, our solution performs better than other methods. When averaged over all the graphs and benchmarks, our approach provides 1.81x and 1.30x speedups over ALL and 1.77x and 1.28x speedups over MS for three-GPU and two-GPU configurations respectively.</p><p>In Figure <ref type="figure">9</ref>, we analyzed the cost of VR queue management versus the savings it provides (in terms of eliminating redundant inter-device vertex communication) by breaking down the processing time into computation time and communication time. To create this plot, we measured the time for each and every kernel execution, memory copy, and outbox-loading/inbox-unloading. Aggregated computation duration refers to the total duration of GPU kernel executions, whereas aggregated communication time refers to the total duration of copies and/or box handling kernels.</p><p>First, it is evident from both plots in Figure <ref type="figure">9</ref> that MS is not an effective solution for reducing communication overhead. In fact, in one case (PR in Figure <ref type="figure">9</ref>(a)) the overhead of outbox handling overcomes the benefits of preselection. Second, unlike MS, VR significantly reduces the total communication duration by refining vertices on-the-fly while adding negligible overhead to the computation duration. Note that even though in VR the vertex information is communicated accompanying its index, the communication duration is still much less compared to ALL and MS for all the cases. Third, by comparing Figure <ref type="figure">9</ref>(a) and Figure <ref type="figure">9</ref>(b), we notice that more time is spent on the communication by employing more GPUs. By adding another GPU, each device needs to send and receive more vertex information to and from more devices, signifying VR's supremacy even further. Especially in the 3-GPU configuration, using host as the hub supports reducing inter-device traffic by passing the data over PCIe only once.</p><p>Scaling to multiple GPUs for smaller graphs: To observe the effect of scaling graph processing procedure from one or two GPUs to three GPUs, we experimented our framework with smaller graphs and more GPUs and reported the speedups in Table <ref type="table" target="#tab_15">IX</ref>. As this table shows, the performance does not scale linearly as we add more GPUs. This is due to comparatively slow PCIe paths and also imperfect load division between different GPUs. Also, the speedup of adding more GPUs greatly depends on the graph algorithm. For example, in PageRank (PR) the chances that a vertex is updated during an iteration is relatively high (especially in earlier iterations) thus more vertices have to be transferred from one GPU to another. As a result, we observe lower speedups in PageRank compared to other algorithms when adding more GPUs.</p><p>We also present the effect of the graph characteristics (graph size and density) on the scalability of our framework in Figure <ref type="figure" target="#fig_7">10</ref>. By comparing large graphs and small graphs  in Figure <ref type="figure" target="#fig_7">10</ref>, we observe that as the graphs get larger with greater number of edges, adding more GPUs produces greater reductions in graph processing time. In addition, higher density in larger graphs signifies the reduction in the processing time when scaling to multiple GPUs by downsizing inter-device vertex transfer volumes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Harish and Narayanan pioneered GPU graph processing in <ref type="bibr" target="#b8">[9]</ref> by privatizing the processing of a vertex to one GPU thread. This solution was is prone to load imbalance (i.e., warp execution inefficiency) and suffers heavily from noncoalesced accesses to edge and vertex indices. Hong et al. improved upon this solution with Virtual-Warp Centric manner of graph processing <ref type="bibr" target="#b10">[11]</ref> [12] nonetheless, as explained in the context, this method does not efficiently utilize available SIMD resources. CuSha <ref type="bibr" target="#b12">[13]</ref> is a generic CUDA graph processing framework that uses G-Shards and CW representation to avoid warp execution inefficiencies and non-coalesced memory accesses. Although effective, such representations consume 2 to 2.5 times more space than CSR which can hinder the framework from processing very large graphs. In addition, CuSha relies on atomic operation in the computation function which can be limiting general applicability of the framework. <ref type="bibr" target="#b28">[29]</ref> proposes an static loadbalancing scheme that puts vertices into multiple bins based on the number of neighbors and assigns appropriate number of threads to each bin accordingly. <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b25">[26]</ref> aim to provide regular GPU-friendly data patterns in order to balance the load however their usage is confined to predictable data structures. Moreover, <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b23">[24]</ref> propose solutions for algorithms that change the structure of the graph. Our framework, in contrast, focuses on the graphs in which the connectivity of vertices via edges do not change at any time.</p><p>Merril et al. recognized the potential of parallel scan on GPUs for graph traversal in order to efficiently construct vertex frontiers and edge frontiers <ref type="bibr" target="#b22">[23]</ref> however their solution is limited to BFS. Similarly, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b20">[21]</ref> suggest workefficient solutions respectively for SSSP and betweenness centrality. Our focus in this work is rather on a scalable and generic framework that allows the expression of numerous iterative vertex-centric algorithms and proposing generallyapplicable techniques. <ref type="bibr" target="#b19">[20]</ref> is also for BFS graph traversal that due to its excessive usage of atomic operations to control the queue does not exhibit acceptable performance. We avoid the contention over the atomic variable by mainly relying on binary prefix sum for vertex refinement and involving only one warp lane in the outbox region reservation process.</p><p>In order to involve more GPUs to process the graph, Medusa <ref type="bibr" target="#b30">[31]</ref> employs METIS <ref type="bibr" target="#b14">[15]</ref> an off-the-shelf graph partitioner in order to distribute vertices between devices and reduce the number of edges that pass the boundaries. TOTEM <ref type="bibr" target="#b6">[7]</ref> is a CPU-GPU hybrid framework that preprocesses the graph and uses the highest order bits of neighbor vertex indices to flag the boundary vertices so the read access is redirected to the device inbox. However, the whole content of the outbox in TOTEM or all the partition vertices in Medusa have to be copied over to the remote device, incurring massive unnecessary traffic over PCIe lanes. <ref type="bibr" target="#b11">[12]</ref> is another CPU-GPU hybrid solution for BFS that after a few iteration, transfers the whole graph from the CPU side to the GPU side. Our work is the first generic multi-GPU framework that reduces inter-device communication by filtering out not-updated and non-boundary vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We introduced a CUDA-based framework for efficient scaling of iterative graph algorithms to larger graphs and multiple GPUs. The graphs are stored in the space-saving CSR form that allows processing large graphs. To overcome the SIMD execution inefficiency we employ Warp Segmentation leading to 1.29x-2.80x speedup over state-of-the-art VWC method. To scale the graph processing over multiple GPUs in our framework, we introduced Vertex Refinement that collects and transfers only those vertices that are boundary and recently updated. Vertex Refinement maximizes inter-device bandwidth utilization leading to 2.71x speedup over existing multi-GPU communication schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A graph with 5 vertices and 8 edges and its CSR representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>VWC with Virtual Warp size 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Discovering segment size and the index within segment by warp lanes for the graph in Figure 1. Warp size is assumed 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>Figure 5 illustrates use of the framework by showing the functions for Single Source Widest Path (SSWP) algorithm as an example. SSWP requires a variable for expressing the edge bandwidth and another variable for specifying maximum visible bandwidth by the vertex from the source.In SSWP, during multiple rounds, the content of a vertex is updated by the maximum bandwidth it observes picked from the minimums between incoming edges and corresponding neighbors. As Figure5shows, this algorithm can be easily expressed in our framework via the above processing functions. First, the vertex content inside the shared memory is initialized by the most updated content of the vertex. Second, for each neighbor a local value is computed, which in this case is the minimum between every connecting edge bandwidth and its corresponding source vertex visible bandwidth. Third, these values are reduced two-by-two using the reduction function and the result is saved to the first argument content. For SSWP, reduction function selects the maximum of visible values through neighbors. Also, at the end of the third step of the processing procedure, the reduction function is executed for the initialized vertex and the final reduction result. Finally, in the fourth step, the framework verifies if the vertex should be updated. If the IsUpdated function returns true-which in case of SSWP is observing a greater bandwidth to the source-the content of the vertex inside global memory is replaced with the reduced vertex content in the current iteration. If any vertex is updated, the host executes another iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. An example of online vertex refinement stages via intra-warp inclusive binary prefix sum -warp size in the figure is 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Profiled average warp execution efficiency of Warp Segmentation compared to VWC's. SSSP is the benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The scalability of our framework over graphs with different number of edges and densities for SSSP benchmark. All the graphs are Rmat created with parameters a = 0.45, b = 0.25, and c = 0.15. y axis is the processing time (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 4.Framework's graph processing procedure pseudo-algorithm. Assumed warp size is 32. Shared memory pointers in the program code are declared with volatile qualifier.</figDesc><table><row><cell cols="2">0. converged = false;</cell></row><row><cell cols="2">1. while( !converged ) {</cell></row><row><cell>2.</cell><cell>converged = true;</cell></row><row><cell>3.</cell><cell>parallel-for warp w {</cell></row><row><cell>5.</cell><cell>__shared__ Vertex V[ blockDim ];</cell></row><row><cell>6.</cell><cell>__shared__ Vertex tLocal_V[ blockDim ];</cell></row><row><cell>7.</cell><cell>__shared__ uint NIdx[ blockDim ];</cell></row><row><cell>8.</cell><cell>w_V = V + warpOffsetInCTA;</cell></row><row><cell>9.</cell><cell>w_tLocal_V = tLocal_V + warpOffsetInCTA;</cell></row><row><cell>10.</cell><cell>w_NIdx = NIdx + warpOffsetInCTA;</cell></row><row><cell></cell><cell>/* 1st major step */</cell></row><row><cell>11.</cell><cell>initVertex( w_V + laneID,</cell></row><row><cell></cell><cell>VertexValues + globalTID );</cell></row><row><cell>12.</cell><cell>w_NIdx[laneID] = NbrIndices[ globalTID ];</cell></row><row><cell>13.</cell><cell>startEIdx = w_NIdx[ 0 ];</cell></row><row><cell>14.</cell><cell>endEIdx = NbrIndices[warpGlobalOffset+32];</cell></row><row><cell></cell><cell>/* 2nd major step */</cell></row><row><cell>15.</cell><cell>for( currEIdx = startEIdx + laneID;</cell></row><row><cell></cell><cell>currEIdx &lt; endEIdx;</cell></row><row><cell></cell><cell>currEIdx += 32 ) {</cell></row><row><cell>16.</cell><cell>nbrIdx = NbrVertexIndices[ currEIdx ];</cell></row><row><cell>17.</cell><cell>srcV = VertexValue[ nbrIdx ];</cell></row><row><cell>18.</cell><cell>belongingVIdx =</cell></row><row><cell></cell><cell>binarySearch( currEIdx, w_NIdx );</cell></row><row><cell>19.</cell><cell>inSegID = min( laneID,</cell></row><row><cell></cell><cell>currEIdx -w_NIdx[ belongingVIdx ] );</cell></row><row><cell>20.</cell><cell>SegSize= inSegID + 1 + min( 31 -laneID,</cell></row><row><cell></cell><cell>( ( belongingVIdx == 31 ) ?</cell></row><row><cell></cell><cell>endEIdx : w_NIdx[ belongingVIdx + 1 ] )</cell></row><row><cell></cell><cell>-currEIdx -1 );</cell></row><row><cell>21.</cell><cell>ComputeNbr( srcV, EdgeValues + currEIdx,</cell></row><row><cell></cell><cell>w_tLocal_V + laneID );</cell></row><row><cell>22.</cell><cell>reduceInsideSegment( w_tLocal_V + laneID,</cell></row><row><cell></cell><cell>inSegID, SegSize );</cell></row><row><cell>23.</cell><cell>if( inSegID == 0 )</cell></row><row><cell>24.</cell><cell>ReduceVertices( w_V + belongingVIdx,</cell></row><row><cell></cell><cell>w_tLocal_V + laneID );</cell></row><row><cell>25.</cell><cell>}</cell></row><row><cell></cell><cell>/* 3rd major step */</cell></row><row><cell>26.</cell><cell>if( IsUpdated( w_tLocal_V + laneID,</cell></row><row><cell></cell><cell>VertexValues + globalTID ) ) {</cell></row><row><cell>27.</cell><cell>atomicExch( VertexValues + globalTID,</cell></row><row><cell></cell><cell>w_tLocal_V[ laneID ] );</cell></row><row><cell>28.</cell><cell>converged = false;</cell></row><row><cell>29.</cell><cell>}</cell></row><row><cell>30.</cell><cell>} sync_device_with_host(); }</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Organization of data structures in multi-GPU processing required for Vertex Refinement. The example graph represented in above configuration has the total number of M + N + P vertices and Q + R + T edges. The letters inside the boxes stand for the number of elements in the buffer. The size of inbox and outbox buffers are determined during Offline Vertex Refinement.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Host Memory</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Odd Buffer</cell><cell></cell><cell></cell><cell cols="2">Even Buffer</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Inbox #0 Inbox #1 Inbox #2</cell><cell></cell><cell cols="3">Inbox #0 Inbox #1 Inbox #2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Values Indices</cell><cell>≤M ≤M</cell><cell>≤N ≤N</cell><cell>≤P ≤P</cell><cell>Values Indices</cell><cell>≤M ≤M</cell><cell>≤N ≤N</cell><cell>≤P ≤P</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PCIe Lanes</cell><cell></cell><cell></cell><cell cols="2">PCIe Lanes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCIe Lanes</cell></row><row><cell>GPU #0</cell><cell></cell><cell></cell><cell></cell><cell>GPU #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GPU #2</cell><cell></cell></row><row><cell>Outbox</cell><cell>Values Indices</cell><cell>≤M ≤M</cell><cell></cell><cell cols="2">Outbox</cell><cell>Values Indices</cell><cell>≤N ≤N</cell><cell></cell><cell></cell><cell>Outbox</cell><cell>Values Indices</cell><cell>≤P ≤P</cell></row><row><cell>VertexValues</cell><cell>M</cell><cell>N</cell><cell>P</cell><cell cols="2">VertexValues</cell><cell>M</cell><cell>N</cell><cell>P</cell><cell></cell><cell>VertexValues</cell><cell>M</cell><cell>N</cell><cell>P</cell></row><row><cell>NbrIndices</cell><cell>M+1</cell><cell></cell><cell></cell><cell cols="2">NbrIndices</cell><cell></cell><cell>N+1</cell><cell></cell><cell></cell><cell>NbrIndices</cell><cell></cell><cell>P+1</cell></row><row><cell>NbrVertexIndices</cell><cell></cell><cell>Q</cell><cell></cell><cell cols="2">NbrVertexIndices</cell><cell></cell><cell>R</cell><cell></cell><cell></cell><cell>NbrVertexIndices</cell><cell></cell><cell>T</cell></row><row><cell>EdgeValues</cell><cell></cell><cell>Q</cell><cell></cell><cell cols="2">EdgeValues</cell><cell></cell><cell>R</cell><cell></cell><cell></cell><cell>EdgeValues</cell><cell></cell><cell>T</cell></row><row><cell>Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table II GRAPHS</head><label>II</label><figDesc>USED IN SINGLE-GPU EXPERIMENTS -ACROSS BENCHMARKS THE SIZE RANGES IN MBYTES FOR CSR AND CW REPRESENTATIONS. SIZES EXCEEDING GPU'S GLOBAL MEMORY CAPACITY ARE BOLDED.</figDesc><table><row><cell cols="3">Input Graph N. Vertices N. Edges CSR size CW size</cell></row><row><cell cols="3">RM33V335E 33 554 432 335 544 320 1611-3087 5503-8321</cell></row><row><cell cols="3">ComOrkut [30] 3 072 441 234 370 166 962-1912 3762-5649</cell></row><row><cell cols="3">ER25V201E 25 165 824 201 326 592 1007-1913 3322-5033</cell></row><row><cell cols="3">RM25V201E 25 165 824 201 326 592 1007-1913 3322-5033</cell></row><row><cell cols="3">RM16V201E 16 777 216 201 326 592 940-1812 3288-4966</cell></row><row><cell cols="3">RM16V134E 16 777 216 134 217 728 671-1275 2215-3355</cell></row><row><cell cols="3">LiveJournal [1] 4 847 571 68 993 773 315-610 1123-1695</cell></row><row><cell cols="3">SocPokec [27] 1 632 803 30 622 564 136-265 496-748</cell></row><row><cell cols="3">HiggsTwitter [6] 456 631 14 855 875 63-124 240-360</cell></row><row><cell>RoadNetCA [17] 1 965 214 5 533 214</cell><cell>38-68</cell><cell>96-149</cell></row><row><cell>WebGoogle [17] 875 713 5 105 039</cell><cell>27-51</cell><cell>85-130</cell></row><row><cell>Amazon0312 [18] 400 727 3 200 440</cell><cell>16-30</cell><cell>53-80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IV SPEEDUP</head><label>IV</label><figDesc>RANGES OF WARP SEGMENTATION OVER VWC EXCLUDING DATA TRANSFER TIMES. SINCE BOTH METHODS USE CSR</figDesc><table /><note><p>REPRESENTATION, THEIR DATA TRANSFER TIMES ARE EQUAL.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table III RAW</head><label>III</label><figDesc>Table V and having small graphs in Table VI. For the large graphs, CW representation cannot fit the whole graph inside GPU global memory. For these combinations, CuSha fails; RUNNING TIMES (MS) OF WARP SEGMENTATION (WS) AND VWC INCLUDING KERNEL EXECUTIONS AND HOST-DEVICE DATA TRANSFERS FOR DIFFERENT ALGORITHMS AND DIFFERENT GRAPHS.</figDesc><table><row><cell>Input Graph</cell><cell></cell><cell>BFS</cell><cell>CC</cell><cell>CS</cell><cell>HS</cell><cell>NN</cell><cell>PR</cell><cell>SSSP</cell><cell>SSWP</cell></row><row><cell>RM33V335E</cell><cell cols="4">WS VWC 1428-1811 1270-1680 2012-2562 1257 1118 1629</cell><cell cols="5">2812 3501-4412 2030-2506 6563-8275 3237-3959 6740-8268 1416 6056 2882 5505</cell></row><row><cell>ComOrkut</cell><cell>WS VWC</cell><cell>403 455-664</cell><cell>351 382-572</cell><cell>4162 5566-8847</cell><cell>681 692-1056</cell><cell cols="4">904 989-1634 6296-13334 1515-2519 1029-1626 4290 1398 931</cell></row><row><cell>ER25V201E</cell><cell>WS VWC</cell><cell>837 976-1385</cell><cell>644 710-1045</cell><cell>704 748-1313</cell><cell cols="2">8330 9499-16047 805-1160 773</cell><cell cols="3">5004 5287-6095 2386-3505 2574-3756 2181 2462</cell></row><row><cell>RM25V201E</cell><cell>WS VWC</cell><cell>845 933-1231</cell><cell>835 935-1233</cell><cell>1052 1287-1709</cell><cell cols="5">4782 5619-8716 1190-1529 4183-5491 2080-2653 4787-5991 1023 3856 1802 4216</cell></row><row><cell>RM16V201E</cell><cell>WS VWC</cell><cell>667 750-907</cell><cell>663 746-908</cell><cell>959 1187-1438</cell><cell>1762 2058-2337</cell><cell>840 984-1159</cell><cell cols="3">3762 4043-4526 1800-2230 3403-4284 1625 2998</cell></row><row><cell>RM16V134E</cell><cell>WS VWC</cell><cell>512 591-820</cell><cell>514 592-822</cell><cell>660 850-1218</cell><cell>1244 1539-2133</cell><cell>572 691-913</cell><cell cols="3">4068 4448-5656 1402-1832 2427-3267 1159 2028</cell></row><row><cell>LiveJournal</cell><cell>WS VWC</cell><cell>172 215-296</cell><cell>154 201-273</cell><cell>535 807-1084</cell><cell>346 378-536</cell><cell cols="2">2061 2297-4746 2498-4043 2326</cell><cell>446 619-814</cell><cell>772 1059-1345</cell></row><row><cell>SocPokec</cell><cell>WS VWC</cell><cell>75 90-107</cell><cell>66 80-106</cell><cell>121 175-203</cell><cell>226 264-329</cell><cell>464 614-761</cell><cell>1145 1302-2817</cell><cell>194 237-327</cell><cell>194 236-314</cell></row><row><cell>HiggsTwitter</cell><cell>WS VWC</cell><cell>48 54-170</cell><cell>37 49-178</cell><cell>117 157-495</cell><cell>75 95-294</cell><cell>159 192-812</cell><cell>483 927-2433</cell><cell>100 113-432</cell><cell>77 98-355</cell></row><row><cell>RoadNetCA</cell><cell>WS VWC</cell><cell>386 480-3400</cell><cell cols="2">330 493-3437 2392-23659 1694</cell><cell>41 45-301</cell><cell>193 191-1668</cell><cell>55 62-448</cell><cell>465 619-4402</cell><cell>1077 118-5619</cell></row><row><cell>WebGoogle</cell><cell>WS VWC</cell><cell>41 81-109</cell><cell>36 75-99</cell><cell>61 124-186</cell><cell>15 23-35</cell><cell>84 124-167</cell><cell>109 145-248</cell><cell>63 113-172</cell><cell>108 186-288</cell></row><row><cell>Amazon0312</cell><cell>WS VWC</cell><cell>17 25-46</cell><cell>17 26-46</cell><cell>263 419-797</cell><cell>81 142-237</cell><cell>41 42-78</cell><cell>44 63-110</cell><cell>33 63-90</cell><cell>38 57-92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table V THE</head><label>V</label><figDesc>SPEEDUP OF WARP SEGMENTATION OVER CUSHA'S [13] CW FOR large GRAPHS. THE SHARDS RESIDE INSIDE THE HOST PINNED BUFFERS (X MEANS GRAPH IS SMALL -FITS IN GPU MEMORY).. For the experiments in this section, we created 12 Rmat and Erdős-Rényi graphs with different sizes and densities, shown in TableVII. Six of these graphs can be fit inside two of our GPUs and Six require three GPUs. Finally, we analyze the performance when smaller graphs are processed on multiple GPUs.</figDesc><table><row><cell>Input Graph N. Vertices</cell><cell>N. Edges</cell><cell></cell></row><row><cell cols="2">RM54V704E 54 525 952 704 643 072</cell><cell></cell></row><row><cell cols="2">ER50V671E 50 331 648 671 088 640</cell><cell></cell></row><row><cell cols="2">RM50V671E 50 331 648 671 088 640</cell><cell></cell></row><row><cell cols="2">RM46V671E 46 137 344 671 088 640</cell><cell></cell></row><row><cell cols="2">RM46V603E 46 137 344 603 979 776</cell><cell></cell></row><row><cell cols="2">RM41V536E 41 943 040 536 870 912</cell><cell></cell></row><row><cell cols="2">RM41V503E 41 943 040 503 316 480</cell><cell></cell></row><row><cell cols="2">ER39V469E 39 845 888 469 762 048</cell><cell></cell></row><row><cell cols="2">RM39V469E 39 845 888 469 762 048</cell><cell></cell></row><row><cell cols="2">RM37V469E 37 748 736 469 762 048</cell><cell></cell></row><row><cell cols="2">RM37V436E 37 748 736 436 207 616</cell><cell></cell></row><row><cell cols="2">RM35V402E 35 651 584 402 653 184</cell><cell></cell></row><row><cell></cell><cell cols="3">Input Graph BFS CC CS HS NN PR SSSP SSWP</cell></row><row><cell></cell><cell cols="3">RM33V335E 3.41 3.21 8.44 14.14 4.02 5.38 4.36 4.66</cell></row><row><cell></cell><cell>ComOrkut</cell><cell cols="2">5.11 5.91 1.72 10.76 5.23 6.85 7.92 5.72</cell></row><row><cell></cell><cell cols="3">ER25V201E 3.47 3.36 6.20 10.43 3.72 2.59 4.46 4.34</cell></row><row><cell></cell><cell cols="3">RM25V201E 3.07 2.76 7.71 9.65 3.55 3.54 3.99 4.14</cell></row><row><cell></cell><cell cols="3">RM16V201E 3.45 3.06 6.53 8.42 3.87 4.50 4.63 4.41</cell></row><row><cell></cell><cell cols="2">RM16V134E x</cell><cell>x 3.19 4.97 x 3.93 x</cell><cell>x</cell></row><row><cell></cell><cell>Average</cell><cell cols="2">3.70 3.66 5.63 9.73 4.08 4.47 5.07 4.65</cell></row><row><cell></cell><cell cols="3">Input Graph BFS CC CS HS NN PR SSSP SSWP</cell></row><row><cell></cell><cell cols="3">RM16V134E 0.74 0.80 x</cell><cell>x 0.88 x 0.67 0.56</cell></row><row><cell></cell><cell cols="3">LiveJournal 1.06 1.21 0.74 1.10 1.03 0.60 0.86 0.82</cell></row><row><cell></cell><cell>SocPokec</cell><cell cols="2">0.92 1.02 1.04 0.81 0.41 0.34 0.73 0.67</cell></row><row><cell></cell><cell cols="3">HiggsTwitter 1.48 2.30 1.48 1.64 2.20 1.19 1.65 2.03</cell></row><row><cell></cell><cell cols="3">RoadNetCA 0.67 1.13 0.98 0.92 1.02 1.20 0.76 0.91</cell></row><row><cell></cell><cell cols="3">WebGoogle 0.58 0.82 0.78 1.74 1.69 0.59 0.61 0.74</cell></row><row><cell></cell><cell cols="3">Amazon0312 1.05 1.47 0.39 0.91 0.91 0.97 1.21 1.20</cell></row><row><cell></cell><cell>Average</cell><cell cols="2">0.93 1.25 0.90 1.19 1.16 0.82 0.93 0.99</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Table VI</cell></row><row><cell></cell><cell cols="3">THE SPEEDUP OF WARP SEGMENTATION OVER CUSHA'S [13] CW FOR</cell></row><row><cell></cell><cell cols="3">small GRAPHS. THE SHARDS RESIDE INSIDE THE GPU'S GLOBAL</cell></row><row><cell></cell><cell cols="3">MEMORY (X MEANS GRAPH IS LARGE -REQUIRES HOST MEMORY).</cell></row></table><note><p>benefits</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table VII GRAPHS</head><label>VII</label><figDesc>FOR MULTI-GPU EXPERIMENTS: TOP 6 GRAPHS USED IN EXPERIMENTS WITH 3 GPUS; REST USED WITH 2 GPUS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table VIII THE</head><label>VIII</label><figDesc>SPEED-UP OF VR OVER ALL AND MS FOR THREE-GPU ANDTWO-GPU CONFIGURATIONS.</figDesc><table><row><cell>Input Graph</cell><cell>BFS CC CS HS NN PR SSSP SSWP</cell></row><row><cell>RM54V704E</cell><cell>over ALL 1.85 1.81 2.53 1.64 1.66 1.48 1.75 2.03 over MS 1.82 1.78 2.46 1.59 1.63 1.47 1.71 1.98</cell></row><row><cell>ER50V671E</cell><cell>over ALL 1.64 1.36 2.19 1.55 1.43 1.22 1.72 2.02 over MS 1.67 1.4 2.24 1.49 1.48 1.21 1.76 2.05</cell></row><row><cell>RM50V671E</cell><cell>over ALL 1.83 1.76 2.51 1.68 1.63 1.49 1.72 1.98 over MS 1.78 1.74 2.47 1.6 1.6 1.36 1.68 1.93</cell></row><row><cell>RM46V671E</cell><cell>over ALL 1.78 1.77 2.48 1.7 1.62 1.43 1.72 1.98 over MS 1.75 1.74 2.42 1.64 1.6 1.41 1.69 1.93</cell></row><row><cell>RM46V603E</cell><cell>over ALL 1.84 1.82 2.58 1.67 1.67 1.43 1.79 2.07 over MS 1.81 1.8 2.51 1.59 1.64 1.37 1.75 2.01</cell></row><row><cell>RM41V536E</cell><cell>over ALL 1.89 1.84 2.71 1.62 1.69 1.44 1.8 2.1 over MS 1.82 1.81 2.63 1.58 1.66 1.39 1.75 2.04</cell></row><row><cell>RM41V503E</cell><cell>over ALL 1.29 1.29 1.61 1.23 1.21 1.18 1.24 1.35 over MS 1.27 1.28 1.57 1.21 1.2 1.15 1.21 1.32</cell></row><row><cell>ER39V469E</cell><cell>over ALL 1.21 1.06 1.49 1.18 1.14 1.19 1.23 1.21 over MS 1.23 1.09 1.53 1.16 1.17 1.15 1.25 1.24</cell></row><row><cell>RM39V469E</cell><cell>over ALL 1.29 1.3 1.64 1.28 1.21 1.39 1.26 1.38 over MS 1.28 1.28 1.61 1.24 1.2 1.29 1.23 1.35</cell></row><row><cell>RM37V469E</cell><cell>over ALL 1.26 1.26 1.6 1.24 1.2 1.23 1.22 1.36 over MS 1.25 1.26 1.57 1.21 1.19 1.18 1.2 1.33</cell></row><row><cell>RM37V436E</cell><cell>over ALL 1.33 1.32 1.66 1.27 1.22 1.25 1.28 1.41 over MS 1.31 1.29 1.63 1.23 1.22 1.24 1.26 1.39</cell></row><row><cell>RM35V402E</cell><cell>over ALL 1.32 1.31 1.72 1.28 1.23 1.21 1.25 1.41 over MS 1.3 1.29 1.66 1.23 1.22 1.2 1.22 1.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Processing-time break down into computation time and communication time for the Vertex Refinement (VR) compared to ALL and MS. Computation time is the total duration of kernel execution, and communication time is the total duration of inbox/outbox management plus inter-device memory copies. For each benchmark, the times are normalized with respect to the longest time. Note that this times cannot be used to infer the overall speedup due to asynchronicity of devices.</figDesc><table><row><cell></cell><cell cols="12">Aggregated Computation Duration</cell><cell></cell><cell cols="12">Aggregated Communication Duration</cell></row><row><cell>Normalized Aggregated Time</cell><cell>0 0.2 0.4 0.6 0.8 1</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell cols="2">ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BFS</cell><cell></cell><cell></cell><cell></cell><cell>CC</cell><cell></cell><cell></cell><cell>CS</cell><cell></cell><cell></cell><cell>HS</cell><cell></cell><cell></cell><cell>NN</cell><cell></cell><cell></cell><cell>PR</cell><cell></cell><cell cols="3">SSSP</cell><cell cols="3">SSWP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">(a) RM54V704E graph with 3 GPUs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="12">Aggregated Computation Duration</cell><cell></cell><cell cols="12">Aggregated Communication Duration</cell></row><row><cell>Normalized Aggregated Time</cell><cell>0 0.2 0.4 0.6 0.8 1</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell cols="2">ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell><cell>ALL</cell><cell>MS</cell><cell>VR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BFS</cell><cell></cell><cell></cell><cell></cell><cell>CC</cell><cell></cell><cell></cell><cell>CS</cell><cell></cell><cell></cell><cell>HS</cell><cell></cell><cell></cell><cell>NN</cell><cell></cell><cell></cell><cell>PR</cell><cell></cell><cell cols="3">SSSP</cell><cell cols="3">SSWP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">(b) RM41V503E graph with 2 GPUs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="27">Input Graph GPUs BFS CC CS HS NN PR SSSP SSWP</cell></row><row><cell cols="27">RM41V503E 3 vs. 2 1.39 1.38 1.32 1.23 1.21 1.12 1.32 1.35</cell></row><row><cell cols="27">ER39V469E 3 vs. 2 1.36 1.11 1.44 1.19 1.33 1.09 1.28 1.26</cell></row><row><cell cols="27">RM39V469E 3 vs. 2 1.3 1.37 1.42 1.22 1.28 1.13 1.42 1.29</cell></row><row><cell cols="27">RM37V469E 3 vs. 2 1.21 1.27 1.32 1.24 1.38 1.19 1.34 1.43</cell></row><row><cell cols="27">RM37V436E 3 vs. 2 1.22 1.18 1.32 1.17 1.21 1.18 1.28 1.34</cell></row><row><cell cols="27">RM35V402E 3 vs. 2 1.5 1.37 1.42 1.23 1.29 1.14 1.33 1.39</cell></row><row><cell cols="6">RM33V335E</cell><cell cols="21">3 vs. 1 1.75 1.56 1.95 1.33 1.52 1.1 1.55 1.59 2 vs. 1 1.27 1.24 1.4 1.07 1.12 1.06 1.21 1.2</cell></row><row><cell cols="4">ComOrkut</cell><cell></cell><cell></cell><cell cols="21">3 vs. 1 1.65 1.81 1.95 1.28 1.97 1.43 1.96 1.85 2 vs. 1 1.19 1.31 1.65 1.15 1.36 1.32 1.4 1.39</cell></row><row><cell cols="5">ER25V201E</cell><cell></cell><cell cols="21">3 vs. 1 1.5 1.55 1.44 1.19 1.38 1.18 1.48 1.58 2 vs. 1 1.14 1.33 1.16 1.07 1.13 1.15 1.11 1.19</cell></row><row><cell cols="6">RM25V201E</cell><cell cols="21">3 vs. 1 1.47 0.96 1.56 1.29 1.38 0.93 1.29 1.17 2 vs. 1 1.08 0.97 1.26 1.1 1.08 1.01 1.07 0.94</cell></row><row><cell cols="6">RM16V201E</cell><cell cols="21">3 vs. 1 1.45 1.64 1.74 1.3 1.56 1.02 1.42 1.6 2 vs. 1 1.26 1.36 1.44 1.12 1.21 1.06 1.17 1.34</cell></row><row><cell cols="6">RM16V134E</cell><cell cols="21">3 vs. 1 1.36 1.58 1.86 1.36 1.47 1.19 1.46 1.66 2 vs. 1 1.21 1.21 1.44 1.14 1.11 1.12 1.12 1.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table IX THE</head><label>IX</label><figDesc>SPEEDUP OF OUR FRAMEWORK WHEN SCALING TO MORE GPUS: FROM 2 TO 3 GPUS FOR THE TOP 6 GRAPHS; AND FROM 2 TO 3 AND FROM 1 TO 2 GPUS FOR THE REST OF THE GRAPHS.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1089-795X/15 $31.00 © 2015 IEEE DOI 10.1109/PACT.2015.15</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is supported by NSF Grants CCF-0905509, CNS-1157377, CCF-1318103, and CCF-1524852.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: Membership, growth, and evolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing cuda workloads using a detailed gpu simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<biblScope unit="page" from="163" to="174" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Mat</forename></persName>
		</author>
		<title level="m">A Recursive Model for Graph Mining</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="442" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dymaxion: Optimizing memory access patterns for heterogeneous systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Workefficient parallel GPU methods for single source shortest paths</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPDPS</title>
		<imprint>
			<biblScope unit="page" from="349" to="359" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The anatomy of a scientific rumor</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Domenico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mougel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musolesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A yoke of oxen and a thousand chickens for heavy lifting graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gharaibeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beltrão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santos-Neto</surname></persName>
		</author>
		<author>
			<persName><surname>Ripeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient large-scale graph processing on hybrid CPU and GPU systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gharaibeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ripeanu</surname></persName>
		</author>
		<idno>abs/1312.3018</idno>
		<ptr target="http://arxiv.org/abs/1312.3018" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating large graph algorithms on the gpu using cuda</title>
		<author>
			<persName><forename type="first">P</forename><surname>Harish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HiPC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optimizing parallel prefix operations for the fermi architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
	<note>in gpu computing gems, jade edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating cuda graph algorithms at maximum warp</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient parallel graph exploration on multi-core cpu and gpu</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PACT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cusha: Vertex-centric graph processing on gpus</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPDC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Parmat: A parallel generator for large r-mat graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://github.com/farkhor/PaRMAT" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-threaded graph partitioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lasalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPDPS</title>
		<imprint>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="123" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The dynamics of viral marketing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Web</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Challenges in parallel graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPL</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An effective gpu implementation of breadth-first search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable and high performance betweenness centrality on the gpu</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="572" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A gpu implementation of inclusion-based points-to analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mendez-Lojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable gpu graph traversal</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grimshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Morph algorithms on gpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nasre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>SIDL-WP-1999-0120</idno>
		<ptr target="http://ilpubs.stanford.edu:8090/422/" />
	</analytic>
	<monogr>
		<title level="m">Stanford InfoLab</title>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive input-aware compilation for graphics engines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hormati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data analysis in public social networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zabovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Scientific Conference AND International Workshop Present Day Trends of Innovations</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The cuda handbook: A comprehensive guide to gpu programming</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wilt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Pearson Education</publisher>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient pagerank and spmv computation on amd gpus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPP</title>
		<imprint>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Medusa: Simplified graph processing on gpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
