<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature integration analysis of bag-of-features model for image retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-03-29">29 March 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
							<email>zcqin@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature integration analysis of bag-of-features model for image retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-03-29">29 March 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">A4AF971191F662E01ECC716B338924D5</idno>
					<idno type="DOI">10.1016/j.neucom.2012.08.061</idno>
					<note type="submission">Received 25 January 2012 Received in revised form 17 July 2012 Accepted 24 August 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Bag-of-features (BoF) Image retrieval Weighted K-means SIFT-LBP HOG-LBP Histogram intersection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the biggest challenges in content based image retrieval is to solve the problem of "semantic gaps" between low-level features and high-level semantic concepts. In this paper, we aim to investigate various combinations of mid-level features to build an effective image retrieval system based on the bag-offeatures (BoF) model. Specifically, we study two ways of integrating the SIFT and LBP descriptors, HOG and LBP descriptors, respectively. Based on the qualitative and quantitative evaluations on two benchmark datasets, we show that the integrations of these features yield complementary and substantial improvement on image retrieval even with noisy background and ambiguous objects. Two integration models are proposed: the patch-based integration and image-based integration. By using a weighted K-means clustering algorithm, the image-based SIFT-LBP integration achieves the best performance on the given benchmark problems comparing to the existing algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Content-based image retrieval (CBIR) is a technique to search for the most visually similar images to a given query image from a large image database. It has received increasing attentions in recent years and become a hot topic in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and multimedia <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The main idea of image retrieval is to extract low-level features from the images and measure the degree of similarity between them to find the most similar ones in terms of visual contents. Colors, textures, and shapes have been used to describe the image contents <ref type="bibr" target="#b3">[4]</ref>. There are advantages and disadvantages of using these low-level features on CBIR systems. Color features have high computational efficiency and are invariant to rotation and scale. However, they do not consider image content and spacial distribution of colors. The incomplete color space describing the color characteristics in human imperfect visual perception has yet to be solved. Texture features, describing the spatial variation in pixel intensities, can reflect the surface attributes of an object.</p><p>In <ref type="bibr" target="#b6">[7]</ref>, a number of different methods are proposed to describe image textures. However, texture segmentation still remains a difficult problem to meet human perception <ref type="bibr" target="#b7">[8]</ref>. Shape-based features are relatively consistent with the intuitive feeling but lacking perfect mathematical foundations to deal with the target deformation <ref type="bibr" target="#b3">[4]</ref>. Therefore, only using low-level visual features can hardly capture the semantic concepts of images. In recent years, mid-level features have attracted more attentions. Among them, the scale-invariant feature transform (SIFT) operator <ref type="bibr" target="#b8">[9]</ref> and its improved versions, such as PCA-SIFT <ref type="bibr" target="#b9">[10]</ref>, SURF <ref type="bibr" target="#b10">[11]</ref>, affine invariant SURF <ref type="bibr" target="#b11">[12]</ref> are invariant to rotation, scaling, translation and small distortions. Nevertheless, perfect retrieval results cannot be achieved in practice because of complex background in image data. The histogram of oriented gradients (HOG) <ref type="bibr" target="#b12">[13]</ref> descriptor is similar to the SIFT and shape contexts, but differs in that it is computed on a dense grid of uniformly spaced cells and uses overlapping local contrast normalization for improved accuracy. HOG descriptor counts occurrences of gradient orientation in localized portions of an image. The basic idea behind the HOG descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. To increase the applicability of HOG, an extension of it has been proposed in <ref type="bibr" target="#b5">[6]</ref> that makes use of a gradient decomposition and combination strategy to improve the human detection accuracy. The local binary pattern (LBP) descriptor <ref type="bibr" target="#b13">[14]</ref> is considered as one of the most popular texture features due to its discriminative power and computational simplicity. Moreover, the most important property of the LPB operator is its robustness to monotonic gray-scale changes caused, for example, by illumination variations, making it is desirable to analyze images in challenging real-world applications.</p><p>All these three types of features have been proven to be very powerful descriptors to detect and describe local features in images. Both the SIFT and HOG features are capable of capturing local object shape or edge with the distributions of intensity gradients. For an image with simple background, the SIFT and HOG features are able to accurately represent the foreground Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom objects without noise interference. However, the SIFT and HOG will perform poorly when the image contains complex background due to the fact that a portion of extracted features may come from the noisy background. On the contrary, the LBP descriptor does not take into account shape information in images. In addition, it can filter out background noise through local binary texture patterns <ref type="bibr" target="#b14">[15]</ref>, in which uniform patterns with consistent changes are labeled with separate labels while all the non-uniform patterns are considered as a single label. In this fashion, noisy background does not significantly affect the similarity measure between images. We believe that combining the shape information, such as obtained from the SIFT or HOG, with the texture information captured by the LBP operator provides a more robust and precise representation of images compared to a single feature.</p><p>Recently, there are many feature fusion algorithms published in literature. For example, Wang et al. <ref type="bibr" target="#b15">[16]</ref> proposed an integrated HOG-LBP feature on the cell-level as a human detector. It has been reported that the method achieves good performance in the INRIA dataset. Heikkilä et al. <ref type="bibr" target="#b16">[17]</ref> derived a new descriptor combining the strengths of the SIFT and LBP, in which center-symmetric local binary patterns were used to replace the gradient operator used by the SIFT operator for region detection and image matching applications. Zheng et al. <ref type="bibr" target="#b17">[18]</ref> combined the SIFT and rotationinvariant LBP for an image matching problem, where the LBP descriptor was used to describe the local region centered at the keypoints which were found by the SIFT feature. In addition, Schwartz et al. <ref type="bibr" target="#b18">[19]</ref> introduced a human detection method by integrating edge-based features with texture and color information to form a relative large descriptor set. Hiremath and Pujari <ref type="bibr" target="#b3">[4]</ref> considered the combination of color, texture, and shape information to build an efficient framework for image retrieval, which is also used as a reference method to be compared to our proposed feature integration models.</p><p>In this paper, we design two local semantic descriptors by integrating the SIFT, HOG with the LBP descriptor. The new features do not rely on the image segmentation method and are able to automatically detect interest points and regions within an image. The proposed integration model is based on the bag-offeatures (BoF) representations <ref type="bibr" target="#b19">[20]</ref>. Features that are computed for each image are combined to form high-dimensional descriptors. These descriptors are clustered into several key points which are referred to as visual words. Each image is then represented by a distribution on the visual words. Given a query image, we are able to find a list of similar images ranked by similarity scores based on visual word distributions.</p><p>The remaining paper is organized as follows: Section 2 describes the framework of bag-of-features model and the similarity matching method. Section 3 provides a brief review of the SIFT, LBP and HOG features. We introduce two integration methods and apply them to combine the SIFT and LBP, HOG and LBP, respectively. The experimental results are presented in Section 4. The conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bag-of-features model</head><p>The bag-of-features method is largely inspired by the concept of bag-of-words (BoW) <ref type="bibr" target="#b20">[21]</ref> which has been used in text mining. In the BoW model, each word is assumed to be independent. Though it is counterintuitive, the BoW has been well known in spam filtering and topic modeling <ref type="bibr" target="#b21">[22]</ref> with outstanding performance. In the BoF model, each image is described by a set of orderless local features <ref type="bibr" target="#b22">[23]</ref>. It includes four key concepts: (1) local features: the essential aspect of the BoF concept is to extract global image descriptors and represent images as a collection of local properties calculated from a set of small sub-images called patches. For example, the SIFT patches are small rectangular regions centered on interest points, while the LBP patches are small round regions with the desired radius and a number of sampling points. (2) Codebook representation: codebook is a way that images can be represented as a set of local features <ref type="bibr" target="#b19">[20]</ref>. The idea is to group the feature descriptors of all patches, and the representatives of resulting clusters are then used as entries of an unified codebook <ref type="bibr" target="#b20">[21]</ref>. The entries are called "visual words". (3) Feature quantization: after obtaining the codebook, each local feature is quantized to one "visual word" using unsupervised learning methods, such as K-nearest neighbor (KNN) classifier. ( <ref type="formula" target="#formula_7">4</ref>) Image representation: as all the features are mapped to the codebook, an image can be represented by the BoF frequency histogram of the "visual words" in the codebook. The similarity measure between two images can be computed by comparing these two BoF histograms. A histogram intersection is used to compute the similarity between two histograms of given images A and B, which is defined by</p><formula xml:id="formula_0">dðA; BÞ ¼ 1-∑ n i ¼ 1 minða i ; b i Þ ð<label>1Þ</label></formula><p>where a i and b i represent the probabilities of visual words of image A and B, respectively. For a given query image Q, the distance between Q and each image in the database will be calculated. A set of images are selected and sorted in ascending order based on their distance values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature integration</head><p>The scale-invariant feature transform has been proven to be one of the most robust among the local invariant feature descriptors with respect to different geometrical changes <ref type="bibr" target="#b23">[24]</ref>. It represents blurred image gradients in multiple orientation planes and at multiple scales. The SIFT has showed great success in object recognition and detection due to its invariance in translation, scaling, rotation, and small distortions. The basic idea is to identify the extreme points in the scale space, and filter these extreme points to find the stable feature points known as keypoints. The local attributes of orientation gradient and describe are computed, and the keypoints are described by 4 Â 4 Â 8 matrix.</p><p>The LBP operator <ref type="bibr" target="#b14">[15]</ref> is a texture descriptor that has been widely used in object recognition and achieved good results in face recognition problems. Its key advantages, namely its invariance to monotonic ray level changes and rotation, make it suitable for demanding image analysis tasks <ref type="bibr" target="#b15">[16]</ref> such as object retrieval. The LBP method is schematically shown in Fig. <ref type="figure">1</ref>. Previous research which used uniform patterns representing the most essential texture information showed a strong discriminative ability <ref type="bibr" target="#b14">[15]</ref>. Uniform pattern LBP considers only those LBP codes that have U value of no more than 2 (U refers to the measure of uniformity, that is the number of 0/1 and 1/0 transitions in the circular binary code pattern). In case of LBP u 8;1 (8 and 1 are respectively the number of neighboring sample points and the radius of LBP P;R ,   u means uniform pattern), we get a feature vector of 58 bins instead of original 256 bins. As the remaining un-uniform patterns are accumulated to a single bin, the uniform LBP histogram actually contains 59 bins. That is only a fraction (59/256) of the classic LBP descriptor.</p><p>As a dense version of SIFT feature, histograms of oriented gradients (HOG) feature has shown great performance in object presentation and recognition <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. HOG is accepted to be one of the best features to describe the edge and shape information. Basic procedure of extracting HOG features is presented in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SIFT-LBP features integration</head><p>The SIFT descriptor perform poorly when the background is lack of texture or corrupted with noise due to the fact the SIFT fails to find stable keypoints in these cases. The LBP with uniform patterns has been proven to be a very robust texture feature to supplement the SIFT by filtering out the noises <ref type="bibr" target="#b14">[15]</ref>. We believe that the characteristics of an object in an image can be better captured by combining these two features. Thereby, two SIFT-LBP integrations methods are proposed at patch level and image level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Patch-based SIFT-LBP feature integration</head><p>We define p i ðx; y; s; θÞ as a keypoint detected by SIFT approach, where ðx; yÞ is the location of pixel p i in the original image, s and θ are the scale and main direction of p i respectively. s refers to the certain level of p i in Gaussian Pyramid. Take a region with size of 16 Â 16 as a patch where p i is the center of the patch. The SIFT-LBP descriptor as shown in Fig. <ref type="figure" target="#fig_2">3</ref> is built as follows:</p><p>Step1. Use an 128-dimensional SIFT descriptor to describe each keypoint p i in a patch, denoted as SIFT i in the image.</p><p>Step2. Choose an 8 Â 8 region around p i and compute the uniform pattern LBP u j 8;1 of each pixel. These descriptors are composed as a 64-dimensional vector, i.e.</p><formula xml:id="formula_1">LBP i ¼ ½LBP u 1 8;1 LBP u 2 8;1 ⋯ LBP u 64 8;1</formula><p>Step3. LBP i is directly connected to the end of SIFT i , thus a patch can be described as an 192-dimensional integrated vector</p><formula xml:id="formula_2">SIFT-LBP i ¼ ½SIFT i LBP i 3.1.2. Image-based SIFT-LBP feature integration</formula><p>Patch-based SIFT-LBP may have a serious disadvantage that patches around keypoints are sparse and then much texture information is missing. Therefore, we propose to compute all the SIFT and LBP descriptors in an image independently and link them directly at the image level. The LBP-SIFT descriptor as shown in Fig. <ref type="figure" target="#fig_1">4</ref> is built as follows:</p><p>Step1. The same as Step1 in Section 3.1.1.</p><p>Step2. Compute the uniform pattern LBP u 8;1 for each image pixel. Step3. For each kind of feature, we independently build codebooks for SIFT and LBP features by weighted K-means clustering algorithms introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weighted K-means clustering</head><p>K-means is one of the simplest unsupervised algorithm and has been widely used in image processing <ref type="bibr" target="#b4">[5]</ref>. It is also used to cluster the SIFT descriptors to form codebook in the bag-of-feature model <ref type="bibr" target="#b19">[20]</ref>. Fig. <ref type="figure">5</ref> schematic illustrated the BoF model with SIFT-LBP. The vectors represent independent SIFT and LBP descriptors. Given the above two integration methods, K-means can be applied directly to the patch-based approach. In the image-based integration, the number of LBP keys is much smaller than that of SIFT keys (e.g., given an image of 384 Â 256. It contains 1457 SIFT keys and 384 LBP keys in a descriptor. It is quite possible that LBP features only take up small ratio of the total cluster centers. In this case, the SIFT features may play a key role in the codebook whereas the LBP features are less effective. We then use a weight parameter w (0 ≤w ≤1) to balance the importance between these two set of features</p><formula xml:id="formula_3">N SIFT ¼ w Á N<label>ð2Þ</label></formula><formula xml:id="formula_4">N LBP ¼ ð1-wÞ Á N<label>ð3Þ</label></formula><p>where N is the desired number of cluster centers (the size of codebook). N SIFT and N LBP represent the number of cluster centers selected from SIFT keys and LBP keys, respectively. Then, we can control the weight of each feature in the codebook and develop the more effective integrated features through experiments. A schematic illustration of clustering with weighted K-means is shown in Fig. <ref type="figure">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HOG-LBP feature integration</head><p>As a dense version of SIFT feature, histograms of oriented gradients (HOG) feature have shown great performance in object presentation and recognition <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. HOG is accepted to be one of the best features to describe the edge and shape information. While LBP, a texture feature, has the key advantages of invariance to monotonic grey level change and computational efficiency. An Fig. <ref type="figure">8</ref>. Five samples from 10 categories of the Corel database <ref type="bibr" target="#b29">[30]</ref>. Fig. <ref type="figure">7</ref>. The proposed framework for image retrieval based on the bag-of-features model using the SIFT-LBP integration. The SIFT and LBP features are extracted independently and combined by the proposed methods. A bag-of-features model is trained on these features. Given a new query image, the similarity between the BoF histograms of the query image and the ones in database is calculated to find the most similar images from the database. augment feature vector, which integrating HOG and cell-based LBP, outperforms other state-of-the-art descriptors on human detection <ref type="bibr" target="#b15">[16]</ref>. Inspired by this idea, we propose two kinds of HOG-LBP integration approaches at the image level and the patch level, and apply these features to image retrieval.</p><p>We follow the procedure of <ref type="bibr" target="#b24">[25]</ref> to extract HOG features. Each image is divided into small cells of 4 Â 4 pixel and for each cell we accumulate a 1-9 histogram of gradient directions. Each block is constructed by 2 Â 2 cells. Then we refer to the normalized 2 Â 2 Â 9¼36 dimensional block descriptors as the HOG descriptors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Africa Beaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Patch-based HOG-LBP integration</head><p>Here we define each block of the HOG feature as a patch. For the construction of block-based LBP, we build uniform pattern LBP histogram in each block. The integration is to directly link the HOG descriptor and uniform LBP descriptor in the same block. Detailed procedure is as follows:</p><p>Step1. Extract a 36-dimensional HOG descriptor in each patch, denoted as HOG i . The building procedure is shown in Fig. <ref type="figure">2</ref>.</p><p>Step2. Choose an 8 Â 8 region around the center of each patch and compute the uniform pattern LBP u j 8;1 of each pixel. These descriptors are composed as a 64-dimensional vector, i.e.</p><formula xml:id="formula_5">LBP i ¼ ½LBP u 1 8;1 LBP u 2 8;1 ⋯ LBP u 64 8;1</formula><p>Step3. In each patch, LBP i is directly linked to the end of HOG i . So far, a patch-based 100-dimensional descriptive vector is built</p><formula xml:id="formula_6">½HOG-LBP i ¼ ½HOG i LBP i 3.3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image-based HOG-LBP integration</head><p>It is also possible to combine HOG and LBP features at the image level. In this case, HOG and LBP features are extracted independently of the whole image, and integration is finished within our newly proposed weighted K-means clustering. The detailed procedure is as follows:</p><p>Step1. The same as Step1. in patch-based HOG-LBP integration.</p><p>Step2. Compute the uniform pattern LBP descriptors for each image pixel in the whole image.</p><p>Step3. For each image, we build codebook for above HOG and LBP features using weighted K-means clustering whose principle idea is the same as the method introduced in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental studies</head><p>A schematic diagram of the proposed framework for image retrieval is shown in Fig. <ref type="figure">7</ref>. In the training process (shown on the left-hand side of Fig. <ref type="figure">7</ref>), the SIFT and LBP features are extracted and integrated to construct a SIFT-LBP descriptor. After weighted K-means clustering, the codebook is then generated using the integrated descriptor. Each image is then mapped to the codebook in order to obtain its BoF histogram. In the retrieval process (shown on the right-hand side of Fig. <ref type="figure">7</ref>), we select a query image from the given database. By comparing its BoF histogram to the other BoF histograms in the database, we can yield a ranked set of the most similar images. For the HOG-LBP integration, we simply replace the SIFT feature with the HOG feature and the remaining process is untouched.</p><p>We first conduct experiments using the Corel database <ref type="bibr" target="#b29">[30]</ref>. The sample images are displayed in Fig. <ref type="figure">8</ref>. We analyze the performance by considering the different codebook sizes and Kmeans weights. The subsequent subsections focus on comparing our new algorithms with a number of state-of-the-art image retrieval methods on two different databases. For the purpose of fair comparison, we adopt the same evaluation criteria reported in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref>.  The Corel database is utilized in this experiment which comprises 1000 images from 10 categories <ref type="bibr" target="#b29">[30]</ref>. The images are with the size of 256 Â 384 or 384 Â 256 pixels. The average retrieval precision (ARP) <ref type="bibr" target="#b3">[4]</ref> is used to quantify the performance. We define A(i) as a set of images having the same category index with the query image i in the database, and B(i) is a collection of retrieved images. The precision is defined as the percentage of the retrieved images belonging to the same category as the query image</p><formula xml:id="formula_7">PðiÞ ¼ jAðiÞ⋂BðiÞj jBðiÞj<label>ð4Þ</label></formula><p>If we choose a dinosaur image from the database as a query, and 70 of the first 100 (we totally have 100 dinosaur images) retrieved images belong to the category of dinosaur. The retrieval precision is 0.7. The ARP of a specific category is defined by</p><formula xml:id="formula_8">ARPðID m Þ ¼ 1 N ∑ idðiÞ ¼ IDm PðiÞ ð<label>5Þ</label></formula><p>where N is the size of the category ID m in the testing database and id(i) is the category index for the query image i.</p><p>We study the effectiveness of different sizes of codebook and K-means weights on the performance for the image-based SIFT-LBP descriptor. We choose the size of codebooks from f50; 100; 150; 200; 250g. The comparison results are shown in Fig. <ref type="figure">9</ref>. By examining the figure, we can see that size 200 gains the best results on nine out of 10 categories. Given the best size of codebook, we test different values of weight w∈f0:4; 0:5; 0:6; 0:7g. The results shown in Fig. <ref type="figure">10</ref> indicate that w¼0.6 outperforms all the other weights. The ARP values for each of the 10 categories are listed in Table <ref type="table" target="#tab_1">1</ref>. For the patch-based SIFT-LBP integration, we have tested patch region with size of 16 Â 16 and 8 Â 8 with different codebook sizes. We found that the best results are given by the patch size of 8 Â 8 and N¼200, which are shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image retrieval performance</head><p>We compare the retrieval precision of the proposed methods to the existing methods using the Corel database <ref type="bibr" target="#b29">[30]</ref>. An integrated matching scheme combining color, texture, and shape features <ref type="bibr" target="#b3">[4]</ref>, which has been reported to be a robust retrieval system, is included for comparison. A block-based LBP method <ref type="bibr" target="#b7">[8]</ref> and a BoF-based SIFT approach <ref type="bibr" target="#b20">[21]</ref> are also used as reference methods. The spatial pyramid matching (SPM) <ref type="bibr" target="#b32">[33]</ref> is one of the most effective extensions of BoF. The SPM divides an image into small sub-regions, and counts the number of features locating in each region. Here we compare our proposed feature integration schemes with the SPM-based SIFT retrieval system.</p><p>Experimental results shown in Table <ref type="table" target="#tab_1">1</ref> indicate that SPM using the SIFT feature improves the retrieval performance on almost all the categories compared with the BoF model. We note that the SPM gains the best performance on "Africa" which contains more human figures than other categories except "Food" because the spatial information is effective in discriminating human shapes. The poor performance from the block-based LBP is due to the fact that the method only takes into account the texture information.</p><p>Our proposed patch-based SIFT-LBP scheme outperforms the color, texture, and shape integration <ref type="bibr" target="#b3">[4]</ref> and block-based LBP <ref type="bibr" target="#b7">[8]</ref>, but is worse than the BoF and SPM. This indicates that the patchbased LBP has a less discriminant capability. The patch-based HOG-LBP integration significantly improves the results compared to the image-based HOG-LBP. The image-based SIFT-LBP achieves the best retrieval result among all these methods.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">11</ref>, we can see that the image-based SIFT-LBP method yields more meaningful matching results than the SPM method. It is clear to see that by integrating SIFT and LBP features at image level allows us to achieve superior retrieval results in  terms of visual and quantitative evaluations. To further verify the performance of the integrated SIFT-LBP feature, we test on the SIMPLIcity database <ref type="bibr" target="#b30">[31]</ref>. It contains 500 images with size 128 Â 85 from six classes based on their semantic contents. Five sample images for six categories are shown in Fig. <ref type="figure" target="#fig_0">12</ref>. We use the precision-recall (PR) curve <ref type="bibr" target="#b31">[32]</ref> to evaluate the retrieval results. The precision is defined in Eq. 4. The recall represents the percentage of relevant images that are retrieved, which is expressed as</p><formula xml:id="formula_9">RðiÞ ¼ jAðiÞ⋂BðiÞj jAðiÞj<label>ð6Þ</label></formula><p>Fig. <ref type="figure" target="#fig_7">13</ref> shows the precision-recall curve on the three models: the circular ring histogram (CRH) <ref type="bibr" target="#b31">[32]</ref>, the BoF model based on SIFT <ref type="bibr" target="#b20">[21]</ref>, and the BoF model based on SIFT-LBP. The CRH is a histogram-based method that is robust to the image rotation and scaling <ref type="bibr" target="#b31">[32]</ref>. The BoF model using the SIFT feature outperforms the CRH method. Again, the image-based SIFT-LBP has shown the best performance for image retrieval.</p><p>It is noted that our feature extraction module takes a longer time than computing one single feature since there are two features (SIFT-LBP, HOG-LBP) to be computed. However, all the features are extracted before the feature integration and image retrieval tasks. In fact, the integration and similarity measure modules are performed run fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a feature integration framework for image retrieval based on the bag of features model and weighted K-means clustering. Patch-based and image-based integration of SIFT, LBP and HOG features are proposed. Based on comprehensive experimental studies on benchmark image retrieval problems, the image-based integration achieves the best performance compared to the existing models when adopting codebooks size N ¼200 and K-means weight w¼ 0.6. The future work will focus on evaluation of more real-world image retrieval problems and improvement on the ranking quality by incorporating the relations between these features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig.1. The classic LBP descriptor. For each pixel, compare the pixel to each of its 8 neighbors. Where the center pixel's value is greater than the neighbor, write "1". Otherwise, write "0". This gives an 8-digit binary number.</figDesc><graphic coords="2,94.74,619.11,396.00,104.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The integration of the image-based SIFT-LBP. First, an 128-dimensional SIFT feature and an uniform pattern LBP feature are computed on the entire image; they are then concatenated to from an image-based SIFT-LBP descriptor; individual codebooks are built for the SIFT and LBP features by a weighted K-means clustering.</figDesc><graphic coords="3,45.50,566.15,85.03,145.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The integration of the patch-based SIFT-LBP. First, an 128-dimensional SIFT descriptor is computed for a patch; An 8 Â 8 region around the keypoint is chosen and an uniform pattern LBP is then computed on each pixel in this patch; the SIFT and LBP features are finally concatenated to form a patch-based SIFT-LBP descriptor.</figDesc><graphic coords="3,46.28,347.03,511.49,145.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The schematic diagram of BoF with SIFT-LBP using the K-means clustering. The vectors represent the independent SIFT and LBP descriptors. In the left bottom coordinate system, red circles and green triangles are SIFT and LBP keys, respectively. The circles in the right top coordinate system show the clustering results where the SIFT feature takes up majority of the cluster centers. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="4,34.61,493.76,247.44,187.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .Fig. 9 .</head><label>109</label><figDesc>Fig.10. Comparison of retrieval results with different choices of K-means weight in the image-based integration. The ARP values for 10 categories with different weights are presented by a histogram plotting. The performance differs in: Flower, the weight plays an important role where the ARP could vary as big as 30% by using different weights; however, in categories like Bus and Dinosaur, the difference is as small as 3%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Comparison of retrieval results obtained by: (1) the image-based SIFT-LBP integration method, and (2) the SPM based SIFT method. The left side lists three query images. The right side shows two rows of retrieval results corresponding to each query image, the first and the second rows are the results from method (1) and method (2), respectively.</figDesc><graphic coords="7,104.55,488.87,396.00,226.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Five samplers from six categories of the SIMPLIcity database<ref type="bibr" target="#b30">[31]</ref>.</figDesc><graphic coords="8,124.69,556.11,336.24,176.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Comparison of the precision and recall rate of the CRH [32], the BoF model based on SIFT [21], and the BoF model based on the image-based SIFT-LBP integration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,44.50,528.75,516.24,203.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.56,131.86,336.00,332.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Comparison of the ARP values obtained by the proposed methods with the standard image retrieval systems.</figDesc><table><row><cell>Class</cell><cell>Color,</cell><cell>Block</cell><cell>BoF</cell><cell>SPM</cell><cell>Patch-</cell><cell>Image-</cell><cell>Patch-</cell><cell>Image-</cell></row><row><cell></cell><cell>texture</cell><cell>based</cell><cell>based</cell><cell>based</cell><cell>based</cell><cell>based</cell><cell>based</cell><cell>based</cell></row><row><cell></cell><cell>shape</cell><cell>LBP</cell><cell>SIFT</cell><cell>SIFT</cell><cell>SIFT-</cell><cell>SIFT-</cell><cell>HOG-</cell><cell>HOG-</cell></row><row><cell></cell><cell>based [4]</cell><cell>[8]</cell><cell>[21]</cell><cell>[33]</cell><cell>LBP</cell><cell>LBP</cell><cell>LBP</cell><cell>LBP</cell></row><row><cell>Africa</cell><cell>0.48</cell><cell>0.23</cell><cell>0.55</cell><cell>0.61</cell><cell>0.54</cell><cell>0.57</cell><cell>0.55</cell><cell>0.29</cell></row><row><cell>Beaches</cell><cell>0.34</cell><cell>0.23</cell><cell>0.47</cell><cell>0.49</cell><cell>0.39</cell><cell>0.58</cell><cell>0.47</cell><cell>0.43</cell></row><row><cell cols="2">Building 0.36</cell><cell>0.23</cell><cell>0.44</cell><cell>0.46</cell><cell>0.45</cell><cell>0.43</cell><cell>0.56</cell><cell>0.30</cell></row><row><cell>Bus</cell><cell>0.61</cell><cell>0.23</cell><cell>0.93</cell><cell>0.93</cell><cell>0.80</cell><cell>0.93</cell><cell>0.91</cell><cell>0.66</cell></row><row><cell cols="2">Dinosaur 0.95</cell><cell>0.23</cell><cell>0.98</cell><cell>0.99</cell><cell>0.93</cell><cell>0.98</cell><cell>0.94</cell><cell>0.97</cell></row><row><cell cols="2">Elephant 0.48</cell><cell>0.23</cell><cell>0.52</cell><cell>0.58</cell><cell>0.30</cell><cell>0.58</cell><cell>0.49</cell><cell>0.36</cell></row><row><cell>Flower</cell><cell>0.61</cell><cell>0.23</cell><cell>0.77</cell><cell>0.83</cell><cell>0.79</cell><cell>0.83</cell><cell>0.85</cell><cell>0.52</cell></row><row><cell>Horses</cell><cell>0.74</cell><cell>0.23</cell><cell>0.65</cell><cell>0.65</cell><cell>0.54</cell><cell>0.68</cell><cell>0.52</cell><cell>0.55</cell></row><row><cell cols="2">Mountain 0.42</cell><cell>0.23</cell><cell>0.34</cell><cell>0.36</cell><cell>0.35</cell><cell>0.46</cell><cell>0.37</cell><cell>0.29</cell></row><row><cell>Food</cell><cell>0.50</cell><cell>0.23</cell><cell>0.52</cell><cell>0.51</cell><cell>0.52</cell><cell>0.53</cell><cell>0.55</cell><cell>0.22</cell></row><row><cell>Total</cell><cell>0.549</cell><cell cols="5">0.230 0.617 0.641 0.561 0.657</cell><cell cols="2">0.621 0.459</cell></row><row><cell>ARP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Yu et al. / Neurocomputing 120 (2013) 355-364</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially funded by the NCET Program of MOE, China, the SRF for ROCS and the China Scholar Council.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tao Wan is working as a researcher associate at the Case Western Reserve University. She was postdoctoral associate at Boston University's Section of Radiology in the School of Medicine. She received her Master degree in Global Computing and Multimedia from the University of Bristol, UK in 2004 and her Ph.D. in Computer Science from the same university in 2009. She spent one year working as a senior researcher in the Samsung Advanced Institute of Technology (SAIT) China before becoming a visiting scholar in the Visualization and Image Analysis Lab in the Robotics Institute, Carnegie Mellon University. Her research interests are statistical models for image segmentation, fusion, and denoising, machine learning, computer-aided diagnosis system, medical image analysis on prostate and breast cancer.</p><p>Zhang Xi was born in Langfang, Hebei Province, in 1990. She was enrolled by Beihang University, Beijing, China, in 2009. Currently she is a senior student in School of Automation Science and Electronic Engineering, Beihang University. Her research interests include image retrieval, guidance, optimal control system and fuzzy control system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scale invariant image matching using triplewise constraint and weighted voting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="64" to="71" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image retrieval: ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A SIFT-LBP image retrieval model based on bagof-features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1161" to="1164" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Content based image retrieval using color, texture and shape features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pujari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Hiremath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Advanced Computing and Communications</title>
		<meeting>the International Conference on Advanced Computing and Communications</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="780" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new technique for summarizing video sequences through histogram evolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPCOM</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust CoHOG feature extraction in human-centered image/video management system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybernet. Part B: Cybernet</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="458" to="468" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture features corresponding to visual perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybernet</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Block-based methods for image retrieval using local binary patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Takala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Scandinavian Conference on Image Analysis</title>
		<meeting>the 14th Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PCA-SIFT: a more distinctive representation for local image descriptors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 85</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SURF: speeded-up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully affine invariant SURF for image matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient HOG human detection, Signal Process</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="773" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiresolution gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Description of interest regions with local binary patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image matching algorithm based on combination of SIFT and rotation invariant LBP</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computer-Aided Design Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="286" to="292" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Huam detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Streicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
		<title level="m">Proceedings of the Fifth International Symposium on Advances in Visual Computing: Part I</title>
		<meeting>the Fifth International Symposium on Advances in Visual Computing: Part I</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
	<note>A bag of features approach for 3D shape retrieval</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="604" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ranking answers by hierarchical topic models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEA/AIE LNCS</title>
		<meeting>IEA/AIE LNCS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5579</biblScope>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Descriptive visual words and visual phrases for image applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="257" to="263" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Detecting pedestrians by learning shapelet features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sabzmeydani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SIMPLIcity: semantics-sensitive integrated matching for picture libraries</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="947" to="963" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="〈http://wang.ist.psu.edu/〉" />
		<title level="m">Group: Modeling Objects, Concepts, and Aesthetics in Images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel circular ring histogram for content-based image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETCS</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">She used to work as an intern in Samsung Electronics (China) R&amp;D Center and Microsoft Research Asia. Her research interests include multimedia information retrieval, computer vision and digital image processing</title>
	</analytic>
	<monogr>
		<title level="m">She is currently working toward her M.S. degree in Pattern Recognition and Intelligent Systems from the School of Automation Science and Electrical Engineering</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Jing Yu received her B.S. degree in Automation Science from MinZu University ; Beihang University ; BT, Optimor Labs and worked as a visiting scholar in University of Oxford and Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests are agent-based modeling, machine learning, computational intelligence and multimedia retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
