<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble Incremental Learning Random Vector Functional Link Network for Short-term Electric Load Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-30">January 30, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueheng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nagaratnam</forename><surname>Ponnuthurai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gehan</forename><forename type="middle">A J</forename><surname>Suganthan</surname></persName>
						</author>
						<author>
							<persName><surname>Amaratunga</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nagaratnam</forename><surname>Suganthan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gehan</forename><forename type="middle">A J</forename><surname>Amaratunga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1TN</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ponnuthurai Nagaratnam Suganthan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ensemble Incremental Learning Random Vector Functional Link Network for Short-term Electric Load Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-30">January 30, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">2C0C913F157E80C934AF2FAC3B5E23EE</idno>
					<idno type="DOI">10.1016/j.knosys.2018.01.015</idno>
					<note type="submission">Received date: 1 August 2017 Revised date: 15 January 2018 Accepted date: 18 January 2018 Preprint submitted to Knowledge-Based Systems</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge-Based Systems Empirical mode decomposition</term>
					<term>Discrete wavelet transform</term>
					<term>Random vector functional link network</term>
					<term>Incremental learning</term>
					<term>Time series forecasting</term>
					<term>Electric load forecasting</term>
					<term>Neural networks</term>
					<term>Random forests</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Short-term electric load forecasting plays an important role in the management of modern power systems. Improving the accuracy and efficiency of electric load forecasting can help power utilities design reasonable operational planning which will lead to the improvement of economic and social benefits of the systems. A hybrid incremental learning approach composed of Discrete Wavelet Transform (DWT), Empirical Mode Decomposition (EMD) and Random Vector Functional Link network (RVFL) is presented in this work. RVFL network is a universal approximator with good efficiency because of the randomly generated weights between input and hidden layers and the close form solution for parameter computation. By introducing incremental learning, along with ensemble approach via DWT and EMD into RVFL network, the forecasting performance can be significantly improved with respect to both efficiency and accuracy. The electric load</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• An ensemble incremental learning method is proposed for electric load forecasting.</p><p>• DWT and EMD are sequentially combined to decompose load time series signal.</p><p>• Each sub-signal is modelled by an incremental learning RVFL network.</p><p>• Temperature data is used to improve the performance of electric load forecasting.</p><p>• The proposed method has advantages based on both accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is well known that electricity power supply planning plays an important role in the management of modern power system. Accurate forecasting is beneficial for unit commitment, power system security, as well as energy transfer scheduling <ref type="bibr" target="#b0">[1]</ref>.</p><p>As stated in <ref type="bibr" target="#b1">[2]</ref>, for short-term electric load forecasting, the ballpark saving from 1% reduction in forecast error for a utility with 1GW peak is roughly $300, 000 per year. Therefore, the goal of load forecasting can be concluded as providing reliable power supply while keeping the operation costs and energy wastage as low as possible. Electric load forecasting belongs to time series (TS) forecasting paradigm, which can be categorized into three types based on the forecasting horizon: shortterm (minutes to one day ahead), medium-term (weeks to months ahead), longterm (years ahead) <ref type="bibr" target="#b2">[3]</ref>. In this paper, we mainly focus on one day ahead load forecasting, which belongs to short-term load forecasting category. Due to various exogenous factors (e.g. special occasions, weather conditions, economic fluctuations, etc.), load data often possesses highly nonlinear and complicated patterns, which causes electric load forecasting a difficult task <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Since the 1940s, various statistical based linear time series forecasting approaches have been proposed. The common goal of these linear models is to use time series analysis for extrapolating the future energy requirement. The most successful methods are based on Holt-Winters exponential smoothing <ref type="bibr" target="#b4">[5]</ref> and Autoregressive Integrated Moving Average (ARIMA) <ref type="bibr" target="#b5">[6]</ref>, as well as Linear Regression <ref type="bibr" target="#b6">[7]</ref>. These statistical learning models are often treated as the baseline for electric load forecasting research nowadays [8, <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For example, Fan and Hyndman <ref type="bibr" target="#b10">[11]</ref> proposed a semi-parametric additive model for short term load forecasting, which is in the regression framework but with some nonlinear relationships and with serially correlated errors.</p><p>Artificial neural network (ANN) has been a popular machine learning approach with universal approximation capability for both classification and regression problems in a variety of research fields, such as control, biomedical, manufacturing, and power system management, etc <ref type="bibr" target="#b11">[12]</ref>. For example, in <ref type="bibr" target="#b6">[7]</ref>, Hong provided a comprehensive study on short term load forecasting using ANNs, as well as regression analysis and fuzzy regression models. One of the most popular way to train the ANN is the back-propagation (BP) supervised learning algorithm <ref type="bibr" target="#b12">[13]</ref>. However, the BP algorithm has several drawbacks, such as slow convergence and often being trapped in a local minimum <ref type="bibr" target="#b13">[14]</ref>.</p><p>A randomised version of neural network was reported in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, which has random weight assignment and functional link between input and output layers. This variant of neural network is named Random Vector Functional Link (RVFL) network, which improves the efficiency of neural network training by randomly generating the weights from the input layer to hidden layer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Another independently developed method, single hidden layer neural network with random</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T weights (RWSLFN), was reported in <ref type="bibr" target="#b17">[18]</ref>, which is different from RVFL by excluding the functional link. However, some research works have proved that the functional link can significantly improve the performance of RVFL, in particular for time series forecasting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Ensemble learning methods, which are also known as hybrid methods, aim to further improve the performance of forecasting methods by strategically combining multiple algorithms. There are three fundamental reasons for advantages of ensemble methods: statistical, computational and representational <ref type="bibr" target="#b19">[20]</ref>. According to the way of combination, ensemble learning can be classified into two categories: parallel and sequential <ref type="bibr" target="#b20">[21]</ref>. In a parallel combined ensemble method, the training signal is first decomposed into a collection of sub-datasets, which are analyzed individually and combined to generate the final forecasting result; while for sequentially combined ensemble methods, the outputs from one model are treated as the inputs for another forecasting model. There are many parallel ensemble methods in the literature, such as wavelet decomposition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and Empirical Mode Decomposition (EMD) <ref type="bibr" target="#b24">[25]</ref>.</p><p>Incremental learning, or online learning, is a machine learning paradigm where the machine learning model is updated according to the new examples whenever they emerge <ref type="bibr" target="#b25">[26]</ref>. Therefore, incremental learning is different from traditional machine learning in the requirement of training dataset. Incremental learning does not totally depend on a sufficient training set in the training phase, but always update its model as new training examples appear over time. In fact, incremental learning is part of the natural learning and quite common in reality. For example, in <ref type="bibr" target="#b26">[27]</ref>, a recurrent neural network was constructed for grammar learning, and the author found that "the network fails to learn the task when the entire data</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>set is presented all at once, but succeeds when the data are presented incrementally". For electric load forecasting, electricity loads can be seen as a stream of incoming data, thereby it is necessary to focus on adaptive methods that are able to learn incrementally. Several incremental learning methods have been proposed for electric load forecasting in the literature. For example, Gabriela Grmanová et al. proposed an incremental heterogeneous ensemble model for time series prediction <ref type="bibr" target="#b27">[28]</ref>. Moreover, in <ref type="bibr" target="#b28">[29]</ref>, an incremental electric load forecasting model based on support vector regression was introduced. "Concept drift", which means that the statistical properties of the target variable change over time in unforeseen ways, is one of the prominent problems and challenges for incremental learning <ref type="bibr" target="#b29">[30]</ref>. It can be gradual or abrupt. Mere concept shift is often addressed by so-called passive methods, i.e. learning technologies which smoothly adapt model parameters such that the current distribution is reliably represented by the model. On the other hand, rapid concept changes often require active methods, which detect concept drift and react accordingly.</p><p>In this work, load datasets from five states of Australia are used for simulations, which have relatively stable patterns due to the state-wide area, and thus only suffer from mere concept shift. Therefore, in incremental learning phase of the proposed method, if the exogenous factors change, the incoming input data will reflect the changes and help updating our model, which belongs to passive methods category.</p><p>There are many EMD based ensemble methods proposed for various research fields in the literature. For example, in [31], EMD and its improved versions are hybridized with SVR and ANN to tackle wind speed forecasting problems.</p><p>According to the experimental results, it is concluded that EMD based models</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>generally perform better than the corresponding single learning models. In our previous wor <ref type="bibr">[32]</ref>, an ensemble deep learning approach is proposed for electric load forecasting, which combines EMD and Deep Belief Networks. Some other EMD based ensemble forecasting models include: EMD-gene expression programming (GEP) method for electric load forecasting [33], EMD-SVR for wind power prediction <ref type="bibr">[34]</ref>, and EMD-RF for acute hypotensive episode prediction <ref type="bibr">[35]</ref>. Wavelet transform is also widely applied for time series forecasting such as wavelet-ARIMA [36] and wavelet-particle swarm optimization (PSO)-adaptive neuro-fuzzy inference system (ANFIS) <ref type="bibr">[37]</ref>. Interested readers are recommended to read the survey paper of ensemble methods <ref type="bibr" target="#b20">[21]</ref>.</p><p>Motivated by the good performance of EMD and wavelet transform based ensemble forecasting models mentioned above, this work presents a novel decomposition method for electric load TS signal, which combines DWT and EMD sequentially. EMD has a major drawback named mode mixing problem, whereas DWT can decompose TS signal into its frequency components. Therefore, the proposed EMD-DWT decomposition method can perform a better decomposition by using DWT to solve the frequency mixing problem of EMD <ref type="bibr" target="#b30">[38]</ref>. Based on this decomposition method, an incremental ensemble learning approach is proposed for electric load forecasting, which is composed of DWT and EMD, along with incremental RVFL network. The proposed incremental DWT-EMD based RVFL network outperforms all of the eight benchmark methods. The effects of functional links, number of hidden neurons and incremental learning of RVFL are also discussed with persuasive comparative experiments.</p><p>The remaining part of this paper is organized as follows. Section 2 explains the theoretical background on forecasting models and ensemble approaches. Sec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T tion 3 presents the algorithm of proposed incremental DWT-EMD based RVFL network. The details of experimental setup are shown in Section 4, followed by the discussion on experiment results in Section 5. In Section 6, two comparative experiments are implemented to evaluate the performance of the proposed method. Finally, the conclusion and future works are stated in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Artificial Neural Network</head><p>ANN, as a machine learning model inspired by human brain and central nervous system <ref type="bibr" target="#b31">[39]</ref>, can be classified into two categories according to the connectionism of network structure: feedforward neural network (FNN) and recurrent neural network (RNN). Single hidden layer feedforward neural network (SLFN) is the simplest version of the FNN, which has three fundamental layers: an input layer with the same number of neurons as the dimension of input features; a hidden layer having neurons with nonlinear activation function; and an output layer which aggregates the outputs from the hidden layer neurons. The direct connections between neurons in the adjacent layers have adjustable weights, which represent the strengths of these links. However, there is no interconnection of neurons within the same layer. The outputs from hidden neurons are calculated by:</p><formula xml:id="formula_2">v j = f ( n i=1 w ij x i + b i )<label>(1)</label></formula><p>and the final output value is:</p><formula xml:id="formula_3">y = g( h j=1 w jo v j + b j )<label>(2)</label></formula><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>where f (•) and g(•) are nonlinear activation functions; v j is the output of hidden layer neuron j; x i is the input to the neuron; y is the output of this SLFN; n and h are the number of input features and the number of the hidden layer neurons, respectively; w ij is the weight of the connection between the input variable i and the neuron j of the hidden layer; w jo is the weight of the connection between the hidden layer neuron j and the output; b i and b j are the biases.</p><p>The weights in SLFN (w ij and w jo ) are traditionally tuned by the back propagation (BP) algorithm. As we have mentioned above, BP-based methods are usually very time consuming due to the iteration of BP. Moreover the gradient descent is often trapped in a local minimum. These disadvantages often limit the performance of SLFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Random Vector Functional Link Network</head><p>RVFL network is a randomized version of the FNN, which has direct connections between input and output neurons (functional link), and uses fixed random weights and closed form least square estimation instead of the BP to tune the weights <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>It is worth noting that all hidden layer weights W h in an RVFL are generated with uniformly distributed random values within the interval [-S,+S], where S is a scale factor to be determined during the parameter tuning stage <ref type="bibr" target="#b18">[19]</ref>. Therefore, the output H from hidden neurons can be calculated based on the activation function. Here logistic sigmoid function is used as an example:</p><formula xml:id="formula_5">H = logsig(W h • X) (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where X is the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The output layer weights W o need to be determined by certain optimization method. Due to the efficiency of closed-form solution, RVFL employs least square estimation to calculate the output layer weights:</p><formula xml:id="formula_7">W o = (H T H) -1 H T Y (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where Y is the training target.</p><p>The predicted values can thus be calculated by applying the obtained W o and W h to testing data:</p><formula xml:id="formula_9">Ŷs = W o • logsig(W h • X s ) (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where Ŷs is the predicted testing values and X s is the testing data <ref type="bibr" target="#b32">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Empirical Mode Decomposition</head><p>EMD <ref type="bibr" target="#b24">[25]</ref>, also known as Hilbert-Huang transform (HHT), is a method to decompose a signal into several intrinsic mode functions (IMF) along with a residue which stands for the trend. EMD is an empirical approach to obtain instantaneous frequency data from non-stationary and nonlinear data sets.</p><p>The system load is a random non-stationary process composed of thousands of individual components. Thus, EMD algorithm can be very effective for load</p><p>forecasting. An IMF is a function that has only one extreme between zero crossings, along with a mean value of zero. After EMD, the original time series signal is decomposed into a set of functions:</p><formula xml:id="formula_11">x(t) = n i=1 (c i ) + r n</formula><p>, where c i is the i th IMF, r n is the residue, and the number of functions n in the set depends on the original signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Wavelet Transform</head><p>Wavelet transform is a popular mathematical tool to decompose TS signal into its frequency components, which are beneficial for signal processing and analy-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>sis <ref type="bibr" target="#b33">[41]</ref>. Wavelet transform is similar to the Fourier transform with a main difference in the merit function. The wavelet transform uses a collection of wavelet functions to represent the original signal, while traditional Fourier transform decomposes the signal into sines and cosines <ref type="bibr" target="#b34">[42]</ref>.</p><p>Maximal Overlap Discrete Wavelet Transform (MODWT) <ref type="bibr" target="#b35">[43]</ref>, as a variant of DWT, applys low-pass and high-pass filters to the input signal at each level, which has some advantages over standard DWT. The main difference between MODWT and traditional DWT is that MODWT is highly redundant and non-orthogonal.</p><p>The redundancy increases the effective degrees of freedom (EDOF) on each scale and thus decreases the variance of statistical estimates, which allows better comparison of TS signal with its decomposition <ref type="bibr" target="#b36">[44]</ref>. Moreover, the MODWT does not decimate the coefficients in order to keep the number of wavelet and scaling coefficients the same as the number of input data samples at every level of the transform. Therefore, the MODWT is well-defined for all sample sizes N .</p><p>However, for a complete decomposition of J levels, the DWT requires N to be a multiple of 2 J [42, 44].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Incremental Learning with RVFL</head><p>Incremental learning RVFL is suitable for real time applications since the learning model needs to be updated whenever the new input patterns are available. As introduced in <ref type="bibr" target="#b37">[45]</ref>, by taking the pseudoinverse of a partitioned matrix, the stepwise updating of the weight in RVFL can be achieved easily due to the advantages of flat structure of RVFL.</p><p>Denote A n as the n×m pattern matrix representing the input matrix consisting of all input vectors combined with enhancement components. a is the m × 1 new sample pattern which should be added to update the RVFL. Thus, the new pattern</p><formula xml:id="formula_12">A C C E P T E D M A N U S C R I P T matrix A n+1 is shown as: A n+1    A n a    (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where n is the number of samples, as well as the discrete time instance, m is the dimension of the input pattern. Therefore, the pseudoinverse of A n+1 can be updated based only on the pseudoinverse of A n and the imported new input vector a . The precedure of calculation is shown as follows:</p><formula xml:id="formula_14">A + n+1 = [A + n -db |d]<label>(7)</label></formula><p>where</p><formula xml:id="formula_15">b = a A + n d = (1 + b b) -1 A + n b<label>(8)</label></formula><p>Same with the pattern matrix A n , we denote the output vector as Y n , weight matrix as W n , and the new output as y . According to the least square solution,</p><p>we can obtain:</p><formula xml:id="formula_16">W n+1 = A + n+1 Y n+1 W n = A + n Y n<label>(9)</label></formula><p>Therefore, the new weight matrix W n+1 can be calculated by:</p><formula xml:id="formula_17">W n+1 = W n + (y -a W n )d (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where d is the obtained optimal learning rate of weight updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed incremental RVFL based ensemble method</head><p>"Divide and conquer" is a parallel ensemble method which works by decomposing the original TS signal into a collection of sub-series until they are simple</p><formula xml:id="formula_19">A C C E P T E D M A N U S C R I P T</formula><p>enough to be solved directly. EMD, as an efficient ensemble method to perform TS signal decomposition, has a major drawback called mode mixing problem:</p><p>one IMF may consist of signal spanning a wide band of frequency, or more than one IMFs contain signals in a similar frequency band <ref type="bibr" target="#b30">[38]</ref>. In the literature, this problem is normally solved by ensemble of EMD (EEMD) <ref type="bibr" target="#b38">[46]</ref>, which works by applying EMD to uncorrelated Gaussian noise added TS signal repetitively and combining the results to remove the noise. In this paper, the MODWT is employed to deal with the frequency issue, followed by EMD to perform better decomposition. Then an RVFL network is trained for each IMF and residue. The outputs of all sub-series are combined and analyzed by another RVFL to formulate the final prediction results. Figure <ref type="figure" target="#fig_1">1</ref> is the schematic diagram of the pre-training phase of this proposed ensemble method, whose procedures can be concluded as follows:</p><p>1. Apply MODWT to decompose the original TS into several frequency components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Apply EMD to decompose each frequency component into several IMFs</head><p>and one residue. Then an incremental RVFL is trained using this new training matrix to formulate the final prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Incremental learning allows the learning model updating itself whenever the new input patterns are available, which guarantee the effectiveness of the proposed method for long term application. Therefore, after the pre-training phase, the learned ensemble model including the pseudoinverse of A n and the weight matrix W n for the incremental RVFL can be updated by the incremental learning phase:</p><p>1. When a new input sample is given to the network, MODWT and EMD can be applied to decompose the signal and obtain new input pattern A n+1 by combining all the new outputs from all sub-series. Then the new weight matrix W n+1 can be updated using equation 10.</p><p>2. The validation data is applied to check the error level. If the error decreases, we keep the updates, otherwise the weight matrix is not changed.</p><p>3. Repeat steps 1 and 2 whenever new samples are presented to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In this paper, the electric load datasets from Australian Energy Market Operator (AEMO) <ref type="bibr" target="#b39">[47]</ref> were used for evaluating the performance of benchmark learning models. There are totally fifteen electric load datasets of year 2013 ∼ 2015 from five states of Australia: New South Wales (NSW), Tasmania (TAS), Queensland (QLD), South Australia (SA) and Victoria (VIC). For each dataset, the first nine months were used for training, while the remaining three months were used for testing. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The electric load data is sampled every half an hour, therefore 48 data points are recorded for one day. To identify cycles and patterns in load TS, we employ</p><formula xml:id="formula_20">A C C E P T E D M A N U S C R I P T Time Series Data DWT W 1 W 2 • • • • • • W m EM D EM D • • • • • • EM D IM F 1 1 • • • IM F 1 n R 1 • • • IM F m 1 • • • IM F m n R m RV F L 1 1 • • • RV F L 1 n RV F L 1 n+1 • • • RV F L m 1 • • • RV F L m n RV F L m n+1 Output 1 1 • • • Output 1 n Output 1 n+1 • • • Output m 1 • • • Output m n Output m n+1</formula><p>Temperature data Incremental RVFL Original TS </p><note type="other">Prediction Results</note><formula xml:id="formula_21">r k = r(X t , X t-k ) = n t=k+1 (X t -X)(X t-k -X) n t=1 (X t -X) 2<label>(11)</label></formula><p>where X is the mean value of all X in the given time series, r k measures the linear correlation of the time series at times t and tk.</p><p>From Figure <ref type="figure" target="#fig_2">2</ref>, which shows the ACF for electricity load data with a time window of one week, three strongest dependent lag variables can be identified: the value of previous half-an-hour (X t-1 ), the value at the same time in the previous week (X t-336 ), as well as the value at the same time in the previous day (X t-48 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Therefore, in order to take all the most informative lag variables into consideration, we select the data points of the whole previous day (X t-48 to X t-96 ) and the same day in the previous week (X t-336 to X t-384 ) to construct the input feature set for one day ahead load forecasting in this work.</p><p>After DWT-EMD decomposition, we get m × n IMFs and m residues. Let's mark the IMFs and residues as X i,j , where 1 i m, 1 j n + 1. For each IMF and residue, when we want to predict the value at time t (marked as X i,j t ), the corresponding input features include the data points of the whole previous day (X i,j t-48 to X i,j t-96 ) and the same day in the previous week (X i,j t-336 to X i,j t-384 ). This is the content of the input matrix for the RVFLs analyzing the sub-series. As a result, the predicted values of all the sub-series for time t are:</p><formula xml:id="formula_22">{ Xi,j t : 1 i m, 1 j n + 1}<label>(12)</label></formula><p>As mentioned above, to construct the input matrix for the incremental RVFL, we combine the predicted values with original input signal (X t-48 to X t-96 , and X t-336 to X t-384 ), and the recently observed temperature data (T t-1 ).</p><p>Temperature data of above five states in Australia is also considered for electric load forecasting in this study, which is provided by Australian Bureau of Meteorology <ref type="bibr" target="#b40">[48]</ref>. Specifically, the temperature data from high demand areas in each state is considered: Sydney and Canberra for NSW, Horbat and Launceston airport for TAS, Brisbane for QLD, Adelaide for SA, and Melbourne for VIC. The daily maximum temperature and daily minimum temperature datasets are used as additional features to help improve the performance of the proposed electric load forecasting method. The influence of temperature data will be discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Variations of RVFL network</head><p>There are eight different RVFL network configurations by varying the network components: input layer and hidden layer biases, along with input-output connections (functional links). In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, the authors compared the performance of all the variations of RVFL network for both regression and classification problems. According to the conclusions made, the direct input-output connections can improve the performance of RVFL network significantly. Moreover, although the input and hidden layer biases have little influence on the performance, it is still necessary to retain biases to ensure the neural networks function properly as a universal approximator. Hence, in this work, the RVFL network with di- rect input-output connections and input/hidden layer biases was selected for our proposed EMD-RVFL network. Moreover, RWSLFN (the RVFL variant without functional links) is also included to perform comparison and verify the results reported in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><formula xml:id="formula_23">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Methodology</head><p>For the time series load datasets, all the training and testing values are linearly scaled to [0, 1]. The scaling is done separately and independently for each year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The scaling formula is: ȳi = y maxy i y maxy min <ref type="bibr" target="#b12">(13)</ref> where y i is the value of the i th point of the dataset, ȳi is the corresponding scaled value, and y max and y min are the maximum and minimum values of the dataset, respectively.</p><p>To implement the simulation, the deep learning toolbox was used for neural networks, including SLFN and EMD based SLFN (EMD-SLFN) <ref type="bibr" target="#b41">[49]</ref>. RF and EMD-RF were developed from the function "TreeBagger" in Matlab. We set the parameter "NumPredictorsToSample" as one third of the number of input features to invoke RF algorithm. RVFL, EMD-RVFL and the proposed incremental DWT-EMD-RVFL were developed by the authors in Matlab based on the work in <ref type="bibr" target="#b18">[19]</ref>.</p><p>For SLFN and EMD-SLFN, the size of neural networks is determined by the size of input vector. Based on the experience in our previous work <ref type="bibr">[32]</ref>, the number of iterations for back propagation is set as 500 to avoid overfitting. For RF and EMD based RF, because of the same reasons as above, the number of decision trees is set as 500. For RVFL, EMD-RVFL and the proposed method, according to suggestion in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, the randomization used a uniform distribution in [-1, 1],</p><p>the number of hidden neurons is selected over 1000 : 10000 with a step-size of 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Error Measurement</head><p>There are two error measures being used to evaluate the performance of learning models in this work: Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE), which are defined as:</p><formula xml:id="formula_24">A C C E P T E D M A N U S C R I P T RM SE = 1 n n i=1 (y i -y i ) 2 M AP E = 1 n n i=1 y i -y i y i<label>(14)</label></formula><p>where y i is the predicted value of corresponding y i , and n is the number of data points in the testing TS dataset.</p><p>Except for above traditional error measures, readers are also suggested to try some new error measures. For example, in <ref type="bibr" target="#b42">[50]</ref>, a new error measure is proposed based on finding a restricted permutation of the original forecast that minimizes the point-wise error, which may be able to reduce the so-called "double penalty" effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of Functional Links and Number of Hidden Neurons</head><p>We focus on the effect of the direct connections in RVFL network in this section. In order to make a reasonable comparison, the other issues (e.g. parameters, activation functions, range of randomization, etc.) in RVFL with and without functional links (RWSLFN) were set the same, except for the number of hidden neurons. Table <ref type="table" target="#tab_1">2</ref> shows the prediction results using RVFL and RWSLFN with different number of hidden neurons. The row "win-tie-lose" in the bottom means the number of times that RVFL wins, ties, and loses to RWSLFN, respectively.</p><p>Several observations can be concluded by this comparison:</p><p>1. The number of hidden neurons have important influence on the performance of RVFL variants. Sufficient number of hidden neurons can ensure the completeness of information obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>2. The superiority of functional links can also be observed, which may due to the reason that the direct links can serve as a regularization for the randomization <ref type="bibr" target="#b16">[17]</ref>.</p><p>3. However, the advantage of functional links decreases as the number of hidden neurons increasing, which means that the functional links and number of hidden neurons complement each other for information gaining.</p><p>Therefore, taking model complexity into consideration, RVFL with direct links and reasonable number of hidden neurons is recommended.</p><p>In this study, several hundreds or thousands hidden neurons are sufficient for load forecasting. However, for practical application, the number of hidden neurons should be chosen based on the characteristics of different datasets, as well as the requirement of accuracy and efficiency. We recommend readers try different RVFL models with various numbers of hidden neurons, and choose the most suitable one according to the actual requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Incremental Learning</head><p>In this part, we concentrate on the effectiveness of incremental learning. As we have mentioned above, for each dataset, the first nine months are used for training, and the last three months are used for testing. For incremental learning, the data points in testing subset are imported to the network one by one, thereby stepwise updating the model obtained in training phase. Meanwhile, for nonincremental learning, the model is fixed during testing phase. The performance comparison of incremental and non-incremental learning is shown in Table <ref type="table" target="#tab_2">3</ref>. All the models have 100 hidden neurons with functional links. Same with section 5.1, the row "win-tie-lose" in the bottom means the number of times that incremental</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T learning wins, ties, and loses to non-incremental learning, respectively. From the comparison results "30 -0 -0", we can conclude that incremental learning is definitely beneficial for short term electric load TS forecasting with RVFL and its ensemble models. Moreover, it also worth noting that the proposed DWT-EMD-RVFL outperforms RVFL and EMD-RVFL in every case. The statistical testing is conducted and discussed in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance Comparison with Benchmarks</head><p>In this section, the performance of the proposed incremental DWT-EMD-RVFL approach is evaluated by comparing with several benchmark methods. First of all, the persistence method, which is the simplest forecasting method, is employed as the baseline for comparing the performance of learning models in this work. For persistence method, the load value at the same hour of last day is used as the forecast for each of the 24 hours of next day, which works well because of the highly periodic characteristic of electric load TS. Moreover, a modified version of another benchmark method GLMLF-B (General Linear Model based Load Forecaster -Benchmark) <ref type="bibr" target="#b43">[51]</ref> is also employed, which can offer a higher baseline for the other machine learning algorithms. In this study, since the forecasting horizon is one day (or 48 steps), to predict X t , we use the corresponding hour, day, month, as well as the load value X t-48 at the same time in the previous day, as input features to construct a multiple linear regression (MLR) model, which shares similar ideas with GLMLF-B. Except the persistence method, all the other benchmark models have made use of the temperature data, which can improve the performance of forecasting.</p><p>The prediction results for one-day-ahead electric load forecasting are shown in Table <ref type="table" target="#tab_3">4</ref>. The numbers in bold mean that the corresponding method achieves</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the best performance for this dataset under this performance measurement. The prediction results generated by the proposed method without temperature data are also recorded, which can be compared with the results from the proposed method with temperature data. From Table <ref type="table" target="#tab_3">4</ref>, we can clearly see that the proposed method achieves the best performance for every case except QLD. The MAPE values of the load prediction for QLD are significantly lower than the ones for all the other regions. This phenomenon proves the fact that the pattern of the load data of QLD is much more stable and simpler than the patterns of the load data of other regions, which is easier for the benchmarks to analyze. Therefore, our proposed method cannot show much advantage on the QLD datasets.</p><p>Moreover, statistical tests are employed to give a detail analysis about the performance differences among all the learning models. The Friedman test ranks the algorithms for each dataset separately, and then assign average ranks in case of ties. The null-hypothesis states that all the algorithms have the same performance.</p><p>If the null-hypothesis is rejected, in order to tell whether the performances of two among totally k learning models are significantly different, the Nemenyi post-hoc test is applied to compare all the learning models with each other. The comparison results of statistical test based on RMSE and MAPE are shown in Figure <ref type="figure" target="#fig_0">3</ref> and Figure <ref type="figure" target="#fig_6">4</ref>, respectively. The methods with better ranks are at the top whereas the methods with worse ranks are at the bottom. It is worth noting that the models within a vertical line whose length is less than or equal to a critical distance have statistically the same performance. The critical distance for Nemenyi test is defined as:</p><formula xml:id="formula_25">CD = q α k(k + 1) 6N (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>where k is the number of algorithms (k = 9 in this experiment), N is the</p><formula xml:id="formula_27">A C C E P T E D M A N U S C R I P T</formula><p>number of data sets (N = 15 here), and q α is the critical value based on the studentized range statistic divided by √ 2 <ref type="bibr" target="#b44">[52]</ref>.  From the results of simulations and statistical tests, several conclusions can be made:</p><p>1. The original load TS data was modeled by SLFN, RF and RVFL without decomposition. Therefore, the advantages of EMD based ensemble methods can be revealed by performing comparisons.  3. The proposed incremental DWT-EMD based RVFL approach achieves the best rank and significantly outperform the non-EMD based benchmarks and EMD-RF with a 95% confidence.</p><p>In order to find where in the forecast the proposed method offers a key advantage in performance, a comparison between the forecasting results for original  From the comparison results, we can conclude that the key improvements caused by the proposed method are located on the data points during the weekends. In fact, the difference between the electric load in weekdays and weekends is one of the dominant challenges for prediction methods. Some published works, such as <ref type="bibr" target="#b22">[23]</ref>, deal with this problem by introducing additional input features of calendar information (e.g. holidays, weekends, etc.). However, in this work, under the help of decomposition algorithms DWT and EMD, as well as the incremental learning, the proposed method can detect the pattern changes caused by weekends,  EMD method performs better than EMD only is that MODWT may help solve the mode mixing problem of EMD. To illustrate our conjecture, we use the electric load data from QLD of the year 2014 as an example. To analyze the frequency spectrum in each sub-signal, Fast Fourier Transform (FFT) is applied to all the IMFs and residue as shown in Figure <ref type="figure">7</ref>. Obviously, the first IMF contains the widest band of frequency, which is hard to predict. In fact, in the literature, some works just treat the first IMF component as the noise and simply ignore it. However, in this work, we employ MODWT to help reduce the frequency range in each IMF. Figure <ref type="figure">8</ref> shows the obtained wavelet coefficients and scaling coefficients for the same electric load data. Then we apply EMD on each wavelet based</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>sub-signal and check the frequency spectrum for the first IMF using FFT. Comparing Figure <ref type="figure" target="#fig_14">9</ref> and the first sub-figure in Figure <ref type="figure">7</ref>, we can clearly see that the band of frequency for the first IMF obtained from DWT-EMD method is significantly narrower than the one obtained by EMD itself. In other words, after the proposed DWT-EMD decomposition method, the obtained sub-signals are much easier to be analyzed and modeled by learning methods compared with signals decomposed by EMD only. Therefore, the proposed DWT-EMD based ensemble learning method can achieve better performance, as demonstrated by the simulation results shown in this section.</p><p>In our previously published paper [32], the benchmarks are evaluated using the same load datasets form AEMO of the year 2013. The prediction results for one day ahead load forecasting are shown in Table <ref type="table" target="#tab_2">3</ref> in that paper. For each area, four months were chosen to reflect the factors of different seasons: January, April, July and October. To show the performance of the proposed method and the benchmark models in different seasons, the same comparison simulations are implemented in this study using load dataset from NSW of the year 2015. The results are shown in Table <ref type="table" target="#tab_4">5</ref>. From the results, by taking the factors of different seasons into consideration, we can tell that the benchmark methods performs relatively stable for the same dataset in different seasons. Moreover, in this case, our proposed method still outperforms all benchmarks models significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Computation time comparison</head><p>The computation time of benchmark methods for electric load forecasting using the datasets of year 2015 is shown in Figure <ref type="figure" target="#fig_15">10</ref>. It is easy to conclude that RVFL is much faster than ANN and RF. ANN needs to be iteratively tuned by BP algorithm to convergence to the optimal weights. RF needs to train a group </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFT for Residue</head><p>Power spectrum density </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparative Experiment</head><p>In this section, three comparative experiments were implemented to test the effectiveness of our proposed method. The comparison conditions such as dataset partition and feature selection were kept the same for the reported methods and the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparative experiment with semi-parametric additive model</head><p>In <ref type="bibr" target="#b10">[11]</ref>   <ref type="table" target="#tab_5">6</ref>, which lead to the conclusion that our proposed method has better performance compared with the benchmark models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparative experiment with dART&amp;HS-ARTMAP model</head><p>In <ref type="bibr" target="#b48">[56]</ref> ing. Table <ref type="table" target="#tab_6">7</ref> shows the forecasting results obtained by the proposed method and benchmarks in <ref type="bibr" target="#b48">[56]</ref>. Obviously, the proposed method outperforms all the benchmark methods, including Improved BP, HS-ARTMAP and dART&amp;HS-ARTMAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparative experiment with DSHW and ARIMA variates</head><p>In the second comparative experiment, the transmission level data obtained from British Columbia with an online service was used for comparison. The system has 292 substations, powering the 4.4 M inhabitants of British Columbia.</p><p>In [8], the data from 2004 throughout 2010 was used for simulation, among which the last year of the TS being divided equally between validation and testing, while the remainder of the TS being used for training. Several benchmark models were tested in [8], including double seasonal Holt-Winters (DSHW) exponential smoothing algorithm, and variations of autoregressive integrated moving average (ARIMA). In this paper, the same TS signal was analyzed by our proposed method with the same comparison conditions. From Table <ref type="table">8</ref>, which shows the comparison results, we can conclude that our proposed method outperforms the benchmark DSHW and ARIMA variates significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, a hybrid incremental learning approach is presented for short- 2. Incremental learning is beneficial for short term electricity load TS forecasting with RVFL and its ensemble models.</p><p>3. The proposed DWT-EMD based ensemble approach outperforms EMD based and single structure models.</p><p>4. The proposed incremental DWT-EMD based RVFL approach achieves the best rank and significantly outperforms the non-EMD based benchmarks and EMD-RF with a 95% confidence.</p><p>For future work, the proposed decomposition approach can be combined with various learning algorithms, such as support vector regression, kernel ridge regression, and even deep learning models. These outcome ensemble models can be tested on various applications, including renewable energy and power system related financial data. For example, electricity price data has a close relationship with electricity load data, thus can be analyzed by the learning models sharing the same concepts of the proposed method. Moreover, probabilistic load forecasting methods, which provides electric load forecasting output in the form of intervals, scenarios, density functions or probabilities, can also been developed by combining the proposed accurate point forecasting models with good classification methods.</p><p>A       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>Construct the training matrix as the input of each RVFL network for each obtained sub-series. Then train an RVFL network for each of the extracted IMF and residue. 4. Construct a new input matrix by combining three aspects: the prediction results of all the sub-series, the original TS signal and the temperature data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic Diagram of the Proposed DWT-EMD based Incremental RVFL Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Autocorrelation function for electricity load data in TAS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Friedman p-value: 1 .411e- 17 • 1 Proposed</head><label>1171</label><figDesc>Different • CritDist: 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nemenyi test for electric load forecasting based on RMSE. The critical distance is 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>By employing incremental learning, RVFL based models have comparable (or even better) performance with traditional NN and RF based models with less computation time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Nemenyi test for electric load forecasting based on MAPE. The critical distance is 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>RVFL and the proposed method was conducted. Comparisons of predicted values with actual values for the proposed method and RVFL network are shown in Figure 5 and Figure 6, respectively. This part of load data is selected from the testing dataset of NSW of the year 2013, with a time window of one week (from Sunday to Saturday).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of predicted values with actual values for the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>and modify the model by itself.As we have mentioned in Section 3, a possible reason why the proposed DWT-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of predicted values with actual values for RVFL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Frequency spectrum for each sub-signal obtained from EMD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>,FFT</head><label></label><figDesc>Fan and Hyndman proposed a semi-parametric additive model for short-term load forecasting, which has been successfully used by AEMO to forecast the short-term loads of two regions with different characteristics in the Australian National Electricity Market. Specifically, the half-hourly demand datasets from Victoria, Australia of the years from January 2004 to September 2008 were for forst IMF of final level scaling coefficient Power spectrum density</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Frequency spectrum for the first IMF of each wavelet based sub-signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Computation time of learning models for electric load forecasting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>term electric load forecasting, which composed of Discrete Wavelet Transform (DWT), Empirical Mode Decomposition (EMD) and Random Vector Functional Link (RVFL) network. Fifteen electric load datasets from AEMO were used for evaluating the performance of the proposed method by comparing with several benchmarks. Moreover, two comparative experiments were also implemented to A C C E P T E D M A N U S C R I P T verify the effectiveness of the proposed method. Based on the experiment results, the following conclusions are made: 1. Both sufficient number of hidden neurons and functional links can benefit the overall performance of RVFL networks. Taking model complexity into consideration, RVFL with direct links and reasonable number of hidden neurons is recommended.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>C C E P T E D M A N U S C R I P T Stanford University, California. He has long collaborations with industry partners in Europe, the U.S., and Asia. He is also a Co-Founder of four spin-out companies, including Nanoinstruments, which is now part of Aixtron. He is one of four Founding Advisers to the Sri Lanka Institute of Nanotechnology. He is currently the 1966 Professor of Engineering and Head of Electronics, Power, and Energy Conversion at the University of Cambridge. Since 2009, he has been a Visiting Professor in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. He has authored or coauthored more than 500 papers. He holds 25 patents. His current research interests include novel materials and device structures for nanotechnology-enhanced batteries, supercapacitors, solar cells, and power electronics for optimum grid connection of photovoltaic electricity generation systems. Dr. Amaratunga is a Fellow of the Royal Academy of Engineering. He was the recipient of awards from the Royal Academy of Engineering, the Institution of Engineering and Technology, and the Royal Society. A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of AEMO load datasets</figDesc><table><row><cell cols="2">Dataset Year Length</cell><cell>Min</cell><cell>Median Mean</cell><cell>Max</cell><cell>Std</cell></row><row><cell>QLD</cell><cell cols="4">2013 17520 4148.7 5752.1 5703.7 8278.4</cell><cell>747.0</cell></row><row><cell></cell><cell cols="4">2014 17520 4073.0 5726.0 5745.7 8445.3</cell><cell>794.0</cell></row><row><cell></cell><cell cols="4">2015 17520 4281.4 6005.6 6035.4 8808.7</cell><cell>777.2</cell></row><row><cell>NSW</cell><cell cols="5">2013 17520 5113.0 8045.0 7981.6 13788 1190.9</cell></row><row><cell></cell><cell cols="5">2014 17520 5138.1 7987.4 7917.8 11846 1170.1</cell></row><row><cell></cell><cell cols="5">2015 17520 5337.4 7990.4 7979.8 12602 1232.7</cell></row><row><cell>TAS</cell><cell>2013 17520</cell><cell cols="3">659.5 1109.0 1129.3 1650.3</cell><cell>142.3</cell></row><row><cell></cell><cell>2014 17520</cell><cell cols="3">569.1 1088.7 1109.7 1630.1</cell><cell>139.0</cell></row><row><cell></cell><cell>2015 17520</cell><cell cols="3">479.4 1112.3 1138.2 1667.2</cell><cell>145.3</cell></row><row><cell>SA</cell><cell>2013 17520</cell><cell cols="3">728.6 1389.3 1426.6 2991.3</cell><cell>301.7</cell></row><row><cell></cell><cell>2014 17520</cell><cell cols="3">682.5 1360.8 1403.3 3245.9</cell><cell>312.8</cell></row><row><cell></cell><cell>2015 17520</cell><cell cols="3">696.3 1352.7 1398.5 2870.4</cell><cell>306.0</cell></row><row><cell>VIC</cell><cell cols="4">2013 17520 3551.6 5458.1 5511.8 9587.5</cell><cell>895.9</cell></row><row><cell></cell><cell cols="4">2014 17520 3272.9 5307.8 5324.4 10240</cell><cell>921.4</cell></row><row><cell></cell><cell cols="4">2015 17520 3369.1 5186.5 5194.6 8579.9</cell><cell>864.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between RVFL variants with and without direct links</figDesc><table><row><cell>Number of hidden neurons</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between incremental and non-incremental learning. I stands for incremental, and N stands for non-incremental.</figDesc><table><row><cell>Learning Models</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Prediction results for one-day-ahead electric load forecasting</figDesc><table><row><cell>Dataset Year Metrics</cell><cell></cell><cell cols="2">Prediction model</cell><cell></cell><cell></cell></row><row><cell>Persistence GLMLF-B</cell><cell>SLFN</cell><cell>RF</cell><cell>RVFL</cell><cell>EMD-SLFN EMD-RF EMD-RVFL</cell><cell>Proposed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Prediction results for different seasons using the load data from NSW of the year 2015</figDesc><table><row><cell cols="2">Month Metrics</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction model</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Persistence GLMLF-B</cell><cell>SLFN</cell><cell>RF</cell><cell>RVFL</cell><cell cols="3">EMD-SLFN EMD-RF EMD-RVFL Proposed</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[51]</cell><cell>[39]</cell><cell>[53]</cell><cell>[15, 16]</cell><cell>[35]</cell><cell>[54]</cell><cell>[55]</cell></row><row><cell>Jan</cell><cell>RMSE</cell><cell>842.732</cell><cell>612.303</cell><cell cols="3">464.061 440.572 428.908</cell><cell>379.871</cell><cell>428.388</cell><cell>403.271</cell><cell>193.800</cell></row><row><cell></cell><cell>MAPE</cell><cell>7.393%</cell><cell>5.599%</cell><cell cols="3">4.094% 3.527% 3.869%</cell><cell>3.319%</cell><cell>3.328%</cell><cell>3.423%</cell><cell>1.857%</cell></row><row><cell>Apr</cell><cell>RMSE</cell><cell>769.606</cell><cell>525.145</cell><cell cols="3">448.827 437.975 425.228</cell><cell>400.455</cell><cell>441.052</cell><cell>411.820</cell><cell>212.703</cell></row><row><cell></cell><cell>MAPE</cell><cell>6.801%</cell><cell>5.259%</cell><cell cols="3">4.294% 3.992% 3.936%</cell><cell>4.031%</cell><cell>3.971%</cell><cell>3.861%</cell><cell>2.030%</cell></row><row><cell>Jul</cell><cell>RMSE</cell><cell>989.372</cell><cell>614.706</cell><cell cols="3">501.107 411.863 493.064</cell><cell>440.431</cell><cell>402.973</cell><cell>423.287</cell><cell>296.743</cell></row><row><cell></cell><cell>MAPE</cell><cell>9.831%</cell><cell>6.135%</cell><cell cols="3">5.145% 4.290% 5.093%</cell><cell>4.719%</cell><cell>4.144%</cell><cell>4.423%</cell><cell>2.961%</cell></row><row><cell>Oct</cell><cell>RMSE</cell><cell>1620.508</cell><cell cols="4">1091.054 1021.127 953.213 1004.394</cell><cell>913.704</cell><cell>911.767</cell><cell>987.330</cell><cell>659.407</cell></row><row><cell></cell><cell>MAPE</cell><cell>14.887%</cell><cell>9.404%</cell><cell cols="3">8.981% 7.139% 8.863%</cell><cell>7.217%</cell><cell>6.817%</cell><cell>7.035%</cell><cell>5.934%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Forecasting results for comparative experiment one. The results of additive model, ANN and Hybrid model are obtained from<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell></cell><cell>Proposed</cell><cell cols="2">Additive model [11]</cell><cell>ANN</cell><cell>Hybrid</cell></row><row><cell cols="3">Month MAE MAPE MAE</cell><cell>MAPE</cell><cell>MAE MAPE MAE MAPE</cell></row><row><cell>Oct</cell><cell cols="2">77.16 1.39% 88.55</cell><cell>1.66%</cell><cell>134.87 2.57% 121.83 2.15%</cell></row><row><cell>Nov</cell><cell cols="2">65.34 1.19% 94.33</cell><cell>1.74%</cell><cell>140.52 2.63% 123.50 2.12%</cell></row><row><cell>Dec</cell><cell cols="2">61.76 1.18% 79.89</cell><cell>1.55%</cell><cell>126.39 2.49% 116.34 2.17%</cell></row><row><cell>Jan</cell><cell cols="2">90.34 1.45% 110.21</cell><cell>1.88%</cell><cell>168.04 2.81% 126.73 2.14%</cell></row><row><cell>Feb</cell><cell cols="2">62.21 1.11% 96.84</cell><cell>1.64%</cell><cell>139.68 2.37% 119.07 1.95%</cell></row><row><cell>Mar</cell><cell cols="2">62.58 1.14% 87.45</cell><cell>1.59%</cell><cell>123.21 2.29% 116.49 1.94%</cell></row><row><cell cols="3">Average 69.90 1.24% 92.82</cell><cell>1.68%</cell><cell>138.79 2.53% 120.66 2.08%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Forecasting results for comparative experiment two</figDesc><table><row><cell cols="5">Metric Improved BP [39] HS-ARTMAP [57] dART&amp;HS-ARTMAP [56] Proposed</cell></row><row><cell>MAPE</cell><cell>3.40%</cell><cell>2.87%</cell><cell>1.91%</cell><cell>1.88%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This project is funded by the National Research Foundation Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Electric load forecasting: literature survey and classification of methods</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Alfares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nazeeruddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Systems Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crystal ball lessons in predictive analytics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EnergyBiz</title>
		<imprint>
			<biblScope unit="page" from="35" to="37" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation and instance based feature selection for electricity load forecasting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Agelidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Short-term load forecasting based on support vector regression and load profiling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Energy Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Short-term electricity demand forecasting using double seasonal exponential smoothing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="799" to="805" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Short-term load forecasting using lifting scheme and ARIMA models</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="5902" to="5911" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graduate Program of Operation Research and Dept. of Electrical and Computer Engineering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Short term electric load forecasting</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Short-term load forecasting with seasonal decomposition using evolution for parameter tuning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Høverstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Öztürk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1904" to="1913" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sparse coding approach to household electricity demand forecasting in smart grids</title>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local short and middle term electricity load forecasting with semi-parametric additive models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nedellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="440" to="446" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Short-term load forecasting based on a semi-parametric additive model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="141" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forecasting the short-term demand for electricity: Do neural networks stand a better chance?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Darbellay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="71" to="83" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks: The state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Patuwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural-net computing and the intelligent control of systems</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="263" to="289" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random vector functional link network for short-term electricity load demand forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page" from="1078" to="1093" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feedforward neural networks with random weights</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR International Conference on Pattern Recognition Conference B: Pattern Recognition Methodology and Systems</title>
		<meeting>the IAPR International Conference on Pattern Recognition Conference B: Pattern Recognition Methodology and Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive evaluation of random vector functional link networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page" from="1094" to="1105" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Multiple classifier systems</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensemble classification and regressionrecent developments, applications and future directions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="53" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>review article</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very shortterm load forecasting: wavelet neural networks with data pre-filtering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Friedland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hybrid intelligent algorithms based short-term load forecasting approach</title>
		<author>
			<persName><forename type="first">R.-A</forename><surname>Hooshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amooshahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical Power &amp; Energy Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="313" to="324" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Short term load forecasting using a hybrid intelligent method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hemmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abdoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="139" to="147" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Roy. Soc. London A</publisher>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="903" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Incremental</forename><surname>Learning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Boston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="731" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: the importance of starting small</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="71" to="99" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental ensemble learning for electricity load forecasting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Grmanová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laurinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rozinajová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ezzeddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lacko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vrablecová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Návrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Polytechnica Hungarica</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="97" to="117" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An incremental electric load forecasting model based on support vector regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="796" to="808" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incremental learning algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gepperth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting>European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A technique to improve the empirical mode decomposition in the hilbert-huang transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Earthquake Engineering and Engineering Vibration</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="796" to="808" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>International edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting wind power ramp with random vector functional link (rvfl) network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium Series on Computational Intelligence (CIEL2015)</title>
		<meeting>IEEE Symposium Series on Computational Intelligence (CIEL2015)<address><addrLine>Cape Town, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decomposition of hardy functions into square integrable wavelets of constant shape</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="723" to="736" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved week-ahead predictions of wind speed using simple linear models with wavelet decomposition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Kiplangat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asokan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable Energy</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="38" to="44" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Percival</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walden</surname></persName>
		</author>
		<title level="m">Wavelet Methods for Time Series Analysis, Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An introduction to wavelet analysis with applications to vegetation time series</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Percival</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Overland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Community Ecology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A rapid learning and dynamic stepwise updating algorithm for flat neural networks and the application to time-series prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ieee A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ensemble empirical mode decomposition: A noiseassisted data analysis method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Adaptive Data Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="http://www.aemo.com.au/" />
		<title level="m">Australian energy market operator</title>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="http://www.bom.gov.au/" />
		<title level="m">Australian bureau of meteorology</title>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Prediction as a candidate for learning deep hierarchical models of data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new error measure for forecasts of household-level, high resolution electrical energy consumption</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Greetham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grindrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="246" to="256" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Global energy forecasting competition 2012</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="363" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Prediction of acute hypotensive episodes using random forest based on genetic programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Evolutionary Computation (CEC2015)</title>
		<meeting>IEEE Conference on Evolutionary Computation (CEC2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Electricity load demand time series forecasting with empirical mode decomposition based random vector functional link network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Systems, Man and Cybernetics (SMC2016)</title>
		<meeting>IEEE Conference on Systems, Man and Cybernetics (SMC2016)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An efficient approach for electric load forecasting using distributed ART (adaptive resonance theory) &amp; HS-ARTMAP (hyper-spherical ARTMAP network) neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1340" to="1350" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A new ARTMAP-based neural network for incremental learning</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2284" to="2300" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Short-term load forecasting methods: An evaluation based on european data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcsharry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2213" to="2219" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">He is currently working towards the Ph.D. degree in the school of Electrical and Electronic Engineering in Nanyang Technological University, His research interests include various ensemble deep learning algorithms for regression and time series forecasting</title>
		<imprint/>
	</monogr>
	<note>Xueheng Qiu received his B.Eng. degree from Nanyang Technological University in 2012</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ponnuthurai Nagaratnam Suganthan received the B.A degree, Postgraduate Certificate and M.A degree in Electrical and Information Engineering from the</title>
		<idno>414.871 424.211 403.132 402.781 396.435 396.394</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
