<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Trajectory Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Federico</forename><surname>Bartoli</surname></persName>
							<email>federico.bartoli@unifi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Lisanti</surname></persName>
							<email>giuseppe.lisanti@unipv.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Pavia</orgName>
								<address>
									<settlement>Pavia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
							<email>lamberto.ballan@unipd.it</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
							<email>alberto.delbimbo@unifi.it</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Florence Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Trajectory Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BCCCF0205F15E4EC2C2A35294212B4BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human motion and behaviour in crowded spaces is influenced by several factors, such as the dynamics of other moving agents in the scene, as well as the static elements that might be perceived as points of attraction or obstacles. In this work, we present a new model for human trajectory prediction which is able to take advantage of both human-human and human-space interactions. The future trajectory of humans, are generated by observing their past positions and interactions with the surroundings. To this end, we propose a "context-aware" recurrent neural network LSTM model, which can learn and predict human motion in crowded spaces such as a sidewalk, a museum or a shopping mall. We evaluate our model on a public pedestrian datasets, and we contribute a new challenging dataset that collects videos of humans that navigate in a (real) crowded space such as a big museum. Results show that our approach can predict human trajectories better when compared to previous state-of-the-art forecasting models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>People usually move in a crowded space, having a goal in mind, such as moving towards another subject active in the scene or reaching a particular destination. These might be, for example, a shopping showcase, a public building or a specific artwork in a museum. While doing so, they are able to consider several factors and adjust their path accordingly. A person can adapt her/his path depending on the space in which is moving, and on the other humans that are walking around him. In the same way, an obstacle can slightly modify the trajectory of a pedestrian, while some other elements may constrain his path.</p><p>Although human behaviour understanding and people tracking have a long tradition in computer vision literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, in the recent years we observe increasing interest also in predictive models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Observing how persons navigate a crowded scenario, and being able to predict their future steps, is an extremely challenging task that can have key applications in robotic, smart spaces and automotive <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>Predicting the motion of human targets, such as pedestrians, or generic agents, such as cars or robots, is a very challenging and open problem. Most of the existing work in this area address the task of predicting the trajectory of a target, by inferring some properties of the scene or trying to encode the interactive dynamics among observed agents. Trajectory prediction is then achieved by modelling and learning humanspace <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref> or human-human interactions <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Pioneering works have tried to parameterise human behaviours with hand-crafted models such as social forces <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b23">[24]</ref>, while more recent approaches have attempted to infer these behaviours in a data-driven fashion <ref type="bibr" target="#b9">[10]</ref>. This idea has proven to be extremely successful for improving performance in multi-target tracking applications, and short-term trajectory prediction. In fact, being able to take into account the interactions of nearby agents, is extremely important to avoid collisions in a crowded scenario <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>. At the same time, a prior knowledge about the interactions between a specific target and the static elements of the scene (e.g. location of sidewalks, buildings, trees, etc.), is essential to obtain reliable prediction models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>. However, a main limitation of these models is that all of them attempt only to model human-human or human-space interactions.</p><p>In this paper, we introduce a novel approach for human trajectory prediction which is based on a "context-aware" recurrent neural network model. Each person trajectory is modelled through a Long-Short Term Memory (LSTM) network. In order to take into account for interactions with other humans and/or with the space, we extend the Social-LSTM model that has been recently proposed by Alahi et al. <ref type="bibr" target="#b9">[10]</ref>, by defining a "context-aware" pooling that allows our model to consider also the static objects in the neighborhood of a person. The proposed model observes the past positions of a human and his interactions with the surroundings in order to predict his near future trajectory. Results demonstrate that considering both human-human and human-space interactions is fundamental for obtaining an accurate prediction of a person' trajectory.</p><p>The main contributions of this paper are two-fold: i) we introduce a trajectory prediction model based on an LSTM architecture and a novel "context-aware" pooling which is able to learn and encode both human-human and humanspace interactions; ii) we demonstrate the effectiveness of the proposed model in comparison to previous state of the art approaches, such as the recent Social-LSTM <ref type="bibr" target="#b9">[10]</ref>, on the UCY dataset <ref type="bibr" target="#b25">[26]</ref> and a new challenging dataset, called MuseumVisits, that we will release publicly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPROACH</head><p>In this section, we describe our "context-aware" model for trajectory prediction. Our key assumption is that the behaviour of each person is strongly influenced by his interactions with the context, both in terms of static elements of the scene and dynamics agents active in the same scenario (such as other pedestrians). To this end, we model each person's trajectory with an LSTM network. Our work builds on top of the recent Social-LSTM model <ref type="bibr" target="#b9">[10]</ref>, but we introduce a more general Fig. <ref type="figure">1</ref>. We aim to learn and predict human behaviours in crowded spaces. To this end, we have collected a new dataset from multiple cameras in a big museum. In contrast to previous ones, our dataset allows experiments in a real crowded scenario where people interact not only with each others, but also with the space which is characterized by a rich semantic.</p><p>formulation which can include both human-human and humanspace interactions.</p><p>A. Context-Aware LSTM Given a video sequence, each trajectory is expressed in the form of spatial coordinates, such that X i t = (x i t , y i t ) represent the location of the i-th person at time instant t. We use an LSTM network to represent the i-th person trajectory, as follows:</p><formula xml:id="formula_0">    i i t f i t o i t ci t     =     σ σ σ tanh     W h i t-1 x i t + b i ,<label>(1)</label></formula><formula xml:id="formula_1">x i t = φ(X i t , W x ),<label>(2)</label></formula><formula xml:id="formula_2">c i t = f i t c i t-1 + i i t ci t ,<label>(3)</label></formula><formula xml:id="formula_3">h i t = o i t tanh(c i t ),<label>(4)</label></formula><p>where x i t ∈ R n represents the input data, h i t ∈ R D is the output state and c t ∈ R D is the hidden state of the LSTM at time t. The input x i t is obtained by applying the ReLU function φ(•) to the spatial coordinates X i t and the weight matrix</p><formula xml:id="formula_4">W x ∈ R N ×2 .</formula><p>At training time <ref type="foot" target="#foot_1">1</ref> the current input and the output from the previous instant are updated according to the weights W ∈ R (4D)×(n+D) and the bias term b ∈ R (n+D) . Then, the updated vectors are regularized through a sigmoid function to obtain i t , f t , o t ∈ R D , respectively representing the input gate vector (which weights the contribute of new information), the forget gate vector (which maintains old information) and the output gate. The tanh function, instead, creates a vector on new candidate values, ct ∈ R D , that can be added to the state.</p><p>The i-th trajectory position (x, y) i t+1 is estimated considering the output state h i t and a bivariate Gaussian distribution at time t:</p><formula xml:id="formula_5">(µ,σ, ρ) i t+1 = W h i t ,<label>(5)</label></formula><formula xml:id="formula_6">(x, y) i t+1 ∼ N (µ i t+1 , σ i t+1 , ρ i t+1 ),<label>(6)</label></formula><p>where µ i t+1 , σ i t+1 are the first and the second order moments of the Gaussian distribution, while ρ i t+1 represent the correlation coefficient. These parameters are obtained by a linear transformation of the output state h i t with the matrix W ∈ R (5×D) . Given the i-th trajectory, the parameters are learned by minimizing the negative log-Likelihood loss:</p><formula xml:id="formula_7">L i (W, W x , W ) = - T pred t=1 log P (x, y) i t |(µ,σ, ρ) i t<label>(7)</label></formula><p>Even though these networks are a really powerful tool for modelling time-dependent phenomenon, they are not able to take into account for other factors that can influence the path of a person, such as interactions with other persons and interactions with static elements of the scene.</p><p>a) Encoding human-human interactions.: A solution for modelling interactions between persons moving in the same space has been recently introduced in <ref type="bibr" target="#b9">[10]</ref>. Here, in order to model human-human interactions, at every time-step, the positions of all neighbors for the i-th trajectory are pooled through an occupancy grid of cells of size</p><formula xml:id="formula_8">m × n, G[m, n]. The occupancy matrix O i t (m, n) is computed as follows: O i t (m, n) = j∈G[m,n] 1 mn [x i t -x j t , y i t -y j t ],<label>(8)</label></formula><p>where 1 mn is the indicator function, used to assign for each cell [m, n] of the grid the corresponding (x, y) j t trajectories. This matrix allows modelling the presence or absence of neighbors for each person. This model is able to modify the trajectory in order to avoid immediate collision. However, in order to have a more smooth prediction a second model has been introduced, which simultaneously take into account for the hidden states of multiple LSTMs using a pooling layer, such as:</p><formula xml:id="formula_9">H i t (m, n, :) = j∈Gm×n 1 mn [x i t -x j t , y i t -y j t ]h j t-1 .<label>(9)</label></formula><p>These two models do not consider the context in which a person is moving, and this may limit their application in real crowded spaces. For this reason, we introduce a contextaware pooling in which both human-human and human-space interactions are explicitly taken into account.</p><p>b) Encoding human-human and human-space interactions.: Considering the space in which a person is moving is fundamental to obtain a more accurate prediction. To this end, we first identify a set of static objects in the scene that can influence the human behavior. These points are manually defined and can comprise just some entry or exit points, but also more complex elements that can influence in a different way the behavior of a person (such as an artwork in a museum). An overview of our approach is given in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>A naive approach for modelling also human-space interactions, can be obtained by modifying Eq. 8 in order to include the static elements in the neighborhood of each person, as in the following:</p><formula xml:id="formula_10">S i t (m, n, k) = j∈Gm×n 1 mn [x i t -p k x , y i t -p k y ],<label>(10)</label></formula><p>where (p x , p y ) k represents the coordinates of the k-th static object in the scene. However, a drawback in this model is that each element in the space contributes equally to the prediction.</p><p>Weighting equally the humans present in the neighborhood is useful to understand whether a trajectory is close to a collision or not. However, this same principle applied to the static objects in the scene is somewhat limited. Indeed, a human trajectory is influenced differently by humans or static objects.</p><p>For this reason we introduce a solution to model this latter case by defining for each person a vector containing the distances with respect to each element in the space:</p><formula xml:id="formula_11">C i t (k) = (x i t -p k x ) 2 + (y j t -p k y ) 2<label>(11)</label></formula><p>Differently from the naive solution of Eq. 10 in this way we can model how much each static elements can influence the path of a person.</p><p>A straightforward way to include this kind of interactions in our basic LSTM model can be obtained by simply modifying the input defined in Eq. 2 so as to include the representation defined in Eq. 11, such that:</p><formula xml:id="formula_12">x i t = φ(X i t , W x ) φ(C i t , W C )<label>(12)</label></formula><p>Finally, depending on which approach we want to exploit for modelling human-human interactions, we can further extend the input to our model by concatenating the representations defined in Eq. 8 and Eq. 9, respectively:</p><formula xml:id="formula_13">x i t = φ(X i t , W x ) φ(C i t , W C ) φ(O i t , W O ) (<label>13</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">x i t = φ(X i t , W x ) φ(C i t , W C ) φ(H i t , W H )<label>(14)</label></formula><p>The loss function defined in Eq. 7 is also modified accordingly, introducing the set of parameters W O , W H , W C in the LSTM optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Trajectory Prediction</head><p>At test time we consider a different set of trajectories, not observed during training time. In particular, we feed our model with the set of locations {x t , y t } t obs 1 of all the persons observed in the interval [1, t obs ]. We then estimate the near future trajectories, {x t , ŷt } t pred t obs +1 , of each person considering: 1) their path until time t obs ; 2) the path of other persons observed in the same period of time; and 3) the distances w.r.t. each static object in the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, we report on a set of experiments to assess the performance of our Context-aware Trajectory Prediction. We first describe the datasets used for the evaluation of our model and the settings used to train our model. Then, we report a comparison between the state-of-the-art and different configurations of our model.</p><p>A. Datasets and Evaluation Protocol a) UCY.: Our initial experiments have been conducted on the standard UCY <ref type="bibr" target="#b25">[26]</ref> dataset. This dataset contains three sequences, namely ZARA-01, ZARA-02 and University, which have been acquired in two different scenarios from a bird's eye view. The first scenario, ZARA, presents a moderately crowd condition. The University scenario, on the other hand, is really crowd, with persons walking in different directions and group of persons, stuck in the scene.</p><p>Since this dataset does not provide any annotation regarding human-space interactions, we have annotated the two ZARA sequences by manually identifying 11 points in the scene. These points are mainly localized in proximity of enter and exit locations. b) MuseumVisits.: Our new dataset has been acquired in the hall of a big art museum using four cameras, with small or no overlap. The cameras are installed so as to observe the artworks present in the hall and capture groups of persons during their visits. Figure <ref type="figure">1</ref> shows three different views of the museum hall. Differently from existing datasets, here we can observe rich human-human and human-space interactions. For example, persons walking together in a group and stopping in front of an artwork for a certain period of time. Trajectories for 57 distinct persons have been manually annotated along with some metadata, such as, the group one person belong to, the artwork a person is observing, etc. These metadata are not exploited in this work.</p><p>Some statistics comparing the datasets used in our experiments are given in Table <ref type="table" target="#tab_0">I</ref>. The number of persons observed in the ZARA sequences is higher with respect to our dataset, but the average length of their trajectories is really small. This is mainly because of the nature of this dataset, since persons enter and leave the scene continuously. Moreover, our dataset is slightly more crowd with an average number of persons per frame of 17 instead of 14 and, most importantly, it includes richer interactions with static objects in the scene. c) Evaluation protocol.: Experiments are conducted as follows: the trajectory of a person is observed for 3.2 seconds, then a trained model is used to predict 4.8 seconds. Trajectories are sampled so as to retain one frame every ten; with a framerate of 0.4, this corresponds to observing 8 frames and predicting 12 frames, as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Results are reported in terms of Average displacement error, which is calculated as the mean square error (MSE) in meters between the points predicted by the model {x i t , y i t } and the ground-truth {g i t , g i t }:</p><formula xml:id="formula_16">M SE = N i=1 t pred t=t obs +1 (x i t -g i t ) 2 + (y j t -g i t ) 2 N (t pred -t obs ) , (<label>15</label></formula><formula xml:id="formula_17">)</formula><p>where N is the total number of trajectories to be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In our Social-LSTM implementation <ref type="bibr" target="#b9">[10]</ref> we use the same configuration of parameters as in the original paper. In particular, the embedding dimension for the spatial coordinates is set to 64 while the hidden state dimension is equal to 128, for all the LSTM models. The pooling size for the neighborhood of a person is set to 32 and the dimension of the windows used for the pooling is 8 × 8. Each model has been trained considering a learning rate of 0.005, the RMSProp optimizer with a decay  rate of 0.95, and a total of 1,600 epochs. All the solutions have been implemented using the TensorFlow framework and train/test have been performed on a single GPU. No further parameters are needed for our context-aware pooling since we consider the distance between a person and all the objects in the scene (this obviously depends only on the dataset under analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>In this section we discuss the results obtained with our solution compared to state-of-the-art. We re-implemented the three methods described in <ref type="bibr" target="#b9">[10]</ref>, namely LSTM, O-LSTM and Social-LSTM (S-LSTM). In particular, the solution based only on LSTM does not consider any kind of interactions while predicting the person trajectory. On the other hand, O-LSTM and S-LSTM are able to consider human-human interactions in their prediction. We then extend these three models with our context-aware pooling. In our experiments we did not perform a fine-tuning using simulated trajectories, although according to <ref type="bibr" target="#b9">[10]</ref> this may help in reducing the overall prediction errors of each sequence of about 50%.</p><p>We divided the MuseumVisits dataset in five sequences and performed a five-fold cross validation while for the UCY datasets, we consider only the ZARA sequences and performed a two-fold cross-validation. Table <ref type="table" target="#tab_1">II</ref> shows experimental results for each sequence, as well as the average over all Finally, in Table <ref type="table" target="#tab_2">III</ref> we show some results obtained using a different implementation of our context-aware pooling which, similarly to O-LSTM, considers the human-space interactions by pooling the co-ordinates of neighbor static objects, as defined in Eq. 10. The prediction errors are higher in this case. This confirm our assumption that each static object influences in different way the trajectory of a person, so weighting them equally, such as in the case of O-LSTM, is somehow limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>Figure <ref type="figure" target="#fig_1">3</ref> shows some qualitative results of our model in comparison to prior work. We show four trajectories (in different colors), obtained using different methods. In black, we report the ground-truth trajectory. In green, a vanilla LSTM that does not use any information about the space and the other persons active in the scene. In red the output of Social-LSTM <ref type="bibr" target="#b9">[10]</ref>, while our Context-aware model is shown in blue.</p><p>We highlight four examples where are depicted some static elements of the scene, such as a particular artwork, as well as the trajectories (in grey) of other subjects that can influence the path of our target. In the first example, Context-aware O-LSTM is able to estimate the real trajectory without being influenced by the artwork. Both LSTM and Social-LSTM are wrongly influenced by a close trajectory and fail in predicting the correct trajectory. More interesting is the second example which shows the capability of our context-aware pooling to also exploit the static objects in the scene. On the contrary, solutions that do not model human-space interactions cannot stop the trajectory. The third example is similar to the first one but our prediction is less accurate. This is mainly caused by the fact that our Context-aware LSTM wrongly estimated the motion model for this target. The last example shows another interesting case in which the prediction is influenced by both static objects and other persons. In this case, our solution estimates a really noisy trajectory, close to the ground truth, while other methods are driven away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We introduced a novel approach for trajectory prediction which is able to model interactions between persons in a scene as well as considering the rich semantic that characterize the space in which a person moves. Inspired by the recent Social-LSTM model <ref type="bibr" target="#b9">[10]</ref>, which only considers human-human interactions, we introduce a more general formulation based on a "context-aware" pooling that is able to estimate how much the elements of the scene can influence the trajectory of a person. We also introduce a new challenging dataset of trajectories, acquired in the hall of a big art museum, in a really crowded and rich scenario.</p><p>Experimental results on this new dataset and on a subset of the UCY dataset, demonstrate that considering both human-human and human-space interactions is fundamental for trajectory prediction. Our solution, indeed, obtains a lower prediction errors compared to other state of the art solutions. Additionally, we qualitatively show that our context-aware pooling is able to take into account for static object in the scene and stop a trajectory while also considering the influence of other trajectory in the neighborhood.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our Context-aware Trajectory Prediction. The input of the LSTM are represented by the trajectory of the person under analysis (green), the grid of human-human interactions for the human-human pooling (red), and the context-aware pooling (blue).</figDesc><graphic coords="3,42.02,77.45,186.26,64.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results on the MuseumVisits dataset, showing the trajectories predicted using LSTM (green circle), Social-LSTM (red circle), Context-aware O-LSTM (blue circle) and the Ground Truth (black circle). Four examples are highlighted in order to appreciate the differences between the three methods. Trajectories of other nearest persons are also shown (grey dashed lines). The green marker specifies that an artwork is observed by a person.</figDesc><graphic coords="5,-78.65,227.38,503.02,327.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>FOR THE MUSEUMVISITS DATASET AND THE ZARA SEQUENCES. TRAJECTORY REPORTED IN FRAMES.</figDesc><table><row><cell></cell><cell cols="2">MuseumVisits ZARA sequeces</cell></row><row><cell>Total number of persons</cell><cell>57</cell><cell>204</cell></row><row><cell>Average trajectory length</cell><cell>422</cell><cell>72</cell></row><row><cell>Minimum trajectory length</cell><cell>26</cell><cell>7</cell></row><row><cell>Maximum trajectory length</cell><cell>1019</cell><cell>617</cell></row><row><cell>Average number of person per frame</cell><cell>17</cell><cell>14</cell></row><row><cell>Interactions with scene elements</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PREDICTION</head><label>II</label><figDesc>ERRORS ON MUSEUMVISITS AND UCY (ZARA SEQUENCES). RESULTS ARE REPORTED IN METERS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MuseumVisits</cell><cell></cell><cell></cell></row><row><cell>Technique</cell><cell>Seq1</cell><cell cols="2">Seq2 Seq3</cell><cell cols="2">Seq4 Seq5</cell><cell>Avg</cell></row><row><cell>LSTM</cell><cell>0.99</cell><cell>1.22</cell><cell>0.99</cell><cell>0.78</cell><cell>1.03</cell><cell>1.00</cell></row><row><cell>O-LSTM</cell><cell>1.60</cell><cell>1.43</cell><cell>0.95</cell><cell>0.76</cell><cell>1.02</cell><cell>1.15</cell></row><row><cell>S-LSTM</cell><cell>1.68</cell><cell>1.26</cell><cell>0.94</cell><cell>0.75</cell><cell>0.88</cell><cell>1.10</cell></row><row><cell>Context-aware LSTM</cell><cell>1.36</cell><cell>1.14</cell><cell>1.21</cell><cell>0.49</cell><cell>0.82</cell><cell>1.00</cell></row><row><cell>Context-aware O-LSTM</cell><cell>1.53</cell><cell>1.08</cell><cell>0.90</cell><cell>0.57</cell><cell>0.80</cell><cell>0.98</cell></row><row><cell>Context-aware S-LSTM</cell><cell>1.48</cell><cell>1.27</cell><cell>0.94</cell><cell>0.54</cell><cell>1.07</cell><cell>1.06</cell></row><row><cell></cell><cell></cell><cell cols="4">UCY (ZARA sequences)</cell><cell></cell></row><row><cell>Technique</cell><cell></cell><cell cols="2">Seq1 Seq2</cell><cell cols="2">Avg</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell><cell>1.32</cell><cell>1.49</cell><cell cols="2">1.40</cell><cell></cell></row><row><cell>O-LSTM</cell><cell></cell><cell>1.65</cell><cell>1.40</cell><cell cols="2">1.52</cell><cell></cell></row><row><cell>S-LSTM</cell><cell></cell><cell>1.30</cell><cell>1.37</cell><cell cols="2">1.34</cell><cell></cell></row><row><cell cols="2">Context-aware LSTM</cell><cell>1.21</cell><cell>1.37</cell><cell cols="2">1.29</cell><cell></cell></row><row><cell cols="2">Context-aware O-LSTM</cell><cell>1.18</cell><cell>1.34</cell><cell cols="2">1.26</cell><cell></cell></row><row><cell cols="2">Context-aware S-LSTM</cell><cell>1.19</cell><cell>1.25</cell><cell cols="2">1.22</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PREDICTION</head><label>III</label><figDesc>ERRORS ON BOTH MUSEUMVISITS AND UCY (ZARA SEQUENCES) USING A DIFFERENT POOLING FOR ENCODING HUMAN-SPACE INTERACTIONS. RESULTS ARE REPORTED IN METERS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MuseumVisits</cell><cell></cell><cell></cell></row><row><cell>Technique</cell><cell cols="2">Seq1 Seq2</cell><cell>Seq3</cell><cell cols="2">Seq4 Seq5</cell><cell>Avg</cell></row><row><cell>Context-aware LSTM (O)</cell><cell>1.66</cell><cell>1.55</cell><cell>1.26</cell><cell>0.81</cell><cell>1.20</cell><cell>1.30</cell></row><row><cell>Context-aware O-LSTM (O)</cell><cell>1.93</cell><cell>1.44</cell><cell>1.32</cell><cell>0.71</cell><cell>1.15</cell><cell>1.31</cell></row><row><cell>Context-aware S-LSTM (O)</cell><cell>1.66</cell><cell>1.55</cell><cell>1.26</cell><cell>0.58</cell><cell>1.20</cell><cell>1.25</cell></row><row><cell></cell><cell></cell><cell cols="4">UCY (ZARA sequences)</cell><cell></cell></row><row><cell>Technique</cell><cell></cell><cell>Seq1</cell><cell>Seq2</cell><cell>Avg</cell><cell></cell><cell></cell></row><row><cell cols="2">Context-aware LSTM (O)</cell><cell>1.30</cell><cell>1.40</cell><cell>1.35</cell><cell></cell><cell></cell></row><row><cell cols="2">Context-aware O-LSTM (O)</cell><cell>1.37</cell><cell>1.40</cell><cell>1.39</cell><cell></cell><cell></cell></row><row><cell cols="2">Context-aware S-LSTM (O)</cell><cell>1.31</cell><cell>1.29</cell><cell>1.30</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_0"><p>24th International Conference on Pattern Recognition (ICPR) Beijing, China, August 20-24, 2018 978-1-5386-3788-3/18/$31.00 ©2018 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>As in previous solutions<ref type="bibr" target="#b9">[10]</ref>,<ref type="bibr" target="#b0">[1]</ref>, we assume that a set of trajectories for all the persons observed in a scene is available a priori for training.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s going on? discovering spatio-temporal dependencies in dynamic scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s prototypes: a convex learning approach for discovering activity patterns in dynamic scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to divide and conquer for online multi-target tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring &quot;dark matter&quot; and &quot;dark energy&quot; from videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextbased pedestrian path prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F P</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social LSTM: Human Trajectory Prediction in Crowded Spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge transfer for scene-specific motion prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">People tracking with human motion predictions from social forces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robot navigation in dense human crowds: the case for cooperation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2153" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Socially-aware large-scale crowd forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Know before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Predict the Effect of Forces in Images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>What happens if</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning driven visual path prediction from a single image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5892" to="5904" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual path prediction in complex scenes with crowded moving objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Forecasting interactive dynamics of pedestrians with fictitious play</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling cooperative navigation in dense human crowds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4282" to="4286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic scene models from observing activity in visual surveillance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="408" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="655" to="664" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
