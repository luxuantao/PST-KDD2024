<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner</title>
				<funder ref="#_9ru2xb8 #_W3D6SPF">
					<orgName type="full">Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-10">10 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yufei</forename><surname>He</surname></persName>
							<email>yufei.he@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxiaod@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
							<email>evgeny.kharlamov@de.bosch.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">WWW &apos;23</orgName>
								<address>
									<addrLine>May 1-5</addrLine>
									<postCode>2023</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">WWW &apos;23</orgName>
								<address>
									<addrLine>May 1-5</addrLine>
									<postCode>2023</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-10">10 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583379</idno>
					<idno type="arXiv">arXiv:2304.04779v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies ? Learning latent representations</term>
					<term>? Information systems ? Data mining Graph Neural Networks</term>
					<term>Self-Supervised Learning</term>
					<term>Graph Representation Learning</term>
					<term>Pre-Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)-one type of generative methods-have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)-that are randomly masked from the input-with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework 1 GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to regularize the feature reconstruction. The multi-view random re-mask decoding is to introduce randomness into reconstruction in the feature space, while the latent representation prediction is to enforce the reconstruction in the embedding space. Extensive experiments show that GraphMAE2 can consistently generate top results on various public datasets, including at least 2.45% improvements over state-of-the-art baselines on ogbn-Papers100M with 111M nodes and 1.6B edges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have found widespread adoption in learning representations for graph-structured data. The success of GNNs has thus far mostly occurred in (semi-) supervised settings, in which task-specific labels are used as the supervision information, such as GCN <ref type="bibr" target="#b24">[25]</ref>, GAT <ref type="bibr" target="#b40">[41]</ref>, and GraphSAGE <ref type="bibr" target="#b12">[13]</ref>. However, it is often arduously difficult to obtain sufficient labels in real-world scenarios, especially for billion-scale graphs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>One natural solution to this challenge is to perform self-supervised learning (SSL) on graphs <ref type="bibr" target="#b29">[30]</ref>, where graph models (e.g., GNNs) are supervised by labels that are automatically constructed from the input graph data. Along this line, generative SSL models that aim to generate one part of the input graph from another part have received extensive exploration <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>. Straightforwardly, it first corrupts the input graph by masking node features or edges and then learns to recover the original input.</p><p>Under the masked prediction framework, a very recent work introduces a masked graph autoencoder GraphMAE <ref type="bibr" target="#b17">[18]</ref> for generative SSL on graphs, which yields outperformance over various baselines on 21 datasets for different tasks. Generally, an autoencoder is made up of an encoder, code/embeddings, and a decoder. The encoder maps the input to embeddings, and the decoder aims to reconstruct the input based on the embeddings under a reconstruction criterion. The main idea of GraphMAE is to reconstruct the input node features that are randomly masked before encoding by using an autoencoding architecture. Its technical contribution lies in the design of 1) masked feature reconstruction and 2) fixed re-mask decoding, wherein the encoded embeddings of previously-masked nodes are masked again before feeding into the decoder. Despite GraphMAE's promising performance, the reconstruction of masked features fundamentally relies on the discriminability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref> of the input node features, i.e., the extent to which the node features are distinguishable. In practice, the features of nodes in a graph are usually generated from data that is associated with each node, such as the embeddings of content posted by users in a social network, making them an approximate description of nodes and thus less discriminative. Note that in vision or language studies, the reconstruction targets are usually a natural description of the data, i.e., pixels of an image and words of a document. Table <ref type="table" target="#tab_0">1</ref> further shows that the performance of GraphMAE drops more significantly than the supervised counterpart when using less discriminative node features (w/ PCA). In other words, Graph-MAE, as a generative SSL framework with feature reconstruction, is relatively more vulnerable to the disturbance of features.</p><p>In this work, we present GraphMAE2 with the goal of improving feature reconstruction for graph SSL. The idea is to impose regularization on target reconstruction. To achieve this, we introduce two decoding strategies: multi-view random re-mask decoding for reducing the overfitting to the input features, and latent representation prediction for having more informative targets.</p><p>First, instead of fixed re-mask decoding used in GraphMAE-remasking the encoded embeddings of masked nodes, we propose to introduce randomness into input feature reconstruction with multiview random re-mask decoding. That is, the encoded embeddings are randomly re-masked multiple times, and their decoding results are all enforced to recover input features. Second, we propose latent representation prediction, which attempts to reconstruct masked features in the embedding space rather than the reconstruction in the input feature space. The predicted embeddings of masked nodes are constrained to match their representations that are directly generated from the input graph. Both designs naturally work as the regularization on target construction in generative graph SSL.</p><p>Inherited from GraphMAE, GraphMAE2 is a simple yet more effective generative self-supervised framework for graphs that can be directly coupled with existing GNN architectures. We perform extensive experiments on public graph datasets representative of different scales and types, including three open graph benchmark datasets. The results demonstrate that GraphMAE2 can consistently offer significant outperformance over state-of-the-art graph SSL baselines under different settings. Furthermore, we show that both decoding strategies contribute to the performance improvements In addition, we extend GraphMAE2 to large-scale graphs with hundreds of millions of nodes, which have been previously less explored for graph SSL. We leverage local clustering strategies that can produce local and dense subgraphs to benefit GraphMAE2 (and GraphMAE) with masked feature prediction. Experiments on ogbn-Papers100M of 111M nodes and 1.6B edges suggest the simple GraphMAE2 framework can generate significant performance improvements over existing methods (Cf. Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>In this section, we first revisit masked autoencoding for graph SSL and identify its deficiency in which the effectiveness of masked feature reconstruction can be vulnerable to the distinguishability of input node features. Then we present our GraphMAE2 to overcome the problem by imposing regularization on the feature decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Masked Autoencoding on Graphs</head><p>Notations. Let G = (V, ?, ? ), where V is the node set, ? = |V | represents the number of nodes, ? ? {0, 1} ? ?? is the adjacency matrix with each element ?(?, ?) = 1 indicating that there exists an edge between ? ? and ? ? . ? ? R ? ?? ?? is the input node feature matrix. In graph autoencoders, we use ? ? to represent the GNN encoder such as GAT <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b24">[25]</ref>. And ? ? represents the decoder which can be a multi-layer perceptron (MLP) or GNN. Denoting the hidden embedding ? ? R ? ?? , the general goal of graph autoencoders is to learn representation ? or a well-initialized ? ? through reconstructing input node features or structure:</p><formula xml:id="formula_0">? = ? ? (?, ? ), G = ? ? (?, ? )<label>(1)</label></formula><p>where G denotes the reconstructed graph characteristics, which can be structure, node features or both. Overview of masked feature reconstruction. The idea of masked autoencoder has seen successful practice in graph SSL <ref type="bibr" target="#b17">[18]</ref>. As a form of more general denoising autoencoders, it removes a portion of data in the graph, e.g., node features or links, with the masking operation and learns to predict the masked content. And it has been demonstrated that reconstructing masked node features as </p><formula xml:id="formula_1">e j E x 0 o X X L X 8 K 8 0 R 3 s = " &gt; A A A C 8 H i c j V H L S s Q w F D 3 W 1 / g e d e m m O A i K M r Q i 6 t L H Q p c j O D r g i K S Z q M X 0 Q Z o K w z A f 4 c 6 d u P U H 3 O p X i H + g f + F N r O A D 0 Z Q 2 5 5 5 7 z 2 l u b p D K M N O e 9 9 z j 9 P b 1 D w y W h o Z H R s f G J 8 q T U w d Z k i s u 6 j y R i W o E L B M y j E V d h 1 q K R q o E i w I p D o O L b Z M / v B Q q C 5 N 4 X 7 d T c R y x s z g 8 D T n T R J 2 U F 5 s R 0 + e c y c 5 O d 7 4 Z J L K V t S P a O p v d J f d z 3 O g u n J Q r X t W z y / 0 J / A J U U K x a U n 5 C E y 0 k 4 M g R Q S C G J i z B k N F z B B 8 e U u K O 0 S F O E Q p t X q C L Y d L m V C W o g h F 7 Q d 8 z i o 4 K N q b Y e G Z W z e k v k l 5 F S h d z p E m o T h E 2 f 3 N t P r f O h v 3 N u 2 M 9 z d n a t A e F V 0 S s x j m x f + k + K v + r M 7 1 o n G L d 9 h B S T 6 l l T H e 8 c M n t r Z i T u 5 + 6 0 u S Q E m d w i / K K M L f K j 3 t 2 r S a z v Z u 7 Z T b / Y i s N a 2 J e 1 O Z 4 N a e k A f v f x / k T H C x X / d W q v 7 d S 2 d g q R l 3 C D G Y x T / N c w w Z 2 U U O d v K 9 w j w c 8 O s q 5 d m 6 c 2 / d S p 6 f Q T O P L c u 7 e A E y F o a E = &lt; / l a t e x i t &gt; G(A, X)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN encoder</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + m X L A 0 B y W b + q r U 8 s t T G 9 c Z (1)   &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q T P S O 5 Z 4 3 q k 7 J g c d 2 2 0 g t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 9 7 N q t 5 s D s 4</p><formula xml:id="formula_2">V P L o c Y = " &gt; A A A C 2 X i c j V H L S s N A F D 2 N r 1 p f 8 b F z E y x C 3 Z R E R F 0 W 3 b i s Y B 9 Y a 0 n S a Q 3 N i 2 Q i 1 N C F O 3 H r D 7 j V H x L / Q P / C O 2 M K a h G d k J k z 5</formula><formula xml:id="formula_3">D i U k 8 4 = " &gt; A A A C 2 X i c j V H L S s N A F D 2 N r 1 p f 8 b F z E y x C 3 Z R E R F 0 W 3 b i s Y B 9 Y a 0 n S a Q 3 N i 2 Q i 1 N C F O 3 H r D 7 j V H x L / Q P / C O 2 M K a h G d k J k z 5</formula><formula xml:id="formula_4">J i A G E H 5 0 b 0 A + i 4 D 0 = " &gt; A A A C 2 X i c j V H L S s N A F D 2 N r 1 p f 8 b F z E y y C q 5 K I q M u i G 5 c V 7 A P b U p J 0 W k P z Y j I R a u n C n b j 1 B 9 z q D 4 l / o H / h n T E F t Y h O y M y Z c + 8 5 M 3 e u E / t e I k z z N a f N z M 7 N L + Q X C 0 v L K 6 t r + v p G L Y l S 7 r K q G / k R b z h 2 w n w v Z F X h C Z 8 1 Y s 7 s w P F Z 3 R m c y n j 9 m v H E i 8 I L M Y x Z O 7 D 7 o d f z X F s Q 1 d G 3 W o 7 N R y 0 n 8 r v J M K B l d D k e d / S i W T L V M K a B l Y E i s l G J 9 B e 0 0 E U E F y k C M I Q Q h H 3 Y S O h r w o K J m L g 2 R s R x Q p 6 K M 4 x R I G 1 K W Y w y b G I H N P d p 1 8 z Y k P b S M 1 F q l 0 7 x 6 e e k N L B L m o j y O G F 5 m q H i q X K W 7 G / e I + U p 7 z a k 1 c m 8 A m I F r o j 9 S z f J / K 9 O 1 i L Q w 7 G q w a O a Y s X I 6 t z M J V W v I m 9 u f K l K k E N M n M R d i n P C r l J O 3 t l Q m k T V L t / W V v E 3 l S l Z u X e z 3 B T v 8 p b U Y O t n O 6 d B b b 9 k H Z a s 8 4 N i + S R r d R 7 b</formula><p>2 M E e 9 f M I Z Z y h g i p 5 3 + A R T 3 j W m t q t d q f d f 6 Z q u U y z i W 9 D e / g A B g 6 Y a A = = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN decoder GraphMAE [MASK]</head><p>[MASK]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN decoder</head><p>Target generator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Projector</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l e o S D 2 m f i x z y D e R p q t a K U S 0 Z 0  the only pretext task could generate promising performance. In this work, we follow the paradigm of masked feature reconstruction and aim to further boost the performance by resolving the potential concerns in existing works.</p><formula xml:id="formula_5">N g = " &gt; A A A C 5 H i c j V H L S s N A F D 2 N r 1 p f U Z c u D B a h g p R E R F 1 W 3 b i s Y B / Q l p K k 0 x q a F 8 l E K K V L d + 7 E r T / g V r 9 F / A P 9 C + + M K V S L 6 I R k z j 3 3 n p O 5 c 6 3 Q d W K u 6 2 8 Z Z W Z 2 b n 4 h u 5 h b W l 5 Z X V P X N 6 p x k E Q 2 q 9 i B G 0 R 1 y 4 y Z 6 / i s w h 3 u s n o Y M d O z X F a z + u c i X 7 t h U e w E / h U f h K z l m T 3 f 6 T q 2 y Y l q q 9 u F p h W 4 n X j g 0 T Y 8 H e 1 P h v X R X l v N 6 0 V d L m 0 a G C n I I 1 3 l Q H 1 F E x 0 E s J H A A 4 M P T t i F i Z i e B g z o C I l r Y U h c R M i R e Y Y R c q R N q I p R h U l s n 7 4 9 i h o p 6 1 M s P G O p t u k v L r 0 R K T X s k i a g u o i w + J s m 8 4 l 0 F u x v 3 k P p K c 4 2 o N 1 K v T x i O a 6 J / U s 3 r v y v T v T C 0 c W J 7 M G h n k L J i O 7 s 1 C W R t y J O r k 1 0 x c k h J E 7 g D u U j w r Z U j u 9 Z k 5 p Y 9 i 7 u 1 p T 5 d 1 k p W B H b a W 2 C D 3 F K G r D x c 5 z T o H p Q N I 6 K x u V h v n S W j j q L L e y g Q P M 8 R g k X K K N C 3 r d 4 w j N e l K 5 y p 9 w r D 1 + l S i b V b O L b U h 4 / A f N m n J Q = &lt; / l a t e x i t &gt; (A, X) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x J 4 e D m J 0 L K Q D Z F G z A P p u 0 S a f W Y M = " &gt; A A A C 9 X i c j V H L a t t A F D 1 R 0 8 Z J H 3 H b Z T a i p u B C M b J x E n t n 2 k 0 X W S R Q J 6 a 2 M a P x 2 B k y e i C N W o z Q b 3 T X X c i 2 P 9 B t 8 w s h f 5 D 8 R e 5 M Z E g p p h 0 h z b n n 3 n M 0 d 6 4 f K 5 l q z 7 t e c x 6 t P 3 6 y U d n c e v r s + Y v t 6 s t X x 2 m U J V z 0 e a S i Z O C z V C g Z i r 6 W W o l B n A g W + E q c + G c f T f 7 k q 0 h S G Y W f 9 S I W 4 4 D N Q z m T n G m i J l V v F D B 9 y p n K D 4 p J L s O i P v I j N U 0 X A W 3 5 l + K 9 + z A e F O 8 m 1 Z r X 2 O 1 2 2 q 2 u + z d o N j y 7 a i j X Y V S 9 w g h T R O D I E E A g h C a s w J D S M 0 Q T H m L i x s i J S w h J m x c o s E X a j K o E V T B i z + g 7 p 2 h Y s i H F x j O 1 a k 5 / U f Q m p H T x l j Q R 1 S W E z d 9 c m 8 + s s 2 F X e e f W 0 5 x t Q b t f e g X E a p w S + y / d s v J / d a Y X j R k 6 t g d J P c W W M d 3 x 0 i W z t 2 J O 7 j 7 o S p N D T J z B U 8 o n h L l V L u / Z t Z r U 9 m 7 u l t n 8 j a 0 0 r I l 5 W Z v h 1 p y S B r y c o r s a H L c a z b 1 G 8 6 h d 6 3 0 o R 1 3 B D t 6 g T v P c R w + f c I g + e X / H L / z G p f P N + e G c O x f 3 p c 5 a q X m N P 5 b z 8 w 7 j S K S O &lt; / l a t e x i t &gt; L in (Z, X) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a f i V w G b Y X E J M + c j N p Q X w c l I x 5 t 8 = " &gt; A A A D B X i c j V H L T t w w F D 0 E W g Z K 2 y l d d h M x q j R I 1 S h B F X Q 5 K p s u u q A S A y P I a O R 4 D E Q 4 D z k O 0 i j K u n / C j h 3 q l h 9 g 2 1 b 9 A / g L r o 2 R W k Z V 6 y j 2 8 b n 3 H P v 6 x o V M S h 0 E v + a 8 + Y U n T x d b S 8 v P V p 6 / e N l + t b p X 5 p X i Y s B z m a t h z E o h k 0 w M d K K l G B Z K s D S W Y j 8 + 3 T b x / T O h y i T P d v W 0 E K O U H W f J U c K Z J m r c 7 k c p 0 y e c y f p z M 6 4 l 0 y L T T T e K c z k p p y k t d R Q z V R 8 0 z T t / h h 0 2 z f q 4 3 Q l 6 g R 3 + L A g d 6 M C N n b z 9 E x E m y M F R I Y V A B k 1 Y g q G k 7 x A h A h T E j V A T p w g l N i 7 Q Y J m 0 F W U J y m D E n t J 8 T L t D x 2 a 0 N 5 6 l V X M 6 R d K v S O n j L W l y y l O E z W m + j V f W 2 b B / 8 6 6 t p 7 n b l N b Y e a X E a p w Q + y / d Q + b / 6 k w t G k f 4 Y G t I q K b C M q Y 6 7 l w q + y r m 5 v 5 v V W l y K I g z e E J x R Z h b 5 c M 7 + 1 Z T 2 t r N 2 z I b v 7 G Z h j V 7 7 n I r 3 J p b U o P D x + 2 c B X s b v X C z F 3 5 5 3 + l / d K 1 u 4 Q 3 W 0 K V + b q G P T 9 j B g L z P c Y 3 v + O F 9 9 S 6 8 S + / b f a o 3 5 z S v 8 c f w r u 4 A x V q r g g = = &lt; / l a t e x i t &gt; L latent ( Z, X) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s q U k 5 4 b g T 7 V a K U R l F K e o f x C 7 r / 8 = " &gt; A A A D B n i c j V H L T t w w F D 0 E K K / S D r B k E 3 W E N E h o l K A K W P L Y s O i C S g y M y t C R 4 z F g j f N Q 7 C C N o u z 5 E 3 b s q m 7 7 A 9 0 W 9 Q / a v + D a D R I U I X C U + N x z 7 z n x 9 Y 0 y J b U J g t 9 j 3 v j E 5 J u p 6 Z n Z u b f z 7 9 4 3 F h a P d F r k X H R 4 q t K 8 G z E t l E x E x 0 i j R D f L B Y s j J Y 6 j 4 Z 7 N H 1 + K X M s 0 O T S j T J z G 7 D y R Z 5 I z Q 1 S / s d P T R d w v Z e X 3 Y m Y u O F P l p 4 r i J C t M 1 e p F q R r o U U x b + a X 6 W r b k a r X 2 k O x W q / 1 G M 2 g H b v l P Q V i D J u p 1 k D Z u 0 c M A K T g K x B B I Y A g r M G h 6 T h A i Q E b c K U r i c k L S 5 Q U q z J K 2 o C p B F Y z Y I X 3 P K T q p 2 Y R i 6 6 m d m t N f F L 0 5 K X 2 s k C a l u p y w / Z v v 8 o V z t u x z 3 q X z t G c b 0 R 7 V X j G x B h f E v q S 7</formula><formula xml:id="formula_6">X i L input (Z (i) , X) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 t L 6 l T / R k 1 R 5 K o a t 0 8 r 3 H m 9 J z 7 c = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d V l 0 4 7 K i f U B b J Z l O a 2 h e T C Z C K A V x 6 w + 4 1 Z 8 S / 0 D / w j t j B L W I T s j M m X P v O T N 3 r h v 7 X i I t 6 6 V g z M z O z S 8 U F 0 t L y y u r a + X 1 j W Y S p Y L x B o v 8 S L R d J + G + F / K G 9 K T P 2 7 H g T u D 6 v O W O T l S 8 d c N F 4 k X h h c x i 3 g u c Y e g N P O Z I o i 6 7 b u T 3 k y y g Z d y e X J U r V t X S w 5 w G d g 4 q y E c 9 K j + j i z 4 i M K Q I w B F C E v b h I K G v A x s W Y u J 6 G B M n C H k 6 z j F B i b Q p Z X H K c I g d 0 T y k X S d n Q 9 o r z 0 S r G Z 3 i 0 y 9 I a W K H N B H l C c L q N F P H U + 2 s 2 N + 8 x 9 p T 3 S 2 j 1 c 2 9 A m I l r o n 9 S / e Z + V + d q k V i g C N d g 0 c 1 x Z p R 1 b H c J d W v o m 5 u f q l K k k N M n M J 9 i g v C T C s / 3 9 n U m k T X r t 7 W 0 f F X n a l Y t W d 5 b o o 3 d U t q s P 2 z n d O g u V e 1 D 6 r 2 2 X 6 l d p y 3 u o g t b G O X + n m I G k 5 R R 4 O 8 B R 7 w i C f j 3 M i M W + P u I 9 U o 5 J p N f B v G / T t c 4 5 V w &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u a + 9 h K f U R G 0 z m g Z 1 Z C t v u + + 6 e C M = " &gt; A A A C 2 X i c j V H L S s N A F D 2 N r 1 p f 8 b F z E y y C q 5 K I q M u i G 5 c V 7 A P a U p J 0 W o N 5 M Z k I N X T h T t z 6 A 2 7 1 h 8 Q / 0 L / w z p i C W k Q n Z O b M u f e c m T v X i X 0 v E a b 5 W t B m Z u f m F 4 q L p a X l l d U 1 f X 2 j k U Q p d 1 n d j f y I t x w 7 Y b 4 X s r r w h M 9 a M W d 2 4 P i s 6 V y d y n j z m v H E i 8 I L M Y p Z N 7 C H o T f w X F s Q 1 d O 3 O k 7 k 9 5 N R Q E v W c W y e t c b j n l 4 2 K 6 Y a x j S w c l B G P m q R / o I O + o j g I k U A h h C C s A 8 b C X 1 t W D A R E 9 d F R h w n 5 K k 4 w x g l 0 q a U x S j D J v a K 5 i H t 2 j k b 0 l 5 6 J k r t 0 i k + / Z y U B n Z J E 1 E e J y x P M 1 Q 8 V c 6 S / c 0 7 U 5 7 y b i N a n d w r I F b g k t i / d J P M / + p k L Q I D H K s a P K o p V o y s z s 1 d U v U q 8 u b G l 6 o E O c T E S d y n O C f s K u X k n Q 2 l S V T t 8 m 1 t F X 9 T m Z K V e z f P T f E u b 0 k N t n 6 2 c x o 0 9 i v W Y c U 6 P y h X T / J W F 7 G N H e x R P 4 9 Q x R l q q J P 3 D R 7 x h G e t</formula><p>Formally, we uniformly sample a subset of nodes V ? V without replacement and replace their feature with a mask token [MASK], i.e. a learnable vector ? [? ] ? R ? ?? . And sampling with a relatively large mask ratio (e.g., 50%) helps eliminate redundancy in graphs and benefit performance. The features ? ? for node ? ? ? V in the corrupted feature matrix ? can be represented as:</p><formula xml:id="formula_7">? ? = ? [? ] ? ? ? V ? ? ? ? ? V</formula><p>Then the corrupted graph (?, ? ) is fed into the encoder ? ? to generate representations ? . And the decoder ? ? decodes the predicted masked features ? from ? . The training objective is to match the predicted ? with the original features ? with a designated criterion, such as (scaled) cosine error.</p><p>Problems in masked feature reconstruction. Despite the excellent performance, there exists potential concern for masked node feature reconstruction due to the inaccurate semantics of node features. A recent study <ref type="bibr" target="#b7">[8]</ref> shows that the performance of GNNs on downstream tasks can be significantly affected by the distinguishability of node features. In masked feature reconstruction, less discriminative reconstruction targets might cause misleading and harm the learning. To verify this assumption, we conduct pilot experiments by comparing the results using original features with less discriminative features. To induce information loss on features, we compress the features by mapping the original features to low dimensional space, i.e., 50 dimensions, using PCA. Table <ref type="table" target="#tab_0">1</ref> shows the results. We observe that the performance of GraphMAE degrades more significantly than the supervised counterpart when using the compressed features. The results indicate that the performance of learning through input feature reconstruction tends to be more vulnerable to the discriminability of the features.</p><p>In CV and NLP, where the philosophy of masked prediction has groundbreaking practices, their inputs are exact descriptions of data without loss of semantic information, e.g., pixels for images and words for texts. However, the input ? of graphs could inevitably and intrinsically contain unexpected noises since they processed products from various raw data, e.g., texts or hand-crafted features. The input ? is and generated by various feature extractors. For example, the node features of Cora <ref type="bibr" target="#b48">[49]</ref> are bag-of-words vectors, ogbn-Arxiv <ref type="bibr" target="#b19">[20]</ref> averages word embeddings of word2vec, and MAG240M <ref type="bibr" target="#b18">[19]</ref> are from pretrained language model. Their discriminability is constrained to the expressiveness of the feature generator and could inherit the substantial noise in the generator. In masked feature reconstruction, the objective of recovering less discriminative node features can guide the model to fit inaccurate targets and unexpected noises, bringing potential negative effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The GraphMAE2 Framework</head><p>We present GraphMAE2 to overcome the aforementioned issue. It follows the masked prediction paradigm and further incorporates regularization to the decoding stage to improve effectiveness.</p><p>To improve feature reconstruction, we propose to randomly remask the encoded representations multiple times and force the decoder to reconstruct input features from the corrupted representations. Then to minimize the direct effects of input features, we also enforce the model to predict representations of masked nodes in the embedding space beyond the input feature space. Both strategies serve as regularization to avoid the model over-fitting to the input features. Moreover, we extend GraphMAE2 to large graphs and propose to sample densely-connected subgraphs to accommodate with GraphMAE2's training, The overall framework of GraphMAE2 is illustrated in Figure <ref type="figure" target="#fig_5">2</ref>.</p><p>Multi-view random re-mask decoding. From the perspective of input feature reconstruction, we introduce randomness in the decoding and require the decoder to restore the input ? from different and partially observed embeddings.</p><p>The decoder maps the latent code ? to the input feature space to reconstruct ? for optimization. GraphMAE <ref type="bibr" target="#b17">[18]</ref> shows that using a GNN as the decoder achieves better performance than using MLP, and the GNN decoder helps the encoder learn high-level latent code when recovering the high-dimension and low-semantic features. The main difference is that GNN involves propagation and recovers the input relying on neighborhood information. Based on this characteristic of the GNN decoder, instead of the fixed re-mask decoding used in GraphMAE, we propose a multi-view random re-mask decoding strategy. It randomly re-masks the encoded representation before they are fed into the decoder, which resembles the random propagation in semi-supervised learning <ref type="bibr" target="#b9">[10]</ref>. Formally, we resample a subset of nodes V ? V following a uniform distribution. V is different from the input masked nodes V and nodes are equally selected for re-masking regardless of whether they are masked before. Then corrupted representation matrix ? is built from ? by replacing the ? ? of node ? ? ? V with another shared mask token [DMASK], i.e., a learnable vector ? [? ] ? R ? :</p><formula xml:id="formula_8">? ? = ? [? ] ? ? ? V ? ? ? ? ? V</formula><p>Then the decoder would reconstruct the input ? from the corrupted ? . The procedure is repeated several times to generate ? different re-masked nodes sets {V ( ?) } 1,...,? and corresponding corrupted representations { ? ( ? )} 1,...,? . Each view contains different information after re-masking, and they are all enforced to reconstruct input node features. The randomness of decoding serves as regularization preventing the network from memorizing unexpected patterns in the input ? , and thus the training would be less sensitive to the disturbance in the input feature. Finally, we employ the scaled cosine error <ref type="bibr" target="#b17">[18]</ref> to measure the reconstruction error and sum over the errors of the ? views for training:</p><formula xml:id="formula_9">L ????? = 1 | V | ? ?? ?=1 ?? ? ? ? V (1 - ? ? ? ? ( ?) ? ?? ? ? ? ?? ( ?) ? ? ) ?<label>(2)</label></formula><p>where ? ? is the ?-th row of ? , ?</p><formula xml:id="formula_10">( ?) ?</formula><p>is the ?-th row of predicted feature ? ( ?) = ? ? (?, ? ( ?) ), and ? &gt;= 1 is the scaled coefficient. In this work, the decoder ? ? for feature reconstruction consists of a light single-layer GAT. Therefore, this strategy is very efficient and only incurs negligible computational costs. Latent representation prediction. In line with the mask-thenpredict, the focus of this part is on constructing an additional informative prediction target that is minimally influenced by the direct effects of input features. To achieve this, we propose to perform the prediction in representation space beyond input feature space.</p><p>Considering that the neural networks can essentially serve as denoising encoders <ref type="bibr" target="#b31">[32]</ref> and encode high-level semantics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">56]</ref>, we propose to employ a network as the target generator to produce latent prediction targets from the unmasked graph. Formally, we denote the GNN encoder as ? ? (?; ? ) = ? ? . We also define a projector ?(?; ? ), corresponding to the decoder ? ? in input feature reconstruction, to map the code ? to representation space for prediction. ? denotes their learnable weights. The target generator network shares the same architecture as the encoder and projector but uses a different set of weights, i.e., ? ? ? (?; ?) and ? ? (?; ?). During the pretraining, the unmasked graph is first passed through the target generator to produce target representation X . Then the encoding results ? of the masked graph G(?, ? ) are projected to representation space, resulting in Z for latent prediction: Z = ?(? ; ? ), X = ? ? (? ? ? (?, ? ; ?); ?)</p><p>The encoder and projector network are trained to match the output of the target generator on masked nodes. Of particular interest, encouraging the correspondence of unmasked nodes would bring slight benefits to our framework. This may attribute to the masking operation implicitly serving as a special type of augmentation. We learn the parameters ? of the encoder and projector by minimizing the following scaled cosine error with gradient descent.</p><formula xml:id="formula_12">L ?????? = 1 ? ? ?? ? (1 - z? ? x? ?z? ? ? x ? ) ?<label>(4)</label></formula><p>And the parameters of target generator ? are updated via an exponential moving average of ? <ref type="bibr" target="#b27">[28]</ref> using weight decay ?:</p><formula xml:id="formula_13">? ? ?? + (1 -?)?<label>(5)</label></formula><p>The target generator shares similarities with the teacher network in self-knowledge distillation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">56]</ref> or contrastive methods <ref type="bibr" target="#b11">[12]</ref>. But there exist differences in both the motivation and implementation: GraphMAE2 aims to direct the prediction of masked nodes with output from the unmasked graph as the target. In contrast, knowledge-distillation and contrastive methods target maximizing the consistency of two augmented views. The characteristic is that our method does not rely on any elaborate data augmentations and thus has no worry about whether the augmentations would alter the semantics in particular graphs. Training and inference. The overall training flow of GraphMAE2 is summarized in Figure <ref type="figure" target="#fig_5">2</ref>. Given a graph, the original graph is passed through the target generator to generate the latent target X . Then we randomly mask the features of a certain portion of nodes and feed the masked graph with partially observed features into the encoder ? ? (?, X ; ? ) to generate the code ? . Next, the decoding consists of two streams. On the one hand, we apply the multi-view random re-masking to replace re-masked nodes in ? with [DMASK] token, and the results are fed into the decoder ? ? to reconstruct the input ? . On the other hand, another decoder ? is adapted to predict the latent target X . We combine the two losses with a mixing coefficient ? during training:</p><formula xml:id="formula_14">L = L ????? + ?L ??????<label>(6)</label></formula><p>Note that the time and space complexity of GraphMAE2 is linear with the number of nodes ? , and thus it can scale to extremely large graphs. When applying to downstream tasks, the decoder and target generator are discarded, and only the GNN encoder is used to generating embeddings or finetuned for downstream tasks.</p><p>Extending to large-scale graph Extending self-supervised learning to large-scale graphs is of great practical significance, yet few efforts have been devoted to this scenario. Existing graph SSL works focus more on small graphs, and current works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref> concerning large graphs simply conduct experiments based on existing graph sampling developed under the supervised setting, e.g., neighborhood sampling <ref type="bibr" target="#b12">[13]</ref> or ClusterGCN <ref type="bibr" target="#b6">[7]</ref>. Though it is a feasible implementation, there exist several challenges that may affect the performance under the self-supervised setting:</p><p>(1) Self-supervised learning generally benefits from relatively larger model capacity, i.e., wider and deeper networks, whereas GNNs suffer from the notorious problem of over-smoothing and oversquashing when stacking more layers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>. One feasible way to circumvent the problems is to decouple the receptive field and depth of GNN by extracting a local subgraph <ref type="bibr" target="#b51">[52]</ref>. <ref type="bibr" target="#b1">(2)</ref> In the context of masked feature prediction, GraphMAE2 has a preference for a well-connected local structure since each node would rely on aggregating messages of its neighboring nodes to generate embedding and reconstruct features.</p><p>Most popular sampling methods tend to generate highly sparse yet wide subgraphs as regularization in supervised setting <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b59">60]</ref>, or only bear shallow GNNs in inference <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. In light of these defects, we imitate the idea from <ref type="bibr" target="#b51">[52]</ref> and are motivated to construct densely connected subgraphs for GraphMAE2 to tackle the scalability on large-scale graphs. Thus, we utilize local clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref> algorithms to seek local and dense subgraphs. Local clustering aims to find a small cluster near a given seed in the large graph. And it has been proven to be very useful for identifying structures at small-scale or meso-scale <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. Though many local clustering algorithms have been developed, we leverage the popular spectral-based PPR-Nibble <ref type="bibr" target="#b1">[2]</ref> for efficient implementation. PPR-Nibble adopts the personalized PageRank (PPR) vector ? ? , which reflects the significance of all nodes V in the graph for the node ? ? , to generate a local cluster for a given node ? ? . Previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b58">59]</ref> provide a theoretical guarantee for the quality of the generated local cluster of PPR-Nibble. The theorem in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b58">59]</ref> (described in Appendix A.1) indicates that the algorithm can generate local clusters of a relatively small conductance, which meets our expectations for densely connected local subgraphs. In our work, we select the ?-largest elements in ? ? to form a local cluster for node ? ? for computational efficiency.</p><p>The PPR-Nibble can be implemented efficiently through fast approximation, and the computational complexity is linear with the number of nodes. One by-product is that this strategy decreases the discrepancy between training and inference since they are both conducted on the extracted subgraphs. In GraphMAE2, the selfsupervised learning is conducted upon all nodes within a cluster. In downstream finetuning or inference, we generate the prediction or embedding for node ? ? using the local cluster induced by ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we compare our proposed self-supervised framework with state-of-the-art methods in the setting of unsupervised representation learning and semi-supervised node classification. In this work, we focus on the node classification task, which aims to predict unlabeled nodes. Note that GraphMAE2 is a general SSL method and can be applied to various graph learning tasks. 1 The source code of GGD is not released and its results on Arxiv and MAG-Scholar-F are not reported in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluating on Large-scale Datasets</head><p>Datasets. The experiments are conducted on four public datasets of different scales, varying from hundreds of thousands of nodes to hundreds of millions. The statistics are listed in Table <ref type="table" target="#tab_1">2</ref>. In the experiments, we follow the official splits in <ref type="bibr" target="#b19">[20]</ref> for ogbn-Arxiv/Products/Papers100M. As for MAG-Scholar-F, we randomly select 5%/5%/40% nodes for training/validation/test, respectively. For ogbn-Products <ref type="bibr" target="#b19">[20]</ref> and MAG-Scholar-F <ref type="bibr" target="#b2">[3]</ref>, their node features are generated by first extracting bag-of-words vectors from the product descriptions or paper abstracts and then conducting Principal Component Analysis (PCA) to reduce the dimension. ogbn-Arxiv and ogbn-Papers100M <ref type="bibr" target="#b19">[20]</ref> are both citation networks, and they leverage word2vec model to obtain node features by averaging the embeddings of words in the paper's title and abstract. Baselines. We compare GraphMAE2 with state-of-the-art selfsupervised graph learning methods, including contrastive methods, GRACE <ref type="bibr" target="#b56">[57]</ref>, BGRL <ref type="bibr" target="#b38">[39]</ref>, CCA-SSG <ref type="bibr" target="#b53">[54]</ref>, and GGD <ref type="bibr" target="#b54">[55]</ref> as well as a generative method GraphMAE <ref type="bibr" target="#b17">[18]</ref>. Other methods are not compared because they are not scalable to large graphs, e.g., MVGRL <ref type="bibr" target="#b13">[14]</ref>, or the source code has not been released, e.g., In-foGCL <ref type="bibr" target="#b47">[48]</ref>. As stated in <ref type="bibr" target="#b39">[40]</ref>, random models can have a strong inductive bias on graphs and are non-trivial baselines. Therefore, we also report the results of the randomly-initialized GNN model and Simplified Graph Convolution (SGC) <ref type="bibr" target="#b45">[46]</ref>, which simply stacks the propagated features of different orders, to examine whether the SSL learns a more effective propagation paradigm. Comparing with them can reflect the contributions of self-supervised learning. To extend to large graphs for baselines, we adopt GraphSAINT <ref type="bibr" target="#b52">[53]</ref> sampling strategy, which is proved to perform better than widelyadopted Neighborhood Sampling <ref type="bibr" target="#b12">[13]</ref> in many cases. GraphMAE and GraphMAE2 are trained based on the presented local clustering algorithm. For all baselines, we employ Graph Attention Network (GAT) <ref type="bibr" target="#b40">[41]</ref> as the backbone of the encoder ? ? and the decoder for input feature reconstruction ? ? .</p><p>Evaluation. We evaluate our approach with two setups: (i) linear probing and (ii) fine-tuning. For linear probing, we first generate node embeddings with the pretrained encoder. Then we discard the encoder and train a linear classifier using the embeddings under the supervised setting. For fine-tuning, we add a linear classifier on top of node representations and fine-tune all parameters under the semi-supervised setting. We randomly sample 1% and 5% labels from the training set to finetune the pretrained model, aiming to test the ability to transfer knowledge learned from unlabeled data to facilitate the downstream performance with a few labels. For both cases, we run the experiments for 10 trials with random seeds and report the average accuracy and standard variance.</p><p>Results. The results of linear probing are illustrated in Table <ref type="table" target="#tab_2">3</ref>. And we interpret the result from 3 aspects. First, GraphMAE2 achieves better results than all self-supervised baselines across all datasets. This manifests that the proposed method can learn more discriminative representations under the unsupervised setting. Notably, GraphMAE2 improves upon GraphMAE by a margin of 1.91% and 2.35% (absolute difference) on MAG-Scholar-F and Papers100M. These results demonstrate the significance of the proposed improvement. Second, our approach, together with most baselines, outperforms the randomly initialized, untrained model by a large margin. This demonstrates that the designed self-supervised pretext task guides the model to better capture the semantic and structural information than the untrained model. As a comparison, improper self-supervised signals can lead the model to perform even worse, yet this phenomenon is ignored in most previous studies. Third, GraphMAE2 consistently generates better performance than SGC. Despite the fact that methods based on decoupled propagation have achieved promising results in the full-supervised setting with the assistance of self-training, we demonstrate that graph neural networks, like GAT, could still be more powerful at generating node representations in the unsupervised setting. Table <ref type="table" target="#tab_3">4</ref> shows the results of finetuning the pretrained model in the semi-supervised setting. On the one hand, it is observed that self-supervised pretraining of GraphMAE2 benefits downstream supervised training with significant performance gains. In ogbn-Products, with the pre-trained model, the performance achieves improvement by above 5.1%. And finetuning with only 5% of data generates comparative performance (80.52%) to many supervised learning methods in OGB leaderboard<ref type="foot" target="#foot_0">2</ref> with all 100% of training data, e.g., GraphSAINT: 80.27, Cluster-GAT: 79.23%. On the other hand, our approach remarkably achieves state-of-the-art performance for all benchmarks. The only exception is on the Products dataset with 1% training data, where GraphMAE2 slightly underperforms GRACE yet still achieves the second-best result. It should be noted that in ogbn-Papers100M, only GraphMAE2 and GraphMAE generate better performance than the random-initialized model, while all contrastive baselines fail to bring improvement with pretraining. One possible reason is that the data augmentation techniques used in baselines fail in this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating on Small-scale Datasets</head><p>Experimental setup. We also report results on small yet widely adopted datasets, i.e., Cora, Citeseer, and PubMed <ref type="bibr" target="#b48">[49]</ref>, to show the generality of our method. We follow the public data splits as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>. We compare GraphMAE2 with state-of-the-art self-supervised graph learning methods, including contrastive methods, DGI <ref type="bibr" target="#b41">[42]</ref>, MVGRL <ref type="bibr" target="#b13">[14]</ref>, GRACE <ref type="bibr" target="#b56">[57]</ref>, BGRL <ref type="bibr" target="#b38">[39]</ref>, InfoGCL <ref type="bibr" target="#b47">[48]</ref>, CCA-SSG <ref type="bibr" target="#b53">[54]</ref>, and GGD <ref type="bibr" target="#b54">[55]</ref> as well as generative methods GAE <ref type="bibr" target="#b23">[24]</ref>, Graph-MAE <ref type="bibr" target="#b17">[18]</ref>. For the evaluation, we employ the linear probing mentioned above and report the average performance of accuracy on the test nodes based on 20 random initialization. The GNN encoder and decoder both use standard GAT as the backbone, and an MLP is employed as the representation projector ?. Results. From Table <ref type="table" target="#tab_4">5</ref>, we can observe that our approach generally outperforms all baselines in all datasets, suggesting that Graph-MAE2 serves as a general and effective framework for graph selfsupervised learning on graphs of varied scales. We observe that the improvement over GraphMAE is not as significant as that in the experiments of large graphs. We guess that the reason lies in the construction of input node features. Bag-of-word vectors behave more like discrete features as words in text and pixels image and, thus are less noisy as reconstruction targets. And this may partially support our assumption that GraphMAE2 is more advantageous than GraphMAE when there is more noise in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>We further conduct ablation studies to verify the contributions of the designs in GraphMAE2. We choose linear probing for evaluation. Ablation on the learning framework. We study the influence of the proposed two strategies-latent representation prediction and multi-view random re-masking. The results are shown in Table <ref type="table" target="#tab_5">6</ref> , where the "w/o random re-mask" represents that we adopt the fixed re-masking strategy as GraphMAE. It is observed that the two strategies both contribute to performance improvement. This demonstrates the effectiveness and further supports our motivation for the effects of input feature quality. Latent representation prediction brings more benefits as the accuracy drops more when the component is removed, e.g., -1.58% in ogbn-Products and -1.91% in ogbn-Papers100M, than the multi-view random re-masking, e.g., -0.45% and -0.73%. The target generator network provides valuable guidance and constraints on the encoded representation.</p><p>We also conduct an experiment by totally removing the input feature reconstruction, and the training only involves latent representation prediction. In such cases, the learning degrades to selfknowledge distillation without heavy data augmentation and causes a significant drop in performance. The network may fall into a trivial solution and may learn collapsed representation as the results are worse than GraphMAE or even worse than the random-initialized model in MAG-Scholar-F and ogbn-Papers100M. This indicates that feature reconstruction substantially supports SSL, and the proposed two strategies serve as auxiliaries to help overcome the deficiency. Overall speaking, the results confirm that the superior performance of GraphMAE2 comes from the design rather than any individual contribution. Ablation on sampling strategies. Table <ref type="table" target="#tab_6">7</ref> shows the influence of different sampling strategies. We compare local clustering against two popular subgraph sampling algorithms-ClusterGCN <ref type="bibr" target="#b6">[7]</ref> and GraphSAINT <ref type="bibr" target="#b52">[53]</ref>. Neighborhood sampling is not included since it is not friendly to masked feature reconstruction, especially with GNN decoder. The local clustering is conducive to the excellent performance of GraphMAE2, as our algorithm shows an advantage over GraphSAINT and Cluster-GCN with 0.57% and 1.49% improvement on average. Recall that GraphSAINT tends to sample nodes globally, and thus the subgraph is more sparse. Although ClusterGCN generates large and connected partitions, it suffers from high information loss as edges between clusters are abandoned. And the results indicate that the densely-connected local subgraph produced can generate better representations. In addition, we compare our approach with the strongest baselines using the same sampling strategy. And GraphMAE2 still generates a 1.49% advantage in ogbn-Products, demonstrating its effectiveness. Ablation on model capacity. The effects of model capacity have attracted significant attention in other fields like CV <ref type="bibr" target="#b14">[15]</ref> and NLP <ref type="bibr" target="#b3">[4]</ref> as it is demonstrated that SSL can largely benefit from increasing model parameters. We take an interest in whether the scaling law of model capacity also applies to GNNs. Specifically, we employ a GAT as the encoder and explore the influence of depth and width. And experiments are conducted on ogbn-Products of around 2 million nodes. The results are shown in Figure <ref type="figure" target="#fig_6">3</ref>. Increasing the hidden size drives the model to achieve better performance. Doubling the hidden size leads to a performance improvement of nearly 2% in accuracy when the hidden size does not exceed 1024. But further enlarging the width only brings very marginal gain.</p><p>Another way to increase the capacity is to stack more network layers. Figure <ref type="figure" target="#fig_6">3</ref> shows that increasing the depth can slightly boost the performance as the accuracy increases by 0.65% when the number of network layers is increased from 2 to 4. And the benefits KLGGHQVL]H 3URGXFWV QXPBOD\HU 3URGXFWV would diminish when stacking more layers. One possible reason is that deeper GNNs are harder to optimize, while current downstream tasks or semantics of homogeneous structured data benefit little from more complex network architecture. It is observed that the influence of the depth is less remarkable than the width of GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>In this section, we introduce related works about graph self-supervised learning and scalable graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Self-Supervised Learning</head><p>Graph self-supervised learning (SSL) can be roughly categorized into two genres, including graph contrastive learning and graph generative learning, based on the learning paradigm. Contrastive methods. Contrastive learning is an important way to learn representations in a self-supervised manner and has achieved successful practices in graph learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55]</ref>. DGI <ref type="bibr" target="#b41">[42]</ref> and InfoGraph <ref type="bibr" target="#b36">[37]</ref> adopt the local-global mutual information maximization to learn node-level and graph-level representations. MVGRL <ref type="bibr" target="#b13">[14]</ref> leverages graph diffusion to generate an additional view of the graph and contrasts node-graph representations of distinct views. GCC <ref type="bibr" target="#b34">[35]</ref> utilizes subgraph-based instance discrimination and adopts InfoNCE as the pre-training objective with MoCo-style dictionary <ref type="bibr" target="#b15">[16]</ref>. GRACE <ref type="bibr" target="#b56">[57]</ref>, GraphCL <ref type="bibr" target="#b50">[51]</ref>, and GCA <ref type="bibr" target="#b57">[58]</ref> learn the node or graph representation by maximizing agreement between different augmentations. GGD <ref type="bibr" target="#b54">[55]</ref> analyzes the defect of existing contrastive methods (i.e., improper usage of Sigmoid function) and proposes a group discrimination paradigm.</p><p>To avoid the expensive computation of negative samples, some researchers propose graph SSL methods that do not require negative samples. BGRL <ref type="bibr" target="#b38">[39]</ref> uses an online encoder and a target encoder to contrast two augmented versions without negative samples. CCA-SSG <ref type="bibr" target="#b53">[54]</ref> leverages a feature-level objective for graph SSL, inspired by Canonical Correlation Analysis methods.</p><p>Most graph contrastive learning methods rely on complex graph augmentation operators to generate two different views, which are used to be contrasted or correlated. However, the theoretical understanding of augmentation techniques on the graph SSL has not been well-studied. The choice of graph augmentation operators mostly depends on the empirical analysis of researchers. Although some works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref> have made attempts to alleviate this reliance, it still remains further exploration. Generative methods. Graph autoencoders (GAE) and VGAE <ref type="bibr" target="#b23">[24]</ref> follow the spirit of autoencoder <ref type="bibr" target="#b16">[17]</ref> to learn node representations. Following VGAE, most GAEs focus on reconstructing the structural information (e.g., ARVGA <ref type="bibr" target="#b32">[33]</ref>) or adopt the reconstruction objective of both structural information and node attributes (e.g., MGAE <ref type="bibr" target="#b42">[43]</ref>, GALA <ref type="bibr" target="#b33">[34]</ref>). NWR-GAE <ref type="bibr" target="#b37">[38]</ref> designs a graph decoder to reconstruct the entire neighborhood information of graph structure. kgTransformer <ref type="bibr" target="#b30">[31]</ref> applies the masked GAE to knowledge graph reasoning. However, these previous GAE models do not perform well on node-level and graph-level classification tasks. To mitigate the performance gap, GraphMAE <ref type="bibr" target="#b17">[18]</ref> leverages masked feature reconstruction as the objective with auxiliary designs and obtains comparable or better performance than contrastive methods. In addition to graph autoencoders, inspired by the success of autoregressive models in natural language processing, GPT-GNN <ref type="bibr" target="#b21">[22]</ref> designs an attributed graph generation task, including attribute and edge generation, for pre-training GNN models. Generative methods can alleviate the deficiency of contrastive rivals since the objective of generative ones is to directly reconstruct the input graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalable Graph Neural Networks</head><p>There are two genres of methods for scalable GNNs. One is based on sampling that trains GNN models on sampled mini-batch data. GraphSAGE <ref type="bibr" target="#b12">[13]</ref> adopts the neighbor sampling method to conduct the mini-batch training. FastGCN <ref type="bibr" target="#b5">[6]</ref> performs layer-wise sampling and leverages importance sampling to reduce variance. GraphSAINT <ref type="bibr" target="#b52">[53]</ref> and ClusterGCN <ref type="bibr" target="#b6">[7]</ref> both produce a subgraph from the original graph for mini-batch training by graph partition or random walks. Another paradigm for scalable GNNs is to decouple the message propagation and feature transformation. SGC <ref type="bibr" target="#b45">[46]</ref> removes the nonlinear functions and is equivalent to a pre-processing K-step propagation and a logistic regression on the propagated features. SIGN <ref type="bibr" target="#b10">[11]</ref> extends SGC to stack the propagated results of different hops and graph filters, and then only trains MLPs for applications. These decoupled methods achieve excellent performance in the supervised setting but have no advantage in generating high-quality embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we explore graph self-supervised learning with masked feature prediction. We first examine its potential concern that the discriminability of input features hinders current attempts to achieve promising performance. Then we present a framework GraphMAE2 to address this issue by imposing regularization on the prediction. We focus on the decoding stage and introduce latent representation target and randomness to input reconstruction. The novel decoding strategy significantly boosts the performance in realistic large-scale benchmarks. Our work further supports that node-level signals could provide abundant supervision for masked graph self-supervised learning and deserves further exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Linear probing results on ogbn-Products and ogbn-Papers100M. GraphMAE2 achieves a significant advantage over previous graph SSL methods on benchmarks with millions of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fixed</head><label></label><figDesc>re-mask [0,2,4,6] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Z T w n 7 b d o o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>9 5 z Z u 5 c K 3 S d m O v 6 a 0 6 Z m p 6 Z n c v P F x Y W l 5 Z X 1 N W 1 e h w k k c 1 q d u A G U d M y Y + Y 6 P q t x h 7 u s G U b M 9 C y X N a z B s Y g 3 r l k U O 4 F / x o c h a 3 t m 3 3 d 6 j m 1 y o j r q x o U V u N 1 4 6 N G S n o 8 u 0 5 K x M + q o R b 2 s y 6 F N A i M D R W S j G q g v u E A X A W w k 8 M D g g x N 2 Y S K m r w U D O k L i 2 k i J i w g 5 M s 4 w Q o G 0 C W U x y j C J H d D c p 1 0 r Y 3 3 a C 8 9 Y q m 0 6 x a U / I q W G b d I E l B c R F q d p M p 5 I Z 8 H + 5 p 1 K T 3 G 3 I a 1 W 5 u U R y 3 F F 7 F + 6 c e Z / d a I W j h 4 O Z Q 0 O 1 R R K R l R n Z y 6 J f B V x c + 1 L V Z w c Q u I E 7 l I 8 I m x L 5 f i d N a m J Z e 3 i b U 0 Z f 5 O Z g h V 7 O 8 t N 8 C 5 u S Q 0 2 f r Z z E t R 3 y 8 Z + 2 T j d K 1 a O s l b n s Y k t l K i f B 6 j g B F X U y P s G j 3 j C s 9 J S b p U 7 5 f 4 z V c l l m n V 8 G 8 r D B 2 F Q l 7 c = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>9 5 z Z u 5 c K 3 S d m O v 6 a 0 6 Z m p 6 Z n c v P F x Y W l 5 Z X 1 N W 1 e h w k k c 1 q d u A G U d M y Y + Y 6 P q t x h 7 u s G U b M 9 C y X N a z B s Y g 3 r l k U O 4 F / x o c h a 3 t m 3 3 d 6 j m 1 y o j r q x o U V u N 1 4 6 N G S n o 8 u 0 5 K + M + q o R b 2 s y 6 F N A i M D R W S j G q g v u E A X A W w k 8 M D g g x N 2 Y S K m r w U D O k L i 2 k i J i w g 5 M s 4 w Q o G 0 C W U x y j C J H d D c p 1 0 r Y 3 3 a C 8 9 Y q m 0 6 x a U / I q W G b d I E l B c R F q d p M p 5 I Z 8 H + 5 p 1 K T 3 G 3 I a 1 W 5 u U R y 3 F F 7 F + 6 c e Z / d a I W j h 4 O Z Q 0 O 1 R R K R l R n Z y 6 J f B V x c + 1 L V Z w c Q u I E 7 l I 8 I m x L 5 f i d N a m J Z e 3 i b U 0 Z f 5 O Z g h V 7 O 8 t N 8 C 5 u S Q 0 2 f r Z z E t R 3 y 8 Z + 2 T j d K 1 a O s l b n s Y k t l K i f B 6 j g B F X U y P s G j 3 j C s 9 J S b p U 7 5 f 4 z V c l l m n V 8 G 8 r D B 1 7 u l 7 Y = &lt; / l a t e x i t &gt; Z (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>r 3 y t z v Z i c I Y t 1 4 O k n j L H 2 O 5 4 7 V K 4 W 7 E n 9 x 9 0 Z c g h I 8 7 i A e V z w t w p 7 + / Z d x r t e r d 3 y 1 z + j 6 u 0 r I 1 5 X V v g r z 0 l D T j 8 f 5 x P w d F 6 O 9 x o h 5 8 / N r d 3 6 1 F P Y x k f 0 K J 5 b m I b + z h A h 7 y v 8 R O / c O t d e T f e N + / 7 v 1 J v r N Y s 4 d H y f t w B N t a r o w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of GraphMAE2 framework. For large-scale graphs, we first run local clustering to produce local clusters for each node as the preprocessing step. During the pre-training, GraphMAE2 corrupts the graph by masking input node features with a mask token[MASK] and then feeds the result to a GNN encoder to generate the code. The decoding involves two objectives: 1) we generate multiple corrupted codes by randomly re-masking the code several times, and they are all forced to reconstruct input features after GNN decoding. 2) we use an MLP as the decoder to predict latent target representations, which are produced by a target generator with the unmasked graph. As a comparison, GraphMAE is trained through input feature reconstruction only with a fixed re-mask decoding strategy.</figDesc><graphic url="image-2.png" coords="3,135.35,87.45,123.08,123.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ablation study on hidden size and the number of GNN layers. The effects of width are more significant than depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results with the original node features (raw) or PCAprocessed node features (w/ PCA). w/ PCA represents that the input features are reduced to 50-dimensional continuous vectors using PCA, relatively less discriminative. GraphMAE can be more sensitive to the discriminability of input features than the supervised one. GAT is used as the backbone for all cases.</figDesc><table><row><cell>Cora</cell><cell>PubMed</cell></row><row><cell>raw ? w/ PCA</cell><cell>raw ? w/ PCA</cell></row><row><cell cols="2">Supervised 83.0 ? 82.3 (? 0.7) 78.0 ? 77.0 (? 1.0)</cell></row><row><cell cols="2">GraphMAE 84.2 ? 82.6 (? 1.6) 81.1 ? 78.9 (? 2.2)</cell></row><row><cell cols="2">GraphMAE2 84.5 ? 83.5 (? 1.0) 81.4 ? 80.1 (? 1.3)</cell></row><row><cell cols="2">compared to GraphMAE. Excitingly, GraphMAE2 as an SSL method</cell></row><row><cell cols="2">offers performance advantages over classic supervised GNNs across</cell></row><row><cell cols="2">all datasets, giving rise to the premise of self-supervised graph</cell></row><row><cell>representation learning and pre-training.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Datasets</cell><cell>#Nodes</cell><cell cols="2">#Edges #Features</cell></row><row><cell>Cora</cell><cell>2,485</cell><cell>5,069</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>2,110</cell><cell>3,668</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>300</cell></row><row><cell>ogbn-Arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell></row><row><cell>ogbn-Products</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>100</cell></row><row><cell>MAG-Scholar-F</cell><cell>12,403,930</cell><cell>358,010,024</cell><cell>128</cell></row><row><cell cols="3">ogbn-Papers100M 111,059,956 1,615,685,872</cell><cell>128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Linear probing results on large-scale datasets with mini-batch training. We report accuracy(%) for all datasets.</figDesc><table><row><cell cols="5">Random-Init represents a random-initialized model without any</cell></row><row><cell cols="2">self-supervised pretraining.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Arxiv</cell><cell>Products</cell><cell>MAG</cell><cell>Papers100M</cell></row><row><cell>MLP</cell><cell cols="4">55.50?0.23 61.06?0.08 39.11?0.21 47.24?0.31</cell></row><row><cell>SGC</cell><cell cols="4">66.92?0.08 74.87?0.25 54.68?0.23 63.29?0.19</cell></row><row><cell cols="5">Random-Init 68.14?0.02 74.04?0.06 56.57?0.03 61.55?0.12</cell></row><row><cell>CCA-SSG</cell><cell cols="4">68.57?0.02 75.27?0.05 51.55?0.03 55.67?0.15</cell></row><row><cell>GRACE</cell><cell cols="4">69.34?0.01 79.47?0.59 57.39?0.02 61.21?0.12</cell></row><row><cell>BGRL</cell><cell cols="4">70.51?0.03 78.59?0.02 57.57?0.01 62.18?0.15</cell></row><row><cell>GGD 1</cell><cell>-</cell><cell>75.70?0.40</cell><cell>-</cell><cell>63.50?0.50</cell></row><row><cell cols="5">GraphMAE 71.03?0.02 78.89?0.01 58.75?0.03 62.54?0.09</cell></row><row><cell cols="5">GraphMAE2 71.89?0.03 81.59?0.02 59.24?0.01 64.89?0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of fine-tuning the pretrained GNN with 1% and 5% labeled training data on large-scale datasets. We report accuracy(%) for all datasets. Random-Init represents a random-initialized model without any self-supervised pretraining.</figDesc><table><row><cell></cell><cell cols="2">Arxiv</cell><cell cols="2">Products</cell><cell cols="2">MAG</cell><cell cols="2">Papers100M</cell></row><row><cell>Label ratio</cell><cell>1%</cell><cell>5%</cell><cell>1%</cell><cell>5%</cell><cell>1%</cell><cell>5%</cell><cell>1%</cell><cell>5%</cell></row><row><cell>Random-Init ??? ??</cell><cell>63.45?0.32</cell><cell>67.67?0.42</cell><cell>72.23?0.44</cell><cell>75.21?0.35</cell><cell>43.55?0.15</cell><cell>51.03?0.13</cell><cell>56.47?0.23</cell><cell>60.63?0.22</cell></row><row><cell>CCA-SSG</cell><cell>64.14?0.21</cell><cell>68.32?0.32</cell><cell>75.89?0.43</cell><cell>78.47?0.42</cell><cell>42.62?0.15</cell><cell>51.32?0.11</cell><cell>55.68?0.24</cell><cell>59.78?0.08</cell></row><row><cell>GRACE</cell><cell>64.53?0.47</cell><cell>69.21?0.45</cell><cell>77.13?0.64</cell><cell>79.67?0.54</cell><cell>43.59?0.13</cell><cell>51.35?0.12</cell><cell>55.45?0.23</cell><cell>59.38?0.15</cell></row><row><cell>BGRL</cell><cell>65.05?1.17</cell><cell>69.01?0.34</cell><cell>76.32?0.54</cell><cell>79.46?0.43</cell><cell>43.92?0.11</cell><cell>51.69?0.15</cell><cell>55.12?0.23</cell><cell>60.40?0.54</cell></row><row><cell>Random-Init ??</cell><cell>64.79?0.45</cell><cell>67.89?0.27</cell><cell>71.87?0.34</cell><cell>74.42?0.43</cell><cell>43.66?0.14</cell><cell>50.86?0.12</cell><cell>57.48?0.23</cell><cell>61.41?0.25</cell></row><row><cell>GraphMAE</cell><cell>65.78?0.69</cell><cell>69.78?0.26</cell><cell>75.87?0.43</cell><cell>79.21?0.33</cell><cell>48.33?0.18</cell><cell>53.12?0.12</cell><cell>58.29?0.15</cell><cell>62.00?0.12</cell></row><row><cell>GraphMAE2</cell><cell cols="2">66.86?0.53 70.16?0.28</cell><cell>76.98?0.36</cell><cell cols="5">80.52?0.23 49.01?0.15 53.58?0.11 58.69?0.38 62.87?0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on small-scale datasets. We report accuracy(%) for all datasets.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell></row><row><cell>GAT</cell><cell>83.0?0.7</cell><cell>72.5?0.7</cell><cell>79.0?0.3</cell></row><row><cell>GAE</cell><cell>71.5?0.4</cell><cell>65.8?0.4</cell><cell>72.1?0.5</cell></row><row><cell>DGI</cell><cell>82.3?0.6</cell><cell>71.8?0.7</cell><cell>76.8?0.6</cell></row><row><cell>MVGRL</cell><cell>83.5?0.4</cell><cell>73.3?0.5</cell><cell>80.1?0.7</cell></row><row><cell>GRACE</cell><cell>81.9?0.4</cell><cell>71.2?0.5</cell><cell>80.6?0.4</cell></row><row><cell>BGRL</cell><cell>82.7?0.6</cell><cell>71.1?0.8</cell><cell>79.6?0.5</cell></row><row><cell>InfoGCL</cell><cell>83.5?0.3</cell><cell>73.5?0.4</cell><cell>79.1?0.2</cell></row><row><cell>CCA-SSG</cell><cell>84.0?0.4</cell><cell>73.1?0.3</cell><cell>81.0?0.4</cell></row><row><cell>GGD</cell><cell>83.9?0.4</cell><cell>73.0?0.6</cell><cell>81.3?0.8</cell></row><row><cell>GraphMAE</cell><cell>84.2?0.4</cell><cell>73.4?0.4</cell><cell>81.1?0.4</cell></row><row><cell>GraphMAE2</cell><cell>84.5?0.6</cell><cell>73.4?0.3</cell><cell>81.4?0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of GraphMAE2 key components.</figDesc><table><row><cell></cell><cell>Products</cell><cell>MAG</cell><cell>Papers100M</cell></row><row><cell>GraphMAE2</cell><cell cols="2">81.59?0.02 59.24?0.01</cell><cell>64.89?0.04</cell></row><row><cell cols="3">w/o random remask 81.04?0.03 59.01?0.02</cell><cell>64.16?0.02</cell></row><row><cell cols="3">w/o latent rep pred. 80.01?0.02 58.87?0.02</cell><cell>62.98?0.01</cell></row><row><cell>w/o input recon.</cell><cell cols="2">76.88?0.02 55.20?0.02</cell><cell>59.20?0.00</cell></row><row><cell>GraphMAE</cell><cell cols="2">78.89?0.01 58.75?0.03</cell><cell>62.54?0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on sampling strategy. "SAINT" refers to GraphSAINT, "Cluster" refers to Cluster-GCN, and "LC" refers the presented local clustering algorithm.</figDesc><table><row><cell></cell><cell cols="2">Strategy Products</cell><cell>MAG Papers100M</cell></row><row><cell>GRACE</cell><cell cols="3">SAINT 79.47?0.59 57.39?0.02 61.21?0.12</cell></row><row><cell>BGRL</cell><cell cols="3">SAINT 78.59?0.02 57.57?0.01 62.18?0.15</cell></row><row><cell cols="4">GraphMAE2 SAINT 80.96?0.03 58.75?0.03 64.21?0.11</cell></row><row><cell cols="4">GraphMAE2 Cluster 79.35?0.05 58.05?0.02 63.77?0.11</cell></row><row><cell>GraphMAE2</cell><cell>LC</cell><cell cols="2">81.59?0.02 59.24?0.01 64.89?0.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://ogb.stanford.edu/docs/leader_nodeprop/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by <rs type="funder">Natural Science Foundation of China (NSFC)</rs> <rs type="grantNumber">61825602</rs> and <rs type="grantNumber">62276148</rs>, <rs type="institution">Tsinghua-Bosch Joint ML Center, and Zhipu.AI</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>* This work was done when the author was visiting <rs type="institution">Tsinghua University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9ru2xb8">
					<idno type="grant-number">61825602</idno>
				</org>
				<org type="funding" xml:id="_W3D6SPF">
					<idno type="grant-number">62276148</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Theorem for Local Clustering.</head><p>Theorem A.1 (Theorem 1 in <ref type="bibr" target="#b58">[59]</ref>; Theorem 4.3 in <ref type="bibr" target="#b49">[50]</ref>). Let ? ? ? be some unknown targeted cluster that we are trying to retrieve from an unweighted graph. Let ? be the inverse mixing time of the random walk on the subgraph induced by ? . Then there exists ? ? ? ? with vol (? ? ) ? vol (? )/2, such that for any seed ? ? ? ? , PPR-Nibble with ? = ?(?) and ? ? A.2 Finetuning Results for Supervised Learning Table <ref type="table">8</ref> shows the results of finetuning the pretrained encoder with all labels in the training set. It is observed that self-supervised learning can still lead to improvements in the supervised setting. GraphMAE2 can bring 1.22%-3.17% improvement compared to the randomly initialized model on three datasets, i.e., ogbn-Arxiv, Products, and MAG-Scholar-F. The only exception is ogbn-Papers100M, in which the improvement is only 0.2% in accuray. The reason could be that the splitting strategy of this dataset (train:val:test = 78%:8%:14%) resulted in an overly well-labeled training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Linear Probing Results on ogbn-Arxiv with</head><p>Full-graph Training.</p><p>As ogbn-Arxiv dataset is relatively small, we can conduct full batch training and inference. Table <ref type="table">9</ref> shows the results of GraphMAE2 as well as baselines under full batch training. Compared to Graph-MAE, GraphMAE2 has a 0.2% improvement. It is worth noting that GraphMAE results are better when trained with full-batch than with mini-batch, while the mini-batch training results of Graph-MAE2 are comparable to the full-batch results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation Notes</head><p>Running Environment. Our proposed framework is implemented via PyTorch. For our methods, the experiments are conducted on a Linux machine with 1007G RAM, and 8 NVIDIA A100 with 80GB GPU memory. As for software versions, we use Python 3.9, PyTorch 1.12.0, OGB 1.3.3, and CUDA 11.3. Model Configuration. For our model and all baselines, we pretrain the model using AdamW Optimizer with cosine learning rate decay without warmup. More details about pre-training hyperparameters are in Table <ref type="table">10</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Baselines</head><p>For large datasets, we choose four baselines GRACE <ref type="bibr" target="#b56">[57]</ref>, BGRL <ref type="bibr" target="#b38">[39]</ref>, CCA-SSG <ref type="bibr" target="#b53">[54]</ref> and GraphMAE <ref type="bibr" target="#b17">[18]</ref>. To have a fair comparison, we download the public source code and use the GAT backbone. We adapted their code to integrate with sampling algorithms to run on large-scale graphs. For GGD <ref type="bibr" target="#b54">[55]</ref>, we can only report the results available in the original paper since the authors have not released the code. The sources of the codes used are as follows:</p><p>? BRGL: https://github.com/Namkyeong/BGRL_Pytorch ? GRACE: https://github.com/CRIPAC-DIG/GRACE ? CCA-SSG: https://github.com/hengruizhang98/CCA-SSG/ ? GraphMAE:https://github.com/THUDM/GraphMAE</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>ICCV. 9650-9660</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive graph encoder for attributed graph embedding</title>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="976" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Fabrizio Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Autoencoders, Minimum Description Length and Helmholtz Free Energy</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<editor>NeurIPS, J. Cowan, G. Tesauro, and J. Alspector</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan-Kaufmann</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GraphMAE: Self-Supervised Masked Graph Autoencoders</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<title level="m">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Think locally, act locally: Detection of small, mediumsized, and large communities in large networks</title>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Lucas Gs Jeub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="29" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selfkg: self-supervised entity alignment in knowledge graphs</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>WWW. 860-870</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD. 1120-1130</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified view on graph neural networks as graph signal denoising</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Jiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR&apos;20</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09025</idno>
		<title level="m">Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-Scale Representation Learning on Graphs via Bootstrapping</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Augmentations in graph contrastive learning: Current methodological flaws &amp; towards better practices</title>
		<author>
			<persName><forename type="first">Puja</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekdeep</forename><surname>Singh Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno>WWW. 1538-1549</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Augmentation-Free Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Masked feature prediction for self-supervised visual pretraining</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14668" to="14678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>WWW. 1070-1079</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Infogcl: Information-aware graph contrastive learning</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local higher-order graph clustering</title>
		<author>
			<persName><surname>Hao Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Decoupling the depth and scope of graph neural networks</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">From canonical correlation analysis to self-supervised graph neural networks</title>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination</title>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ibot: Image bert pre-training with online tokenizer</title>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW. 2069-2080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A local algorithm for finding well-connected clusters</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahab</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
