<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trustworthy Knowledge Graph Completion Based on Multi-sourced Noisy Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-21">21 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
							<email>jchuang@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Ning</surname></persName>
							<email>ningzhen.nz@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Qijin</forename><surname>Chen</surname></persName>
							<email>qijin.cqj@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoxia</forename><surname>Qiu</surname></persName>
							<email>xiaoxia.qxx@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Chengfu</forename><surname>Huo</surname></persName>
							<email>chengfu.huocf@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Weijun</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">National Institute of Healthcare Data Science Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trustworthy Knowledge Graph Completion Based on Multi-sourced Noisy Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-21">21 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511938</idno>
					<idno type="arXiv">arXiv:2201.08580v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge graph completion</term>
					<term>truth inference</term>
					<term>noisy data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs (KGs) have become a valuable asset for many AI applications. Although some KGs contain plenty of facts, they are widely acknowledged as incomplete. To address this issue, many KG completion methods are proposed. Among them, open KG completion methods leverage the Web to find missing facts. However, noisy data collected from diverse sources may damage the completion accuracy. In this paper, we propose a new trustworthy method that exploits facts for a KG based on multi-sourced noisy data and existing facts in the KG. Specifically, we introduce a graph neural network with a holistic scoring function to judge the plausibility of facts with various value types. We design value alignment networks to resolve the heterogeneity between values and map them to entities even outside the KG. Furthermore, we present a truth inference model that incorporates data source qualities into the fact scoring function, and design a semi-supervised learning way to infer the truths from heterogeneous values. We conduct extensive experiments to compare our method with the state-of-the-arts. The results show that our method achieves superior accuracy not only in completing missing facts but also in discovering new facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Data extraction and integration; • Computing methodologies → Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs (KGs), which represent real-world facts in the form of triples, have many important applications, e.g., semantic search, question answering and recommender systems. Some KGs, e.g., DBpedia <ref type="bibr">[2]</ref>, Freebase <ref type="bibr">[4]</ref>, Wikidata <ref type="bibr" target="#b65">[40]</ref> and Probase <ref type="bibr" target="#b70">[45]</ref>, are quite large; however, they are still acknowledged as incomplete <ref type="bibr">[5]</ref>. Existing studies cope with a task called KG completion to discover missing facts for a KG (see surveys <ref type="bibr">[18,</ref><ref type="bibr" target="#b54">29,</ref><ref type="bibr" target="#b66">41]</ref>). The majority of recent works focus on learning the embeddings of entities and relations in the KG, and leveraging them to predict the relations between two entities (or predict an entity given another entity and a relation). However, it is arguable to assume that all the entities and relations have been covered by the KG, as most KGs are built based on the open-world assumption <ref type="bibr" target="#b55">[30]</ref>. In fact, there are many other entities outside the KG but on the Web, and a few open KG completion methods <ref type="bibr">[7,</ref><ref type="bibr">14,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b58">33,</ref><ref type="bibr" target="#b60">35]</ref> attempt to mine facts from web pages. To validate the plausibility of mined facts, these methods are challenged by multi-sourced noisy data with varying quality.</p><p>Current approaches. Conventional KG completion methods, e.g., <ref type="bibr">[1,</ref><ref type="bibr">5,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b67">42]</ref>, learn embeddings for entities and relations within a KG, and define scoring functions to measure the plausibility of facts based on the embeddings. Most methods focus on completing relational facts between entities, e.g., (Boris Johnson, nationality, UK), whereas facts in real-world KGs contain other types of values, e.g., number, datetime, category and string. As far as we know, only a few methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b62">37]</ref> can process one or two extra types of values, but they are limited to completing the values inside the KG.</p><p>Open KG completion methods <ref type="bibr">[7,</ref><ref type="bibr">14,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b58">33,</ref><ref type="bibr" target="#b60">35]</ref> can mine new facts containing unseen entities, which appear in web pages but may not exist in the KG. In this way, open KG completion methods broaden the extent of KG by leveraging external resources such as online encyclopedias and vertical websites. The main challenge is the noises from multi-sourced data, which severely interfere the quality of discovered facts. Several existing methods <ref type="bibr">[14,</ref><ref type="bibr" target="#b58">33,</ref><ref type="bibr" target="#b60">35]</ref> focus on adding new facts that do not conflict with existing ones in the KG. Their shortcoming is that they ignore the quality of data sources. For example, data on Wikipedia are carefully validated, but data on personal websites may be unreliable. A recent solution, OKELE <ref type="bibr">[7]</ref>, models the data source quality through a probabilistic graphical model. However, it cannot leverage prior knowledge in the KG to judge the plausibility of new facts.</p><p>Our approach. In this paper, we propose a novel trustworthy KG completion method called TKGC. Our fundamental idea is to leverage noisy data from diverse web pages and prior knowledge in a KG symbiotically, so that KG completion can benefit from the open Web to not only add missing facts inside the KG, but also discover new facts outside the KG; meanwhile data noises can be resolved by considering existing facts in the KG. Figure <ref type="figure" target="#fig_0">1</ref> depicts the workflow of our method, which consists of three main components to address key challenges:</p><p>Holistic fact scoring measures the plausibility of facts extracted from multi-sourced noisy data by learning embeddings to represent entities, attributes and values. In addition to relational facts, we propose a graph neural network (GNN) with a holistic fact scoring function to handle facts with various types of values.</p><p>Value alignment networks softly resolve the heterogeneity between values, e.g., "UK" versus "United Kindom", by a literal-literal alignment network. Furthermore, literals are mapped into the entity embedding space by a literal-entity alignment network, so that facts with unseen entities can be discovered.</p><p>Semi-supervised truth inference identifies which noisy facts are plausible and adds the trustworthy ones into the KG. It defines the confusion probabilities to capture the relevance between aligned values and infers the truths by a semi-supervised learning process with the holistic fact scoring.</p><p>In summary, our main contributions in this paper are fourfold:</p><p>• We introduce a holistic fact scoring model to measure the plausibility of facts with different types of values such as entity, number and string. (Section 3)</p><p>• We design value alignment networks to resolve heterogeneous values from multiple sources, and align values with entity embeddings to discover facts with unseen entities. (Section 4)</p><p>• We incorporate a new truth inference model into the fact scoring model to realize trustworthy KG completion. Furthermore, we design a semi-supervised learning method to infer plausible facts from noisy data with a KG providing prior knowledge. (Section 5)</p><p>• We carry out extensive experiments to evaluate the effectiveness of our method TKGC. Our experimental results show that TKGC achieves superior performance on a benchmark dataset. Compared with the best-performing competitor, it particularly raises 0.048 of F1-score on relational fact completion and reduces 0.020 of RMSE (root of mean square error) on literal fact completion. (Section 6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Problem Statement</head><p>A KG collects a wealth of structured facts. It uses entities to denote real-world objects and describes the facts about entities in the form of (𝑒𝑛𝑡𝑖𝑡𝑦, 𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒, 𝑣𝑎𝑙𝑢𝑒), or (𝑒, 𝑎, 𝑣) for short. The value in each fact can be either an entity or a literal. A fact with an entity as the value is called a relational fact, and a fact with a literal as the value is called a literal fact. Therefore, a KG 𝐾 can be defined as a 5-tuple (𝐸, 𝐴, 𝐿,𝑇 𝑟𝑒𝑙 ,𝑇 𝑙𝑖𝑡 ), where 𝐸 is the entity set, 𝐴 is the attribute set, 𝐿 is the literal set, 𝑇 𝑟𝑒𝑙 ⊆ 𝐸 × 𝐴 × 𝐸 is the relational fact set, and 𝑇 𝑙𝑖𝑡 ⊆ 𝐸 × 𝐴 × 𝐿 is the literal fact set. We denote the value set by 𝑉 = 𝐿 ∪ 𝐸. For each attribute 𝑎, we denote the domain of its possible values by 𝑉 𝑎 = {𝑣 ∈ 𝑉 | ∃𝑒 ∈ 𝐸, (𝑒, 𝑎, 𝑣) ∈ 𝑇 𝑟𝑒𝑙 ∪ 𝑇 𝑙𝑖𝑡 }. For example, given 𝑎 = gender, we have 𝑉 𝑎 = {"male", "female"}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised truth inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-sourced noisy data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Holistic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>Conventional KG completion. Recent KG completion methods mainly use the embedding techniques to complete missing relational facts <ref type="bibr">[18,</ref><ref type="bibr" target="#b54">29,</ref><ref type="bibr" target="#b66">41]</ref>. There are three general types of methods: geometric models <ref type="bibr">[1,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>, tensor decomposition models <ref type="bibr">[3,</ref><ref type="bibr" target="#b72">47]</ref> and deep learning models <ref type="bibr">[10,</ref><ref type="bibr">11,</ref><ref type="bibr">16,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b57">32,</ref><ref type="bibr" target="#b59">34,</ref><ref type="bibr" target="#b64">39,</ref><ref type="bibr" target="#b67">42]</ref>. Inspired by the recent achievement of GNNs in KG completion (e.g., <ref type="bibr" target="#b57">[32,</ref><ref type="bibr" target="#b59">34,</ref><ref type="bibr" target="#b64">39,</ref><ref type="bibr" target="#b67">42]</ref>), we choose a GNN model for fact scoring. Additionally, a few KG completion methods, e.g., DKRL <ref type="bibr" target="#b71">[46]</ref> and LiteralE <ref type="bibr">[20]</ref>, encode literals to vectors and add them into entity embeddings. However, they are not designed for completing literal facts, rather using literals to improve the quality of relational fact completion. MT-KGNN <ref type="bibr" target="#b62">[37]</ref> and KBLRN <ref type="bibr">[15]</ref> both leverage different fact scoring functions for entities and numbers. MT-KGNN uses multi-task learning, while KBLRN proposes a "product of experts" approach. Generally, conventional KG completion methods cannot complete facts with unseen entities <ref type="bibr" target="#b58">[33,</ref><ref type="bibr" target="#b60">35]</ref>.</p><p>Open KG completion. By leveraging external knowledge, open KG completion methods <ref type="bibr">[7,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b58">33,</ref><ref type="bibr" target="#b60">35]</ref> can find new facts with unseen entities. ConMask <ref type="bibr" target="#b60">[35]</ref> fuses entity names and descriptions to obtain entity embeddings, and combines entity embeddings and relation embeddings to judge the plausibility of relational facts. OWE <ref type="bibr" target="#b58">[33]</ref> extends the conventional KG completion methods, and trains a neural network to map the name embeddings of unseen entities to the KG embedding space. However, ConMask and OWE do not consider the noises derived from external data sources. To address the noise problem, Knowledge Vault <ref type="bibr">[14]</ref> validates new facts by mining rules with a path ranking algorithm and matrix completion with tensor decomposition. OKELE <ref type="bibr">[7]</ref> first predicts missing attributes for long-tail entities, and then extracts true facts with a probabilistic graphical model. Knowledge Vault focuses on leveraging prior knowledge in the KG, while OKELE aims to reduce noises of external knowledge from the Web. Both of them may suffer from error accumulation due to their pipeline architecture. Compared with them, TKGC makes use of both prior and external knowledge in a semi-supervised manner.</p><p>Truth inference. Truth inference methods are proposed to infer truths from multi-sourced noisy data <ref type="bibr">[22,</ref><ref type="bibr" target="#b76">51]</ref>. Majority voting <ref type="bibr" target="#b76">[51]</ref> is a very simple approach choosing the majority as the truth. It treats all data sources equally, therefore many untrustworthy claims would generate inaccurate truths. To overcome this issue, previous studies <ref type="bibr">[21,</ref><ref type="bibr" target="#b52">27,</ref><ref type="bibr" target="#b74">49]</ref> leverage the quality of a data source to represent the ability that the data source provides correct claims, and infer truths based on the high-quality data sources. LTM <ref type="bibr" target="#b75">[50]</ref>, LCA <ref type="bibr" target="#b53">[28]</ref>, MBM <ref type="bibr" target="#b68">[43]</ref> and BWA <ref type="bibr">[23]</ref> further use probabilistic models to measure the trustworthiness of each claim. Compared with all of them, TKGC can leverage prior knowledge provided in the KG to improve the accuracy of truth inference. For instance, nationality and birth place of the same person are highly relevant. Currently, only TKGC can capture this fact relevance. Furthermore, TKGC can make subtle comparisons using the value alignment networks to capture value relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HOLISTIC FACT SCORING</head><p>Inspired by embedding-based KG completion methods <ref type="bibr">[18,</ref><ref type="bibr" target="#b54">29,</ref><ref type="bibr" target="#b66">41]</ref>, we encode the knowledge in a KG through representing its entities, attributes and values as embeddings and learning scoring functions over them to judge whether a fact is plausible. However, the majority of existing methods, such as <ref type="bibr">[1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">16,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b57">32,</ref><ref type="bibr" target="#b59">34,</ref><ref type="bibr" target="#b64">39,</ref><ref type="bibr" target="#b67">42,</ref><ref type="bibr" target="#b72">47]</ref>, only address relational fact completion. Unlike them, we propose a holistic fact scoring function F : 𝑇 𝑟𝑒𝑙 ∪ 𝑇 𝑙𝑖𝑡 ↦ → [0, +∞) to model both relational triples and literal triples, such that F (𝑒, 𝑎, 𝑣) → 0 if and only if fact (𝑒, 𝑎, 𝑣) is plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity Encoding</head><p>It is widely observed that neighboring entities, especially within two hops, are often critical to discover the missing attribute values <ref type="bibr" target="#b57">[32,</ref><ref type="bibr" target="#b61">36]</ref>. For example, two albums created by the same singer often have the similar genre. In this paper, we employ a two-layer GNN to model this graph structure. Considering the scale of real-world KGs which may contain millions of entities, we choose the Graph-Sage framework <ref type="bibr">[17]</ref>, which defines a neighbor sampling function N : 𝐸 → 2 𝐸 to improve the scalability, and an aggregation function A : 𝐸 × 2 𝐸 → R 𝑑 to aggregate the neighboring information. For each entity 𝑒 ∈ 𝐸, its hidden representation at the 𝑘 th layer is denoted by h 𝑘 𝑒 . At the input layer (𝑘 = 0), the hidden representation h 0 𝑒 is called initial entity representation and denoted by e 𝑖𝑛𝑖𝑡 . The aggregation function A () transforms h 𝑘−1 𝑒 to h 𝑘 𝑒 based on the sampled neighbors N (𝑒). Specifically, we use the mean-based aggregation function, which is defined as</p><formula xml:id="formula_0">h 𝑘 𝑒 = A 𝑒, N (𝑒) = 𝜎 W 𝑘 𝑔 MEAN {h 𝑘−1 𝑒 } ∪ {h 𝑘−1 𝑒 ′ | 𝑒 ′ ∈ N (𝑒)} ,<label>(1)</label></formula><p>where 𝜎 () is the sigmoid function, W 𝑘 𝑔 is the weight matrix at the 𝑘 th layer, and MEAN() is the element-wise mean function.</p><p>As the multiple layers of aggregation function make it difficult to learn the initial entity representation e 𝑖𝑛𝑖𝑡 , we use a residual connection and concatenate the initial representation with the output of residual connection as the final representation:</p><formula xml:id="formula_1">e = e 𝑖𝑛𝑖𝑡 ; e 𝑖𝑛𝑖𝑡 + 𝜎 W 𝑟𝑒𝑠 (e 𝑖𝑛𝑖𝑡 + h 𝐾 𝑒 ) ,<label>(2)</label></formula><p>where 𝐾 is the number of aggregation layers, W 𝑟𝑒𝑠 is the weight matrix. Dropout and layer normalization are adopted in Eq. ( <ref type="formula" target="#formula_1">2</ref>) to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Fact Scoring</head><p>We design a multi-class classification model for relational facts and leverage the weighted cross-entropy loss as the scoring function. A key challenge is the class imbalance problem, and we automatically calculate the value weights to address this problem. The scoring function F 𝑟𝑒𝑙 () for a relational fact (𝑒, 𝑎, 𝑣) is defined as</p><formula xml:id="formula_2">F 𝑟𝑒𝑙 (𝑒, 𝑎, 𝑣) = −𝜆 𝑎,𝑣 • log exp(e 𝑇 W 𝑎 v) 𝑣 ′ ∈𝑉 𝑎 exp(e 𝑇 W 𝑎 v ′ ) ,<label>(3)</label></formula><p>where 𝜆 𝑎,𝑣 is the weight for value 𝑣 of attribute 𝑎, e is the vector representation of entity 𝑒, W 𝑎 is the matrix representation of attribute 𝑎, v and v ′ are the vector representations of values 𝑣, 𝑣 ′ , respectively, and 𝑉 𝑎 is the domain of attribute 𝑎. As 𝑉 𝑎 may contain thousands of entities, we use a threshold 𝑁 𝑣 (𝑁 𝑣 &lt; |𝑉 𝑎 |) to restrict the size of 𝑉 𝑎 . Specifically, we sample a subset 𝑉 ′ 𝑎 from 𝑉 𝑎 which contains 𝑣 and other 𝑁 𝑣 − 1 elements.</p><p>Also, there may be thousands of entity and attribute pairs appearing in the relational fact set. Thus, it is impractical to manually tune weights 𝜆 𝑎,𝑣 for different pairs. In this paper, for value 𝑣 of attribute 𝑎, 𝜆 𝑎,𝑣 is automatically calculated as follows:</p><formula xml:id="formula_3">𝜆 𝑎,𝑣 = 𝜆 ′ 𝑎,𝑣 𝑣 ′ ∈𝑉 ′ 𝑎 𝜆 ′ 𝑎,𝑣 ′ , (<label>4</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">𝜆 ′ 𝑎,𝑣 = 1 log 1 + |{𝑒 | (𝑒, 𝑎, 𝑣) ∈ 𝑇 𝑟𝑒𝑙 }| .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Literal Fact Scoring</head><p>The general types of values in literal facts include string, category, number and datetime. The value type can be obtained or inferred with the corresponding attribute based on the schema or existing facts in the KG. For a string, we reuse the relational fact scoring function, by replacing the learnable entity embedding with the vector representation of the string encoded with BERT <ref type="bibr">[12]</ref>. We adopt the pre-trained uncased BERT base model to encode string values.</p><p>As generating texts from a KG is still a challenging problem <ref type="bibr">[19]</ref>, we select correct string values from claims rather than directly generate them. For a category, we treat it as an entity. For a datetime, we transform it to a real number that denotes the time duration in days from 2000-01-01. Note that a datetime before 2000-01-01 is transformed into a negative number. For other value types, we treat them as strings. Thus, we only need to consider numeric fact scoring. The same preprocessing is reused in Section 5.</p><p>For numeric facts, we define the scoring function as the 𝐿 1 -norm loss for a regression model, where attribute embeddings serve as the model parameters. Formally, the scoring function F 𝑙𝑖𝑡 () for a numeric fact (𝑒, 𝑎, 𝑣) is defined as</p><formula xml:id="formula_6">F 𝑙𝑖𝑡 (𝑒, 𝑎, 𝑣) = 𝜎 (e 𝑇 a + 𝑏 𝑎 ) − norm 𝑎 (𝑣) ,<label>(6)</label></formula><p>where 𝜎 () is the sigmoid function, 𝑏 𝑎 is the bias, e is the vector representation of entity 𝑒, a is the vector representation of attribute </p><formula xml:id="formula_7">F 𝑙𝑖𝑡 (𝑒, 𝑎, 𝑣) = min 𝑖=1,2...,𝑘 𝜎 (e 𝑇 a 𝑖 + 𝑏 𝑎,𝑖 ) − norm 𝑎 (𝑣 𝑖 ) .<label>(7)</label></formula><p>We expect to jointly learn entity embeddings based upon both relational and literal facts. Thus, we use F to unify F 𝑟𝑒𝑙 and F 𝑙𝑖𝑡 . Given a set of existing facts, we minimize the following fact loss function:</p><formula xml:id="formula_8">L 𝑓 𝑎𝑐𝑡 = ∑︁ (𝑒,𝑎,𝑣) ∈𝑇 𝑟𝑒𝑙 ∪𝑇 𝑙𝑖𝑡 F (𝑒, 𝑎, 𝑣),<label>(8)</label></formula><p>where 𝑇 𝑟𝑒𝑙 is the relational fact set, and 𝑇 𝑙𝑖𝑡 is the literal fact set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VALUE ALIGNMENT NETWORKS</head><p>As claims are collected from different data sources, values in the claims may be heterogeneous. In order to produce consistent truths based on facts and claims, we have to find which pairs of extracted values have similar meanings. In addition, as entities in the web pages are represented as texts, we also need to align them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Literal-Literal Alignment</head><p>We first normalize values and then align values with the same type.</p><p>For number and datetime, we conduct the same normalization in Section 3.3. For category, it can also be hand-craftily normalized, since they are enumerable. However, due to the heterogeneity, it is hard to normalize string, which is our focus. Given a pair of strings 𝑣, 𝑣 ′ , we design a literal-literal alignment neural network LL-ANN(𝑣, 𝑣 ′ ). Unlike most existing string alignment methods that are proposed to judge whether two strings are the same, we use LL-ANN(𝑣, 𝑣 ′ ) to measure whether value 𝑣 can infer 𝑣 ′ . In this sense, LL-ANN(𝑣, 𝑣 ′ ) is an asymmetric similarity measure <ref type="bibr" target="#b63">[38]</ref>. Based on this asymmetric measure, the subsequent truth inference can increase the trustworthiness of strings which can be inferred from other strings. Consider a set of extracted values {"pop", "rock", "pop rock", "folk rock"}. When measuring the accuracy, once the truth inference algorithm decides "pop rock" or "folk rock" is correct, "pop" is also correct. This inference can help increase the trustworthiness on "pop".</p><p>As illustrated in Figure <ref type="figure">2</ref>, we first encode 𝑣, 𝑣 ′ into two token embedding sequences, then use an attention mechanism to compare the token embeddings and combine the comparison results with LSTM, and finally use an MLP for classification.</p><p>Encoding layer. In the encoding layer, we follow the typical string matching network <ref type="bibr">[6]</ref> to encode word sequences with BERT and obtain token embedding sequences. We still use the pre-trained uncased BERT base model. Given a value 𝑣 = {𝑤 𝑖 } 𝑙 𝑖=1 , where 𝑤 𝑖 is the 𝑖 th word and 𝑙 is the total words, the token embedding sequence Encoding layer BERT encoder</p><formula xml:id="formula_9">𝑣 ! = {𝑤 " ! , 𝑤 # ! , … , 𝑤 $ ! ! } 𝑣 = {𝑤 " , 𝑤 # , … , 𝑤 $ } Encoding layer BERT encoder … difference attention weights similarity BiLSTM BiLSTM MLP output … … … … … … Figure 2: Literal-literal alignment network is {w 1 , w 2 , . . . , w 𝑙 }. For value 𝑣 ′ = {𝑤 ′ 𝑗 } 𝑙 ′ 𝑗=1</formula><p>, the token embedding sequence is defined similarly.</p><p>Alignment layer. In this layer, we align each token embedding w 𝑖 of value 𝑣 with all token embeddings of value 𝑣 ′ , and find its aligned token embedding att 𝑖 . As a result, we obtain the aligned token embedding sequence:</p><formula xml:id="formula_10">att 𝑖 = 𝑙 ′ ∑︁ 𝑗=1 cos(w 𝑖 , w ′ 𝑗 )w ′ 𝑗 (𝑖 = 1, 2, . . . , 𝑙),<label>(9)</label></formula><p>where cos() is the cosine similarity function for vectors. Then, to achieve a subtle comparison, we propose to obtain the similarity signals and difference signals with bidirectional LSTM:</p><formula xml:id="formula_11">diff = BiLSTM(w 1 − att 1 , w 2 − att 2 , . . . , w 𝑙 − att 𝑙 ),<label>(10)</label></formula><formula xml:id="formula_12">sim = BiLSTM w 1 + att 1 2 , w 2 + att 2 2 , . . . , w 𝑙 + att 𝑙 2 . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>We take the last hidden layer of bidirectional LSTM as output.</p><p>Classification layer. We apply an MLP to combine the similarity and difference signals. Finally, the string alignment probability is calculated as follows:</p><formula xml:id="formula_14">LL-ANN(𝑣, 𝑣 ′ ) = MLP(sim, diff).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Literal-Entity Alignment</head><p>A string value may also refer to a real-world object. Current entity linking methods <ref type="bibr" target="#b56">[31,</ref><ref type="bibr" target="#b69">44]</ref> can link texts to corresponding entities in the KG, but we also expect to validate the correctness of facts with corresponding entities outside the KG. In this paper, we propose a literal-entity alignment neural network LE-ANN to approximate literal 𝑣 to its corresponding entity 𝑒 based on the literal and entity embeddings. In this way, facts about unseen entity 𝑒, represented by 𝑣, can be validated by fact scoring based on LE-ANN(𝑣). We use BERT to encode literal 𝑣 into a vector, and feed the vector into MLP. Therefore, LE-ANN is defined as</p><formula xml:id="formula_15">LE-ANN(𝑣) = MLP BERT(𝑣) ,<label>(13)</label></formula><p>where MLP() is a two-layer MLP with sigmoid activation. Given an entity embedding e and a literal value 𝑣, the literal-entity alignment probability is calculated by exp − || LE-ANN(𝑣) − e|| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SEMI-SUPERVISED TRUTH INFERENCE</head><p>While the fact scoring model can complete the KG by maximizing the scoring function F (𝑒, 𝑎, 𝑣) for missing value 𝑣 of attribute 𝑎 w.r.t. entity 𝑒, it is restricted to the internal knowledge within the KG. To leverage external knowledge from the open Web which contains multi-sourced claims, we propose a truth inference model upon the fact scoring and use a semi-supervised learning method to infer the truths. Following most truth inference models <ref type="bibr">[22,</ref><ref type="bibr" target="#b76">51]</ref>, we consider two key factors, i.e., the quality of data sources and the trustworthiness of claims. We design the confusion probability to model how a data source makes errors. Compared with other solutions <ref type="bibr">[21,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b52">27,</ref><ref type="bibr" target="#b53">28,</ref><ref type="bibr" target="#b68">43,</ref><ref type="bibr" target="#b75">50]</ref>, the confusion probability makes sophisticated comparison for complex value types. We also propose a new observed value probability estimation method to estimate the trustworthiness of claims with prior knowledge provided by the fact scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Confusion Probability</head><p>Our truth inference model assumes that noises change the truths with an inherent probability distribution determined by a data source. The probability that a data source 𝑠 changes truth value 𝑣 * to observed value 𝑣, called confusion probability, is denoted by Pr[𝑣 | 𝑣 * , 𝑠]. We can define a difference function 𝑑 (𝑣, 𝑣 * ) to estimate the difference between 𝑣 and 𝑣 * . We assume that the confusion probability is determined by the difference function, and the difference subjects to a Gaussian distribution with zero mean value and</p><formula xml:id="formula_16">(𝑘 𝑎 𝜎 𝑠 ) 2 variance, i.e., Pr[𝑣 | 𝑣 * , 𝑠] = Pr[𝑑 (𝑣, 𝑣 * )] ∼ 𝑁 0, (𝑘 𝑎 𝜎 𝑠 ) 2 ,</formula><p>where 𝑘 𝑎 is a scale factor and 𝜎 𝑠 reflects the noise caused by data source 𝑠. As the value differences corresponding to different attributes often have different scales, 𝑘 𝑎 is used to regularize these differences. For different types of values, we define different difference functions:</p><p>• For entity (and category), we define the difference function as a metric function in the entity (category) embedding space.</p><p>Given two entities (or categories) 𝑣, 𝑣 * and their embeddings v, v * , we first project them into a latent feature space with dimension 𝑑 𝑑𝑒 by a linear transform W 𝑒 , and use 𝐿 1 -norm as the distance measure. Formally, the difference function is defined as</p><formula xml:id="formula_17">𝑑 (𝑣, 𝑣 * ) = ||W 𝑒 v − W 𝑒 v * ||.</formula><p>• For number and datetime, we define the difference function as the absolute difference, i.e., 𝑑 (𝑣, 𝑣 * ) = |𝑣 − 𝑣 * |. With this difference function, our truth inference coincides with the widely-used Gaussian truth inference <ref type="bibr">[21]</ref>, which assumes that the observed value subjects to a Gaussian distribution with mean value 𝑣 * and variance controlled by data source 𝑠, i.e., 𝑣 ∼ 𝑁 (𝑣 * , 𝑘 𝑎 𝜎 𝑠 ). • For string, we reuse the literal-literal alignment model to obtain the difference 𝑑 (𝑣, 𝑣 * ) = 1 − LL-ANN(𝑣, 𝑣 * ). Based on the observed value probability, we define the loss function for claims as the negative logarithm likelihood function of all claims:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Observed Value Probability</head><formula xml:id="formula_18">L 𝑐𝑙𝑎𝑖𝑚 = − ∑︁ (𝑒,𝑎,𝑣,𝑠) ∈𝐶 log Pr[𝑣 | 𝑒, 𝑎, 𝑠], (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>where 𝐶 is the claim set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semi-supervised Learning</head><p>We train our model and discover the truths in a semi-supervised manner. For truth inference, each fact is the label of corresponding claims. As consistent claims help determine the truths, we combine labeled and unlabeled claims together to train our model. Moreover, as claims may contain noises but facts may not, we first train the fact scoring model with facts, and then jointly train the fact scoring model and the truth inference model with facts and claims.</p><p>As shown in Algorithm 1, we design a semi-supervised learning algorithm to discover a new fact set 𝑇 𝑛𝑒𝑤 from a claim set 𝐶 with the help of existing facts 𝑇 in the KG. In Lines 2-4, we learn the parameters of fact scoring model up to 𝐹𝐴𝐶𝑇 _𝐸𝑃𝑂𝐶𝐻𝑆 epochs. In Lines 5-12, we learn the parameters of both fact scoring model and truth inference model up to 𝐼 𝑁 𝐹 𝐸𝑅𝐸𝑁𝐶𝐸_𝐸𝑃𝑂𝐶𝐻𝑆 epochs. In Lines 7-11, we traverse each claim (𝑒, 𝑎, 𝑣, 𝑠) ∈ 𝐶 and compute the observed value probability. For each claim (𝑒, 𝑎, 𝑣, 𝑠), we first find all possible truth value set 𝑉 * 𝑒,𝑎 (Line 8), then compute the confusion probability Pr[𝑣 | 𝑣 * , 𝑠] with the value alignment model and fact plausibility Pr[𝑣 * | 𝑒, 𝑎] with the fact scoring model (Lines 9-10), and finally update the claim loss with the observed value probability Pr[𝑣 | 𝑒, 𝑎, 𝑠] using Eq. ( <ref type="formula">16</ref>) (Line 11). In Lines 13-14, for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>In this section, we assess the proposed TKGC and report our experimental results. Dataset and source code are accessible online (https://github.com/nju-websoft/TKGC).</p><p>Hyperparameters. We implement TKGC on a server with 4 CPUs, 32GB memory and a NVIDIA Tesla V100 graphics card. Due to the space limitation, we report the hyperparameters in Appendix B.</p><p>Datasets. We choose a recent open KG completion dataset created by OKELE <ref type="bibr">[7]</ref> to evaluate the performance of TKGC, because this dataset contains noisy facts crawled from web pages and manuallylabeled true facts. Table <ref type="table" target="#tab_1">1</ref> depicts its statistical data. This dataset contains 10 popular classes of entities in Freebase <ref type="bibr">[4]</ref>, and each class contains 1,000 entities for training, 100 entities for validation and 100 entities for testing. In total, it contains 191,759 facts. We reuse these 12,000 entities as seeds, and collect their one-hop and two-hop facts that do not appear in the test set to construct a subgraph of Freebase, which serves as the prior knowledge for TKGC. Evaluation metrics. Following <ref type="bibr">[15,</ref><ref type="bibr" target="#b62">37]</ref>, we use precision (P), recall (R) and F1-score as the metrics for relational facts; and mean absolute error (MAE) and root of mean square error (RMSE) as the metrics for literal facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Performance</head><p>Competitors. For comparison, we choose 10 competing methods from four categories: (i) Four conventional KG completion methods only for relational facts, namely TransE <ref type="bibr">[5]</ref>, BoxE <ref type="bibr">[1]</ref>, DualE <ref type="bibr">[8]</ref>, ConEx <ref type="bibr">[10]</ref> and M 2 GNN <ref type="bibr" target="#b67">[42]</ref>, in which BoxE, DualE, ConEx and M 2 GNN are the state-of-the-arts achieving leading performance on benchmark datasets. In this experiment, we force them to treat literals as entities. (ii) Two KG completion methods for both relational and literal facts, MT-KGNN <ref type="bibr" target="#b62">[37]</ref> and KBLRN <ref type="bibr">[15]</ref>. (iii) Two open KG completion methods supporting external texts, OWE <ref type="bibr" target="#b58">[33]</ref> and Con-Mask <ref type="bibr" target="#b60">[35]</ref>. We adopt the TransE version of OWE in this experiment.</p><p>(iv) Two open KG completion methods addressing noisy claims, Knowledge Vault <ref type="bibr">[14]</ref> and OKELE <ref type="bibr">[7]</ref>. Note that, for all of them, we carefully read their papers and tune the (hyper)parameters.</p><p>Results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of Fact Scoring</head><p>To evaluate the effectiveness of fact scoring functions for different types of values, we modify two variants of TKGC, namely TKGC without literal fact scoring function (w/o F 𝑙𝑖𝑡 ) and TKGC without relational fact scoring function (w/o F 𝑟𝑒𝑙 ). We compare their performance on KG completion to the full TKGC. Notice that TKGC w/o F 𝑙𝑖𝑡 cannot complete missing literal values (marked as "N/A"), thus we only test its performance on relational facts. Similarly, we only test the performance of TKGC w/o F 𝑟𝑒𝑙 on completing missing literal facts. This experiment can be regarded as an ablation study.</p><p>As shown in Table <ref type="table" target="#tab_5">3</ref>, we have the following finding: TKGC w/o F 𝑙𝑖𝑡 loses 0.099 of precision, 0.010 of recall and 0.056 of F1-score when completing relational facts, and TKGC w/o F 𝑟𝑒𝑙 increases 0.017 of MAE and 0.021 of RMSE. These significant performance drops show that literal fact scoring and relational fact scoring both contribute to the overall performance and can benefit each other. For example, Titanic and Britannic have the same overall length "269m", as both of their ship types are Olympic Class Ocean Liner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation of Value Alignment</head><p>Effectiveness of literal-literal alignment. We test the literalliteral alignment network with several string matching methods. We manually label each pair of values in the dataset corresponding to the same attribute of the same entity, and create a literal-literal alignment dataset containing 1,126 literal pairs and 298 matches.</p><p>We split the dataset into three parts with the ratio of 3:1:1 for training, validation and evaluation, respectively. In this experiment, we choose six competitors from three categories: (i) Two conventional string similarity measures <ref type="bibr">[13]</ref>, exact matching and Levenshtein edit distance. (ii) Two deep text matching models, ESIM <ref type="bibr">[9]</ref> and RE2 <ref type="bibr" target="#b73">[48]</ref>. (iii) Two deep entity matching models based on literals, Deep-Matcher <ref type="bibr">[24]</ref> and EMTransformer <ref type="bibr">[6]</ref>. As EMTransformer directly employs BERT, we do not include BERT <ref type="bibr">[12]</ref> as a baseline. Table <ref type="table" target="#tab_6">4</ref> lists the comparison results, and we gain a few observations: (i) Exact matching obtains the best precision while the worst recall, simply because it cannot handle different literals expressing the same meaning. (ii) The five deep models perform better than the two similarity measures, as they can tolerate heterogeneity with token embeddings. (iii) Among these deep models, TKGC achieves the best precision, recall and F1-score.</p><p>Effectiveness of literal-entity alignment. The target of literalentity alignment is to validate the correctness of facts with corresponding entities outside the KG. By feeding the entity embeddings produced by literal-entity alignment into the fact scoring function, we evaluate the performance of TKGC on completing relational facts with entities inside and outside the KG.</p><p>Table <ref type="table" target="#tab_7">5</ref> presents the results, and we have some findings: (i) All the methods achieve higher precision, recall and F1-score on completing the facts with entities inside the KG than those outside the KG. This shows the difficulty of KG completion outside the KG. (ii) For finding the facts with entities outside the KG, TKGC gains the best performance. However, this is still a challenging problem and worth further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation of Truth Inference</head><p>We choose eight popular truth inference methods from four categories <ref type="bibr">[22,</ref><ref type="bibr" target="#b76">51]</ref>: (i) a direct computation method, majority voting <ref type="bibr" target="#b76">[51]</ref>, (ii) two iterative methods, TruthFinder <ref type="bibr" target="#b74">[49]</ref> and PooledInvestment <ref type="bibr" target="#b52">[27]</ref>, (iii) an optimization-based method, CATD <ref type="bibr">[21]</ref>, and (iv) four probabilistic graphical models, LTM <ref type="bibr" target="#b75">[50]</ref>, LCA <ref type="bibr" target="#b53">[28]</ref>, MBM <ref type="bibr" target="#b68">[43]</ref> and BWA <ref type="bibr">[23]</ref>. It is worth mentioning that majority voting, TruthFinder, PooledInvestment, LTM, LCA and MBM only address single-choice truth inference. Following <ref type="bibr">[21,</ref><ref type="bibr" target="#b76">51]</ref>, we adapt them to conduct truth inference on multi-valued attributes for a fair comparison.</p><p>As shown in Table <ref type="table" target="#tab_9">6</ref>, we get the following findings: (i) For literal facts, compared to the best competitor LTM, TKGC reduces 0.017 of MAE and 0.031 of RMSE. The main reason is that TKGC leverages correlation with existing facts to determine literal values. For example, Mount Everest and Nanga Parbat have similar elevations,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Sensitivity Analysis</head><p>Proportion of noises. We add random noises into the claim set to investigate how TKGC can tolerate noisy data. For each value in the claim set, we randomly substitute it by another value with the probability of 0%, 5%, 10%, 15%, 20% and 25%. We repeat this experiment five times, and report the results on average. We see in Figure <ref type="figure" target="#fig_3">3</ref>(a) that, while the noises increase up to 25%, the RMSE only increases 0.027 and the F1-score drops 0.092, indicating that TKGC can infer correct values from noisy claims to varying degrees.</p><p>Proportion of claims. We analyze how the proportion of claims affects TKGC. We randomly preserve 0%, 20%, 40%, 60%, 80% and 100% of claims, and evaluate the performance. We repeat this experiment five times, and present the results on average. Based on Figure <ref type="figure" target="#fig_3">3</ref>(b), we obtain two findings: (i) Compared TKGC with all claims, TKGC without claims increases 0.031 of RMSE, but significantly drops 0.293 of precision and 0.211 of F1-score. The main reason is that claims can provide facts with unseen entities. (ii) As the amount of claims increases, the performance of TKGC consistently improves. One important reason is that the truth inference in TKGC improves its accuracy based on redundancy. When there are not enough claims, TKGC cannot rule out wrong facts.</p><p>Proportion of prior knowledge. We also analyze how the proportion of facts in the KG affects TKGC. We randomly preserve 0%, 20%, 40%, 60%, 80% and 100% of facts in the KG, and evaluate the performance. Again, we repeat this experiment five times, and report the results on average. As shown in Figure <ref type="figure" target="#fig_3">3</ref>(c), we receive two observations: (i) When there is no prior knowledge, TKGC only increases 0.015 of RMSE, but significantly loses 0.283 of recall and 0.102 of F1-score. The main reason is that TKGC cannot learn entity embeddings to capture related entities without existing facts in the KG, thus it fails to find these entities. (ii) As the proportion of prior knowledge increases, the performance of TKGC largely improves. This shows that the embeddings learned by prior knowledge can improve the quality of truth inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain variance.</head><p>We present the performance variance of TKGC on different domains in Figure <ref type="figure" target="#fig_3">3(d)</ref>. We observe that the F1-score of TKGC varies from 0.198 to 0.682, and the standard deviation of F1-score is 0.173. The main factor is the quality of data sources. For example, TKGC obtains the highest F1-score on the "film" class, which includes some high-quality sources like IMDB and Metacritic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose a trustworthy KG completion method based on multi-sourced noisy data. We introduce a holistic scoring function that measures the plausibility of both relational facts and literal facts with various value types. We design value alignment networks to resolve the heterogeneous values in claims and predict some of them as entities. We propose a truth inference model to incorporate data source qualities into the fact scoring model and infer the truths from inconsistent values by semi-supervised learning. The experimental results show that our method consistently achieves the best performance. Compared with the best competitors for KG completion, our method reduces 0.020 of RMSE for completing literal facts, and improves 0.048 of F1-score for completing relational facts. Our models for value alignment, fact scoring with unseen entities and truth inference also gain superior results. In future work, we plan to extend our method to choose data sources for claim retrieval. We also want to apply our method to validate the facts from relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MODEL COMPLEXITY</head><p>In fact scoring, the GNN contains (𝐾 − 1)𝑑 2 ℎ + 𝑑 ℎ 𝑑 𝑒 parameters, initial entity embeddings contain 1  2 𝑁 𝑒 𝑑 𝑒 parameters, attribute embeddings contain 𝑁 𝑟𝑒𝑙 𝑑 2 𝑒 + 𝑁 𝑙𝑖𝑡 (𝑑 𝑒 + 1) parameters, and string encoder contains 𝑑 𝐵𝐸𝑅𝑇 𝑑 𝑒 parameters, where 𝑑 ℎ is the dimension of GNN hidden layers, 𝑁 𝑒 is the number of entities, 𝑁 𝑟𝑒𝑙 is the number of attributes used in relational fact scoring, 𝑁 𝑙𝑖𝑡 is the number of attribute used in literal fact scoring, and 𝑑 𝐵𝐸𝑅𝑇 is the output dimension of BERT.</p><p>In value alignment networks, the literal-literal alignment network contains 8𝑑 2  𝐵𝐸𝑅𝑇 + 𝑑 𝐵𝐸𝑅𝑇 𝑑 𝑙𝑙 + 𝑑 𝑙𝑙 parameters, where 𝑑 𝑙𝑙 is the dimension of its hidden layer. The literal-entity alignment network contains (𝑑 𝐵𝐸𝑅𝑇 + 𝑑 𝑒 )𝑑 𝑙𝑒 parameters, where 𝑑 𝑙𝑒 is the dimension of its hidden layer.</p><p>The truth inference contains 𝑁 𝑙𝑖𝑡 + 𝑁 𝑟𝑒𝑙 + 𝑁 𝑠 +𝑑 𝑒 𝑑 𝑑𝑒 parameters, where 𝑁 𝑠 is the number of data sources, and 𝑑 𝑑𝑒 is the hidden layer dimension of entity difference function.</p><p>In total, the parameter number of our method is (𝐾 − 1)𝑑 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPERPARAMETER SETTING</head><p>We search the following hyperparameter values for model training: the learning rate in {0.0001, 0.0005, 0.001, 0.005, 0.01} and the batch size in {64, 128, 256, 512, 1024}. The selected learning rate is 0.005, and the picked batch size is 128.</p><p>For holistic fact scoring, we try number 𝐾 of GNN layers in {1, 2, 3, 4}, the dimension of hidden layers in {100, 200, 300, 400, 500}, and the dimensions of entity and value embeddings in {25, 50, 100, 200, 300}. We randomly sample 50 neighbors for each entity, and choose a two-layer GNN (i.e., 𝐾 = 2) with the dimension of hidden layers 100. The dimensions of entity and value embeddings are both set to 100. The dimension of relational attribute embeddings is 100 × 100, and the dimension of literal attribute embeddings is 100. The threshold for value set size is 𝑁 𝑣 = 10. For value alignment networks, we attempt the dimension of MLP hidden layers in {100, 200, 300, 400, 500}. The chosen dimension for both literal-literal alignment network and literal-entity alignment network is 100.</p><p>For semi-supervised truth inference, we test the output dimension of linear transform W 𝑒 in the difference function in {25, 50, 75, 100}, and set W 𝑒 = 100 × 25. The 𝐹𝐴𝐶𝑇 _𝐸𝑃𝑂𝐶𝐻𝑆 is set to 20, and 𝐼 𝑁 𝐹 𝐸𝑅𝐸𝑁𝐶𝐸_𝐸𝑃𝑂𝐶𝐻𝑆 is also set to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C RUNTIME ANALYSIS</head><p>We compare the overall runtime of TKGC with two existing open KG completion methods, OKELE and Knowledge Vault, as well as two variants of TKGC, namely TKGC w/o F 𝑙𝑖𝑡 and TKGC w/o F 𝑟𝑒𝑙 .</p><p>As shown in Table <ref type="table" target="#tab_11">7</ref>, we have the following findings: (i) OKEKE runs relatively faster because it only uses facts in the KG for property prediction, but does not use them for truth inference. Differently, TKGC learns prior knowledge inside the KG with fact scoring to enhance truth inference, making it spend more time. (ii) Unlike Knowledge Vault that only considers the trustworthiness of facts, TKGC additionally considers the trustworthiness of data sources.</p><p>The more complex truth inference model causes TKGC to run a little slower than Knowledge Vault. (iii) Compared with TKGC w/o F 𝑙𝑖𝑡 , TKGC spends 0.3 minutes to process 5,727 literal facts. Compared with TKGC w/o F 𝑟𝑒𝑙 , TKGC spends 4.8 minutes to process 186,032 relational facts. The remaining runtime of TKGC is mainly spent by the GNN module that aggregates the neighboring information of entities. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow of the method A data source 𝑠 provides a collection of noisy facts called claims that may be inaccurate or incorrect. A claim extracted from a data source 𝑠 is denoted by (𝑒, 𝑎, 𝑣, 𝑠), which means that source 𝑠 claims "attribute 𝑎 of entity 𝑒 has value 𝑣". Trustworthy KG completion aims to resolve inconsistent claims (e.g., different values of a given pair of entity and attribute) from multiple data sources, and identify the trustworthy claims called truths. Problem definition. Given a KG 𝐾 = (𝐸, 𝐴, 𝐿,𝑇 𝑟𝑒𝑙 ,𝑇 𝑙𝑖𝑡 ) as prior knowledge and a set of claims 𝐶 = (𝑒, 𝑎, 𝑣, 𝑠) 𝑖 𝐼 𝑖=1 extracted from a set of data sources 𝑆 = {𝑠 𝑗 } 𝐽 𝑗=1 , the task of trustworthy KG completion is to identify the truths 𝐶 ′ ⊆ 𝐶 and add them into 𝐾.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Since the truth value 𝑣 * is unknown in the truth inference problem, we seek to estimate the conditional probability Pr[𝑣 | 𝑒, 𝑎, 𝑠] instead of Pr[𝑣 | 𝑣 * , 𝑠]. We intend to leverage truth value 𝑣 * to bridge the observed value 𝑣 and the condition (𝑒, 𝑎, 𝑠). Based on the law of total probability, we can integrate over all possible 𝑣 * to evaluate the conditional probability: Pr[𝑣 | 𝑒, 𝑎, 𝑠] = ∫ 𝑣 * Pr[𝑣 | 𝑣 * , 𝑒, 𝑎, 𝑠] Pr[𝑣 * | 𝑒, 𝑎, 𝑠]𝑑𝑣 * . (14) As we assume that the observed value 𝑣 is directly determined by truth value 𝑣 * and data source 𝑠, 𝑣 is conditionally independent with 𝑒, 𝑎, i.e., Pr[𝑣 | 𝑣 * , 𝑒, 𝑎, 𝑠] = Pr[𝑣 | 𝑣 * , 𝑠]. Moreover, as the truth value 𝑣 * is determined by entity 𝑒 and attribute 𝑎, we have Pr[𝑣 * | 𝑒, 𝑎, 𝑠] = Pr[𝑣 * | 𝑒, 𝑎]. Together, we have Pr[𝑣 | 𝑒, 𝑎, 𝑠] = ∫ 𝑣 * Pr[𝑣 | 𝑣 * , 𝑠] Pr[𝑣 * | 𝑒, 𝑎]𝑑𝑣 * .(15)While the truth value space is undetermined in truth inference, we can use the possible truth value set 𝑉 * 𝑒,𝑎 = {𝑣 * | (𝑒, 𝑎, 𝑣 * , 𝑠 ′ ) ∈ 𝐶} to approximate it, where 𝐶 is the claim set. Because we assume that observed values are close to the truth value, they should have larger probabilities than unobserved values. Thus, the unobserved values have lower impact, and we integrate over the observed value space instead of the truth value space:Pr[𝑣 | 𝑒, 𝑎, 𝑠] ≈ 𝑣 * ∈𝑉 * 𝑒,𝑎 Pr[𝑣 | 𝑣 * , 𝑠] Pr[𝑣 * | 𝑒, 𝑎] 𝑣 * ∈𝑉 * 𝑒,𝑎 Pr[𝑣 * | 𝑒, 𝑎] ,(16)where 𝑉 * 𝑒,𝑎 is the possible truth value set for entity 𝑒 and attribute 𝑎. Note that 𝑣 * ∈𝑉 * 𝑒,𝑎 Pr[𝑣 * | 𝑒, 𝑎] is used to normalize the conditional probability such that it falls into [0, 1]. To evaluate the above observed value probability, we use the confusion probability to obtain Pr[𝑣 | 𝑣 * , 𝑠] and the fact scoring function F to obtain Pr[𝑣 * | 𝑒, 𝑎] = exp(−F (𝑒, 𝑎, 𝑣)). For literal values, Pr[𝑣 * | 𝑒, 𝑎] = F (𝑒, 𝑎, 𝑣 * ), and for entities, Pr[𝑣 * | 𝑒, 𝑎] = F (𝑒, 𝑎, LE-ANN(𝑣 * )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sensitivity analysis of noises, claims, prior knowledge and domains</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝑎, and norm 𝑎 () is a normalization function defined as norm 𝑎 (𝑣) = 𝑣−min 𝑉 𝑎 max 𝑉 𝑎 −min 𝑉 𝑎 . When max 𝑉 𝑎 = min 𝑉 𝑎 , we simply discard the facts containing attribute 𝑎, as these values are statistically meaningless. When 𝑎 is a 𝑘-valued attribute, we apply 𝑘 different embeddings {a 𝑖 } 𝑘 𝑖=1 and biases {𝑏 𝑎,𝑖 } 𝑘 𝑖=1 to fit 𝑘 values {𝑣 𝑖 } 𝑘 𝑖=1 of entity 𝑒, and the scoring function is</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Semi-supervised truth inference algorithm Input: Fact set 𝑇 = 𝑇 𝑙𝑖𝑡 ∪ 𝑇 𝑟𝑒𝑙 and claim set 𝐶 Output: New fact set 𝑇 𝑛𝑒𝑤</figDesc><table><row><cell>8 9</cell><cell>𝑉  *  𝑒,𝑎 ← {𝑣  *  | (𝑒, 𝑎, 𝑣  *  , 𝑠 ′ ) ∈ 𝐶}; foreach 𝑣  *  ∈ 𝑉  *  𝑒,𝑎 do</cell></row></table><note>1 Initialize model parameters, and let 𝑇 𝑛𝑒𝑤 ← ∅; 2 for 𝑒𝑝𝑜𝑐ℎ = 1, 2, . . . , 𝐹𝐴𝐶𝑇 _𝐸𝑃𝑂𝐶𝐻𝑆 do 3 Compute L 𝑓 𝑎𝑐𝑡 based on Eq. (8); 4 Update model parameters based on L 𝑓 𝑎𝑐𝑡 ; 5 for 𝑒𝑝𝑜𝑐ℎ = 1, 2, . . . , 𝐼 𝑁 𝐹 𝐸𝑅𝐸𝑁𝐶𝐸_𝐸𝑃𝑂𝐶𝐻𝑆 do 6 Compute L 𝑓 𝑎𝑐𝑡 based on Eq. (8), and let L 𝑐𝑙𝑎𝑖𝑚 ← 0; 7 foreach (𝑒, 𝑎, 𝑣, 𝑠) ∈ 𝐶 do // Compute L 𝑐𝑙𝑎𝑖𝑚 10 Compute Pr[𝑣 | 𝑣 * , 𝑠] and Pr[𝑣 * | 𝑒, 𝑎]; 11 L 𝑐𝑙𝑎𝑖𝑚 ← L 𝑐𝑙𝑎𝑖𝑚 − log Pr[𝑣 | 𝑒, 𝑎, 𝑠]; 12 Update model parameters based on L 𝑓 𝑎𝑐𝑡 + L 𝑐𝑙𝑎𝑖𝑚 ; 13 foreach (𝑒, 𝑎, 𝑣, 𝑠) ∈ 𝐶 do 14 if Pr[𝑣 | 𝑒, 𝑎] &gt; 0.5 then 𝑇 𝑛𝑒𝑤 ← 𝑇 𝑛𝑒𝑤 ∪ {(𝑒, 𝑎, 𝑣)}; 15 return 𝑇 𝑛𝑒𝑤 ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics 𝐵𝐸𝑅𝑇 +𝑑 𝑙𝑒 (𝑑 𝐵𝐸𝑅𝑇 +𝑑 𝑒 ) + 𝑁 𝑙𝑖𝑡 + 𝑁 𝑟𝑒𝑙 + |𝑆 | +𝑑 𝑒 𝑑 𝑑𝑒 . See Appendix A for more details.</figDesc><table><row><cell>Classes</cell><cell cols="2">Facts Literal Relational</cell><cell>Classes</cell><cell cols="2">Facts Literal Relational</cell></row><row><cell>actor book drug food ship</cell><cell>330 499 1,002 842 852</cell><cell>64,983 10,776 26,432 23,041 1,805</cell><cell>album building film mountain software</cell><cell>155 361 576 623 487</cell><cell>5,897 2,823 45,233 2,720 2,322</cell></row></table><note>each claim (𝑒, 𝑎, 𝑣, 𝑠), we add (𝑒, 𝑎, 𝑣) in the new fact set 𝑇 𝑛𝑒𝑤 if the fact plausibility of (𝑒, 𝑎, 𝑣) exceeds the threshold 0.5.The time complexity of Algorithm 1 is 𝑂 𝐹𝐴𝐶𝑇 |𝑇 | + 𝐼 𝑁 𝐹 𝐸𝑅𝐸𝑁𝐶𝐸_𝐸𝑃𝑂𝐶𝐻𝑆 |𝐶 | . The total parameter number of our method is (𝐾−1)𝑑 2 ℎ +𝑑 ℎ 𝑑 𝑒 +𝑑 𝑒 ( 1 4 𝑑 𝑒 + 1 2 |𝐸|+𝑑 𝐵𝐸𝑅𝑇 +𝑁 𝑟𝑒𝑙 +𝑁 𝑙𝑖𝑡 )+𝑁 𝑙𝑖𝑡 + 𝑑 𝑙𝑙 (𝑑 𝐵𝐸𝑅𝑇 + 1) + 8𝑑 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall performance</figDesc><table><row><cell>Methods TransE BoxE DualE ConEX M 2 GNN</cell><cell cols="2">MAE ↓ RMSE ↓ 0.270 0.289 0.183 0.192 0.162 0.173 0.154 0.164 0.167 0.175</cell><cell>P ↑ 0.259 0.327 0.289 R ↑ F1 ↑ 0.343 0.361 0.351 0.379 0.354 0.366 0.399 0.346 0.371 0.393 0.372 0.382</cell></row><row><cell>MT-KGNN KBLRN</cell><cell>0.096 0.109</cell><cell>0.105 0.116</cell><cell>0.377 0.354 0.365 0.323 0.322 0.322</cell></row><row><cell>OWE (TransE) ConMask</cell><cell>0.113 0.094</cell><cell>0.118 0.102</cell><cell>0.351 0.421 0.383 0.376 0.443 0.407</cell></row><row><cell>Knowledge Vault OKELE</cell><cell>0.115 0.078</cell><cell>0.123 0.082</cell><cell>0.385 0.455 0.417 0.436 0.485 0.459</cell></row><row><cell>TKGC (ours)</cell><cell>0.054</cell><cell>0.062</cell><cell>0.524 0.491 0.507</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>ConEX, M 2 GNN, MT-KGNN and KBLRN, the open KG completion methods, including OWE, ConMask, Knowledge Vault, OKELE and TKGC, increase at least 0.049 of recall. Those methods without external knowledge just choose entities inside the KG for</figDesc><table><row><cell>shows the comparison results, and we obtain the following findings: For literal fact completion, (i) TransE, BoxE, DualE, ConvEX and M 2 KGNN perform worst, due to that these four methods only focus on modeling relational facts. The other methods with specific design of literal fact scoring functions can deal with the literal fact completion better. (ii) TKGC achieves the best performance. Compared with the second-best method OKELE, TKGC improves 0.024 of MAE and 0.020 of RMSE. The key reason is that TKGC additionally considers the correlation with existing facts in the KG to enhance truth inference. (iii) The remaining five meth-ods, namely MT-KGNN, KBLRN, OWE, ConMask and Knowledge Vault, obtain comparable results. Each method has its own pros and cons. MT-KGNN and KBLRN leverage dedicated fact scoring functions for numbers, but they cannot handle entities outside the KG. OWE, ConMask and Knowledge Vault do not process numbers separately, but they can leverage external knowledge. For relational fact completion, (i) compared with TransE, BoxE, DualE,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of fact scoring functions</figDesc><table><row><cell>TKGC w/o F 𝑙𝑖𝑡 TKGC w/o F 𝑟𝑒𝑙 TKGC (full)</cell><cell>MAE ↓ RMSE ↓ N/A N/A 0.071 0.083 0.054 0.062</cell><cell>P ↑ 0.425 0.481 0.451 R ↑ F1 ↑ N/A N/A N/A 0.524 0.491 0.507</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of literal-literal alignment methods , which narrows the range of candidate entities. In contrast, the open KG completion methods can leverage the external knowledge from web pages to discover new facts with entities outside the KG. (ii) Compared with OWE and ConMask, Knowledge Vault, OKELE and TKGC achieve better precision. The reason is that OWE and ConMask neglect the noises in claims, while Knowledge Vault, OKELE and TKGC cope with the noises and eliminate incorrect ones. (iii) Compared with the best competitor OKELE, TKGC improves 0.088 of precision, 0.006 of recall and 0.048 of F1score. The root cause is that TKGC not only leverages facts in the KG to improve truth inference, but also makes subtle comparisons with entities based on value alignment. Differently, OKELE ignores the entity and attribute information when validating the values in claims, and its pipeline workflow causes more errors.See Appendix C for the runtime comparison.</figDesc><table><row><cell cols="2">Methods Exact matching 0.949 0.293 0.448 P ↑ R ↑ F1 ↑ Edit distance 0.691 0.721 0.706</cell></row><row><cell>ESIM RE2</cell><cell>0.718 0.728 0.723 0.792 0.755 0.773</cell></row><row><cell cols="2">DeepMatcher EMTransformer 0.890 0.783 0.833 0.814 0.782 0.798</cell></row><row><cell>TKGC</cell><cell>0.929 0.813 0.867</cell></row></table><note>completion</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of literal-entity alignment methods</figDesc><table><row><cell>Methods OWE ConMask</cell><cell>Inside KG R ↑ 0.358 0.428 0.390 0.314 0.384 0.346 Outside KG P ↑ F1 ↑ P ↑ R ↑ F1 ↑ 0.376 0.457 0.413 0.373 0.367 0.370</cell></row><row><cell cols="2">Knowledge Vault 0.386 0.471 0.424 0.381 0.369 0.375 OKELE 0.436 0.491 0.462 0.434 0.452 0.443</cell></row><row><cell>TKGC</cell><cell>0.537 0.492 0.514 0.461 0.483 0.471</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Analytical results of domain sensitivity</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ta</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00 0.10 0.20 0.30 0.40 0.50 0.60</cell><cell>0</cell><cell>20</cell><cell>40 % of noise 60</cell><cell>80</cell><cell>100 RMSE MAE P R F1</cell><cell>0.00 0.10 0.20 0.30 0.40 0.50 0.60</cell><cell>0</cell><cell>20</cell><cell>40 % of claims 60</cell><cell>80</cell><cell>100 RMSE MAE P R F1</cell><cell>0.00 0.10 0.20 0.30 0.40 0.50 0.60</cell><cell>0</cell><cell>20</cell><cell>40 % of triples in the KG 60</cell><cell>80</cell><cell>100 MAE RMSE P R F1</cell><cell>Classe film.ac music. book.b archite medici film.fil food.fo geogra boats.s compu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Claims</cell><cell></cell><cell></cell><cell>(c) Prior Knowledge</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">observe TKGC ha the perfo to addre names b errors. In same nam Figure 1: Sensitivity Analysis of Noises, Claims and Prior Know</cell></row><row><cell>MA E RMS E P R F1</cell><cell>0.00 0.10 0.20 0.30 0.40 0.50 0.60</cell><cell>0</cell><cell>2 0</cell><cell>4 0 % of facts in KG 6 0</cell><cell>8 0</cell><cell>1 0 0 MA E RMS E P R F1</cell><cell cols="8">Classes actor album book building drug film food mountain 0.035 MAE ↓ RMSE ↓ 0.066 0.076 0.020 0.026 0.036 0.041 0.043 0.050 0.067 0.079 0.072 0.083 0.053 0.061 0.041 ship 0.071 0.083 software 0.035 0.039</cell><cell cols="4">P ↑ 0.635 0.601 0.618 R ↑ F1 ↑ 0.203 0.192 0.198 0.348 0.335 0.341 0.403 0.390 0.396 0.669 0.608 0.637 0.706 0.660 0.682 0.515 0.478 0.496 0.329 0.311 0.320 0.682 0.640 0.660 0.348 0.307 0.326</cell><cell>Sensitiv claim se value in with pro Fig 1(a), of TKGC TKGC sl Sensitiv knowled 0%, 20%, the perf that even RMSE an inferenc that as t of TKGC by prior</cell></row><row><cell>(b) Claims</cell><cell></cell><cell></cell><cell cols="3">(c) Prior knowledge</cell><cell></cell><cell cols="13">RMSE and 0.405 of F1-score. It shows that TKGC is an effective truth inference methods to process noisy claims. In addition, we notice that as the amount of prior knowledge increases, the performance of TKGC consistently improves. It shows that embeddings learned by prior knowledge can improve the quality of truth inference.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of truth inference methods</figDesc><table><row><cell>Methods Majority voting</cell><cell cols="2">MAE ↓ RMSE ↓ 0.134 0.173</cell><cell>P ↑ 0.321 0.419 0.364 R ↑ F1 ↑</cell></row><row><cell>TruthFinder PooledInvestment</cell><cell>0.129 0.091</cell><cell>0.153 0.108</cell><cell>0.279 0.374 0.320 0.397 0.380 0.388</cell></row><row><cell>CATD</cell><cell>0.127</cell><cell>0.145</cell><cell>0.432 0.423 0.427</cell></row><row><cell>LTM LCA MBM BWA</cell><cell>0.071 0.106 0.104 0.088</cell><cell>0.093 0.130 0.125 0.102</cell><cell>0.262 0.394 0.315 0.364 0.404 0.383 0.340 0.539 0.417 0.414 0.408 0.411</cell></row><row><cell>TKGC</cell><cell>0.054</cell><cell>0.062</cell><cell>0.524 0.491 0.507</cell></row></table><note>as they are both located in the Himalayas mountain range. (ii) For relational facts, compared with the best competitor CATD, TKGC improves 0.092 of precision, 0.068 of recall and 0.080 of F1-score. One reason is that TKGC can leverage facts in the KG to judge the correctness of claims. Another reason is that all competitors process entities independently, but TKGC makes subtle comparisons between entities, e.g., (Born This Way, genre, Pop Rock) can infer (Born This Way, genre, Pop).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2 ℎ + 𝑑 ℎ 𝑑 𝑒 + 𝑑 𝑒 ( 1 4 𝑑 𝑒 + 1 2 𝑁 𝑒 + 𝑁 𝑟𝑒𝑙 + 𝑑 𝐵𝐸𝑅𝑇 + 𝑁 𝑙𝑖𝑡 ) + 𝑁 𝑙𝑖𝑡 + 𝑑 𝑙𝑙 (𝑑 𝐵𝐸𝑅𝑇 + 1) + 8𝑑 2 𝐵𝐸𝑅𝑇 + 𝑑 𝑙𝑒 (𝑑 𝐵𝐸𝑅𝑇 + 𝑑 𝑒 ) + 𝑁 𝑙𝑖𝑡 + 𝑁 𝑟𝑒𝑙 + 𝑁 𝑠 + 𝑑 𝑒 𝑑 𝑑𝑒 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Runtime of open KG completion methods</figDesc><table><row><cell>Methods</cell><cell>Runtime (min.)</cell></row><row><cell>OKELE Knowledge Vault</cell><cell>7.3 16.5</cell></row><row><cell>TKGC w/o F 𝑙𝑖𝑡 TKGC w/o F 𝑟𝑒𝑙 TKGC (full)</cell><cell>17.8 13.3 18.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by National Natural Science Foundation of China (No. 61872172), and Alibaba Group through Alibaba Research Fellowship Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC/ASWC</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno>SIGMOD. 1247-1250</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph representation and reasoning</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.05.101</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.05.101" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">461</biblScope>
			<biblScope unit="page" from="494" to="496" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open knowledge enrichment for long-tail entities</title>
		<author>
			<persName><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xiaochun Cao, and Qingming Huang. 2021. Dual quaternion knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<biblScope unit="page" from="6894" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Philip Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Complex Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-77385-4_24</idno>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Phy</surname></persName>
		</author>
		<author>
			<persName><surname>Strohmann</surname></persName>
		</author>
		<title level="m">Shaoh A web-scale approach to probab</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García</surname></persName>
		</author>
		<title level="m">Durán and Math knowledge base representations In UAI</title>
				<imprint>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Zhitao Yin tation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Hajishirzi</surname></persName>
		</author>
		<title level="m">Text generation In NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su Han</surname></persName>
		</author>
		<title level="m">A confidence-aware Proc. VLDB Endow</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bo ing conflicts in heterogeneous estimation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<idno>SIGMOD. 1187-11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rubinste scale: A bayesian model for adju In WWW</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Benjamin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1028" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Truth discovery by claim a Eng</title>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1264" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open-wor interaction attention</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenpeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<idno>1007/s11280-020-00847-2</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">World Wid</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An Open-World Ex In AAAI</title>
		<author>
			<persName><forename type="first">Haseeb</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Villmow Shafait</surname></persName>
		</author>
		<idno>1609/aaai.v33i01.33013044</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>AAAI Press</publisher>
			<pubPlace>Honolulu</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<ptr target="aaai.org/ocs/index.php/AAAI/A" />
		<title level="m">20 In AAAI</title>
				<meeting><address><addrLine>New Orle</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><surname>Tuan</surname></persName>
		</author>
		<title level="m">Minh C Neural Network for Non-discret CIKM</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1029" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">K</forename><surname>Knowledgebase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Embedding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey of Approac and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">20 2017.2754499</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bi Mixed-Curvature Multi-Relation Completion</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cícer</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">O</forename><surname>Arnold</surname></persName>
		</author>
		<idno>WWW. ACM / 3442381.3450118</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Whose vote should count m of unknown expertise</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<idno>NIPS. 2</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Haixu probabilistic taxonomy for text (d) Domains REFERENCES</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BoxE: A Box Embedding Model for Knowledge Base Completion</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9649" to="9661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC/ASWC</title>
				<meeting><address><addrLine>Busan, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP. ACL</title>
				<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. ACM</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Entity Matching with Transformer Architectures -A Step Forward in Data Integration</title>
		<author>
			<persName><forename type="first">Ursin</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Stockinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
				<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open Knowledge Enrichment for Long-tail Entities</title>
		<author>
			<persName><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual Quaternion Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convolutional Complex Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<editor>ESWC. Springer</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="409" to="424" />
			<pubPlace>Heraklion, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
				<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Principles of Data Integration</title>
		<author>
			<persName><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>Waltham, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD. ACM</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">KBLRN: End-to-end Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Survey on Knowledge Graphs: Representation, Acquisition and Applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems early access</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
				<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Incorporating Literals into Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Agustinus</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Asif</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<meeting><address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="347" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Confidence-aware Approach for Truth Discovery on Long-tail Data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Demirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="425" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuishi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey on Truth Discovery. ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Truth Inference at Scale: A Bayesian Model for Adjudicating Highly Redundant Crowd Annotations</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1028" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Learning for Entity Matching: A Design Space Exploration</title>
		<author>
			<persName><forename type="first">Sidharth</forename><surname>Mudgal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngchoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Deep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Arcaute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Raghavendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. ACM</title>
				<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
				<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Open-world Knowledge Graph Completion with Multiple Interaction Attention</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenpeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Knowing What to Believe (when you already know something)</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pasternack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING. ACL</title>
				<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="877" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Latent Credibility Analysis</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pasternack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno>WWW. IW3C2</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1009" to="1020" />
			<pubPlace>Rio de Janeiro, Brazil</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding for Link Prediction: A Comparative Analysis</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donatella</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Matinata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
				<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>4th ed.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Old is Gold: Linguistic Driven Approach for Entity and Relation Linking of Short Text</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Sakor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeedeh</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Esther</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
				<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2336" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An Open-world Extension to Knowledge Graph Completion Models</title>
		<author>
			<persName><forename type="first">Haseeb</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Villmow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3044" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end Structure-aware Convolutional Networks for Knowledge Base Completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Open-world Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Features of Similarity</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Composition-based Multi-relational Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Wikidata: A Free Collaborative Knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mixed-curvature Multi-relational Graph Neural Network for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cícero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">O</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">F</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1761" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An Integrated Bayesian Approach for Effective Multi-truth Discovery</title>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susie</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
				<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scalable Zero-shot Entity Linking with Dense Entity Retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL, online</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Probase: A Probabilistic Taxonomy for Text Understanding</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><surname>Qili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. ACM</title>
				<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Representation Learning of Knowledge Graphs with Entity Descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR. OpenReview.net</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Simple and Effective Text Matching with Richer Alignment Features</title>
		<author>
			<persName><forename type="first">Runqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
				<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4699" to="4709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Truth Discovery with Multiple Conflicting Information Providers on the Web</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="796" to="808" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A Bayesian Approach to Discovering Truth from Conflicting Sources for Data Integration</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="550" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Yudian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caihua</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<title level="m">Truth Inference in Crowdsourcing: Is the Problem Solved? Proceedings of the VLDB Endowment</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="541" to="552" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
