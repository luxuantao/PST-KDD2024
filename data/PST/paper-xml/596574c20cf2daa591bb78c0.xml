<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilinear Convolutional Neural Networks for Fine-grained Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
						</author>
						<title level="a" type="main">Bilinear Convolutional Neural Networks for Fine-grained Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9DFCD46C083DAFA6AFF5E84CC524BE6F</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2723400</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2723400, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained recognition</term>
					<term>Texture representations</term>
					<term>Second order pooling</term>
					<term>Bilinear models</term>
					<term>Convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple and effective architecture for fine-grained recognition called Bilinear Convolutional Neural Networks (B-CNNs). These networks represent an image as a pooled outer product of features derived from two CNNs and capture localized feature interactions in a translationally invariant manner. B-CNNs are related to orderless texture representations built on deep features but can be trained in an end-to-end manner. Our most accurate model obtains 84.1%, 79.4%, 84.5% and 91.3% per-image accuracy on the Caltech-UCSD birds [66], NABirds [63], FGVC aircraft <ref type="bibr" target="#b39">[42]</ref>, and Stanford cars <ref type="bibr" target="#b30">[33]</ref> dataset respectively and runs at 30 frames-per-second on a NVIDIA Titan X GPU. We then present a systematic analysis of these networks and show that (1) the bilinear features are highly redundant and can be reduced by an order of magnitude in size without significant loss in accuracy, (2) are also effective for other image classification tasks such as texture and scene recognition, and (3) can be trained from scratch on the ImageNet dataset offering consistent improvements over the baseline architecture. Finally, we present visualizations of these models on various datasets using top activations of neural units and gradient-based inversion techniques. The source code for the complete system is available at http://vis-www.cs.umass.edu/bcnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>F INE-GRAINED recognition involves classification of in- stances within a subordinate category. Examples include recognition of species of birds, models of cars, or breeds of dogs. These tasks often require recognition of highly localized attributes of objects while being invariant to their pose and location in the image. For example, distinguishing a "California gull" from a "Ringed-bill gull" requires the recognition of patterns on their bill, or subtle color differences of their feathers <ref type="bibr">[1]</ref>. There are two broad classes of techniques that are effective for these tasks.</p><p>Part-based models construct representations by localizing parts and extracting features conditioned on their detected locations. This makes subsequent reasoning about appearance easier since the variations due to location, pose, and viewpoint changes are factored out. Holistic models on the other hand construct a representation of the entire image directly. These include classical image representations, such as Bag-of-Visual-Words <ref type="bibr" target="#b9">[12]</ref> and their variants popularized for texture analysis. Most modern approaches are based on representations extracted using Convolutional Neural Networks (CNNs) pre-trained on the ImageNet dataset <ref type="bibr" target="#b51">[54]</ref>. While part-based models based on CNNs are more accurate, they require part annotations during training. This makes them less applicable in domains where such annotations are difficult or expensive to obtain, including categories without a clearly defined set of parts such as textures and scenes.</p><p>In this paper we argue that the effectiveness of partbased reasoning on fine-grained recognition tasks is due to their invariance to translation and pose of the object. Texture representations are translationally invariant by design as they are based on aggregation of local image features in • T.-Y. Lin, A. RoyChowdhury, and S. Maji are with the College of Information and Computer Sciences, University of Massachusetts Amherst, USA. E-mails: {tsungyulin, arunirc, smaji}@cs.umass.edu an orderless manner. While classical texture representations based on SIFT <ref type="bibr" target="#b37">[40]</ref> and their recent extensions based on CNN features <ref type="bibr" target="#b8">[11]</ref>, <ref type="bibr" target="#b21">[24]</ref>, have been shown to be effective at fine-grained recognition, they have not matched the performance of part-based approaches. A potential reason for this gap is that the underlying features in texture representations are not learned in an end-to-end manner and are likely to be suboptimal for the recognition task. We present Bilinear CNNs (B-CNNs) that address several drawbacks of existing deep texture representations. Our key insight is that several widely-used texture representations can be written as a pooled outer product of two suitably designed features. When these features are based on CNNs the resulting architecture consists of standard CNN units for feature extraction, followed by a specially designed bilinear layer and a pooling layer. The output is a fixed highdimensional representation which can be combined with a fully-connected layer to predict class labels. The simplest bilinear layer is one where two identical features are combined with an outer product. This is closely related to the Second Order Pooling approach <ref type="bibr" target="#b5">[8]</ref> popularized for semantic image segmentation. We also show that other texture representations can be written as B-CNNs once suitable non-linearities are applied to the underlying features. This results in a family of layers which can be plugged into existing CNNs for end-to-end training on large datasets, or domain-specific fine-tuning for transfer learning. B-CNNs outperform existing models, including those trained with part-level supervision, on a variety of fine-grained recognition datasets. Moreover, these models are fairly efficient. Our most accurate model implemented in MatConvNet <ref type="bibr" target="#b62">[65]</ref> runs at 30 frames-per-second on a NVIDIA Titan X GPU and obtains 84.1%, 79.4%, 84.5% and 91.3% per-image accuracy on Caltech-UCSD birds <ref type="bibr" target="#b63">[66]</ref>, NABirds <ref type="bibr" target="#b60">[63]</ref>, FGVC aircraft <ref type="bibr" target="#b39">[42]</ref>, and Stanford cars <ref type="bibr" target="#b30">[33]</ref> dataset respectively. This manuscript combines the analysis of our earlier works <ref type="bibr" target="#b33">[36]</ref>, <ref type="bibr" target="#b34">[37]</ref> and extends them in a number of ways. We present an account of related work, including extensions published subsequently (Section 2). We describe the B-CNN architecture (Section 3), and present a unified analysis of exact and approximate end-to-end trainable formulations of Second Order Pooling (O2P), Fisher Vector (FV), Vectorof-Locally-Aggregated Descriptors (VLAD), Bag-of-Visual-Words (BoVW) in terms of their accuracy on a variety of fine-grained recognition datasets (Section 3.2-4). We show that the approach is general-purpose and is effective at other image classification tasks such as material, texture, and scene recognition (Section 4). We present a detailed analysis of dimensionality reduction techniques and provide trade-off curves between accuracy and dimensionality for different models, including a direct comparison with the recently proposed compact bilinear pooling technique <ref type="bibr" target="#b16">[19]</ref> (Section 5.1). Moreover, unlike prior texture representations based on networks pre-trained on the ImageNet dataset, B-CNNs can be trained from scratch and offer consistent improvements over the baseline architecture with a modest increase in the computation cost (Section 5.2). Finally we visualize the top activations of several units in the learned models and apply the gradient-based technique of Mahendran and Vedaldi <ref type="bibr" target="#b38">[41]</ref> to visualize inverse images on various texture and scene datasets (Section 5.3). We have released the complete source code for the system at http://vis-www.cs.umass.edu/bcnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Fine-grained recognition techniques. After AlexNet's <ref type="bibr" target="#b31">[34]</ref> impressive performance on the ImageNet classification challenge, several authors <ref type="bibr" target="#b10">[13]</ref>, <ref type="bibr" target="#b49">[52]</ref> have demonstrated that features extracted from layers of a CNN are effective at finegrained recognition tasks. Building on prior work on partbased techniques <ref type="bibr" target="#b2">[5]</ref>, <ref type="bibr" target="#b12">[15]</ref>, <ref type="bibr" target="#b67">[70]</ref>, Zhang et al. <ref type="bibr" target="#b66">[69]</ref>, and Branson et al. <ref type="bibr" target="#b3">[6]</ref> showed the benefits of combining CNN-based part detectors <ref type="bibr" target="#b20">[23]</ref> and CNN-based features for fine-grained recognition tasks. Other approaches have used segmentation to guide part discovery in a weakly-supervised manner to train part-based models <ref type="bibr" target="#b28">[31]</ref>. Among the non part-based techniques, texture descriptors such as FV and VLAD have traditionally been effective for fine-grained recognition. For example, the top performing method on FGCOMP'12 challenge used SIFT-based FV representation <ref type="bibr" target="#b22">[25]</ref>.</p><p>Recent improvements in deep architectures have also resulted in improvements in fine-grained recognition. This include architectures that have increased depth such as the "deep" <ref type="bibr" target="#b6">[9]</ref> and "very deep" <ref type="bibr" target="#b56">[59]</ref> networks from the Oxford's VGG group, inception networks <ref type="bibr" target="#b57">[60]</ref>, and "ultra deep" residual networks <ref type="bibr" target="#b23">[26]</ref>. Spatial Transformer Networks <ref type="bibr" target="#b26">[29]</ref> augment CNNs with parameterized image transformations and are highly effective at fine-grained recognition tasks. Other techniques augment CNNs with "attention" mechanisms that allow focused reasoning on regions of an image <ref type="bibr" target="#b1">[4]</ref>, <ref type="bibr" target="#b40">[43]</ref>. B-CNNs can be viewed as an implicit spatial attention model since the outer product modulates one feature based on the other, similar to the multiplicative feature interactions in attention mechanisms. Although not directly comparable, Krause et al. <ref type="bibr" target="#b29">[32]</ref> showed that the accuracy of deep networks can be improved significantly by using two orders of magnitude more training data obtained by querying category labels on search engines. Recently, Moghimi et al. <ref type="bibr" target="#b41">[44]</ref> showed boosting B-CNNs offers consistent improvements on fine-grained tasks.</p><p>Texture representations and second order features. Texture representations have been widely studied for decades. Early work <ref type="bibr" target="#b32">[35]</ref> represent the texture by computing the statistics of linear filter-bank responses (e.g., wavelets and steerable pyramids). The use of second-order features of filter bank responses was pioneered by Portilla and Simoncelli <ref type="bibr" target="#b47">[50]</ref>. FV <ref type="bibr" target="#b43">[46]</ref> and O2P <ref type="bibr" target="#b5">[8]</ref> with SIFT were shown to be a highly effective for image classification and semantic segmentation <ref type="bibr" target="#b11">[14]</ref> tasks respectively.</p><p>The advantages of combining orderless texture representations and deep features have been studied in a number of recent works. Gong et al. performed a multi-scale orderless pooling of CNN features <ref type="bibr" target="#b21">[24]</ref> for scene classification. Cimpoi et al. <ref type="bibr" target="#b8">[11]</ref> performed a systematic analysis of texture representations by replacing linear filter-banks with non-linear filter-banks derived from a CNN and showed it results in significant improvements on various texture, scene, and fine-grained recognition tasks. They found that orderless aggregation of CNN features was more effective than the commonly-used fully-connected layers on these tasks. However, a drawback of these approaches is that the filter-banks are not trained in an end-to-end manner. Our work is also related to the cross-layer pooling approach of Liu et al. <ref type="bibr" target="#b35">[38]</ref> who showed that second-order aggregation of features from two different layers of a CNN is effective at fine-grained recognition. Our work showed that feature normalization and domain-specific fine-tuning offers additional benefits, improving the from 77.0% to 84.1% accuracy using identical networks on the Caltech-UCSD Birds dataset <ref type="bibr" target="#b63">[66]</ref>. Another subsequently published work of interest is the NetVLAD architecture <ref type="bibr" target="#b0">[3]</ref> which provides an end-to-end trainable approximation of VLAD and was applied to image-based geolocation problem. NetVLAD can be written as a B-CNN and we include a comparison in Section 4.</p><p>Texture synthesis and style transfer. Concurrent to our work, Gatys et al. showed that the Gram-matrix representation of CNN features is an effective texture representation, and by matching the Gram-matrix of a target image one can create novel images with the same texture <ref type="bibr" target="#b17">[20]</ref> and transfer styles <ref type="bibr" target="#b18">[21]</ref>. While the Gram-matrix is identical to a pooled bilinear representation when the two features are the same, the emphasis of our work is recognition and not generation. This distinction is important since Ustyuzhaninov et al. <ref type="bibr" target="#b59">[62]</ref> show that the Gram-matrix of a shallow CNN with random filters is a good representations for texture synthesis, while discriminative pre-training and subsequent fine-tuning are essential to achieve high performance for recognition.</p><p>Polynomial kernels and sum-product networks. An alternate strategy for combining features from two networks is to concatenate them and learn their pairwise interactions through a series of layers on top. However, doing this naively requires a large number of parameters since there are O(n 2 ) interactions over O(n) features requiring a layer with O(n 3 ) parameters. Our explicit representation using an outer product has no parameters and is similar to a quadratic kernel expansion used in kernel support vector machines <ref type="bibr" target="#b52">[55]</ref>. However, one might be able to achieve similar approximations using alternate architectures such as sum-product networks that efficiently model multiplicative interactions <ref type="bibr" target="#b19">[22]</ref>.</p><p>Bilinear model variants and extensions. Bilinear models were used by Tanenbaum and Freeman <ref type="bibr" target="#b58">[61]</ref> to model twofactor variations such as "style" and "content" for images. While we also model two factor variations in location and appearance of parts, our goal is classification and not the explicit modeling of these factors. Our work is related to bilinear classifiers <ref type="bibr" target="#b46">[49]</ref> that express the classifier as a product of two low-rank matrices. Our models based on low dimensional representations described in Section 5.1 can be interpreted as bilinear classifiers. Our model is related to "two-stream" architectures used to analyze videos where one network models the temporal aspect, while the other models the spatial aspect <ref type="bibr" target="#b14">[17]</ref>, <ref type="bibr" target="#b55">[58]</ref>. The idea of combining two features using the outer product has also been shown to be effective for other tasks such as visual questionanswering <ref type="bibr" target="#b15">[18]</ref> where text and visual features are combined, action recognition <ref type="bibr" target="#b13">[16]</ref> where optical flow and image features are combined.</p><p>Low-dimensional bilinear features. A drawback of the bilinear features is the memory overhead of storing the high-dimensional features. For example, the outer product of 512 dimensional features results in a 512×512 dimensional representation. Our earlier work <ref type="bibr" target="#b34">[37]</ref> showed that the overall representation can be reduced to 512×64 dimensions by projection one of the features two a lower-dimensional space. Alternatively, the compact bilinear pooling <ref type="bibr" target="#b16">[19]</ref> applies tensor sketching <ref type="bibr" target="#b45">[48]</ref> to aggregate low-dimensional embeddings that approximate the bilinear features. In Section 5.1 we compare the two approaches and find that the projection method is simpler, faster, and equally effective. In most cases features size can be reduced 8-32× without significant loss in accuracy.</p><p>Scalability and speed. B-CNNs compare favorably to traditional CNN architectures in terms of speed since they replace several fully-connected layers with a bilinear pooling layer and a linear layer. Our MatConvNet-based <ref type="bibr" target="#b62">[65]</ref> implementation runs between 30 to 100 frames per second on a NVIDIA Titan X GPU with cudnn-v5 depending on the model architecture. Even with faster object detection modules such as Faster R-CNNs <ref type="bibr" target="#b50">[53]</ref> or Single-Shot Detector (SSD) <ref type="bibr" target="#b36">[39]</ref>, part-based models for fine-grained recognition are 2-10× slower. The main advantage of B-CNNs is that they require image labels only and can be easily applied to different fine-grained datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">B-CNNS FOR IMAGE CLASSIFICATION</head><p>In this section we introduce the B-CNN architecture for image classification and then show that various widely used texture representations can be written as B-CNNs. f : L × I → R K×D , that takes an image I ∈ I and a location l ∈ L and outputs a feature of size K × D. We refer to locations generally, which can include position and scale. The feature outputs are combined at each location using the matrix outer product, i.e., the bilinear combination of f A and f B at a location l is given by bilinear(l,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The B-CNN architecture</head><formula xml:id="formula_0">I, f A , f B ) = f A (l, I) T f B (l, I).<label>(1)</label></formula><p>Both f A and f B must have the same feature dimension K to be compatible. The value of K depends on the particular model. For example, K = 1 for BoVW model and equals the number of clusters in a FV model (details in Section 2). The pooling function P aggregates the bilinear combination of features across all locations in the image to obtain a global image representation Φ(I). We use sum pooling in all our experiments, i.e.,</p><formula xml:id="formula_1">Φ(I) = l∈L bilinear(l, I, f A , f B ) = l∈L f A (l, I) T f B (l, I).</formula><p>(2) Since the location of features is ignored during pooling, the bilinear feature Φ(I) is an orderless representation. If f A and f B extract features of size K × M and K × N at each location respectively, then Φ(I) is of size M ×N . The bilinear feature is a general-purpose image representation that can be used with a classifier C (Figure <ref type="figure" target="#fig_0">1</ref>). Intuitively, the outer product conditions the outputs of features f A and f B on each other by considering their pairwise interactions, similar to the feature expansion in a quadratic kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature functions</head><p>A natural candidate for the feature function f is a CNN consisting of a hierarchy of convolutional and pooling layers. In our experiments we use CNNs pre-trained on the ImageNet dataset truncated at an intermediate layer as feature functions. By pre-training we benefit when domainspecific data is limited. This has been shown to be effective for a number of tasks ranging from object detection, texture recognition, to fine-grained classification <ref type="bibr" target="#b7">[10]</ref>, <ref type="bibr" target="#b10">[13]</ref>, <ref type="bibr" target="#b20">[23]</ref>, <ref type="bibr" target="#b49">[52]</ref>. Another advantage of using CNNs is the resulting network can process images of an arbitrary size and produce outputs indexed by image location and feature channel.</p><p>Our earlier work <ref type="bibr" target="#b34">[37]</ref> experimented with models where the feature functions f A and f B were either independent or fully shared. Here we also experiment with feature functions that share a part of the feed-forward computation as seen in Figure <ref type="figure" target="#fig_3">3</ref>. The feature functions used to approximate classical texture representations we present in Section 3.2, as well as the low-dimensional B-CNNs we present in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Normalization and classification</head><p>We perform additional normalization steps where the bilinear feature x = Φ(I) is passed through a signed squareroot (y ← sign(x) |x|), followed by 2 normalization (z ← y/||y|| 2 ) inspired by <ref type="bibr" target="#b44">[47]</ref>. This improves performance in practice (see our earlier work <ref type="bibr" target="#b34">[37]</ref> for an evaluation of the effect of normalization). For classification we use logistic regression or linear SVM <ref type="bibr" target="#b52">[55]</ref>. Although this can be replaced with an arbitrary multi-layer network, we found that linear models are effective on top of bilinear features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">End-to-end training</head><p>Since the overall architecture is a directed acyclic graph, the parameters can be trained by back-propagating the gradients of the classification loss (e.g., cross-entropy). The bilinear form simplifies the gradient computations. If the outputs of the two networks are matrices A and B of size L × M and L × N respectively, then the bilinear feature is x = A T B of size M × N . Let d /dx be the gradient of the loss function with respect to x, then by chain rule of gradients we have:</p><formula xml:id="formula_2">d dA = B d dx T , d dB = A d dx .<label>(3)</label></formula><p>As long as the gradients of the features A and B can be computed efficiently the entire model can be trained in an end-to-end manner. The scheme is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation to classical texture representations</head><p>In this section we show that various orderless texture descriptors can be written in the bilinear form and derive variants that are end-to-end trainable. Since the properties of texture are usually translationally invariant, most texture representations are based on orderless aggregation of local image features, e.g., sum or max operation. A non-linear encoding is typically applied before aggregation of local features to improve their representation power. Additionally, a normalization of the aggregated feature (e.g., power and 2 ) is done to increase invariance. Thus, texture representations can be defined by the choice of the local features, the encoding function, the pooling function, and the normalization function.</p><p>To simplify further analysis, we will decompose the feature function f as f (l, I) = g(h(l, I)) = g(x) to denote the explicit dependency on the image and the location of h and additional non-linearities g. One of the earliest representation used for texture is the Bag-of-Visual-Words (BoVW) <ref type="bibr" target="#b9">[12]</ref>. It was shown to be effective at several recognition tasks beyond texture. While variants differ on how the visual words are learned, a popular approach is to obtain a set of k centers by clustering image features (e.g., using k-means). Each feature x is then assigned to the closest cluster center (also called "hard assignment") and the image is represented as a histogram denoting frequencies of each visual word. If we denote η(x) as the one-hot encoding that is 1 at the index of the closest center of x and zero elsewhere, then BoVW can be written as a bilinear model with g A (x) = 1 and g B (x) = η(x).</p><p>The VLAD representation <ref type="bibr" target="#b27">[30]</ref> encodes a descriptor x as (xµ k ) ⊗ η(x), where ⊗ is the kronecker product, µ k is the closest center to x, and η(x) is the one-hot encoding of x as before. These encodings are aggregated across spatial locations by sum pooling. Thus VLAD can be written as a bilinear model with</p><formula xml:id="formula_3">g A (x) = [x -µ 1 ; x -µ 2 ; . . . ; x -µ k ].</formula><p>Here, g A has k rows each corresponding to a center. And g B (x) = diag(η(x)), a matrix with η(x) in the diagonal and 0 elsewhere. Notice that the feature functions for VLAD output a matrix with k &gt; 1 rows at each location.</p><p>The FV representation <ref type="bibr" target="#b44">[47]</ref> computes both the first order</p><formula xml:id="formula_4">α i = Σ -1 2 i (x -µ i ) and second order β i = Σ -1 i (x -µ i ) (x -µ i ) -1 statistics,</formula><p>which are aggregated weighted by the Gaussian mixture model (GMM) posteriors θ(x). Here µ i and Σ i are the mean and covariance of the i th GMM component respectively and denotes element-wise multiplication. Thus, FV can be written as a bilinear model with g A = [α 1 β 1 ; α 2 β 2 ; . . . ; α k β k ] and g B = diag(θ(x)).</p><p>The O2P representation <ref type="bibr" target="#b5">[8]</ref> computes the covariance statistics of SIFT features within a region, followed by log-Euclidean mapping and power normalization. Their approach was shown to be effective for semantic segmentation. Thus, O2P can can be written as a bilinear model with symmetric features, i.e., f A = f B = f sift , followed by pooling and non-linearities.</p><p>The appearance-based cluster centers learned by the encoder, η(x) or θ(x), in the BoVW, VLAD and FV representations can be thought of as part detectors. Indeed it has been observed that the cluster centers tend to localize facial landmarks when trained on faces <ref type="bibr" target="#b42">[45]</ref>. Thus, by modeling the joint statistics of the encoder η(x) or θ(x), and the appearance x, the models can effectively describe appearance of parts regardless of where they appear in the image. This is particularly useful for fine-grained recognition where objects are not localized in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">End-to-end trainable formulations</head><p>Prior work on fine-grained recognition using texture encoders <ref type="bibr" target="#b8">[11]</ref>, <ref type="bibr" target="#b22">[25]</ref> did not learn the features in an end-toend manner. Below we describe a recently proposed end-toend trainable approximation of VLAD, and present similar formulations for all texture representations described in the earlier section. The ability to directly fine-tune these models leads to significant improvements in accuracy across a variety of fine-grained datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Texture encoders such as VLAD, FV, BoVW and O2P can be written as outer products of the form g A T g B . On the right are their end-to-end trainable formulations that simplify gradient computations by replacing "hard assignment" (η, θ) with "soft assignment" η, ignoring variance normalization for FV, etc. For the symmetric case (i.e., when f A = f B ) bilinear pooling is identical to O2P. See Section 3.2.1 for details.  Recently, an end-to-end trainable formulation of VLAD called NetVLAD was proposed by Arandjelović et al. <ref type="bibr" target="#b0">[3]</ref>. The first simplification was to replace the "hard assignment" η(x) in g B by a differentiable "soft assignment" η(x). Given the k-th cluster center µ k , the k-th component of the soft assignment vector for an input x is given by,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exact formulation End-to-end trainable formulation Model</head><formula xml:id="formula_5">g B (x) g A (x) g B (x) g A (x) VLAD diag(η(x)) x -µ diag(η(x)) x -µ FV diag(θ(x)) [α, β] diag(η(x)) [x -µ, (x -µ) (x -µ)] BoVW η(x) 1 η(x) 1 O2P x x x x image CNN A CNN B f A f B image CNN A CNN B f A f B CNN image CNN f A f B<label>(</label></formula><formula xml:id="formula_6">ηk (x) = e -γ||x-µ k || 2 k e -γ||x-µ k || 2 = e w k T x+b k k e w k T x+b k<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">w k = 2γµ k , b k = -γ||µ k || 2</formula><p>and γ is a parameter of the model. This is simply the softmax operation applied after a convolution layer with a bias term, and can be implemented using standard CNN building blocks. The function</p><formula xml:id="formula_8">g A remains unchanged [x -µ 1 ; x -µ 2 ; . . . ; x -µ k ].</formula><p>The second simplification is to decouple the dependence on µ of both the g A and g B during training which makes gradient computation easier. Thus in NetVLAD during training the weights w k , b k and µ k are independent parameters. We extend the NetVLAD to NetFV by appending the second order statistics to the feature g A , i.e.,</p><formula xml:id="formula_9">g A = [x - µ 1 , (x -µ 1 ) 2 ; x -µ 2 , (x -µ 2 ) 2 ; . . . ; x -µ k , (x -µ k ) 2 ].</formula><p>Here, the squaring is done in an element-wise manner, i.e., (x-µ i ) 2 = (x-µ i ) (x-µ i ). The feature g B is kept identical to NetVLAD. This simplification discards the covariances and priors present in the true GMM posterior used in the FV model. Similarly, the NetBoVW approximation to BoVW replaces the hard assignments by soft assignments η(x) computed in a manner similar to NetVLAD.</p><p>The O2P representation is identical to B-CNN when the feature functions f A and f B are identical. However, the O2P representation applies a log-Euclidean (matrixlogarithm) mapping to the pooled representation which is rather expensive to compute since it involves an Eigenvalue decomposition and currently does not have efficient implementation on GPUs. This significantly slows the forward and gradient computations of the entire network. Skipping this step allows us to efficiently fine-tune the model. We also note that concurrent to our publication <ref type="bibr" target="#b34">[37]</ref>, Ionescu et al. <ref type="bibr" target="#b25">[28]</ref> proposed a DeepO2P approach noted similar difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMAGE CLASSIFICATION EXPERIMENTS</head><p>We outline the models used in our experiments in Section 4.1. We then provide a comparison of various B-CNNs to prior work on fine-grained recognition in Section 4.2, and texture and scene recognition in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>Below we describe various models used in our experiments:</p><p>FV with SIFT. We implemented a FV based using dense SIFT features <ref type="bibr" target="#b44">[47]</ref> extracted using VLFEAT <ref type="bibr" target="#b61">[64]</ref>. The image is first resized to 448×448 and SIFT features with binsize of 8 pixels are computed densely across the image with a stride of 4 pixels. The features are PCA projected to 80 dimensions before learning a GMM with 256 components.</p><p>CNN with fully-connected (FC) layers. This is standard baseline where the features are extracted from the last FC layer, i.e., before the k-way classification layer of a CNN. The input image is resized to 224×224 (the input size of the CNN) and mean-subtracted before propagating it though the CNN. We consider two different representations: 4096 dimensional relu7 layer outputs of both the VGG-M network <ref type="bibr" target="#b6">[9]</ref> and the 16-layer VGG-D network <ref type="bibr" target="#b56">[59]</ref>.</p><p>FV/NetFV with CNNs. This denotes the method of <ref type="bibr" target="#b8">[11]</ref> that builds a descriptor using FV pooling of CNN filter bank responses with 64 GMM components. One modification over <ref type="bibr" target="#b8">[11]</ref> is that we first resize the image to 448×448 pixels, i.e., twice the resolution the CNNs were trained on, and pool features from a single-scale. This leads to a slight reduction in performance over the multi-scale approach. But we choose the single-scale setting because this keeps the feature extraction for all our methods identical making comparisons easier. We consider two representations based VGG-M and VGG-D networks. Unlike the earlier FC models, the features are extracted from the relu5 and relu5 3 layers of the VGG-M and VGG-D networks respectively.</p><p>VLAD/NetVLAD with CNNs. Similar to FV, this approach builds VLAD descriptors on CNN filter banks responses. We resized the image to 448×448 pixels before passing it through the network and aggregate the features obtained from the CNN using VLAD/NetVLAD pooling with 64 cluster centers. Identical to the FV models, we consider VLAD models with VGG-M and VGG-D networks truncated at relu5 and relu5 3 layers respectively.</p><p>BoVW/NetBoVW with CNNs. For BoVW we construct a vocabulary of 4096 words using k-means on top of the CNN features. For NetBoVW we use a 4096-way softmax layer as an approximation to the hard assignment. We use the same setting as FV and VLAD for feature extraction.</p><p>B-CNNs. These are models presented in Section 3 where features from two CNNs are pooled using an outer product. When the two CNNs are identical the model is a extension of O2P using deep features without the log-Euclidean normalization. We consider several B-CNNs -(i) one with two identical VGG-M networks truncated at the relu5 layer, (ii) one with a VGG-D and VGG-M network truncated at the relu5 3 and relu5 layer respectively, and (iii) initialized with two identical VGG-D networks truncated at the relu5 3 layer. Identical to the setting in FV and VLAD, the input images are resized to 448×448 and features are extracted using the two CNNs. The VGG-D network produces 28×28 output compared to 27×27 of the VGG-M network, so we downsample the VGG-D output by ignoring a row and column when combining it with the VGG-M output. The bilinear feature for all these models is of size 512×512. Note that the symmetric models, i.e., option (i) and (iii), the two networks share all parameters (which is also the case when they are fine-tuned due to symmetry), so in practice these models have the same memory overhead and speed as a single network evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Fine-tuning</head><p>For fine-tuning we add k-way linear + softmax layer where k is the number of classes in the fine-grained dataset. The parameters of the linear layer are initialized randomly. We adopt a two step training procedure of <ref type="bibr" target="#b3">[6]</ref> where we first train the linear layer using logistic regression, a convex optimization problem, followed by fine-tuning the entire model using back-propagation for several epochs (about 45 -100 depending on the dataset and model) at a relatively small learning rate (η = 0.001). Across the datasets we found the hyperparameters for fine-tuning were fairly consistent. Although, the exact VLAD, FV, BoVW models cannot be directly fine-tuned, we report results using indirect fine-tuning where the networks are fine-tuned with FC layers. We found this improves accuracy. For example, the indirectly finetuned FV models with CNN features outperforms the multiscale but not fine-tuned results reported in <ref type="bibr" target="#b8">[11]</ref>. However, direct fine-tuning using NetFV is significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">SVM training and evaluation</head><p>In all our experiments before and after fine-tuning, training and validation sets are combined and one-vs-all linear SVMs on the extracted features are trained by setting the learning hyperparameter C svm = 1. Since our features are 2 normalized, the optimal of C svm is likely to be independent of the dataset. The trained classifiers are calibrated by scaling the weight vector such that the median scores of positive and negative training examples are at +1 and -1 respectively. For each dataset we double the training data by flipping images and at test time average the predictions of the image and its flipped copy. Directly using the softmax predictions (logistic regression) results in a small drop in accuracy compared to linear SVMs. Performance is measured as the percentage of correct image predictions for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-grained recognition</head><p>We evaluate methods on following fine-grained datasets and report the per-image accuracy in Table <ref type="table">2</ref>.</p><p>CUB-200-2011 <ref type="bibr" target="#b63">[66]</ref> dataset contains 11,788 images of 200 bird species which are split into roughly equal train and test sets with detail annotation of parts and bounding boxes. As birds appear in different poses and viewpoints and occupy small portion of image in cluttered background, classifying bird species is challenging. Notice that in all our experiments, we only use image labels during training without any part or bounding box annotation. In the following sections, "birds" refers to the results on this dataset.</p><p>FGVC-aircraft dataset <ref type="bibr" target="#b39">[42]</ref> consists of 10,000 images of 100 aircraft variants, and was introduced as a part of the FGComp 2013 challenge. The task involves discriminating variants such as the Boeing 737-300 from Boeing 737-400. The differences are subtle, e.g., one may be able to distinguish them by counting the number of windows in the model. Unlike birds, airplanes tend to occupy a significantly larger portion of the image and appear in relatively clear background. Airplanes also have a smaller representation in the ImageNet dataset compared to birds.</p><p>Stanford cars dataset <ref type="bibr" target="#b30">[33]</ref> contains 16,185 images of 196 classes. Categories are typically at the level of Make, Model, Year, e.g., "2012 Tesla Model S" or '2012 BMW M3 coupe." Compared to aircrafts, cars are smaller and appear in a more cluttered background. Thus object and part localization may play a more significant role here. This dataset was also part of the FGComp 2013 challenge.</p><p>NABirds <ref type="bibr" target="#b60">[63]</ref> is larger than the CUB dataset consisting of 48,562 images of 555 spices of birds that include most that are found in North America. The work engaged citizen scientists to produce high-quality annotations in a cost-effective manner. This dataset also provides parts and bounding-box annotations, but we only use category labels for training our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bird species classification</head><p>Comparison to baselines. Table 2 "bird" column shows results on the CUB-200-2011 dataset. The end-to-end approximations of texture representations (NetBoWV, NetVLAD, NetFV) improve significantly after fine-tuning. Exact models with indirect fine-tuning (i.e., fine-tuned with FC layers) also improve, but the improvement is smaller (shown in gray italics in Table <ref type="table">2</ref>). With fine-tuning the single-scale FV models outperforms the multi-scale results reported in <ref type="bibr" target="#b8">[11]</ref> -49.9% using VGG-M and 66.7% using VGG-D network. B-CNNs offer the best accuracy across all models with the best performing model obtaining 84.1% accuracy (VGG-M + VGG-D). The next best approach is the NetVLAD with 81.9% accuracy. We found that increasing the cluster centers does not improve performance of NetVLAD (see Section 5.1).</p><p>We also trained the B-CNN (VGG-M + VGG-D) model on the much larger NABirds dataset. For this experiment we skipped the SVM training and test time flipping and report the accuracy using softmax layer predictions. This model achieves 79.4% accuracy outperforming a fine-tuned VGG-D network that obtains 63.7% accuaracy. Van Horn et al. <ref type="bibr" target="#b60">[63]</ref> obtains 75% accuracy using AlexNet and part annotations at test time, while the "neural activation constellations" approach <ref type="bibr" target="#b54">[57]</ref> obtains 76.3% accuracy using a GoogLeNet architecture <ref type="bibr" target="#b57">[60]</ref>.</p><p>Comparison to other techniques.  ). Two early methods that performed well when bounding-boxes are not available at test time are 73.9% of the "part-based R-CNN" <ref type="bibr" target="#b66">[69]</ref> and 75.7% of the "pose-normalized CNN" <ref type="bibr" target="#b3">[6]</ref>. These methods are based on AlexNet <ref type="bibr" target="#b31">[34]</ref> and can be improved with deeper and more accurate networks such as the VGG-D. For example, the SPDA-CNN <ref type="bibr" target="#b65">[68]</ref> trains better part detectors and feature representations using the VGG-D network and report 84.6% accuracy. Krause et al. <ref type="bibr" target="#b28">[31]</ref> report 82.0% accuracy using a weakly-supervised method to learn part detectors, followed by the part-based analysis of <ref type="bibr" target="#b66">[69]</ref> using the VGG-D network. However, our approach is simpler and faster since it does not rely on training and evaluating part detectors. The "cross-layer pooling" technique <ref type="bibr" target="#b35">[38]</ref> that considers pairwise features extracted from two different layers of a CNN report an accuracy of 73.5% using AlexNet and 77.0% accuracy with VGG-D. The approach uses bounding-boxes during training and testing.</p><p>One of the top-performing approaches that does not rely on additional annotations is the Spatial Transformer Networks <ref type="bibr" target="#b26">[29]</ref>. It obtains 84.1% accuracy using the batchnormalized Inception network <ref type="bibr" target="#b24">[27]</ref>. The PD+SWFV-CNN approach combines unsupervised part detection with FV pooling of CNN features to obtain 83.6% accuracy, and 84.5% accuracy with FC and FV pooling.</p><p>Since its publication, the B-CNN model has been improved by others in several ways. First, the compact bilinear pooling approach <ref type="bibr" target="#b16">[19]</ref> was proposed to reduce the size of the bilinear features (we evaluate this in Section 5.1). Zhang et al. <ref type="bibr" target="#b68">[71]</ref> combine B-CNNs with part annotations and improve results to 85.9%. Moghimi et al. <ref type="bibr" target="#b41">[44]</ref> boost B-CNNs trained on images of varying resolutions to obtain 86.2% accuracy. Although not directly comparable, Krause et al. <ref type="bibr" target="#b29">[32]</ref> show that by training on two orders of magnitude more labeled data obtained by querying category labels on search engines, the performance of deep architectures can be improved to 92.1%.  classes that are confused by our fine-tuned B-CNN (VGG-M + VGG-D) model. The most confused pair of classes is "American crow" and "Common raven". The differences lie in the wing-spans, habitat, and voice, none of which are easy to measure from the image. Other confused classes are also similar -various Shrikes, Terns, Flycatchers, Cormorants and Gulls. We note that the dataset has an estimated 4.4% label noise hence some of these errors may come from incorrect labeling <ref type="bibr" target="#b60">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Aircraft variant classification</head><p>Comparison to baselines. The trends among the baselines are similar to those in birds with a few exceptions. The FV with SIFT is remarkably good (61.0%) and comparable to some of the CNN baselines. Compared to the birds, the effect of fine-tuning is significantly larger for models based on the VGG-D network suggesting a larger domain shift from the ImageNet dataset. The best performance of 84.5% is achieved by the B-CNN (VGG-M + VDD-D) model. NetVLAD obtains 81.4% accuracy.</p><p>Comparison to other techniques. This dataset does not come with part annotations hence several top performing methods for the birds dataset are not applicable here. We also compare against the results for "track 2", i.e., w/o bounding-boxes, at the FGComp 2013 challenge <ref type="bibr">[2]</ref>. The best performing method <ref type="bibr" target="#b22">[25]</ref> is a heavily engineered FV-SIFT which achieves 80.7% accuracy. Notable differences between our baseline FV-SIFT and theirs are (i) larger dictionary (256 → 1024), (ii) Spatial pyramid pooling (1×1 → 1×1 + 3×1), (iii) multiple SIFT variants, and (iv) multi-scale SIFT. Wang et al. <ref type="bibr" target="#b64">[67]</ref> report 88.4% accuracy by mining discriminative patch triplets, but require bounding boxes during training and testing. Boosting B-CNNs <ref type="bibr" target="#b41">[44]</ref> obtains the current state of the art with 88.5% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>Per-image accuracy on the birds <ref type="bibr" target="#b63">[66]</ref>, aircrafts <ref type="bibr" target="#b39">[42]</ref>, and cars <ref type="bibr" target="#b30">[33]</ref> dataset for various methods described in Section 4.1. We compare various texture representations and prior work (separated by a double line). The first column lists the features used in the encoding followed by the pooling strategy. Features are extracted from the relu5 and relu5 3 layer of VGG-M and VGG-D networks respectively. FC denotes fully-connected layers on top of these intermediate layer features, corresponding to the penultimate layer (relu7 ) of the original network. The second and third columns show additional annotations used during training and testing. Results are shown without and with domain-specific fine-tuning. Directly fine-tuning the approximate models leads to better performance than indirectly fine-tuning (shown in gray italics), where features are constructed from the corresponding layers of fine-tuned FC models. B-CNN models achieve the best accuracy across texture representations. The first, second, and third best texture models are marked with red, blue and yellow colors respectively. Comparison to other techniques. The best accuracy on this dataset is by Krause et al. <ref type="bibr" target="#b28">[31]</ref> which obtains 92.6% accuracy. This is closely matched by 92.5% accuracy of the discriminative patch triplets <ref type="bibr" target="#b64">[67]</ref>, and 92.1% of Boosted B-CNNs <ref type="bibr" target="#b41">[44]</ref>. Unike the other approaches, Boosted B-CNNs do not rely on bounding-boxes at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>birds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Texture and scene recognition</head><p>We experiment on three texture datasets -the Describable Texture Dataset (DTD) <ref type="bibr" target="#b7">[10]</ref>, Flickr Material Dataset (FMD) <ref type="bibr" target="#b53">[56]</ref>, and KTH-TISP2-b (KTH-T2b) <ref type="bibr" target="#b4">[7]</ref>. DTD consists of 5640 images labeled with 47 describable texture attributes. FMD consists of 10 material categories, each of which contains 100 images. Unlike DTD and FMD where images are collected from the Internet, KTH-T2b contains 4752 images of 11 materials captured under controlled scale, pose, and illumination. The KTH-T2b dataset splits the images into four samples for each category. We follow the standard protocol by training on one sample and test the three. On and FMD, we randomly divide the dataset into 10 splits and report the mean accuracy across splits. Besides these, we also evaluate our models on MIT indoor scene dataset <ref type="bibr" target="#b48">[51]</ref>. Indoor scenes are weakly structured and orderless texture representations have been shown to be effective here. The dataset consists of 67 indoor categories and a defined training and test split.</p><p>We compare B-CNN to the prior state-of-the-art approach of FV pooling of CNN features <ref type="bibr" target="#b8">[11]</ref> using the VGG-D network. These results are without fine-tuning. On the MIT indoor dataset fine-tuning B-CNNs leads to a small improvement 72.8% → 73.8% using relu5 3 at s = 1, while on the other datasets the improvements were negligible, likely due to the relatively small size of these datasets. Table <ref type="table" target="#tab_3">3</ref> shows the results obtained by features from a single scale and features from multiple scales 2 s , s ∈ {1.5:-0.5:-3} relative to the 224×224 image using B-CNN and FV representations. We discard scales for which the image is smaller than the size of the receptive fields of the filters, or larger than 1024 2 pixels for efficiency. Across all scales of the input image the performance of the two approaches are identical. Multiple scales consistently lead to an improvement in accuracy. . The multi-scale FV results reported here are comparable (±1%) to the results reported in Cimpoi et al. <ref type="bibr" target="#b8">[11]</ref> for all datasets except KTH-T2b (-4%). These differences are due to the choice of the CNN (they use the conv5 4 layer of the 19-layer VGG network) and the range of scales. These results show that the B-CNNs are comparable to the FV pooling for texture recognition. One drawback is that the FV features with 64 GMM components has smaller in size (64×2×512) than the bilinear features (512×512). However, bilinear features are highly redundant and their dimensionality can be reduced by an order of magnitude without loss in performance (see Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS OF BILINEAR CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dimensionality reduction</head><p>The outer product of CNN features generates very high dimensional image descriptors, e.g., 262K for the B-CNN models in Table <ref type="table">2</ref>. Our earlier work <ref type="bibr" target="#b33">[36]</ref> showed that the features are highly redundant and their dimensionality can be reduced by an order of magnitude without loss in classification performance. Prior work <ref type="bibr" target="#b27">[30]</ref> has also shown that in the context of SIFT-based FV and VLAD, highly compact representations can be obtained.</p><p>In this section we investigate the trade-off between accuracy and feature dimension for various texture models proposed in Section 3 for fine-grained recognition. For NetVLAD and NetFV the feature dimension can be varied by changing the number of cluster centers. For B-CNNs, consider the case where the outer product is computed among features x and y. There are several strategies for reducing the feature dimension:</p><p>(1) Projecting the outer product into a lower dimensional space, i.e., Φ(x, y) = vec(x T y)P, where P is a projection matrix and the vec operator reshapes the matrix into a vector. (2) Projecting both the features into a lower-dimensional space and computing outer product, Φ(x, y) = (xA) T (yB), where A, B are projection matrices.</p><p>(3) Projecting one of the features into a lower-dimensional space and computing the outer product, i.e., by setting B to an identity matrix in the previous approach.</p><p>In each case, the projection matrices can be initialized using Principal Component Analysis (PCA). Although the first approach is straightforward, computing the PCA is computationally expensive due to the high dimensionality of the features (the covariance matrix of the outer product has d 4 entries for d-dimensional features). The second approach is computationally attractive but the outer product of two PCA projected features results in a significant reduction in accuracy as shown in our earlier work <ref type="bibr" target="#b34">[37]</ref>, and more recently in <ref type="bibr" target="#b16">[19]</ref>. We believe this is because after the PCA rotation, the features are no longer correlated across dimensions. Remarkably, reducing the dimension of only one feature using PCA (third option) works well in practice. While the projection can be initialized using PCA, they can be trained jointly with the classification layers. This technique was used in our earlier work <ref type="bibr" target="#b34">[37]</ref> to reduce the feature size. It breaks the symmetry of the features when identical networks are used and is an example of a partially shared feature pipeline (Figure <ref type="figure" target="#fig_3">3b</ref>). It also resembles the computations of VLAD and FV representations where both f A and f B are based on the same underlying feature.</p><p>The accuracy as a function of feature dimension shown in Figure <ref type="figure">5</ref> for NetFV and NetVLAD. These results are obtained by varying the number of cluster centers. The results indicate the performance of NetVLAD and NetFV do not improve with more cluster centers beyond 32.</p><p>Figure <ref type="figure">6</ref> shows the same for B-CNN features. We also compare our PCA approach to the recently-proposed "compact bilinear pooling" (CBP) <ref type="bibr" target="#b16">[19]</ref>. CBP approximates the outer product using a product of sparse linear projections of features with a Tensor Sketch <ref type="bibr" target="#b45">[48]</ref>. The performance of the full model with 512×512 dimensions with and without finetuning is shown as a straight line. On birds and aircrafs the dimensionality can be reduced by 16× (i.e., to 32×512) with a less than 1% loss in accuracy. In comparison, NetVLAD with the same feature size (i.e., with 32 components) is about 3-4% less accurate. Overall, for a given budget of Fig. <ref type="figure">6</ref>. Performance of B-CNNs using VGG-M relu5 features as function of feature dimension before (dotted lines) and after (solid lines) fine-tuning. One of the 512 dimensional feature is projected using PCA to k dimensions leading to a outer product of size k × 512 (see Section 5.1 for details).</p><p>The performance of compact bilinear pooling (CBP) <ref type="bibr" target="#b16">[19]</ref> is shown in magenta, and the full 512 × 512-dimensional B-CNN model is shown in black.</p><p>dimensions the projected B-CNNs outperform NetVLAD and NetFV representations. The PCA approach is slightly worse than CBP. However, one advantage is that PCA can be implemented as a dense matrix multiplication which is empirically 1.5× faster than CBP which involves computing Fourier transforms and their inverses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training B-CNNs on ImageNet LSVRC</head><p>Here we experiment with training a B-CNN model from scratch on the ImageNet LSRVC 2012 dataset <ref type="bibr" target="#b51">[54]</ref>. We train a B-CNN with a relu5 layer output of VGG-M network and compare it to the standard VGG-M network. This allows us to compare bilinear pooling with FC pooling since the rest of the architecture is identical in both models. Additionally, we compare the effect of implicit translational invariance obtained in CNNs by spatially jittering the data, as well a explicit translation invariance of B-CNNs due to the orderless pooling.</p><p>We train the two networks to classify 224×224 images with different amounts of spatial jittering -"f1" for flip, "f5" for flip + 5 translations, and "f25" for flip + 25 translations.</p><p>Training is done with stochastic sampling where one of the jittered copies is randomly selected for each example. The parameters are randomly initialized and trained using stochastic gradient descent with momentum for a number of epochs. We start with a high learning rate and reduce it by a factor of 10 when the validation error stops decreasing till no further improvement is observed.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the "top1" and "top5" validation errors and compares the B-CNN and VGG-M. The validation error is reported on a single center cropped image. Note that we train all networks with neither PCA color jittering nor batch normalization and our baseline results are within 2% of the top1 errors reported in <ref type="bibr" target="#b6">[9]</ref>. The VGG-M model achieves 46.4% top1 error with flip augmentation during training. The performance improves significantly to 39.6% with f25 augmentation. The B-CNN achieves 38.7% top1 error with f1 augmentation, outperforming VGG-M trained with f25 augmentation. The results show that B-CNN feature is discriminative and robust to translation and suggests that explicit translation invariance is more effective on the ImageNet dataset. This trend is also reflected in the latest deep architectures such as Residual Networks <ref type="bibr" target="#b23">[26]</ref> that replace the last fully-connected layers with global pooling of CNN features. Both these networks contain units that activate strongly on highly localized features. For example, the last row of VGG-D detects "tufted heads", while the fourth row in the same column detects a "red-yellow stripe" on birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualizing learned models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Top activations of B-CNN units</head><p>Similarly for airplanes, the units localize different types of windows, noses, vertical stabilizers, with some specializing in detecting particular airliner logos. For cars units activate on different kinds of head/tail lights, wheels, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Inverting categories</head><p>To understand the properties are learned by the B-CNNs we visualize pre-images of a category by "inversion". We use the framework of Mahendran and Vedaldi <ref type="bibr" target="#b38">[41]</ref> to generate an image that produces a high score for a target category Ĉ based on a B-CNN classifier. Specifically, for an image x and a layer r i , i = 1, . . . , n, we compute the bilinear features B ri using a B-CNN. Let C ri be the class prediction probabilities obtained using linear classifier trained on B ri in a supervised manner. We obtain an image that maximizes a target label Ĉ by solving the following optimization: (x i,j+1x i,j ) 2 + (x i+1,jx i,j ) 2</p><formula xml:id="formula_10">β 2 . (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>The exponent β = 2 was empirically found to lead to fewer "spike" artifacts in the optimization <ref type="bibr" target="#b38">[41]</ref>. We use B-CNNs based on the VGG-D network. In our experiments, an input image is resized to 224×224 pixels before computing the target bilinear features. We solve for x ∈ R 224×224×3 on the optimization. The lower resolution is primarily for speed since the dimension of the bilinear features are independent of the size of the image. We optimize the log-likelihood of the class probability using classifiers trained on bilinear   , FMD <ref type="bibr" target="#b53">[56]</ref>, MIT Indoor dataset <ref type="bibr" target="#b48">[51]</ref> (first three rows, two columns each from left to right), and the CUB dataset <ref type="bibr" target="#b63">[66]</ref> (last two rows, all columns). Best viewed in color and with zoom.</p><p>features at relu2 2, relu3 3, relu4 3, relu5 3. We use L-BFGS for optimization and compute the gradients of the objective with respect to x using back-propagation. The hyperparameter γ = 10 -8 was found empirically to lead to good inverse images. We also refer readers to our earlier work <ref type="bibr" target="#b33">[36]</ref> where this framework was applied to texture synthesis and style transfer using attributes.</p><p>Figure <ref type="figure" target="#fig_9">8</ref> shows some inverse images for various categories for the DTD, FMD, MIT indoor, and CUB-200-2011 dataset. These images reveal how B-CNNs represents various categories as textures. For instance, the dotted category of DTD contains images of various colors and dot sizes and the inverse image is composed of multi-scale multi-colored dots. The inverse images of water and wood from FMD are highly representative of these categories. The inverse images of the MIT indoor dataset reveal key properties of a category -a bookstore has a of racks of books, a laundromat has laundry machines at various scales and locations. The inverse images of various bird species capture distinctive colors and patterns on their bodies. Figure <ref type="figure" target="#fig_10">9</ref> visualizes reconstructions by incrementally adding layers in the bilinear representation. Even though the relu5 3 layer provides the best recognition accuracy, simply using that layer did not produce good inverse images (not shown) as the color information was missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented B-CNNs that aggregate second order statistics of CNN features to construct an orderless texture representation of an image. These networks can be trained in an endto-end manner allowing both training from scratch on large datasets, and domain-specific fine-tuning for transfer learning. Moreover, these models are fairly efficient, processing 448×448 resolution images at 30-100 FPS on a NVIDIA Titan X GPU. We also compared B-CNNs to both exact and approximate variants of deep texture representations and studied the accuracy and memory trade-offs they offer. The main conclusion was that variants of outer product representations are highly effective at various fine-grained, texture and scene recognition tasks. Moreover, these representations are redundant and in some cases their dimension can be reduced by an order of magnitude without significant loss in accuracy. A visualization of B-CNNs showed that these models effectively represent objects as texture and its units capture localized attributes useful for fine-grained recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Image classification using a B-CNN. An image is passed through CNNs A and B, and their outputs at each location are combined using the matrix outer product and average pooled to obtain the bilinear feature representation. This is passed through a linear + softmax layer to obtain class predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flow of gradients in a B-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a) no sharing (b) partially shared (c) fully shared</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Feature functions in B-CNNs can (a) share no computations (e.g., B-CNN model based on VGG-M and VGG-D), (b) share computations partially (e.g., NetVLAD, B-CNN PCA model described in Section 5.1), and (c) share all computations (e.g., B-CNN model based on VGG-M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top six pairs of classes that are most confused with each other on the CUB dataset. In each row we show the images in the test set that were most confidently classified as the class in the other column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7 shows the top activations of several units of the relu5 3 layer of VGG-D and the relu5 layer of VGG-M network for the fine-tuned B-CNN (VGG-D + VGG-M) model.Both these networks contain units that activate strongly on highly localized features. For example, the last row of VGG-D detects "tufted heads", while the fourth row in the same column detects a "red-yellow stripe" on birds. Similarly for airplanes, the units localize different types of windows, noses, vertical stabilizers, with some specializing in detecting particular airliner logos. For cars units activate on different kinds of head/tail lights, wheels, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>ri , Ĉ + γΓ(x).<ref type="bibr" target="#b2">(5)</ref> Here, L is a loss function such as the negative log-likelihood of the label Ĉ and γ is a tradeoff parameter. The image prior Γ(x) encourages the smoothness of output image. We use the TV β norm with β = 2: Γ(x) = i,j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Patches with the highest activations for several filters of the fine-tuned B-CNN (VGG-D + VGG-M) model on birds, aircrafts and cars classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visualizing various categories by inverting the B-CNN based on VGG-D network trained on DTD<ref type="bibr" target="#b7">[10]</ref>, FMD<ref type="bibr" target="#b53">[56]</ref>, MIT Indoor dataset<ref type="bibr" target="#b48">[51]</ref> (first three rows, two columns each from left to right), and the CUB dataset<ref type="bibr" target="#b63">[66]</ref> (last two rows, all columns). Best viewed in color and with zoom.</figDesc><graphic coords="12,51.02,388.11,83.33,83.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Inverse images obtained from a multilayer B-CNN. From left to right different layers (shown on top) are added one by one.</figDesc><graphic coords="13,49.49,178.89,60.48,60.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 2 shows other top-performing methods on this dataset. The dataset also provides bounding-box and part annotations and techniques differ based on what annotations are used at training and test time (also shown in the Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Mean per-class accuracy on DTD, FMD, KTH-T2b and MIT indoor datasets using FV and B-CNN representations constructed on top of relu5 3 layer outputs of the 16-layer VGG-D network<ref type="bibr" target="#b56">[59]</ref>. Results for input images at different scales s = 1, s = 2 and ms correspond to a size of 224×224, 448×448 and multiple sizes respectively.</figDesc><table><row><cell></cell><cell></cell><cell>FV</cell><cell></cell><cell></cell><cell>B-CNN</cell></row><row><cell>dataset</cell><cell cols="6">s = 1 s = 2 ms s = 1 s = 2 ms</cell></row><row><cell>DTD</cell><cell cols="6">67.8 70.6 73.6 69.6 71.5 72.9</cell></row><row><cell></cell><cell>±0.9</cell><cell>±0.9</cell><cell>±1.0</cell><cell>±0.7</cell><cell>±0.8</cell><cell>±0.8</cell></row><row><cell>FMD</cell><cell cols="6">75.1 79.0 80.8 77.8 80.7 81.6</cell></row><row><cell></cell><cell>±2.3</cell><cell>±1.4</cell><cell>±1.7</cell><cell>±1.9</cell><cell>±1.5</cell><cell>±1.7</cell></row><row><cell>KTH-T2b</cell><cell cols="6">74.8 75.9 77.9 75.1 76.4 77.9</cell></row><row><cell></cell><cell>±2.6</cell><cell>±2.4</cell><cell>±2.0</cell><cell>±2.8</cell><cell>±3.5</cell><cell>±3.1</cell></row><row><cell cols="7">MIT indoor 70.1 78.2 78.5 72.8 77.6 79.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Performance of NetVLAD and NetFV models encoding VGG-M relu5 features with different number of cluster centers on fine-grained datasets before (dotted lines) and after (solid lines) fine-tuning. Given the same number of cluster centers, the feature dimension of NetFV representation is twice as large as NetVLAD.</figDesc><table><row><cell></cell><cell></cell><cell>Birds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Aircrafts</cell><cell></cell><cell></cell><cell></cell><cell>Cars</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NetVLAD-ft NetVLAD</cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NetFV-ft</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell>NetFV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>Accuracy</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell cols="2">0.7</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.75</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell cols="2">NetVLAD-ft NetVLAD</cell><cell></cell><cell>0.7</cell><cell>NetVLAD-ft NetVLAD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NetFV-ft</cell><cell></cell><cell></cell><cell>NetFV-ft</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NetFV</cell><cell></cell><cell></cell><cell>NetFV</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell></cell><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of components</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number of components</cell><cell></cell><cell></cell><cell>Number of components</cell></row><row><cell cols="2">0.6 0.65 0.7 0.75 0.8 Fig. 5. 8x512 Accuracy</cell><cell cols="3">16x512 Birds 32x512 BCNN-PCA-ft 64x512 BCNN-PCA CBP-ft CBP BCNN-ft BCNN</cell><cell>Accuracy</cell><cell>0.6 0.65 0.7 0.75 0.8</cell><cell>8x512</cell><cell cols="3">16x512 Aircrafts 32x512 BCNN-PCA-ft 64x512 BCNN-PCA CBP-ft CBP BCNN-ft BCNN</cell><cell>Accuracy</cell><cell>0.65 0.7 0.75 0.8 0.85</cell><cell>8x512</cell><cell>16x512 Cars 32x512 BCNN-PCA-ft 64x512 BCNN-PCA CBP-ft CBP BCNN-ft BCNN</cell></row><row><cell></cell><cell></cell><cell cols="2">Feature dimension</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Feature dimension</cell><cell></cell><cell></cell><cell>Feature dimension</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Accuracy on the ILSVRC 2014 validation set using bilinear pooling and FC layers on top of relu5 layer output of a VGG-M network. Both networks are trained from scratch on the ILSVRC 2012 training set with varying amounts of data augmentation.</figDesc><table><row><cell></cell><cell></cell><cell>B-CNN</cell><cell></cell><cell>FC</cell><cell></cell></row><row><cell>data aug.</cell><cell>f1</cell><cell>f5</cell><cell>f25</cell><cell>f1</cell><cell>f25</cell></row><row><cell>error@1</cell><cell>38.7</cell><cell>37.1</cell><cell>36.6</cell><cell>46.4</cell><cell>39.6</cell></row><row><cell>error@5</cell><cell>17.0</cell><cell>16.3</cell><cell>16.0</cell><cell>22.5</cell><cell>17.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research was supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract number 2014-14071600010, National Science Foundation grant IIS-1617917, and a gift from Facebook. The GPUs used in this research were generously donated by NVIDIA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2016. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing people: A poseletbased approach to attribute classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2008">2014. 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-specific material categorisation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004">2012. 1, 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition, description, and segmentation. International</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="2009">2016. 1, 2, 4, 5, 6, 7, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The PASCAL visual obiect classes challenge 2007 (VOC2007) results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Pascal Challenge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>2016. 3</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno>CoRR, abs/1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009">2016. 2, 3, 7, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative learning of sum-product networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3248" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting the fisher vector for fine-grained classification</title>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="92" to="98" />
			<date type="published" when="2008">2014. 2, 4, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-02-11">2016. 2, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2017-2025. 2015. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2015. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Representation and Recognition Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2012. 2, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding deep texture representations</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009">2016. 2, 9</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009">2015. 2, 3, 4, 5, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-convolutionallayer pooling for image recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008">2016. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing deep convolutional neural networks using natural pre-images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016-02-11">2016. 2, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Boosted convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2008">2016. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A compact and discriminative face track descriptor</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeepVision workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Material perceprion: What can you see in a brief glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Texture synthesis using shallow convolutional networks with random filters</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00021</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in finegrained dataset collection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">VLFeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR- 2011-001</idno>
	</analytic>
	<monogr>
		<title level="j">CalTech</title>
		<imprint>
			<date type="published" when="2011">2011. 1, 2, 6, 8, 12</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2016. 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2016. 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008">2014. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pose pooling kernels for subcategory recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fine-grained pose prediction, normalization, and recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In The IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
