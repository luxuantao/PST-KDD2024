<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Temam Inria</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Zidong Du SKLCA</orgName>
								<orgName type="institution">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Ninghui Sun SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Jia Wang SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Chengyong Wu SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Yunji Chen SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/10.1145/2541940.2541967</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machinelearning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.</p><p>Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.</p><p>We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, designing accelerators which realize the best possible tradeoff between flexibility and efficiency is becoming a prominent issue.</p><p>The first question is for which category of applications one should primarily design accelerators ? Together with the architecture trend towards accelerators, a second simultaneous and significant trend in high-performance and embedded applications is developing: many of the emerging high-performance and embedded applications, from image/video/audio recognition to automatic translation, business analytics, and all forms of robotics rely on machinelearning techniques. This trend even starts to percolate in our community where it turns out that about half of the benchmarks of PARSEC <ref type="bibr" target="#b1">[2]</ref>, a suite partly introduced to highlight the emergence of new types of applications, can be implemented using machine-learning algorithms <ref type="bibr" target="#b3">[4]</ref>. This trend in application comes together with a third and equally remarkable trend in machine-learning where a small number of techniques, based on neural networks (especially Convolutional Neural Networks <ref type="bibr" target="#b26">[27]</ref> and Deep Neural Networks <ref type="bibr" target="#b15">[16]</ref>), have been proved in the past few years to be state-ofthe-art across a broad range of applications <ref type="bibr" target="#b24">[25]</ref>. As a result, there is a unique opportunity to design accelerators which can realize the best of both worlds: significant application scope together with high performance and efficiency due to the limited number of target algorithms.</p><p>Currently, these workloads are mostly executed on multicores using SIMD <ref type="bibr" target="#b40">[41]</ref>, on GPUs <ref type="bibr" target="#b4">[5]</ref>, or on FPGAs <ref type="bibr" target="#b2">[3]</ref>. However, the aforementioned trends have already been identified by a number of researchers who have proposed accelerators implementing Convolutional Neural Networks <ref type="bibr" target="#b2">[3]</ref> or Multi-Layer Perceptrons <ref type="bibr" target="#b37">[38]</ref>; accelerators focusing on other domains, such as image processing, also propose efficient implementations of some of the primitives used by machinelearning algorithms, such as convolutions <ref type="bibr" target="#b32">[33]</ref>. Others have proposed ASIC implementations of Convolutional Neural Networks <ref type="bibr" target="#b12">[13]</ref>, or of other custom neural network algorithms <ref type="bibr" target="#b20">[21]</ref>. However, all these works have first, and successfully, focused on efficiently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, or they directly plug their computational accelerator to memory via a more or less sophisticated DMA <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>While efficient implementation of computational primitives is a first and important step with promising results, inefficient memory transfers can potentially void the throughput, energy or cost advantages of accelerators, i.e., an Amdahl's law effect, and thus, they should become a first-order concern, just like in processors, rather than an element factored in accelerator design on a second step. Unlike in processors though, one can factor in the specific nature of memory transfers in target algorithms, just like it is done for accelerating computations. This is especially important in the domain of machine-learning where there is a clear trend towards scaling up the size of neural networks in order to achieve better accuracy and more functionality <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>In this study, we investigate an accelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional Neural Networks (CNNs) and Deep Neural Networks (DNNs). We focus the design of the accelerator on memory usage, and we investigate an accelerator architecture and control both to minimize memory transfers and to perform them as efficiently as possible. We present a design at 65nm which can perform 496 16-bit fixed-point operations in parallel every 1.02ns, i.e., 452 GOP/s, in a 3.02mm 2 , 485mW footprint (excluding main memory accesses). On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is 117.87x faster and 21.08x more energy-efficient (including main memory accesses) on average than an 128-bit SIMD core clocked at 2GHz.</p><p>In summary, our main contributions are the following:</p><p>• A synthesized (place &amp; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machinelearning algorithms. • The accelerator achieves high throughput in a small area, power and energy footprint.</p><p>• The accelerator design focuses on memory behavior, and measurements are not circumscribed to computational tasks, they factor in the performance and energy impact of memory transfers.</p><p>The paper is organized as follows. In Section 2, we first provide a primer on recent machine-learning techniques and introduce the main layers composing CNNs and DNNs. In Section 3, we analyze and optimize the memory behavior of these layers, in preparation for both the baseline and the accelerator design. In section 4, we explain why an ASIC implementation of large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs. We introduce our accelerator design in Section 5. The methodology is presented in Section 6, the experimental results in Section 7, related work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Primer on Recent Machine-Learning Techniques</head><p>Even though the role of neural networks in the machinelearning domain has been rocky, i.e., initially hyped in the 1980s/1990s, then fading into oblivion with the advent of Support Vector Machines <ref type="bibr" target="#b5">[6]</ref>. Since 2006, a subset of neural networks have emerged as achieving state-of-the-art machine-learning accuracy across a broad set of applications, partly inspired by progress in neuroscience models of computer vision, such as HMAX <ref type="bibr" target="#b36">[37]</ref>. This subset of neural networks includes both Deep Neural Networks (DNNs) <ref type="bibr" target="#b24">[25]</ref> and Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b26">[27]</ref>. DNNs and CNNs are strongly related, they especially differ in the presence and/or nature of convolutional layers, see later.</p><p>Processing vs. training. For now, we have implemented the fast processing of inputs (feed-forward) rather than training (backward path) on our accelerator. This derives from technical and market considerations. Technically, there is a frequent and important misconception that on-line learning is necessary for many applications. On the contrary, for many industrial applications off-line learning is sufficient, where the neural network is first trained on a set of data, and then shipped to the customer, e.g., trained on hand-   written digits, license plate numbers, a number of faces or objects to recognize, etc; the network can be periodically taken off-line and retrained. While, today, machine-learning researchers and engineers would especially want an architecture that speeds up training, this represents a small market, and for now, we focus on the much larger market of end users, who need fast/efficient feed-forward networks. Interestingly, machine-learning researchers who have recently dipped into hardware accelerators <ref type="bibr" target="#b12">[13]</ref> have made the same choice. Still, because the nature of computations and access patterns used in training (especially back-propagation) is fairly similar to that of the forward path, we plan to later augment the accelerator with the necessary features to support training. General structure. Even though Deep and Convolutional Neural Networks come in various forms, they share enough properties that a generic formulation can be defined. In general, these algorithms are made of a (possibly large) number of layers; these layers are executed in sequence so they can be considered (and optimized) independently. Each layer usually contains several sub-layers called feature maps; we then use the terms input feature maps and output feature maps. Overall, there are three main kinds of layers: most of the hierarchy is composed of convolutional and pooling (also called sub-sampling) layers, and there is a classifier at the top of the network made of one or a few layers.</p><p>Convolutional layers. The role of convolutional layers is to apply one or several local filters to data from the input (previous) layer. Thus, the connectivity between the input and output feature map is local instead of full. Consider the case where the input is an image, the convolution is a 2D transform between a K x × K y subset (window) of the input layer and a kernel of the same dimensions, see Figure <ref type="figure" target="#fig_0">1</ref>. The kernel values are the synaptic weights between an input layer and an output (convolutional) layer. Since an input layer usually contains several input feature maps, and since an output feature map point is usually obtained by applying a convolution to the same window of all input feature maps, see Figure <ref type="figure" target="#fig_0">1</ref>, the kernel is 3D, i.e., K x × K y × N i , where N i is the number of input feature maps. Note that in some cases, the connectivity is sparse, i.e., not all input feature maps are used for each output feature map. The typical code of a convolutional layer is shown in Figure <ref type="figure" target="#fig_6">7</ref>, see Origi-nal code. A non-linear function is applied to the convolution output, for instance f (x) = tanh(x). Convolutional layers are also characterized by the overlap between two consecutive windows (in one or two dimensions), see steps s x , s y for loops x, y.</p><p>In some cases, the same kernel is applied to all K x × K y windows of the input layer, i.e., weights are implicitly shared across the whole input feature map. This is characteristic of CNNs, while kernels can be specific to each point of the output feature map in DNNs <ref type="bibr" target="#b25">[26]</ref>, we then use the term private kernels.</p><p>Pooling layers. The role of pooling layers is to aggregate information among a set of neighbor input data. In the case of images again, it serves to retain only the salient features of an image within a given window and/or to do so at different scales, see Figure <ref type="figure" target="#fig_0">1</ref>. An important side effect of pooling layers is to reduce the feature map dimensions. An example code of a pooling layer is shown in Figure <ref type="figure" target="#fig_8">8</ref> (see Original code). Note that each feature map is pooled separately, i.e., 2D pooling, not 3D pooling. Pooling can be done in various ways, some of the preferred techniques are the average and max operations; pooling may or may not be followed by a non-linear function.</p><p>Classifier layers. Convolution and pooling layers are interleaved within deep hierarchies, and the top of the hierarchies is usually a classifier. This classifier can be linear or a multi-layer (often 2-layer) perceptron, see Figure <ref type="figure" target="#fig_0">1</ref>. An example perceptron layer is shown in Figure <ref type="figure" target="#fig_3">5</ref>, see Original code. Like convolutional layers, a non-linear function is applied to the neurons output, often a sigmoid, e.g., f (x) = 1 1+e −x ; unlike convolutional or pooling layers, classifiers usually aggregate (flatten) all feature maps, so there is no notion of feature maps in classifier layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Processor-Based Implementation of (Large) Neural Networks</head><p>The distinctive aspect of accelerating large-scale neural networks is the potentially high memory traffic. In this section, we analyze in details the locality properties of the different layers mentioned in Section 2, we tune processor-based implementations of these layers in preparation for both our baseline, and the design and utilization of the accelerator. We apply the locality analysis/optimization to all layers, and we illustrate the bandwidth impact of these transformations with 4 of our benchmark layers (CLASS1, CONV3, CONV5, POOL3); their characteristics are later detailed in Section 6.</p><p>For the memory bandwidth measurements of this section, we use a cache simulator plugged to a virtual computational structure on which we make no assumption except that it is capable of processing T n neurons with T i synapses each every cycle. The cache hierarchy is inspired by Intel Core i7: L1 is 32KB, 64-byte line, 8-way; the optional L2 is 2MB, 64byte, 8-way. Unlike the Core i7, we assume the caches have enough banks/ports to serve T n × 4 bytes for input neurons, and T n × T i × 4 bytes for synapses. For large T n , T i , the cost of such caches can be prohibitive, but it is only used for our limit study of locality and bandwidth; in our experiments, we use T n = T i = 16.  We consider the perceptron classifier layer, see Figures <ref type="figure" target="#fig_3">2 and 5</ref>; the tiling loops ii and nn simply reflect that the computational structure can process T n neurons with T i synapses simultaneously. The total number of memory transfers is (inputs loaded + synapses loaded + outputs written):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classifier Layers</head><formula xml:id="formula_0">N i × N n + N i × N n + N n .</formula><p>For the example layer CLASS1, the corresponding memory bandwidth is high at 120 GB/s, see CLASS1 -Original in Figure <ref type="figure" target="#fig_4">6</ref>. We explain below how it is possible to reduce this bandwidth, sometimes drastically.</p><p>Input/Output neurons. Consider Figure <ref type="figure" target="#fig_1">2</ref> and the code of Figure <ref type="figure" target="#fig_3">5</ref> again. Input neurons are reused for each output neuron, but since the number of input neurons can range anywhere between a few tens to hundreds of thousands, they will often not fit in an L1 cache. Therefore, we tile loop ii (input neurons) with tile factor T ii . A typical tradeoff of tiling is that improving one reference (here neuron[i] for input neurons) increases the reuse distance of another reference (sum[n] for partial sums of output neurons), so we need to tile for the second reference as well, hence loop nnn and the tile factor T nn for output neurons partial sums. As expected, tiling drastically reduces the memory bandwidth requirements of input neurons, and those of output neurons increase, albeit marginally. The layer memory behavior is now dominated by synapses. Synapses. In a perceptron layer, all synapses are usually unique, and thus there is no reuse within the layer. On the other hand, the synapses are reused across network invocations, i.e., for each new input data (also called "input row") presented to the neural network. So a sufficiently large L2 could store all network synapses and take advantage of that locality. For DNNs with private kernels, this is not possible as the total number of synapses are in the tens or hundreds of millions (the largest network to date has a billion synapses <ref type="bibr" target="#b25">[26]</ref>). However, for both CNNs and DNNs with shared kernels, the total number of synapses range in the millions, which is within the reach of an L2 cache. In Figure <ref type="figure" target="#fig_4">6</ref>, see CLASS1 -Tiled+L2, we emulate the case where reuse across network invocations is possible by considering only the perceptron layer; as a result, the total bandwidth requirements are now drastically reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Layers</head><p>We consider two-dimensional convolutional layers, see Figures 3 and 7. The two distinctive features of convolutional layers with respect to classifier layers are the presence of input and output feature maps (loops i and n) and kernels (loops k x , k y ).</p><p>Inputs/Outputs. There are two types of reuse opportunities for inputs and outputs: the sliding window used to scan the (two-dimensional (x, y)) input layer, and the reuse across the N n output feature maps, see Figure <ref type="figure" target="#fig_14">3</ref>. The former corresponds to Kx×Ky sx×sy reuses at most, and the latter to N n reuses. We tile for the former in Figure <ref type="figure" target="#fig_6">7</ref> (tiles T x , T y ), but we often do not need to tile for the latter because the data to be reused, i.e., one kernel of K x × K y × N i , fits in the L1 data cache since K x , K y are usually of the order of 10 and N i can vary between less than 10 to a few hundreds; naturally, when this is not the case, we can tile input feature maps (ii) and introduce an second-level tiling loop iii again.</p><p>Synapses. For convolutional layers with shared kernels (see Section 2), the same kernel parameters (synaptic weights) are reused across all xout, yout output feature maps locations. As a result, the total bandwidth is already for (int kx = 0; kx &lt; Kx; kx++) for (int ii = 0; ii &lt; Ni; ii += Ti) for (int n = nn; n &lt; nn + Tn; n++) for low, as shown for layer CONV3 in Figure <ref type="figure" target="#fig_4">6</ref>. However, since the total shared kernels capacity is K x ×K y ×N i ×N o , it can exceed the L1 cache capacity, so we tile again output feature maps (tile T nn ) to bring it down to K x × K y × N i × T nn . As a result, the overall memory bandwidth can be further reduced, as shown in Figure <ref type="figure" target="#fig_4">6</ref>.</p><formula xml:id="formula_1">(int i = ii; i &lt; ii + Ti; i++) // version with shared kernels sum[n] += synapse[ky][kx][n][i] * neuron[ky + y][kx + x][i]; // version with private kernels sum[n] += synapse[yout][xout][ky][kx][n][i]} * neuron[ky + y][kx + x][i]; for (int n = nn; n &lt; nn + Tn; n++) neuron[yout][xout][n] = non linear transform(sum[n]); } xout++; } yout++; } } } }</formula><p>For convolutional layers with private kernels, the synapses are all unique and there is no reuse, as for classifier layers, hence the similar synapses bandwidth of CONV5 in Figure <ref type="figure" target="#fig_4">6</ref>. As for classifier layers, reuse is still possible across network invocations if the L2 capacity is sufficient. Even though step coefficients (s x , s y ) and sparse input to output feature maps (see Section 2) can drastically reduce the number of private kernels synaptic weights, for very large layers such as CONV5, they still range in the hundreds of megabytes and thus will largely exceed L2 capacity, implying a high memory bandwidth, see Figure <ref type="figure" target="#fig_4">6</ref>.</p><p>It is important to note that there is an on-going debate within the machine-learning community about shared vs. private kernels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>, and the machine-learning importance of having private instead of shared kernels remains unclear. Since they can result in significantly different architecture performance, this may be a case where the architecture/performance community could weigh in on the machine-learning debate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pooling Layers</head><p>We now consider pooling layers, see Figures <ref type="figure" target="#fig_8">4 and 8</ref> for (int kx = 0; kx &lt; Kx; kx++) for (int i = ii; i &lt; ii + Ti; i++) // version with average pooling;</p><formula xml:id="formula_2">value[i] += neuron[ky + y][kx + x][i];</formula><p>// version with max pooling; maps is the same, and more importantly, there is no kernel, i.e., no synaptic weight to store, and an output feature map element is determined only by K x × K y input feature map elements, i.e., a 2D window (instead of a 3D window for convolutional layers). As a result, the only source of reuse comes from the sliding window (instead of the combined effect of sliding window and output feature maps). Since there are less reuse opportunities, the memory bandwidth of input neurons are higher than for convolutional layers, and tiling (T x , T y ) brings less dramatic improvements, see Figure <ref type="figure" target="#fig_4">6</ref>.</p><formula xml:id="formula_3">value[i] = max(value[i], neuron[ky + y][kx + x][i]); } } } } // for average pooling; neuron[xout][yout][i] = value[i] / (Kx * Ky); xout++; } yout++; } } }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Accelerator for Small Neural Networks</head><p>In this section, we first evaluate a "naive" and greedy approach for implementing a hardware neural network accelerator where all neurons and synapses are laid out in hardware, memory is only used for input rows and storing results. While these neural networks can potentially achieve the best energy efficiency, we show that they are not scalable. Still, we use such networks to investigate the maximum number of neurons which can be reasonably implemented in hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hardware Neural Networks</head><p>The most natural way to map a neural network onto silicon is simply to fully lay out the neurons and synapses, so that the hardware implementation matches the conceptual representation of neural networks, see Figure <ref type="figure">9</ref>. The neurons are each implemented as logic circuits, and the synapses are implemented as latches or RAMs. This approach has been recently used for perceptron or spike-based hardware neural networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. It is compatible with some embedded applications where the number of neurons and synapses can be small, and it can provide both high speed and low energy because the distance traveled by data is very small: from one neuron to a neuron of the next layer, and from one synaptic latch to the associated neuron. For instance, an execution time of 15ns and an energy reduction of 974x over a core has been reported for a 90-10-10 (90 inputs, 10 hidden, 10 outputs) perceptron <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Maximum Number of Hardware Neurons ?</head><p>However, the area, energy and delay grow quadratically with the number of neurons. We have synthesized the ASIC versions of neural network layers of various dimensions, and we report their area, critical path and energy in Figure <ref type="figure" target="#fig_0">10</ref>.</p><p>We have used Synopsys ICC for the place and route, and the TSMC 65nm GP library, standard VT. A hardware neuron performs the following operations: multiplication of inputs and synapses, addition of all such multiplications, followed by a sigmoid, see Figure <ref type="figure">9</ref>. A T n × T i layer is a layer of T n neurons with T i synapses each. A 16x16 layer requires less than 0.71 mm 2 , but a 32x32 layer already costs 2.66 mm 2 . Considering the neurons are in the thousands for large-scale neural networks, a full hardware layout of just one layer would range in the hundreds or thousands of mm 2 , and thus, this approach is not realistic for large-scale neural networks. For such neural networks, only a fraction of neurons and synapses can be implemented in hardware. Paradoxically, this was already the case for old neural network designs such as the Intel ETANN <ref type="bibr" target="#b17">[18]</ref> at the beginning of the 1990s, not because neural networks were already large at the time, but because hardware resources (number of transistors) were naturally much more scarce. The principle was to timeshare the physical neurons and use the on-chip RAM to store synapses and intermediate neurons values of hidden layers. However, at that time, many neural networks were small enough that all synapses and intermediate neurons values could fit in the neural network RAM. Since this is no longer the case, one of the main challenges for large-scale neural network accelerator design has become the interplay between the computational and the memory hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Accelerator for Large Neural Networks</head><p>In this section, we draw from the analysis of Sections 3 and 4 to design an accelerator for large-scale neural networks.</p><p>The main components of the accelerator are the following: an input buffer for input neurons (NBin), an output buffer for output neurons (NBout), and a third buffer for synaptic weights (SB), connected to a computational block (performing both synapses and neurons computations) which we call the Neural Functional Unit (NFU), and the control logic (CP), see Figure <ref type="figure" target="#fig_10">11</ref>. We first describe the NFU below, and then we focus on and explain the rationale for the storage elements of the accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Computations: Neural Functional Unit (NFU)</head><p>The spirit of the NFU is to reflect the decomposition of a layer into computational blocks of T i inputs/synapses and T n output neurons. This corresponds to loops i and n for both classifier and convolutional layers, see Figures 5 and Figure <ref type="figure" target="#fig_6">7</ref>, and loop i for pooling layers, see Figure <ref type="figure" target="#fig_8">8</ref>.</p><p>Arithmetic operators. The computations of each layer type can be decomposed in either 2 or 3 stages. For classifier layers: multiplication of synapses × inputs, additions of all  multiplications, sigmoid. For convolutional layers, the stages are the same; the nature of the last stage (sigmoid or another non-linear function) can vary. For pooling layers, there is no multiplication (no synapse), and the pooling operations can be average or max. Note that the adders have multiple inputs, they are in fact adder trees, see Figure <ref type="figure" target="#fig_10">11</ref>; the second stage also contains shifters and max operators for pooling layers. Staggered pipeline. We can pipeline all 2 or 3 operations, but the pipeline must be staggered: the first or first two stages (respectively for pooling, and for classifier and convolutional layers) operate as normal pipeline stages, but the third stage is only active after all additions have been performed (for classifier and convolutional layers; for pooling layers, there is no operation in the third stage). From now on, we refer to stage n of the NFU pipeline as NFU-n.</p><p>NFU-3 function implementation. As previously proposed in the literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>, the sigmoid of NFU-3 (for classifier and convolutional layers) can be efficiently implemented using piecewise linear interpolation (f</p><formula xml:id="formula_4">(x) = a i × x + b i , x ∈ [x i ; x i+1 ]</formula><p>) with negligible loss of accuracy (16 segments are sufficient) <ref type="bibr" target="#b23">[24]</ref>, see Figure <ref type="figure">9</ref>. In terms of operators, it corresponds to two 16x1 16-bit multiplexers (for segment boundaries selection, i.e., x i , x i+1 ), one 16-bit multiplier (16-bit output) and one 16-bit adder to perform the interpolation. The 16-segment coefficients (a i , b i ) are stored in a small RAM; this allows to implement any function, not just a sigmoid (e.g., hyperbolic tangent, linear functions, etc) by just changing the RAM segment coefficients a i , b i ; the segment boundaries (x i , x i+1 ) are hardwired.</p><p>16-bit fixed-point arithmetic operators. We use 16-bit fixed-point arithmetic operators instead of word-size (e.g., 32-bit) floating-point operators. While it may seem surpris- ing, there is ample evidence in the literature that even smaller operators (e.g., 8 bits or even less) have almost no impact on the accuracy of neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. To illustrate and further confirm that notion, we trained and tested multi-layer perceptrons on data sets from the UC Irvine Machine-Learning repository, see Figure <ref type="figure" target="#fig_11">12</ref>, and on the standard MNIST machine-learning benchmark (handwritten digits) <ref type="bibr" target="#b26">[27]</ref>, see Table <ref type="table" target="#tab_2">1</ref>, using both 16-bit fixed-point and 32bit floating-point operators; we used 10-fold cross-validation for testing. For the fixed-point operators, we use 6 bits for the integer part, 10 bits for the fractional part (we use this fixed-point configuration throughout the paper). The results are shown in Figure <ref type="figure" target="#fig_11">12</ref> and confirm the very small accuracy impact of that tradeoff. We conservatively use 16-bit fixedpoint for now, but we will explore smaller, or variable-size, operators in the future. Note that the arithmetic operators are truncated, i.e., their output is 16 bits; we use a standard n-bit truncated multiplier with correction constant <ref type="bibr" target="#b21">[22]</ref>. As shown in Table <ref type="table" target="#tab_3">2</ref>, its area is 6.10x smaller and its power 7.33x lower than a 32-bit floating-point multiplier at 65nm, see Section 6 for the CAD tools methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Storage: NBin, NBout, SB and NFU-2 Registers</head><p>The different storage structures of the accelerator can be construed as modified buffers of scratchpads. While a cache is an excellent storage structure for a general-purpose processor, it is a sub-optimal way to exploit reuse because of the cache access overhead (tag check, associativity, line size, speculative read, etc) and cache conflicts <ref type="bibr" target="#b38">[39]</ref>. The efficient alternative, scratchpad, is used in VLIW processors but it is known to be very difficult to compile for. However a scratchpad in a dedicated accelerator realizes the best of both worlds: efficient storage, and both efficient and easy exploitation of locality because only a few algorithms have to be manually adapted. In this case, we can almost directly translate the locality transformations introduced in Section 3 into mapping commands for the buffers, mostly modulating the tile factors. A code mapping example is provided in Section 5.3.2</p><p>We explain below how the storage part of the accelerator is organized, and which limitations of cache architectures it overcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Split buffers.</head><p>As explained before, we have split storage into three structures: an input buffer (NBin), an output buffer (NBout) and a synapse buffer (SB). Width. The first benefit of splitting structures is to tailor the SRAMs to the appropriate read/write width. The width of both NBin and NBout is T n × 2 bytes, while the width of SB is T n × T n × 2 bytes. A single read width size, e.g., as with a cache line size, would be a poor tradeoff. If it's adjusted to synapses, i.e., if the line size is T n × T n × 2, then there is a significant energy penalty for reading T n × 2 bytes out of a T n × T n × 2-wide data bank, see Figure <ref type="figure" target="#fig_12">13</ref> which indicates the SRAM read energy as a function of bank width for the TSMC process at 65nm. If the line size is adjusted to neurons, i.e., if the line size is T n × 2, there is a significant time penalty for reading T n × T n × 2 bytes out. Splitting storage into dedicated structures allows to achieve the best time and energy for each read request.</p><p>Conflicts. The second benefit of splitting storage structures is to avoid conflicts, as would occur in a cache. It is especially important as we want to keep the size of the storage structures small for cost and energy (leakage) reasons. The alternative solution is to use a highly associative cache. Consider the constraints: the cache line (or the number of ports) needs to be large (T n ×T n ×2) in order to serve the synapses at a high rate; since we would want to keep the cache size small, the only alternative to tolerate such a long cache line is high associativity. However, in an n-way cache, a fast read is implemented by speculatively reading all n ways/banks in parallel; as a result, the energy cost of an associative cache increases quickly. Even a 64-byte read from an 8-way associative 32KB cache costs 3.15x more energy than a 32-byte read from a direct mapped cache, at 65nm; measurements done using CACTI <ref type="bibr" target="#b39">[40]</ref>. And even with a 64-byte line only, the first-level 32KB data cache of Core i7 is already 8-way associative, so we need an even larger associativity with a very large line (for T n = 16, the line size would be 512byte long). In other words, a highly associative cache would be a costly energy solution in our case. Split storage and precise knowledge of locality behavior allows to entirely remove data conflicts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Exploiting the locality of inputs and synapses.</head><p>DMAs. For spatial locality exploitation, we implement three DMAs, one for each buffer (two load DMAs, one store DMA for outputs). DMA requests are issued to NBin in the form of instructions, later described in Section 5.3.2. These requests are buffered in a separate FIFO associated with each buffer, see Figure <ref type="figure" target="#fig_10">11</ref>, and they are issued as soon as the DMA has sent all the memory requests for the previous instruction. These DMA requests FIFOs enable to decouple the requests issued to all buffers and the NFU from the current buffer and NFU operations. As a result, DMA requests can be preloaded far in advance for tolerating long latencies, as long as there is enough buffer capacity; this preloading is akin to prefetching, albeit without speculation. Due to the combined role of NBin (and SB) as both scratchpads for reuse and preload buffers, we use a dual-port SRAM; the TSMC 65nm library rates the read energy overhead of dual port SRAMs for a 64-entry NB at 24%. Rotating NBin buffer for temporal reuse of input neurons. The inputs of all layers are split into chunks which fit in NBin, and they are reused by implementing NBin as a circular buffer. In practice, the rotation is naturally implemented by changing a register index, much like in a software implementation, there is no physical (and costly) movement of buffer entries.</p><p>Local transpose in NBin for pooling layers. There is a tension between convolutional and pooling layers for the data structure organization of (input) neurons. As mentioned before, K x , K y are usually small (often less than 10), and N i is about an order of magnitude larger. So memory fetches are more efficient (long stride-1 accesses) with the input feature maps as the innermost index of the three-dimensional neurons data structure. However, this is inconvenient for pooling layers because one output is computed per input feature map, i.e., using only K x × K y data (while in convolutional layers, all K x × K y × N i data are required to compute one output data). As a result, for pooling layers, the logical data structure organization is to have k x , k y as the innermost dimensions so that all inputs required to compute one output are consecutively stored in the NBin buffer. We resolve this tension by introducing a mapping function in NBin which has the effect of locally transposing loops k y , k x and loop i so that data is loaded along loop i, but it is stored in NBin and thus sent to NFU along loops k y , k x first; this is accom-plished by interleaving the data in NBin as it is loaded, see Figure <ref type="figure" target="#fig_13">14</ref>.</p><p>For synapses and SB, as mentioned in Section 3, there is either no reuse (classifier layers, convolutional layers with private kernels and pooling layers), or reuse of shared kernels in convolutional layers. For outputs and NBout, we need to reuse the partial sums, i.e., see reference sum[n] in Figure <ref type="figure" target="#fig_3">5</ref>. This reuse requires additional hardware modifications explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Exploiting the locality of outputs.</head><p>In both classifier and convolutional layers, the partial output sum of T n output neurons is computed for a chunk of input neurons contained in NBin. Then, the input neurons are used for another chunk of T n output neurons, etc. This creates two issues.</p><p>Dedicated registers. First, while the chunk of input neurons is loaded from NBin and used to compute the partial sums, it would be inefficient to let the partial sum exit the NFU pipeline and then re-load it into the pipeline for each entry of the NBin buffer, since data transfers are a major source of energy expense <ref type="bibr" target="#b13">[14]</ref>. So we introduce dedicated registers in NFU-2, which store the partial sums.</p><p>Circular buffer. Second, a more complicated issue is what to do with the T n partial sums when the input neurons in NBin are reused for a new set of T n output neurons. Instead of sending these T n partial sums back to memory (and to later reload them when the next chunk of input neurons is loaded into NBin), we temporarily rotate them out to NBout. A priori, this is a conflicting role for NBout which is also used to store the final output neurons to be written back to memory (write buffer). In practice though, as long as all input neurons have not been integrated in the partial sums, NBout is idle. So we can use it as a temporary storage buffer by rotating the T n partial sums out to NBout, see Figure <ref type="figure" target="#fig_10">11</ref>. Naturally, the loop iterating over output neurons must be tiled so that no more output neurons are computing their partial sums simultaneously than the capacity of NBout, but that is implemented through a second-level tiling similar to loop nnn in Figure <ref type="figure" target="#fig_3">5</ref> and Figure <ref type="figure" target="#fig_6">7</ref>. As a result, NBout is not only connected to NFU-3 and memory, but also to NFU-2: one entry of NBout can be loaded into the dedicated registers of NFU-2, and these registers can be stored in NBout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Control and Code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">CP.</head><p>In this section, we describe the control of the accelerator. One approach to control would be to hardwire the three target layers. While this remains an option for the future, for now, we have decided to use control instructions in order to explore different implementations (e.g., partitioning and scheduling) of layers, and to provide machine-learning researchers with the flexibility to try out different layer implementations. A layer execution is broken down into a set of instructions. Roughly, one instruction corresponds to the loops ii, i, n for classifier and convolutional layers, see Figures <ref type="figure" target="#fig_6">5 and 7</ref>, and to the loops ii, i in pooling layers (using the interleaving mechanism described in Section 5.2.3), see Figure <ref type="figure" target="#fig_8">8</ref>. The instructions are stored in an SRAM associated with the Control Processor (CP), see Figure <ref type="figure" target="#fig_10">11</ref>. The CP drives the execution of the DMAs of the three buffers and the NFU. The term "processor" only relates to the aforementioned "instructions", later described in Section 5.3.2, but it has very few of the traditional features of a processor (mostly a PC and an adder for loop index and address computations); from a hardware perspective, it is more like a configurable FSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Layer Code.</head><p>Every instruction has five slots, corresponding to the CP itself, the three buffers and the NFU, see Table <ref type="table" target="#tab_4">3</ref>.</p><p>Because of the CP instructions, there is a need for code generation, but a compiler would be overkill in our case as only three main types of codes must be generated. So we have implemented three dedicated code generators for the three layers. In Table <ref type="table" target="#tab_5">4</ref>, we give an example of the code generated for a classifier/perceptron layer. Since T n = 16 (16×16-bit data per buffer row) and NBin has 64 rows, its capacity is 2KB, so it cannot contain all the input neurons (N i = 8192, so 16KB). As a result, the code is broken down to operate on chunks of 2KB; note that the first instruction of NBin is a LOAD (data fetched from memory), and that it is marked as reused (flag immediately after load); the next instruction is a read, because these input neurons are rotated in the buffer for the next chunk of T n neurons, and the read is also marked as reused because there are 8 such rotations ( 16KB 2KB ); at the same time, notice that the output of NFU-2 for the first (and next) instruction is NBout, i.e., the partial output neurons sums are rotated to NBout, as explained in Section 5.2.3, which is why the NBout instruction is WRITE; notice also that the input of NFU-2 is RESET (first chunk of input neurons, registers reset). Finally, when the last chunk of input neurons are sent (last instruction in table), the (store) DMA of NBout is set for writing 512 bytes (256 outputs), and the NBout instruction is STORE; the NBout write operation for the next instructions will be NOP (DMA set at first chunk and automatically storing data back to memory until DMA elapses).</p><p>Note that the architecture can implement either per-image or batch processing <ref type="bibr" target="#b40">[41]</ref>, only the generated layer control code would change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Methodology</head><p>Measurements. We use three different tools for performance/energy measurements.</p><p>Accelerator simulator. We implemented a custom cycleaccurate, bit-accurate C++ simulator of the accelerator fabric, which was initially used for architecture exploration, and which later served as the specification for the Verilog implementation. This simulator is also used to measure time in number of cycles. It is plugged to a main memory model allowing a bandwidth of up to 250 GB/s.</p><p>CAD tools. For area, energy and critical path delay (cycle time) measurements, we implemented a Verilog version of the accelerator, which we first synthesized using the Synopsys Design Compiler using the TSMC 65nm GP standard VT library, and which we then placed and routed using the Synopsys ICC compiler. We then simulated the design using Synopsys VCS, and we estimated the power using Prime-Time PX.</p><p>SIMD. For the SIMD baseline, we use the GEM5+McPAT <ref type="bibr" target="#b27">[28]</ref> combination. We use a 4-issue superscalar x86 core with a 128-bit (8×16-bit) SIMD unit (SSE/SSE2), clocked at 2GHz. The core has a 192-entry ROB, and a 64-entry load/store queue. The L1 data (and instruction) cache is 32KB and the L2 cache is 2MB; both caches are 8-way associative and use a 64-byte line; these cache characteristics correspond to those of the Intel Core i7. The L1 miss latency to the L2 is 10 cycles, and the L2 miss latency to memory is 250 cycles; the memory bus width is 256 bits. We have aligned the energy cost of main memory accesses of our accelerator and the simulator by using those provided by McPAT (e.g., <ref type="bibr" target="#b16">17</ref>.6nJ for a 256-bit read memory access).</p><p>We implemented a SIMD version of the different layer codes, which we manually tuned for locality as explained in Section 3 (for each layer, we perform a stochastic exploration to find good tile factors); we compiled these programs using the default -O optimization level but the inner loops were written in assembly to make the best possible use of the SIMD unit. In order to assess the performance of the SIMD core, we also implemented a standard C++ version of the different benchmark layers presented below, and on average (geometric mean), we observed that the SIMD core provides  Benchmarks. For benchmarks, we have selected the largest convolutional, pooling and/or classifier layers of several recent and large neural network structures. The characteristics of these 10 layers plus a description of the associated neural network and task are shown in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Accelerator Characteristics after Layout</head><p>The current version uses T n = 16 (16 hardware neurons with 16 synapses each), so that the design contains 256 16bit truncated multipliers in NFU-1 (for classifier and convolutional layers), 16 adder trees of 15 adders each in NFU-2 (for the same layers, plus pooling layer if average is used), as well as a 16-input shifter and max in NFU-2 (for pooling layers), and 16 16-bit truncated multipliers plus 16 adders in NFU-3 (for classifier and convolutional layers, and optionally for pooling layers). For classifier and convolutional layers, NFU-1 and NFU-2 are active every cycle, achieving 256 + 16 × 15 = 496 fixed-point operations every cycle; at 0.98GHz, this amounts to 452 GOP/s (Giga fixed-point OPerations per second). At the end of a layer, NFU-3 would be active as well while NFU-1 and NFU-2 process the remaining data, reaching a peak activity of 496 + 2 × 16 = 528 operations per cycle (482 GOP/s) for a short period.</p><p>We have done the synthesis and layout of the accelerator with T n = 16 and 64-entry buffers at 65nm using Synopsys tools, see Figure <ref type="figure" target="#fig_15">15</ref>. The main characteristics and power/area breakdown by component type and functional block are shown in Table <ref type="table" target="#tab_8">6</ref>. We brought the critical path delay down to 1.02ns by introducing 3 pipeline stages in NFU-1 (multipliers), 2 stages in NFU-2 (adder trees), and 3 stages in NFU-3 (piecewise linear function approximation) for a total of 8 pipeline stages. Currently, the critical path is in the issue   logic which is in charge of reading data out of NBin/NBout; next versions will focus on how to reduce or pipeline this critical path. The total RAM capacity (NBin + NBout + SB + CP instructions) is 44KB (8KB for the CP RAM). The area and power are dominated by the buffers (NBin/NBout/SB) at respectively 56% and 60%, with the NFU being a close second at 28% and 27%. The percentage of the total cell power is 59.47%, but the routing network (included in the different components of the table breakdown) accounts for a significant share of the total power at 38.77%. At 65nm, due to the high toggle rate of the accelerator, the leakage power is almost negligible at 1.73%. Finally, we have also evaluated a design with T n = 8, and thus 64 multipliers in NFU-1. The total area for that design is 0.85 mm 2 , i.e., 3.59x smaller than for T n = 16 due to the reduced buffer width and the fewer number of arithmetic operators. We plan to investigate larger designs with T n = 32 or 64 in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Time and Throughput</head><p>In Figure <ref type="figure" target="#fig_16">16</ref>, we report the speedup of the accelerator over SIMD, see SIMD/Acc. Recall that we use a 128-bit SIMD processor, so capable of performing up to 8 16-bit operations every cycle (we naturally use 16-bit fixed-point operations in the SIMD as well). As mentioned in Section 7.1, the accelerator performs 496 16-bit operations every cycle for both classifier and convolutional layers, i.e., 62x more ( <ref type="formula">496</ref>8 ) than the SIMD core. We empirically observe that on these two types of layers, the accelerator is on average 117.87x faster than the SIMD core, so about 2x above the ratio of computational operators (62x). We measured that, for classifier and convolutional layers, the SIMD core performs 2.01 16-bit operations per cycle on average, instead of the upper bound of 8 operations per cycle. We traced this back to two major reasons.</p><p>First, better latency tolerance due to an appropriate combination of preloading and reuse in NBin and SB buffers; note that we did not implement a prefetcher in the SIMD core, which would partly bridge that gap. This explains the high performance gap for layers CLASS1, CLASS3 and CONV5 which have the largest feature maps sizes, thus the most spatial locality, and which then benefit most from preloading, giving them a performance boost, i.e., 629.92x on average, about 3x more than other convolutional layers; we expect that a prefetcher in the SIMD core would cancel that performance boost. The spatial locality in NBin is exploited along the input feature map dimension by the DMA, and with a small N i , the DMA has to issue many short memory requests, which is less efficient. The rest of the convolutional layers (CONV1 to CONV4) have an average speedup of 195.15x; CONV2 has a lesser performance (130.64x) due to private kernels and less spatial locality. Pooling layers have less performance overall because only the adder tree in NFU-2 is used (240 operators out of 496 operators), 25.73x for POOL3 and 25.52x for POOL5.</p><p>In order to further analyze the relatively poor behavior of POOL1 (only 2.17x over SIMD), we have tested a configuration of the accelerator where all operands (inputs and synapses) are ready for the NFU, i.e., ideal behavior of NBin, SB and NBout; we call this version "Ideal", see Figure <ref type="figure" target="#fig_16">16</ref>. We see that the accelerator is significantly slower on POOL1 and CONV2 than the ideal configuration (respectively 66.00x and 16.14x). This is due to the small size of their input/output feature maps (e.g., N i = 12 for for POOL1), combined with the fewer operators used for POOL1. So far, the accelerator has been geared towards large layers, but we can address this weakness by implementing a 2D or 3D DMA (DMA requests over i, k x , k y loops); we leave this optimization for future work.</p><p>The second reason for the speedup over SIMD beyond 62x lays in control and scheduling overhead. In the accelerator, we have tried to minimize lost cycles. For instance, when output neurons partial sums are rotated to NBout (before being sent back to NFU-2), the oldest buffer row (T n partial sums) is eagerly rotated out to the NBout/NFU-2 input latch, and a multiplexer in NFU-2 ensures that either this latch or the NFU-2 registers are used as input for the NFU-2 stage computations; this allows a rotation without any pipeline stall. Several such design optimizations help achieve a slowdown of only 4.36x over the ideal accelerator, see Figure <ref type="figure" target="#fig_16">16</ref>, and in fact, 2.64x only if we exclude CONV2 and POOL1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Energy</head><p>In Figure <ref type="figure" target="#fig_17">17</ref>, we provide the energy ratio between the SIMD core and the accelerator. While high at 21.08x, the average energy ratio is actually more than an order of magnitude smaller than previously reported energy ratios between processors and accelerators; for instance Hameed et al. <ref type="bibr" target="#b13">[14]</ref> report an energy ratio of about 500x, and 974x has been reported for a small Multi-Layer Perceptron <ref type="bibr" target="#b37">[38]</ref>. The smaller ratio is largely due to the energy spent in memory accesses, which was voluntarily not factored in the two aforementioned studies. Like in these two accelerators and others, the energy cost of computations has been considerably reduced by a combination of more efficient computational operators (especially a massive number of small 16-bit fixed-point  truncated multipliers in our case), and small custom storage located close to the operators (64-entry NBin, NBout, SB and the NFU-2 registers). As a result, there is now an Amdahl's law effect for energy, where any further improvement can only be achieved by bringing down the energy cost of main memory accesses. We tried to artificially set the energy cost of the main memory accesses in both the SIMD and accelerator to 0, and we observed that the average energy reduction of the accelerator increases by more than one order of magnitude, in line with previous results. This is further illustrated by the breakdown of the energy consumed by the accelerator in Figure <ref type="figure" target="#fig_18">18</ref> where the energy of main memory accesses obviously dominates. A distant second is the energy of NBin/NBout for the convolutional layers with shared kernels (CONV1, CONV3, CONV4). In this case, a set of shared kernels are kept in SB so the memory traffic due to synapses becomes very low, as explained in Section 3 (shared kernels + tiling), but the input neurons must still be reloaded for each new set of shared kernels, hence the still noticeable energy expense. The energy of the computational logic in pooling layers (POOL1, POOL3, POOL5) is similarly a distant second expense, this time because there is no synapse to load. The slightly higher energy reduction of pooling layers (22.17x on average), see Figure <ref type="figure" target="#fig_17">17</ref>, is due to the fact the SB buffer is not used (no synapse), and the accesses to NBin alone are relatively cheap due to its small width, see Figure <ref type="figure" target="#fig_12">13</ref>.</p><p>The SIMD energy breakdown is in sharp contrast, as shown in Figure <ref type="figure" target="#fig_19">19</ref>, with about two thirds of the energy spent in computations, and only one third in memory accesses.</p><p>While finding a computationally more efficient approach to SIMD made sense, future work for the accelerator should focus on reducing the energy spent in memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work</head><p>Due to stringent energy constraints, such as Dark Silicon <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, there is a growing consensus that future highperformance micro-architectures will take the form of heterogeneous multi-cores, i.e., combinations of cores and accelerators. Accelerators can range from processors tuned for certain tasks, to ASIC-like circuits such as H264 <ref type="bibr" target="#b13">[14]</ref>, or more flexible accelerators capable of targeting a broad range of, but not all, tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref> such as QsCores <ref type="bibr" target="#b41">[42]</ref>, or accelerators for image processing <ref type="bibr" target="#b32">[33]</ref>.</p><p>The accelerator proposed in this article follows this spirit of targeting a specific, but broad, domain, i.e., machinelearning tasks here. Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks <ref type="bibr" target="#b24">[25]</ref> and Convolutional Neural Networks <ref type="bibr" target="#b26">[27]</ref> have become state-of-the-art machine-learning techniques <ref type="bibr" target="#b25">[26]</ref> across a broad range of applications such as web search <ref type="bibr" target="#b18">[19]</ref>, image analysis <ref type="bibr" target="#b30">[31]</ref> or speech recognition <ref type="bibr" target="#b6">[7]</ref>.</p><p>While many implementations of hardware neurons and neural networks have been investigated in the past two decades <ref type="bibr" target="#b17">[18]</ref>, the main purpose of hardware neural networks has been fast modeling of biological neural networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> for implementing neurons with thousands of connections. While several of these neuromorphic architectures have been applied to computational tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref>, the specific bio-inspired information representation (spiking neural networks) they rely on may not be competitive with stateof-the-art neural networks, though this remains an open debate at the threshold between neuroscience and machinelearning.</p><p>However, recently, due to simultaneous trends in applications, machine-learning and technology constraints, hardware neural networks have been increasingly considered as potential accelerators, either for very dedicated functionalities within a processor, such as branch prediction <ref type="bibr" target="#b0">[1]</ref>, or for their fault-tolerance properties <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38]</ref>. The latter property has also been leveraged to trade application accuracy for energy efficiency through hardware neural processing units <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>The focus of our accelerator is on large-scale machinelearning tasks, with layers of thousands of neurons and millions of synapses, and for that reason, there is a special emphasis on interactions with memory. Our study not only confirms previous observations that dedicated storage is key for achieving good performance and power <ref type="bibr" target="#b13">[14]</ref>, but it also highlights that, beyond exploiting locality at the level of registers located close to computational operators <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, considering memory as a prime-order concern can profoundly affect accelerator design.</p><p>Many of the aforementioned studies stem from the architecture community. A symmetric effort has started in the machine-learning community where a few researchers have been investigating hardware designs for speeding up neural network processing, especially for real-time applications. Neuflow <ref type="bibr" target="#b12">[13]</ref> is an accelerator for fast and low-power implementation of the feed-forward paths of CNNs for vision systems. It organizes computations and register-level storage according to the sliding window property of convolutional and pooling layers; but in that respect, it also ignores much of the first-order locality coming from input and output feature maps. Its interplay with memory remains limited to a DMA, there is no significant on-chip storage, though the DMA is capable of performing complex access patterns. A more complex architecture, albeit with similar performance as Neuflow, has been proposed by Kim et al. <ref type="bibr" target="#b20">[21]</ref> and consists of 128 SIMD processors of 16 PEs each; the architecture is significantly larger and implements a specific neural vision model (neither CNNs nor DNNs), but it can achieve 60 frame/sec (real-time) multi-object recognition for up to 10 different objects. Maashri et al. <ref type="bibr" target="#b28">[29]</ref> have also investigated the implementation of another neural network model, the bio-inspired HMAX for vision processing, using a set of custom accelerators arranged around a switch fabric; in the article, the authors allude to locality optimizations across different orientations, which are roughly the HMAX equivalent of feature maps. Closer to our community again, but solely focusing on CNNs, Chakradhar et al. <ref type="bibr" target="#b2">[3]</ref> have also investigated the implementation of CNNs on reconfigurable circuits; though there is little emphasis on locality exploitation, they pay special attention to properly mapping a CNN in order to improve bandwidth utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>In this article we focus on accelerators for machine-learning because of the broad set of applications and the few key state-of-the-art algorithms offer the rare opportunity to combine high efficiency and broad application scope. Since state-of-the-art CNNs and DNNs mean very large networks, we specifically focus on the implementation of large-scale layers. By carefully exploiting the locality properties of such layers, and by introducing storage structures custom designed to take advantage of these properties, we show that it is possible to design a machine-learning accelerator capable of high performance in a very small area footprint. Our measurements are not circumscribed to the accelerator fabric, they factor in the performance and energy overhead of main memory transfers; still, we show that it is possible to achieve a speedup of 117.87x and an energy reduction of 21.08x over a 128-bit 2GHz SIMD core with a normal cache hierarchy. We have obtained a layout of the design at 65nm.</p><p>Besides a planned tape-out, future work includes improving the accelerator behavior for short layers, slightly altering the NFU to include some of the latest algorithmic im-provements such as Local Response Normalization, further reducing the impact of main memory transfers, investigating scalability (especially increasing T n ), and implementing training in hardware.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Neural network hierarchy containing convolutional, pooling and classifier layers.</figDesc><graphic url="image-1.png" coords="2,317.01,72.00,255.11,104.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Classifier layer tiling.</figDesc><graphic url="image-2.png" coords="3,106.68,105.34,59.69,59.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Pooling layer tiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Pseudo-code for a classifier (here, perceptron) layer (original loop nest + locality optimization).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Memory bandwidth requirements for each layer type (CONV3 has shared kernels, CONV5 has private kernels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>for (int yy = 0; yy ¡ Nyin; yy += Ty) { for (int xx = 0; xx ¡ Nxin; xx += Tx) { for (int nnn = 0; nnn ¡ Nn; nnn += Tnn) { // -Original code -(excluding nn, ii loops) int yout = 0; for (int y = yy; y &lt; yy + Ty; y += sy) { // tiling for y; int xout = 0; for (int x = xx; x &lt; xx + Tx; x += sx) { // tiling for x; for (int nn = nnn; nn &lt; nnn + Tnn; nn += Tn) { for (int n = nn; n &lt; nn + Tn; n++) sum[n] = 0; // sliding window; for (int ky = 0; ky &lt; Ky; ky++)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Pseudo-code for convolutional layer (original loop nest + locality optimization), both shared and private kernels versions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>. Unlike convolutional layers, the number of input and output feature for (int yy = 0; yy ¡ Nyin; yy += Ty) { for (int xx = 0; xx ¡ Nxin; xx += Tx) { for (int iii = 0; iii ¡ Ni; iii += Tii) // -Original code -(excluding ii loop) int yout = 0; for (int y = yy; y &lt; yy + Ty; y += sy) { int xout = 0; for (int x = xx; x &lt; xx + Tx; x += sx) { for (int ii = iii; ii &lt; iii + Tii; ii += Ti) for (int i = ii; i &lt; ii + Ti; i++) value[i] = 0; for (int ky = 0; ky &lt; Ky; ky++)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Pseudo-code for pooling layer (original loop nest + locality optimization).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure 9. Full hardware implementation of neural networks.</figDesc><graphic url="image-153.png" coords="6,198.74,113.69,70.52,98.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Accelerator.</figDesc><graphic url="image-262.png" coords="6,331.34,187.66,221.36,65.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. 32-bit floating-point vs. 16-bit fixed-point accuracy for UCI data sets (metric: log(Mean Squared Error)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Read energy vs. SRAM width.</figDesc><graphic url="image-278.png" coords="8,54.00,72.00,255.13,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Local transpose (Ky = 2, Kx = 1, Ni = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>a 3 .</head><label>3</label><figDesc>92x improvement in execution time and 3.74x in energy over the x86 core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Layout (65nm).</figDesc><graphic url="image-279.png" coords="11,88.51,72.00,170.08,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Speedup of accelerator over SIMD, and of ideal acover accelerator.</figDesc><graphic url="image-280.png" coords="11,317.01,72.00,283.47,176.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Energy reduction of accelerator over SIMD.</figDesc><graphic url="image-281.png" coords="12,54.00,72.00,283.47,159.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Breakdown of accelerator energy.</figDesc><graphic url="image-282.png" coords="12,317.01,72.00,255.12,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Breakdown of SIMD energy.</figDesc><graphic url="image-283.png" coords="12,317.01,220.44,255.13,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>32-bit floating-point vs. 16-bit fixed-point accuracy for MNIST (metric: error rate).</figDesc><table><row><cell>Type</cell><cell>Error Rate</cell></row><row><cell>32-bit floating-point</cell><cell>0.0311</cell></row><row><cell>16-bit fixed-point</cell><cell>0.0337</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Characteristics of multipliers.</figDesc><table><row><cell>Type</cell><cell cols="2">Area (µm 2 ) Power (µW )</cell></row><row><cell cols="2">16-bit truncated fixed-point multiplier 1309.32</cell><cell>576.90</cell></row><row><cell>32-bit floating-point multiplier</cell><cell>7997.76</cell><cell>4229.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Control instruction format. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table><row><cell>CP</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SB</cell><cell></cell><cell></cell><cell></cell><cell cols="3">NBin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">NBout</cell><cell></cell><cell></cell><cell></cell><cell>NFU</cell></row><row><cell>END</cell><cell cols="2">READ OP</cell><cell cols="2">REUSE</cell><cell>ADDRESS</cell><cell>SIZE</cell><cell>READ OP</cell><cell>REUSE</cell><cell>STRIDE</cell><cell>STRIDE BEGIN</cell><cell>STRIDE END</cell><cell>ADDRESS</cell><cell cols="2">SIZE</cell><cell>READ OP</cell><cell cols="2">WRITE OP</cell><cell cols="2">ADDRESS</cell><cell>SIZE</cell><cell>NFU-1 OP</cell><cell cols="2">NFU-2 OP</cell><cell>NFU-2 IN</cell><cell>NFU-2 OUT</cell><cell>NFU-3 OP</cell><cell>OUTPUT BEGIN</cell><cell>OUTPUT END</cell></row><row><cell>CP</cell><cell></cell><cell></cell><cell cols="2">SB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">NBin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">NBout</cell><cell></cell><cell></cell><cell>NFU</cell></row><row><cell>NOP</cell><cell>LOAD</cell><cell cols="2">0</cell><cell cols="2">0</cell><cell>32768</cell><cell>LOAD</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">4194304</cell><cell>2048</cell><cell cols="2">NOP</cell><cell cols="2">WRITE</cell><cell cols="2">0</cell><cell>0</cell><cell>MULT</cell><cell>ADD</cell><cell>RESET</cell><cell>NBOUT</cell><cell>SIGMOID</cell><cell>1</cell><cell>0</cell></row><row><cell>NOP</cell><cell>LOAD</cell><cell cols="2">0</cell><cell cols="2">32768</cell><cell>32768</cell><cell>READ</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0</cell><cell>0</cell><cell cols="2">NOP</cell><cell cols="2">WRITE</cell><cell cols="2">0</cell><cell>0</cell><cell>MULT</cell><cell>ADD</cell><cell>RESET</cell><cell>NBOUT</cell><cell>SIGMOID</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</cell><cell></cell></row><row><cell>NOP</cell><cell>LOAD</cell><cell cols="2">0</cell><cell cols="2">7864320</cell><cell>32768</cell><cell>LOAD</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">4225024</cell><cell>2048</cell><cell cols="2">READ</cell><cell cols="2">STORE</cell><cell cols="2">8388608</cell><cell>512</cell><cell>MULT</cell><cell>ADD</cell><cell>NBOUT</cell><cell>NFU3</cell><cell>SIGMOID</cell><cell>1</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Subset of classifier/perceptron code (Ni = 8192,</figDesc><table /><note>No = 256, Tn = 16, 64-entry buffers).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Characteristics of accelerator and breakdown by com-</figDesc><table><row><cell>ponent type (first 5 lines), and functional block (last 7 lines).</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by a Google Faculty Research Award, the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), the French ANR MHANN and NEMESIS grants, the NSF of China (under Grants 61003064, 61100163, 61133004, 61222204, 61221062, 61303158), the 863 Program of China (under Grant 2012AA012202), the Strategic Priority Research Program of the CAS (under Grant XDA06010403), the 10,000 and 1,000 talent programs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-power, highperformance analog neural branch prediction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Como</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The PARSEC benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A dynamically configurable coprocessor for convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jakkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on Computer Architecture</title>
				<meeting><address><addrLine>Saint Malo, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BenchNN: On the Broad Potential Application Scope of Hardware Neural Network Accelerators</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning with cots hpc systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-Vector Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Deep Neural Networks for LVCSR using Rectified Linear Units and Dropout</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the capabilities of neural networks using limited precision weights</title>
		<author>
			<persName><forename type="first">S</forename><surname>Draghici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="414" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging the Error Resilience of Machine-Learning Applications for Designing Highly Energy Efficient Accelerators</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lingamneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Palem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia and South Pacific Design Automation Conference</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dark Silicon and the End of Multicore Scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 38th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Acceleration for General-Purpose Approximate Programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bridging the computation gap between programmable processors and hardwired accelerators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NeuFlow: A runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Akselrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding sources of inefficiency in general-purpose chips</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solomatnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A case for neuromorphic ISAs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<idno>arXiv: . .</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finite Precision Error Analysis of Neural Network Hardware Implementations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Holi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An electrically trainable artificial neural network (ETANN) with 10240 floating gate synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial neural networks</title>
				<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SpiNNaker: Mapping neural networks onto a massively-parallel chip multiprocessor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Painkras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2849" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A 201.4 GOPS 496 mW Real-Time Multi-Object Recognition Processor With Bio-Inspired Neural Perception Engine</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="45" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-dependent truncation scheme for parallel multipliers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Swartzlander</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals, Systems &amp; Computers, 1997. Conference Record of the Thirty-First Asilomar Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1178" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Efficient Hardware Architecture for a Neural Network Activation Function Generator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kinane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>J. Wang, Z. Yi, J. M. Zurada, B.-L. Lu, and H. Yin</editor>
		<imprint>
			<biblScope unit="volume">3973</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1319" to="1327" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kinane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP (3)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1178" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building High-level Features Using Large Scale Unsupervised Learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">McPAT: an integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 42nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accelerating neuromorphic vision algorithms for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Maashri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chandramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Design Automation Conference on -DAC &apos;12</title>
				<meeting>the 49th Annual Design Automation Conference on -DAC &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">579</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Akopyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Custom Integrated Circuits Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-09">Sept. 2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Label Aerial Images from Noisy Data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
				<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dark Silicon and the Internet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EE Times &quot;Designing with ARM&quot; virtual conference</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolution engine: balancing efficiency &amp; flexibility in specialized computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wafer-scale integration of analog neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="431" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks Applied to House Numbers Digit Classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale Convolutional Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Defect-Tolerant Accelerator for Emerging High-Performance Applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Software assistance for data caches</title>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Drach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="536" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<title level="m">CACTI 5.1. HP Labs</title>
				<meeting><address><addrLine>Palo Alto, Tech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on CPUs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">QsCORES : Trading Dark Silicon for Scalable Energy Efficiency with Quasi-Specific Cores Categories and Subject Descriptors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goulding-Hotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamically reconfigurable silicon array of spiking neurons with conductance-based synapses</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reconciling specialization and flexibility through compound circuits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yehia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
				<meeting><address><addrLine>Raleigh, North Carolina</addrLine></address></meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009-02">Feb. 2009</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
