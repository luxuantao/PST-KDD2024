<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recent Advances in End-to-End Automatic Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-02">2 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Recent Advances in End-to-End Automatic Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-02">2 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.01690v2[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>end-to-end</term>
					<term>automatic speech recognition</term>
					<term>streaming</term>
					<term>attention</term>
					<term>transducer</term>
					<term>transformer</term>
					<term>adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The accuracy of automatic speech recognition (ASR) has been significantly boosted since deep neural network (DNN) based hybrid modeling <ref type="bibr" target="#b0">[1]</ref> was adopted a decade ago. This breakthrough used DNN to replace the traditional Gaussian mixture model for the acoustic likelihood evaluation, while still keeping all the components such as acoustic model, language model, and lexicon model, etc., as the hybrid ASR system. Recently, the speech community has a new breakthrough by transiting from hybrid modeling to end-to-end (E2E) modeling <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> which directly translates an input speech sequence into an output token sequence using a single network. Such a breakthrough is even more revolutionary because it overthrows all the modeling components in traditional ASR systems, which have been used for decades.</p><p>There are several major advantages of E2E models over traditional hybrid models. First, E2E models use a single objective function which is consistent with the ASR objective to optimize the whole network, while traditional hybrid models optimize individual components separately, which cannot guarantee the global optimum. Therefore, E2E models have been shown to outperform traditional hybrid models not only in academics <ref type="bibr" target="#b9">[10]</ref> but also in the industry <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Second, because E2E models directly output characters or even words, it greatly simplifies the ASR pipeline. In contrast, the design of traditional hybrid models is complicated, requiring lots of expert knowledge with years of ASR experience. Third, because a single network is used for ASR, E2E models are much more compact than traditional hybrid models. Therefore, E2E models can be deployed to devices with high accuracy.</p><p>Address: 1 Microsoft Way, Redmond, WA, 98052, USA Corresponding author: Jinyu Li Email: jinyli@microsoft.com While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the time of writing because the ASR accuracy is not the only factor for the production choice between hybrid and E2E models. There are lots of practical factors such as streaming, latency, adaptation capability, etc., which affect the commercial model deployment decision. Traditional hybrid models, optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. To that end, in this paper, we overview popular E2E models with a focus on the technologies addressing those challenges from the perspective of the industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. END-TO-END MODELS</head><p>The most popular E2E techniques for ASR are: (a) Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b12">[13]</ref>, (b) Attentionbased Encoder-Decoder (AED) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and (c) recurrent neural network Transducer (RNN-T) <ref type="bibr" target="#b15">[16]</ref>. Among them, RNN-T provides a natural solution to streaming ASR with high accuracy and low latency, ideal for industrial application. These three most popular E2E techniques are illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. In this section, we will give a short overview of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Connectionist Temporal Classification</head><p>The Connectionist Temporal Classification (CTC) technique for ASR <ref type="bibr" target="#b1">[2]</ref> was designed to map the speech input sequence into an output label sequence. Because the length of output labels is smaller than that of the input speech sequence, a blank label is inserted between output labels with allowable repetition of labels to construct CTC paths that have the same length as the input speech sequence. Figure <ref type="figure" target="#fig_1">2</ref> shows an example of three CTC paths for the word "team". Figure <ref type="figure" target="#fig_0">1a</ref> shows the architecture of CTC. We denote the input speech sequence as x, the original output label sequence as y, and all of the CTC paths mapped from y as B −1 (y). The encoder network is used to convert the acoustic feature x t into a high-level representation h enc t . The CTC loss function is defined as the negative log probabilities of correct labels given the input speech sequence:</p><formula xml:id="formula_0">L CT C = −lnP (y|x),<label>(1)</label></formula><p>with P (y|x) = q∈B −1 (y)</p><formula xml:id="formula_1">P (q|x),<label>(2)</label></formula><p>where q is a CTC path. With the conditional independence assumption, P (q|x) can be decomposed into a product of frame posterior as</p><formula xml:id="formula_2">P (q|x) = T t=1 P (q t |x),<label>(3)</label></formula><p>where T is the length of the speech sequence. CTC is the first E2E technology widely used in ASR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. However, the conditional independence assumption in CTC is most criticized. One way to relax that assumption is to use the attention mechanism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> which can introduce implicit language modeling across speech frames. Such attention-based CTC models implicitly relax the conditional independence assumption by improving the encoder without changing the CTC objective function, and therefore enjoy the simplicity of CTC modeling. By replacing the underlying long short-term memory (LSTM) <ref type="bibr" target="#b24">[25]</ref> with Transformer <ref type="bibr" target="#b25">[26]</ref> in the encoder, which allows a more powerful attention mechanism to be used, CTC thrives again in recent studies <ref type="bibr" target="#b26">[27]</ref>. It gets further boosted by the emerged self-supervised learning technologies <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> which can learn a very good representation that carries semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Attention-based Encoder-Decoder</head><p>The attention-based encoder-decoder (AED) model is another type of E2E ASR model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1b</ref>, AED has an encoder network, an attention module, and a decoder network. The AED model calculates the probability as</p><formula xml:id="formula_3">P (y|x) = u P (y u |x, y 1:u−1 ), (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where u is the output label index. The training objective is also to minimize −lnP (y|x).</p><p>The encoder network performs the same function as the encoder network in CTC by converting input feature sequences into high-level hidden feature sequences. The attention module computes attention weights between the previous decoder output and the encoder output of each frame using attention functions such as additive attention <ref type="bibr" target="#b5">[6]</ref> or dot-product attention <ref type="bibr" target="#b6">[7]</ref>, and then generates a context vector as a weighted sum of the encoder outputs. The decoder network takes the previous output label together with the context vector to generate its output to calculate P (y u |x, y 1:u−1 ), which is operated in an autoregressive way as a function of the previous label outputs without the conditional independence assumption.</p><p>While the attention on the full sequence in AED is a natural solution to machine translation which has the word order switching between source and target languages, it may not be ideal to ASR because the speech signal and output label sequence are monotonic. In order to have better alignment between the speech signal and label sequence, the AED model is optimized together with a CTC model in a multi-task learning framework by sharing the encoder <ref type="bibr" target="#b33">[34]</ref>. Such a training strategy greatly improves the convergence of the attention-based model and mitigates the alignment issue. It becomes the standard training recipe for most AED models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a further improvement was proposed by combining the scores from both the AED model and the CTC model during decoding.</p><p>Most commercial setups need ASR systems to be streaming with low latency, which means ASR systems should produce the recognition results at the same time as the user is speaking. In vanilla AED models, the attention is applied to the whole utterance in order to achieve good performance. The latency can be significant because such a setup needs to obtain the full utterance before decoding, and it is impractical for streaming ASR scenarios where the speech signal comes in a continuous mode without segmentation. There are lots of attempts to build streaming AED models. The basic idea of these methods is applying attention on the chunks of the input speech. The difference between these attempts is the way the chunks are determined and used for attention. Figure <ref type="figure" target="#fig_2">3a</ref> shows the full attention of AED, which spreads attention weights across the whole utterance. One major school of streaming AED models is to use monotonic attention. In <ref type="bibr" target="#b42">[43]</ref>, a monotonic attention mechanism was proposed to use integrated decision for triggering ASR. Attention is only on the time step corresponding to the trigger point as shown in Figure <ref type="figure" target="#fig_2">3b</ref>. The method was improved in <ref type="bibr" target="#b43">[44]</ref>, where a monotonic chunkwise attention (MoChA) method was proposed to stream the attention by splitting the encoder outputs into small fixed-size chunks so that the soft attention is only applied to those small chunks, as shown in Figure <ref type="figure" target="#fig_2">3c</ref>. Adaptive-size instead of fixed-size chunks were used in <ref type="bibr" target="#b44">[45]</ref>. Recently, monotonic infinite lookback (MILK) attention <ref type="bibr" target="#b45">[46]</ref> was proposed to attend the entire sequence preceding the trigger point, as shown in Figure <ref type="figure" target="#fig_2">3d</ref>. Another popular streaming AED method is triggered attention <ref type="bibr" target="#b46">[47]</ref> which uses CTC segments to decide the chunks and is applied to lots of AED models <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>.</p><p>While all these methods can achieve the goal of streaming ASR to some extent, they usually do not enforce low-latency which is another important factor for commercial ASR systems. This challenge was addressed in <ref type="bibr" target="#b50">[51]</ref>, which proposed to train low-latency streaming AED models by leveraging the external hard alignment. In <ref type="bibr" target="#b51">[52]</ref>, a scout network was used to predict word boundary which is then used by the ASR network to predict the next subword by utilizing the information from all speech frames before it for low latency streaming. All streaming AED models need some complicated strategies to decide the trigger point for streaming. A comparison of streaming ASR methods was conducted in <ref type="bibr" target="#b52">[53]</ref> which shows RNN Transducer has advantages over MoChA in terms of latency, inference time, and training stability. AED models also cannot perform well on long utterances <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Therefore, although there are still many activities <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref> in the area of streaming AED models, the industry tends to choose RNN Transducer introduced next as the dominating streaming E2E model while AED has its position in some non-streaming scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C) RNN Transducer</head><p>RNN Transducer (RNN-T) <ref type="bibr" target="#b15">[16]</ref> provides a natural way for streaming ASR because its output conditions on the previous output tokens and the speech sequence until the current time step (i.e., frame) as P (y u |x 1:t , y 1:u−1 ). In this way, it also removes the conditional independence assumption of CTC. With the natural streaming capability, RNN-T becomes the most popular E2E model in the industry <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>.</p><p>Illustrated in Figure <ref type="figure" target="#fig_0">1c</ref>, RNN-T contains an encoder network, a prediction network, and a joint network. The encoder network is the same as that in CTC and AED, generating a high-level feature representation h enc t . The prediction network produces a high-level representation h pre u based on RNN-T's previous output label y u−1 . The joint network is a feed-forward network that combines h enc t and h pre u as</p><formula xml:id="formula_5">z t,u = ψ(Qh enc t + Vh pre u + b z ),<label>(5)</label></formula><p>where Q and V are weight matrices, b z is a bias vector, and ψ is a non-linear function (e.g., RELU or Tanh). z t,u is JINYU LI Fig. <ref type="figure">4</ref>.: Example alignment paths of RNN-T.</p><p>connected to the output layer with a linear transform</p><formula xml:id="formula_6">h t,u = W y z t,u + b y ,<label>(6)</label></formula><p>where W y and b y denote a weight matrix and a bias vector, respectively. The probability of each output token k is</p><formula xml:id="formula_7">P (y u = k|x 1:t , y 1:u−1 ) = sof tmax(h k t,u ).<label>(7)</label></formula><p>The loss function of RNN-T is also −lnP (y|x), with</p><formula xml:id="formula_8">P (y|x) = a∈A −1 (y) P (a|x)<label>(8)</label></formula><p>as the sum of all possible alignment paths that are mapped to the label sequence y. The mapping from the alignment path a to the label sequence y is defined as A(a) = y. <ref type="foot" target="#foot_0">1</ref>In Figure <ref type="figure">4</ref>, three example alignment paths are plotted for speech sequence x = (x 1 , x 2 , ......, x 8 ) and label sequence y = ( s , t, e, a, m), where s is a token for sentence start. All valid alignment paths go from the bottom left corner to the top right corner of the T xU grid, hence the length of each alignment path is T + U . In an alignment path, the horizontal arrow advances one time step with a blank label by retaining the prediction network state while the vertical arrow emits a non-blank output label.</p><p>The posteriors of the alignment grid composed by the encoder and prediction networks need to be calculated at each grid point. This a three-dimensional tensor that requires much more memory than what is needed in the training of other E2E models such as CTC and AED. Eq. ( <ref type="formula" target="#formula_8">8</ref>) is calculated based on the forward-backward algorithm described in <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b69">[70]</ref>, to improve training efficiency, the forward and backward probabilities can be vectorized with a loop skewing transformation, and the recursions can be computed in a single loop instead of two nested loops. Function merging was proposed in <ref type="bibr" target="#b65">[66]</ref> to significantly reduce the training memory cost so that larger minibatches can be used to improve the training efficiency.</p><p>It is worth noting that ASR latency is a very important metric that affects user experience. If the latency is large, users will feel the ASR system is not responding. Therefore, practical systems need to have a small latency in order to give users a good experience <ref type="bibr" target="#b70">[71]</ref>. <ref type="bibr">Shangguan et al.</ref> recently gave a very good study of the factors affecting user perceived latency in <ref type="bibr" target="#b71">[72]</ref>. In <ref type="bibr" target="#b72">[73]</ref>, RNN-T is designed to generate end of sentence (EOS) token together with the ASR transcription. The latency of EOS detection is improved in <ref type="bibr" target="#b73">[74]</ref> with early and late penalties. While RNN-T is now the most natural E2E model for streaming, the vanilla RNN-T still has latency challenges because RNN-T tends to delay its label prediction until it is very confident by visiting more future frames of the current label. The green path in Figure <ref type="figure">4</ref> is an example alignment of such a delayed decision. In order to ensure low latency for RNN-T, constrained alignment <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref> was proposed to restrict the training alignment within a delay threshold of the ground truth time alignment and forbid other alignment paths. Strict alignment between the input speech sequence and output label sequence is enforced in <ref type="bibr" target="#b76">[77]</ref> to generate better alignment for streaming RNN-T. In addition to the benefit of latency, another advantage of all these alignment restricted RNN-T methods is GPU-memory saving and training speed-up because less alignment paths are used during training. FastEmit <ref type="bibr" target="#b77">[78]</ref> was proposed to reduce latency without the need of the ground truth alignment. It encourages the emission of vocabulary tokens and discourages the blank transition at all positions in the time-label grid. Therefore, it pushes all alignment paths to the left direction of the time-label grid. However, this operation is a little aggressive at reducing latency, which can result in recognition accuracy loss. In <ref type="bibr" target="#b78">[79]</ref>, a self-alignment method was proposed to reduce latency in a mild way. It uses the self alignment of the current model to find the lower latency alignment direction. In Figure <ref type="figure">4</ref>, the blue path indicates a self-alignment path and the red path is one frame left to the self-alignment path. During training, the method encourages the left-alignment path, pushing the model's alignment to the left direction. The proposed method was reported to have better accuracy and latency tradeoff than the constrained-alignment method and FastEmit.</p><p>In addition to these three popular E2E models, there are also other E2E models such as neural segmental model <ref type="bibr" target="#b79">[80]</ref> and recurrent neural aligner <ref type="bibr" target="#b80">[81]</ref>, to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ENCODER</head><p>In all E2E ASR models, the most important component is the encoder which converts speech input sequences into high-level feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) LSTM</head><p>In early E2E ASR works, LSTM is the most popular model unit. The encoder can be either a multi-layer unidirectional LSTM-RNN as Eq. ( <ref type="formula" target="#formula_9">9</ref>)</p><formula xml:id="formula_9">h l t = LSTM(x l t , h l t−1 )<label>(9)</label></formula><p>or a multi-layer bidirectional LSTM (BLSTM)-RNN as Eq. ( <ref type="formula" target="#formula_10">10</ref>)</p><formula xml:id="formula_10">h l t = [LSTM(x l t , h l t−1 ), LSTM(x l t , h l t+1 )],<label>(10)</label></formula><p>where LSTM() denotes the standard LSTM unit with an optional projection layer <ref type="bibr" target="#b81">[82]</ref>. Here, h l t is the hidden output of the l-th (l = 1...L) layer at time t and x l t is the input vector for the l-th layer with</p><formula xml:id="formula_11">x l t = h l−1 t , if l &gt; 1 x t , if l = 1 ,<label>(11)</label></formula><p>where x t is the speech input at time step t. The last layer output, h L t , is used as the encoder output. Because of the streaming request in most commercial ASR systems, unidirectional LSTM is used more widely. When the encoder is an LSTM-RNN, CTC and RNN-T work in the streaming mode by default while AED still needs streaming attention strategies in order to work in the streaming mode. In contrast, when the encoder is a BLSTM-RNN, all E2E models are non-streaming models.</p><p>There is a clear accuracy gap between the LSTM encoder and the BLSTM encoder because the latter uses the whole utterance information to generate the encoder output. In order to reduce the gap, it is a natural idea to use future context frames to generate more informative encoder output with the methods such as latency controlled BLSTM (LC-BLSTM) <ref type="bibr" target="#b82">[83]</ref> or contextual LSTM (cLSTM) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b83">84]</ref>. LC-BLSTM works in an overlapped multi-frame chunk mode. BLSTM is still used within a chunk, while the state of the forward direction is carried across chunks. In contrast, cLSTM works in a frame-by-frame uni-directional way by taking the lower layer's output from future frames as the input of the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Transformer</head><p>Although LSTM can capture short-term dependencies, Transformer is much better at capturing long-term dependencies because its attention mechanism sees all context directly. Transformer was shown to outperform LSTM <ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref> and is now replacing LSTM in all E2E models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref>. The encoder of Transformer-based E2E models is composed of a stack of Transformer blocks, where each block has a multi-head self-attention layer and a feedforward network (FFN), as shown in Figure <ref type="figure" target="#fig_3">5a</ref>. Residual connections <ref type="bibr" target="#b89">[90]</ref> and layer normalization <ref type="bibr" target="#b90">[91]</ref> are used to connect different layers and blocks. The input of a Transformer block can be linearly transformed to the query q, key k, and value v vectors with matrices W q , W k , and W v , respectively. Self-attention is used to compute the attention distribution over the input speech sequence with the dot-product similarity function as</p><formula xml:id="formula_12">α t,τ = exp(β(W q x t ) T (W k x τ )) τ exp(β(W q x t ) T (W k x τ )) = sof tmax(βq T t k τ ),<label>(12)</label></formula><p>where β = 1 √ d is a scaling factor and d is the dimension of the feature vector for each head. Then, the attention weights are used to combine the value vectors to generate the layer </p><formula xml:id="formula_13">z t = τ α tτ W v x τ = τ α tτ v τ .<label>(13)</label></formula><p>Multi-head self-attention (MHSA) is used to further improve the model capacity by applying multiple parallel self-attentions on the input sequence and the outputs of each attention module are then concatenated. By replacing the LSTM modules with Transformer in the encoder, the RNN-T model becomes Transformer Transducer <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref><ref type="bibr" target="#b93">[94]</ref> which has better accuracy than RNN-T due to the modeling power of Transformer. While Transformer is good at capturing global context, it is less effective in extracting local patterns. To further improve the modeling capability, convolutional neural network (CNN) <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref> which works on local information is combined with Transformer as Conformer <ref type="bibr" target="#b96">[97]</ref> which is plotted in Figure <ref type="figure" target="#fig_3">5b</ref>. The conformer block contains two half-step FFNs sandwiching the MHSA and convolution modules. Although having higher model complexity than Transformer, Conformer has been reported to outperform Transformer in various tasks <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98]</ref>.</p><p>Again, the self-attention on the full speech sequence cannot be used for streaming ASR. In <ref type="bibr" target="#b92">[93]</ref>, a masking strategy was proposed to flexibly control how attention is performed. A binary attention mask m t,τ is introduced to the self-attention in Eq. ( <ref type="formula" target="#formula_12">12</ref>) as</p><formula xml:id="formula_14">α t,τ = m t,τ exp(β(W q x t ) T (W k x τ )) τ m t,τ exp(β(W q x t ) T (W k x τ )) = sof tmax(βq T t k τ , m t,τ ).<label>(14)</label></formula><p>Figure <ref type="figure">6</ref> shows several examples of how the binary mask is used for different computational costs and latency configurations when predicting the output for x 10 . The full attention case is plotted in Figure <ref type="figure">6a</ref>, where the reception field is the full sequence, and every element of the JINYU LI mask matrix is 1. For the strict streaming setup without any latency, the attention should not be performed on any future frame, as shown in Figure <ref type="figure">6b</ref>. The major drawback of this attention mechanism is that the memory and runtime cost increases linearly as the utterance history grows. The context operation <ref type="bibr" target="#b88">[89]</ref> was proposed to address the issue by working on a limited history at every layer. Shown in 6c, because of context expansion, the model still can access a very long history. If the attention operates on F history frames at every layer and the model has L Transformer layers, then the model can access F xL history frames. In order to improve modeling accuracy, it is better to also include some future frames into the prediction. In <ref type="bibr" target="#b88">[89]</ref>, the same context expansion strategy is applied to not only the history but also the future frames. However, because of context expansion, it needs a large amount of future frames when there are many Transformer layers, introducing significant latency. A better future access strategy was proposed in <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b98">99]</ref> where the input speech sequence is segmented into fixed-size chunks. The frames in the same chunk can see each other and each frame can see fixed numbers of the left chunks so that the left reception field propagates. Plotted in Figure <ref type="figure">6d</ref>, this strategy helps to build high accuracy models with a very small latency. In addition to these methods, there are lots of variations <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref> of localizing full self-attention in order to have a streaming Transformer encoder.</p><p>Another popular method is Gaussian soft-mask <ref type="bibr" target="#b102">[103]</ref> which changes the softmax calculation in self-attention in Eq. ( <ref type="formula" target="#formula_12">12</ref>) as</p><formula xml:id="formula_15">α t,τ = sof tmax(βq T t k τ + M t,τ ),<label>(15)</label></formula><p>where M t,τ is a mask. It can be either a hard mask which sets all weights outside the band b to 0 as</p><formula xml:id="formula_16">M t,τ = 0, if |t − τ | &lt; b 2 −∞, otherwise<label>(16)</label></formula><p>or a soft mask as</p><formula xml:id="formula_17">M t,τ = −(t − τ ) 2 2σ , (<label>17</label></formula><formula xml:id="formula_18">)</formula><p>where σ is a trainable parameter. While both masks can enforce the locality which is good for the speech signal, the soft-mask method cannot reduce computational cost and latency because it sill attends the full sequence, although the weights of far-away time steps are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OTHER TRAINING CRITERION</head><p>In addition to the standard training loss −lnP (y|x) for E2E models, there are other losses used in the popular teacher-student learning and minimum word error rate training. In every sub figure, the left side is the reception field, and the right side is the attention mask matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Teacher-Student Learning</head><p>The concept of teacher-student (T/S) learning was originally introduced in <ref type="bibr" target="#b103">[104]</ref> but became popular from the deep learning era. The most popular T/S learning strategy is to minimize the KL divergence between the output distributions of the teacher and student networks, first proposed in 2014 by Li et al. <ref type="bibr" target="#b104">[105]</ref>. Later, Hinton et al. <ref type="bibr" target="#b105">[106]</ref> branded it with a new name, knowledge distillation, by introducing a temperature parameter (like chemical distillation) to scale the posteriors.</p><p>In the context of E2E modeling, the token-level loss function of T/S learning is</p><formula xml:id="formula_19">− u k P T (k | y 1:u−1 , x) log P S (k | y 1:u−1 , x),<label>(18)</label></formula><p>where P T and P S denote the posterior distributions of the teacher and student networks, respectively. The most popular usage of T/S learning in E2E modeling is to learn a small student E2E model from a large teacher E2E model <ref type="bibr" target="#b106">[107]</ref><ref type="bibr" target="#b107">[108]</ref><ref type="bibr" target="#b108">[109]</ref><ref type="bibr" target="#b109">[110]</ref>.</p><p>Another usage is to learn a streaming E2E model from a non-streaming E2E model <ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref><ref type="bibr" target="#b114">[115]</ref>. It is a little tricky here because the streaming E2E models usually generate delayed decisions. For example, the streaming CTC and non-streaming CTC have different output spike patterns. Therefore, delicate work has been done in order to make the non-streaming E2E models generate friendly output spikes that can be learned by the streaming E2E models <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b113">114]</ref>. Another way to circumvent the spike mismatch is to train the streaming E2E model by using the ASR hypotheses generated from decoding the unlabeled data with a non-streaming E2E model <ref type="bibr" target="#b115">[116]</ref>.</p><p>T/S learning can also be used to adapt an E2E model from a source environment to a target environment if the target-domain acoustic data x can be paired with the source-domain acoustic data x either by recording at the same time or from simulation <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b117">118]</ref>. The loss function in Eq. ( <ref type="formula" target="#formula_19">18</ref>) is re-written as</p><formula xml:id="formula_20">− u k P T (k | y 1:u−1 , x) log P S (k | y 1:u−1 , x).</formula><p>(19) While standard T/S learning follows a two-step process in which we first train a teacher model and then use the T/S criterion to train a student model, there are recent studies <ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr" target="#b121">[122]</ref> that train the teacher and student models at the same time. Such a co-learning strategy not only simplifies the training process but also boosts the accuracy of student models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Minimum Word Error Rate Training</head><p>The minimum word error rate (MWER) training criterion tries to mitigate the discrepancy between the training criterion and the evaluation metric of a speech recognition model. Instead of minimizing the negative log loss of the probability P (y|x), MWER minimizes the expected word errors <ref type="bibr" target="#b122">[123]</ref> as</p><formula xml:id="formula_21">L MWER = hi P (h i | x)R(h i , h r ),<label>(20)</label></formula><p>where P (h i | x) denotes the posterior probability of an hypothesis h i , R(•) denotes the risk function which measures the edit-distance between the hypothesis h i and the reference transcription h r at word-level. While the exact posterior probability is computationally intractable, in practice, an N-best list of hypotheses from beam search decoding are used to compute the empirical posterior probability <ref type="bibr" target="#b123">[124]</ref> as</p><formula xml:id="formula_22">P (h i | x) = P (h i | x) hi P (h i |x) ,<label>(21)</label></formula><p>where P (h i | x) is the probability of an hypothesis h i , computed in Eq. ( <ref type="formula" target="#formula_3">4</ref>) for AED models or Eq. ( <ref type="formula" target="#formula_8">8</ref>) for RNN-T models.</p><p>MWER has been shown to improve the accuracy of AED models <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b124">125]</ref>, RNN-T models <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b126">127]</ref>, and hybrid autoregressive transducer (HAT) models <ref type="bibr" target="#b127">[128]</ref>. However, the gain is not as significant as what has been reported in hybrid models. One possible reason is that the MWER training of hybrid models follows the crossentropy training which is frame-based. In contrast, E2E models have already been optimized with sequence-level training, therefore further sequence-level MWER training gives less gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MULTILINGUAL MODELING</head><p>Building a multilingual ASR system with traditional hybrid models is difficult because every language usually has its own language-specific phonemes and word inventories. Most hybrid multilingual works focus on building an acoustic model with shared hidden layers <ref type="bibr" target="#b128">[129]</ref><ref type="bibr" target="#b129">[130]</ref><ref type="bibr" target="#b130">[131]</ref>, while every language has its own lexicon model and language model. In contrast, it is very easy to build a multilingual E2E ASR system by simply taking a union of the token (e.g., character, subword, or even byte) sets of all languages as the output token set and then training an E2E model with all the data <ref type="bibr" target="#b131">[132]</ref><ref type="bibr" target="#b132">[133]</ref><ref type="bibr" target="#b133">[134]</ref><ref type="bibr" target="#b134">[135]</ref><ref type="bibr" target="#b135">[136]</ref>. Such an E2E model is a universal ASR model which is capable of recognizing the speech from any language as long as that language has been used during training. However, pooling all languages together to train a multilingual model is a double-edged sword. Although it is simple and maximizes the sharing across languages, it also brings confusion between languages during recognition.</p><p>If the multilingual model can condition on the language identity (LID), significant improvement can be obtained over the universal multilingual model without LID because the one-hot LID guides the ASR model to generate the transcription of the target language by reducing the confusion from other languages <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b136">[137]</ref><ref type="bibr" target="#b137">[138]</ref><ref type="bibr" target="#b138">[139]</ref><ref type="bibr" target="#b139">[140]</ref><ref type="bibr" target="#b140">[141]</ref>. However, such a multilingual E2E model with LID relies on the prior knowledge of which language the user will speak for every utterance, working more like a monolingual model. To remove the dependency on knowing one-hot LID in advance, one way is to estimate the LID and use it as the additional input to E2E multilingual models. However, the gain is very limited, especially for streaming E2E models, because the estimation is not very reliable <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b142">143]</ref>.</p><p>Because there are more multilingual users than monolingual users <ref type="bibr" target="#b143">[144]</ref>, there is an increasing demand of building multilingual systems which can recognize multiple languages without knowing in advance which language each individual utterance is from. One solution is to build a corresponding multilingual E2E model for any set of language combinations. However, this is impractical because the number of possible language combinations is huge given so many languages in the world. Zhou et al. proposed a configurable multilingual model (CMM) <ref type="bibr" target="#b144">[145]</ref> which is trained only once by pooling the data from all languages, but can be configured to recognize speeches from any combination of languages. As shown in Figure <ref type="figure" target="#fig_5">7</ref>, the hidden output of CMM is calculated as the weighted sum of the output from a universal module and the outputs from all language-specific modules. At runtime, the universal module together with corresponding language-specific modules are activated based on the user choice. Because multilingual users usually speak a few languages, CMM reduces the language confusion space from dozens to a few. Therefore, it performs much better than the multilingual model without LID, and can serve the multilingual users well.</p><p>There are several factors to be considered when designing multilingual E2E models. When scaling the multilingual model to cover a large amount of languages (e.g., 50 languages in <ref type="bibr" target="#b139">[140]</ref>), the data from those languages is heavily imbalanced, which usually leads to a model performing well on resource-rich languages while failing on low-resource languages. To solve that issue, data sampling is usually used to balance the training data amount <ref type="bibr" target="#b137">[138,</ref><ref type="bibr" target="#b139">140]</ref>. The model capacity should also be expanded to recognize a large amount of languages. In <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b145">146]</ref>, multilingual models are scaled to billions of parameters.</p><p>Because there are differences across languages, it may not be optimal if a simple E2E structure is used to blindly model all the multilingual data. In <ref type="bibr" target="#b137">[138]</ref>, language-specific adapt layers are used to further boost the multilingual model performance. Similar ideas are used with the mixture of expert structure for multilingual models <ref type="bibr" target="#b146">[147,</ref><ref type="bibr" target="#b147">148]</ref>.</p><p>While E2E models have achieved remarkable success in multilingual modeling, an even more challenging topic is the ASR of code-switching (CS) utterances which contain mixed words or phrases from multiple distinct languages. There are two categories of CS: one is intra-sentential CS where switches happen within an utterance and the other is inter-sentential CS where switches occur between utterances with monolingual words inside each utterance. The former one is more difficult, attracting most studies. In <ref type="bibr" target="#b148">[149]</ref>, synthetic utterance-level CS text was generated to improve the AED model's ASR accuracy on intersentential CS utterances. The challenge of intra-sentential CS to E2E models was addressed in <ref type="bibr" target="#b149">[150]</ref> by using the LID model to linearly adjust the posterior of a CTC model, followed by lots of recent studies (e.g., <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b150">[151]</ref><ref type="bibr" target="#b151">[152]</ref><ref type="bibr" target="#b152">[153]</ref><ref type="bibr" target="#b153">[154]</ref><ref type="bibr" target="#b154">[155]</ref><ref type="bibr" target="#b155">[156]</ref><ref type="bibr" target="#b156">[157]</ref>). One major challenge to CS ASR is the lack of the CS data while E2E models are more data hungry. Therefore, methods have been studied to effectively leverage monolingual data. A constrained output embedding method <ref type="bibr" target="#b150">[151]</ref> was proposed to encourage the output embedding of monolingual languages similar in order to let the ASR model easily switch between languages. In <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b152">153]</ref>, a bi-encoder which has a separate encoder for each language is used to train the model from monolingual data. This was further extended with both language-specific encoder and language-specific decoder to recognize CS speech <ref type="bibr" target="#b157">[158]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ADAPTATION</head><p>The performance of ASR systems can degrade significantly when the test conditions differ from training. Adaptation algorithms are designed to address such challenging issues. A comprehensive overview of adaptation technologies in ASR can be found in <ref type="bibr" target="#b158">[159]</ref>. In this section, we highlight some adaptation technologies for E2E models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Speaker Adaptation</head><p>Speaker adaptation adapts ASR models to better recognize a target speaker's speech. It is a common practice to adapt the acoustic encoder of CTC <ref type="bibr" target="#b159">[160,</ref><ref type="bibr" target="#b160">161]</ref>, AED <ref type="bibr" target="#b161">[162,</ref><ref type="bibr" target="#b162">163]</ref>, RNN-T <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b164">165]</ref> and Conformer Transducer <ref type="bibr" target="#b165">[166]</ref>. Another popular methodology is to augment the input speech with speaker embeddings <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b166">167,</ref><ref type="bibr" target="#b167">168]</ref>.</p><p>The biggest challenge of speaker adaptation is that the adaptation data amount from the target speaker is usually very small. There are several ways to address such a challenge. The first one is using regularization techniques such as Kullback-Leibler (KL) divergence regularization <ref type="bibr" target="#b168">[169]</ref>, maximum a posteriori adaptation <ref type="bibr" target="#b169">[170]</ref>, or elastic weight consolidation <ref type="bibr" target="#b170">[171]</ref>, to ensure the adapted models do not overfit the limited adaptation data <ref type="bibr" target="#b159">[160,</ref><ref type="bibr" target="#b161">[162]</ref><ref type="bibr" target="#b162">[163]</ref><ref type="bibr" target="#b163">[164]</ref>. The second approach is multi-task learning. E2E models typically use subword units as the output target in order to achieve high recognition accuracy. Sometimes, the number of subword units is at the scale of thousands or even more. These units usually cannot be fully observed in the limited speaker adaptation data. In contrast, the small number of character units usually can be fully covered. Therefore, multi-task learning using both character and subword units can significantly alleviate such sparseness issues <ref type="bibr" target="#b159">[160,</ref><ref type="bibr" target="#b162">163]</ref>. The third approach is to use multispeaker text-to-speech (TTS) to expand the adaption set for the target speaker <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b164">165]</ref>. Because the transcription used to generate the synthesized speech is also used for model adaptation, this approach also alleviates the hypothesis error issue in unsupervised adaptation when combining the synthesized speech with the original speech from the target speaker.</p><p>It is desirable to use E2E models on devices because of the compact model size. There are interesting works <ref type="bibr" target="#b171">[172,</ref><ref type="bibr" target="#b172">173]</ref> which study how to do secure adaptation on devices with continuous learning so that the user data never leaves devices. Several engineering practices were described in order to deal with the limited memory and computation power on devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Domain Adaptation</head><p>Domain adaptation is the task of adapting ASR models to the target domain which has content mismatch from the source domain in which the ASR models were trained. Because E2E models tend to memorize the training data well, their performance usually degrades a lot in a new domain. Such a challenge attracts much more works on domain adaptation than speaker adaptation. This is not a severe issue for hybrid models because their language model (LM) is trained with a much larger amount of text data than that is used in the paired speech-text setup for E2E model training. The big challenge of adapting E2E models to a new domain is that it is not easy to get enough paired speech-text data in the new domain, which requires transcribing the new-domain speech. However, it is much easier to get text data in the new domain. Therefore, the mainstream of domain adaptation methods for E2E models focuses on the new-domain text only.</p><p>A widely adopted approach to adapt E2E models to a new domain is fusing E2E models with an external LM trained with the new-domain text data. There are several LM fusion methods, such as shallow fusion <ref type="bibr" target="#b173">[174]</ref>, deep fusion <ref type="bibr" target="#b173">[174]</ref>, and cold fusion <ref type="bibr" target="#b174">[175]</ref>. Among them, the simplest and most popular method is shallow fusion in which the external LM is log-linearly interpolated with the E2E model at inference time <ref type="bibr" target="#b175">[176]</ref>.</p><p>However, shallow fusion does not have a clear probabilistic interpretation. As an improvement, a maximum a posteriori based decoding method <ref type="bibr" target="#b176">[177,</ref><ref type="bibr" target="#b177">178]</ref> was proposed for the integration of an external LM for CTC. A density ratio approach based on Bayes' rule was proposed in <ref type="bibr" target="#b178">[179]</ref> for RNN-T. During inference, the output of the E2E model is modified by the ratio of the external LM trained with the new-domain text data and the source-domain LM trained with the transcript of the training set which has the paired speech-text data for E2E model training. Another similar model is the hybrid autoregressive transducer (HAT) model <ref type="bibr" target="#b179">[180]</ref> designed to improve the RNN-T model. In the HAT model, the label distribution is derived by normalizing the score functions across all labels excluding blank. Hence, it is mathematically justified to integrate the HAT model with an external LM using the density ratio method.</p><p>In <ref type="bibr" target="#b180">[181]</ref>, an internal LM estimation (ILME)-based fusion was proposed to enable a more effective LM integration. During inference, the internal LM score of an E2E model is estimated by zeroing out the contribution of the encoder and is subtracted from the log-linear interpolation between E2E and external LM scores. An internal LM training (ILMT) method <ref type="bibr" target="#b181">[182]</ref> was further proposed to minimize an additional internal LM loss by updating only the components that estimate the internal LM. ILMT increases the modularity of the E2E model and alleviates the mismatch between training and ILME-based fusion.</p><p>Tuning the LM weights on multiple development sets is computationally expensive and time-consuming. To eliminate the weights tuning, the MWER training with LM fusion was proposed in <ref type="bibr" target="#b182">[183,</ref><ref type="bibr" target="#b183">184]</ref> where the LM fusion is performed during MWER training. During inference, LM weights pre-set in training enables a robust LM fusion on test sets from different domains.</p><p>Because LM fusion methods require interpolating with an external LM, both the computational cost and footprint are increased, which may not be applicable to ASR on devices. With the advance of TTS technologies, a new trend is to adapt E2E models with the synthesized speech generated from the new-domain text <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b185">186]</ref>. This is especially useful for adapting RNN-T, in which the prediction network works similarly to an LM. It was shown that such domain adaptation method with TTS is more effective than LM fusion methods <ref type="bibr" target="#b11">[12]</ref>.</p><p>The TTS-based adaptation method also has its drawbacks. The first is that TTS speech is different from the real speech. It sometimes also degrades the recognition accuracy on real speech <ref type="bibr" target="#b186">[187]</ref>. This issue was alleviated in <ref type="bibr" target="#b187">[188]</ref> by inserting a mapping network before the encoder network when the input is TTS audio in both the training and adaption stage. This mapping network works as feature space transformation to map the space of TTS audio to the space of real speech. During testing with real speech, the mapping network was removed. The second is that the speaker variation in the TTS data is far less than that in the largescale ASR training data. The third is the cost of training a multi-speaker TTS model and the generation of synthesized speech from the model is large. These issues were solved by a spliced data method <ref type="bibr" target="#b188">[189]</ref> which was used to adapt E2E models with better performance than the TTSbased adaptation method. For any text sentence in the new domain, the method randomly extracts the corresponding audio segments from the source training data, and then concatenates them to form new utterances. Figure <ref type="figure">8</ref> gives an example of how this spliced data method works. Similar ideas have been used for data augmentation by replacing some word segments in an utterance with new word segments from another utterance to train a general E2E model <ref type="bibr" target="#b189">[190,</ref><ref type="bibr" target="#b190">191]</ref>.</p><p>Both the TTS-based adaptation and the spliced data method need to synthesize audios from text, and then update E2E models. Recently, a fast text adaptation method <ref type="bibr" target="#b191">[192]</ref> was proposed to adapt the prediction network of RNN-T by treating it as an LM and then using the text from the new domain to update it. In <ref type="bibr" target="#b192">[193]</ref>, ILMT <ref type="bibr" target="#b181">[182]</ref> was used to ensure that the internal LM inside RNN-T behaves similarly to a standalone neural LM, and then it can be adapted with text only data. However, as shown in <ref type="bibr" target="#b193">[194]</ref>, the prediction network in RNN-T does not fully function as an LM. Chen et al. showed this is because the prediction network needs to predict both normal tokens and blank, and therefore proposed a factorized neural transducer which separates the prediction of normal tokens and blank <ref type="bibr" target="#b194">[195]</ref>. As shown in Figure <ref type="figure">9</ref>, the modules in the dashed box function exactly as an LM, predicting the probability P (y u |y 1:u−1 ). Therefore, it can be updated with the text from the new domain by using any well-established neural LM adaptation methods. Finally, the adapted factorized neural transducer with the branch output P (y u |x 1:t , y 1:u−1 ) is used to recognize the speeches from the new domain with large accuracy improvement <ref type="bibr" target="#b194">[195]</ref>.</p><p>While these domain adaptation technologies are powerful, it is desirable that the adaptation of E2E models to the new domain does not degrade the models' performance on the source domain. Therefore, it is worth looking at the concept of lifelong learning which enables E2E models to learn new tasks without forgetting the previously learned knowledge <ref type="bibr" target="#b195">[196]</ref>. If the source-domain data is available, it can be mixed with the new-domain data for adaptation <ref type="bibr" target="#b188">[189]</ref>. Otherwise, the popular solution is to use regularization that prevents the adapted model from moving too far away from the source model <ref type="bibr" target="#b192">[193]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C) Customization</head><p>Different from speaker adaptation which adjusts a speaker independent E2E model toward the speaker audio and domain adaptation which lets an E2E model recognize the content of a new domain better, customization here specifically refers to the technologies that leverage context such as contacts, location, music playlist etc., of a specific user to significantly boost the ASR accuracy for this user. For example, an English ASR system usually cannot recognize the contact names of a Chinese person well. However, if the English ASR system is presented with the contact list of this Chinese person, the ASR output can be biased toward the contact names. Such biasing is even more effective when designed with context activation phrases such as "call", "email", "text", etc.</p><p>While it is relatively easier for hybrid systems to do so with the on-the-fly rescoring strategy which dynamically adjusts the LM weights of a small number of n-grams which are relevant to the particular recognition context, it is more challenging to E2E systems. One solution is to add a context bias encoder in addition to the original audio encoder into the E2E model, which was first proposed in <ref type="bibr" target="#b196">[197]</ref> to bias a keyword spotting E2E model toward a specific keyword. The idea was then extended into a contextual LAS (CLAS) ASR model <ref type="bibr" target="#b197">[198]</ref>, plotted in figure <ref type="figure" target="#fig_0">10</ref>. LAS (Listen, Attend and Spell) <ref type="bibr" target="#b6">[7]</ref> is one particular instantiation of AED. Compared to the standard AED model in figure <ref type="figure" target="#fig_0">1b</ref>, CLAS adds the context encoder in the bottom right of the figure which takes the context list z 1:N as the input, where z i denotes the i-th phrase. An attention module is used to generate the contextual vector c z u as one of the inputs to the decoder. In this way, the network output conditions not only on the speech signal and previous label sequence but also on the contextual phrase list. It was further extended into a two-step memory enhanced model in <ref type="bibr" target="#b198">[199]</ref>. It is common that a contextual phrase list contains rare words, especially names which are not observed during training. In such a case, E2E models have not learned to map the rare words' acoustic signal to words. This issue can be alleviated by further adding a phoneme encoder in addition to the text encoder for the contextual phrase list <ref type="bibr" target="#b199">[200,</ref><ref type="bibr" target="#b200">201]</ref>. The same idea has also been applied to RNN-T <ref type="bibr" target="#b201">[202]</ref>.</p><p>However, as shown in <ref type="bibr" target="#b197">[198]</ref>, it becomes challenging for the bias attention module to focus if the biasing list is too large (e.g., more than 1000 phrases). Therefore, a more popular way to handle a large contextual phrase list is shallow fusion with the contextual biasing LM <ref type="bibr" target="#b202">[203,</ref><ref type="bibr" target="#b203">204]</ref>. A challenge for the fusion-based biasing method is that it usually benefits from prefix which may not be available all Fig. <ref type="figure" target="#fig_0">10</ref>.: Flowchart of Contextual LAS <ref type="bibr" target="#b197">[198]</ref> the time. In <ref type="bibr" target="#b204">[205]</ref>, class tags are inserted into word transcription during training to enable context aware training. During inference, class tags are used to construct contextual bias finite-state transducer. In <ref type="bibr" target="#b205">[206]</ref>, trie-based deep biasing, WFST shallow fusion and neural network LM contextualization are combined together to reach good biasing performance for tasks with and without prefix. As discussed in Section VI.B, density ratio and HAT perform better than shallow fusion when integrating an external LM. Contextual biasing with density ratio and HAT were also shown to have better biasing performance <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b206">207]</ref>.</p><p>The most challenging part of context biasing is how to deal with a very large bias list which contains more than 1000 phrases. Wang et al. <ref type="bibr" target="#b207">[208]</ref> provided a nice solution by extending spelling correction <ref type="bibr" target="#b208">[209]</ref> to contextual spelling correction (CSC), which is shown in Figure <ref type="figure" target="#fig_0">11</ref>. Both the embeddings of the ASR hypothesis and the contextual phrase list are used as the input to the decoder, which generates a new word sequence. A filtering mechanism based on the distance between ASR hypothesis and contextual phrases is used to trim the very large phrase list to a relatively small one so that the attention can perform well. Such filtering is the key to the success which cannot be done inside ASR models such as CLAS because the filtering relies on the ASR decoding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ADVANCED MODELS</head><p>In this section, we discuss several advanced models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Non-Autoregressive Models</head><p>While most E2E models use autoregressive (AR) modeling to predict target tokens in a left-to-right manner as Eqs. ( <ref type="formula" target="#formula_3">4</ref>), there is a recent trend of using non-autoregressive (NAR) modeling which generates all target tokens simultaneously with one-shot or iteratively without replying on predicted tokens in early steps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b209">[210]</ref><ref type="bibr" target="#b210">[211]</ref><ref type="bibr" target="#b211">[212]</ref><ref type="bibr" target="#b212">[213]</ref><ref type="bibr" target="#b213">[214]</ref><ref type="bibr" target="#b214">[215]</ref>. These NAR methods root from the assumption that the feature sequence generated by the acoustic encoder contains not only acoustic Fig. <ref type="figure" target="#fig_0">11</ref>.: Flowchart of Context Spelling Correction information but also some language semantic information. However, such an assumption is not very strong, resulting in worse performance of NAR modeling compared to AR modeling in general. The biggest advantage of NAR models is that its decoding speed is much faster than that of AR models because there is no dependency on previous tokens. All target tokens can be predicted in parallel while the decoding of AR models is more complicated because of the token dependency.</p><p>A typical way to generate all target tokens is described in <ref type="bibr" target="#b209">[210]</ref>. The target token sequence length L is predicted or set as a constant value. Then the NAR model assumes that each token is independent of each other as</p><formula xml:id="formula_23">P (y|x) = L u=1 P (y u |x). (<label>22</label></formula><formula xml:id="formula_24">)</formula><p>At decoding time, the predicted token at each position is the one with the highest probability. The independence assumption in Eq. ( <ref type="formula" target="#formula_23">22</ref>) is very strong. The token sequence L is also hard to predict usually. Mask CTC <ref type="bibr" target="#b26">[27]</ref> was proposed to solve these issues. It predicts a set of masked tokens y mask , conditioning on the observed tokens y obs = y \ y mask and the input speech sequence x as P (y mask |y obs , x) = y∈y mask P (y|y obs , x).</p><p>(</p><formula xml:id="formula_25">)<label>23</label></formula><p>Figure <ref type="figure" target="#fig_1">12</ref> shows an example of how Mask CTC works. The target sequence was first initialized with the CTC outputs.</p><p>Then tokens with low confidence scores are masked and are iteratively optimized conditioning on the unmasked tokens and input speech sequence. The length of the final token sequence generated by Mask CTC is the same as the length of the CTC initial output, therefore Mask CTC can only deal with the substitution error. In <ref type="bibr" target="#b214">[215]</ref>, the length of a Fig. <ref type="figure" target="#fig_1">12</ref>.: An example of Mask CTC <ref type="bibr" target="#b26">[27]</ref>. The CTC model generates a token sequence "C A T" in which "A" has a low confidence score. Then "A" is masked. The speech sequence and unmasked tokens are used to predict the token "U".</p><p>partial target sequence is predicted to enable Mask CTC's ability of handling deletion and insertion errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Unified Models</head><p>The most significant component in E2E models is the encoder, which is well studied in literature. There are lots of encoder designs depending on the requirements of streaming, latency, and computational cost in different target scenarios. The development cost is formidable if we train a separate E2E model for each application scenario. Therefore, an emerging trend is to train a single unified model which can be configured with multiple streaming, latency, and computational cost setups during decoding. In <ref type="bibr" target="#b119">[120]</ref>, a dual E2E model was proposed with shared weights for both streaming and non-streaming ASR. In order to have a dual-mode Conformer encoder, the authors designed dual-mode modules for the components such as convolution, pooling, and attention layers inside Conformer. An in-place T/S learning was used to train the student streaming mode from the full-context non-streaming mode. Such training even brought accuracy and latency benefits to the streaming ASR. Another model unifying streaming and non-streaming modes was proposed in <ref type="bibr" target="#b215">[216]</ref> which decomposes the Transformer's softmax attention into left-only causal attention and right-only anti-causal attention. For the streaming scenario, it just uses the causal attention while the non-streaming mode uses both attentions.</p><p>There are several studies working on a unified model with dynamic computational cost during inference. In <ref type="bibr" target="#b118">[119]</ref>, a dynamic sparsity network is used as the encoder of RNN-T. It only needs to be trained once and then can be set with any predefined sparsity configuration at runtime The training and decoding of dynamic encoder <ref type="bibr" target="#b216">[217]</ref> in order to meet various computational cost requirements on different devices. An even more direct way to have a dynamic encoder was proposed in <ref type="bibr" target="#b216">[217]</ref>. During training, layer dropout is applied to randomly drop out encoder layer as shown in Figure <ref type="figure" target="#fig_2">13a</ref>. The pruned model can be used to decode full utterances for constant low computational cost as shown in Figure <ref type="figure" target="#fig_2">13b</ref>. An example of advanced usage of the dynamic encoder is plotted in Figure <ref type="figure" target="#fig_2">13c</ref>, where the dynamic encoder is configured with a small number of layers in the beginning of the utterance, and then configured with full layers in the remaining. The idea of having different computational costs within an utterance was also studied in <ref type="bibr" target="#b217">[218]</ref> for RNN-T. The model has a fast encoder and a slow encoder. An arbitrator is used to select which encoder should be used given an input speech frame.</p><p>It requires a small latency for applications such as voice search and command control, while the applications such as dictation and video transcription usually can afford a larger latency. Variable context training <ref type="bibr" target="#b218">[219]</ref><ref type="bibr" target="#b219">[220]</ref><ref type="bibr" target="#b220">[221]</ref> was proposed to build a unified model which can be configured for different latency requirements at runtime. During training, the Transformer encoder is provided with different right context lengths which correspond to different theoretic latency. In <ref type="bibr" target="#b219">[220]</ref>, the alignment constraint method <ref type="bibr" target="#b75">[76]</ref> was also used to flexibly set latency thresholds for different tasks. A task ID is used at runtime to configure the Transducer model with different encoder segments and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C) Two-pass Models</head><p>Although a single E2E model can already achieve very good ASR performance, its performance can be further improved with a second-pass model. Spelling correction methods <ref type="bibr" target="#b208">[209,</ref><ref type="bibr" target="#b221">222]</ref> were proposed by using TTS data to train a separate translation model which is used to correct the hypothesis errors made by the first-pass E2E model. The spelling correction model is a pure text-to-text model without using the speech input. As E2E models need to be trained with paired speech-text data, the language modeling power is always a concern of E2E models. Wang et al. proposed a two-pass RNN-T model in which the firstpass RNN-T transcribes speech into syllables while the Fig. <ref type="figure" target="#fig_0">14</ref>.: An RNN-T model with a cascaded encoder. second-pass RNN-T converts syllable sequences into character sequences <ref type="bibr" target="#b222">[223]</ref>. Because the second-pass RNN-T uses text instead of speech as input, therefore it can leverage a much larger amount of text data to build a more powerful capability for language modeling.</p><p>The second-pass processing with decoding hypothesis only cannot leverage the speech input. In <ref type="bibr" target="#b223">[224]</ref>, a twopass model was proposed to use an AED decoder to attend the encoder output of a streaming RNN-T model. In this way, the first-pass RNN-T model provides immediate recognition results while the second-pass AED model can provide better accuracy with small additional perceived latency. This work was extended as the deliberation model which uses an LSTM AED decoder <ref type="bibr" target="#b224">[225]</ref> or a Transformer AED decoder <ref type="bibr" target="#b225">[226]</ref> to attend both the encoder output and first-pass decoding hypothesis.</p><p>However, AED models cannot perform well on long utterances <ref type="bibr" target="#b53">[54]</ref>. Therefore, an RNN-T model with the cascaded encoder was proposed <ref type="bibr" target="#b226">[227]</ref> as shown in Figure <ref type="figure" target="#fig_0">14</ref>. Such a model can also be considered as a unified model which provides both streaming and non-streaming solutions, and is a special case of Y-model <ref type="bibr" target="#b218">[219]</ref> which has both streaming and non-streaming branches in a Y-shape. The causal encoder output h c t is fed into a non-causal encoder to generate h nc t . Depending on applications, a switch is used to let h c t or h nc t go to the joint network. In the context of two-pass modeling, the causal output h c t is used in the first pass and the non-causal output h nc t is used in the second pass. Such a cascaded model is trained in one stage, while the first-pass and second-pass models in deliberation are trained in two stages. Another advantage of the RNN-T model with a cascaded encoder is its better performance on long utterances because the RNN-T decoder better handles long utterances than the AED decoder.</p><p>Note that the second-pass model brings additional latency and computational costs to ASR systems, therefore, careful designs have been studied to hide these costs in commercial systems <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b227">228,</ref><ref type="bibr" target="#b228">229]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D) Multi-talker Models</head><p>While ASR systems have achieved very high recognition accuracy in most single-speaker applications, it is still very difficult to achieve satisfactory recognition accuracy in scenarios with multiple speakers talking at the same time. A common practice in the industry is to separate the overlapped speech first and then use an ASR model to recognize the separated speech streams <ref type="bibr" target="#b229">[230]</ref>. The biggest challenge to multi-talker overlapping ASR is the permutation problem which occurs when the mixing sources are symmetric and the model cannot predetermine the target signal for its outputs. Deep clustering <ref type="bibr" target="#b230">[231]</ref> and permutation invariant training (PIT) <ref type="bibr" target="#b231">[232]</ref> were proposed to address such a challenge to separate overlapping speech signals. Specifically, PIT is simpler to implement and easier to be integrated with other methods. Therefore, it becomes the most popular speech separation method. Instead of the two-stage processing, Yu et al. <ref type="bibr" target="#b232">[233]</ref> proposed to directly optimize the ASR criterion with a single model using PIT without having an explicit speech separation step.</p><p>The first E2E model for overlapping speech was proposed in <ref type="bibr" target="#b233">[234]</ref> with PIT loss in the label level without using the source signal from each speaker. It was further extended in <ref type="bibr" target="#b234">[235]</ref> without pretraining, in <ref type="bibr" target="#b235">[236]</ref> for multichannel input and multi-channel output, and in <ref type="bibr" target="#b236">[237]</ref> with Transformer. Figure <ref type="figure" target="#fig_3">15a</ref> shows the network architecture of these methods. The overlapping speech goes through a mixture encoder for separation followed by two branches which generate the recognition results from two speakers. Most modules are shared between two branches except the attention module and speaker encoder. For the overlapping speech with S speakers, the training loss is calculated by considering all possible speaker permutation Φ(1, ..., S) as</p><formula xml:id="formula_26">L P IT = min φ∈Φ(1,...,S) S s=1 CE(y s , r φ[s] ),<label>(24)</label></formula><p>where CE() is the cross entropy function with the s-th output y s and the permuted reference r φ [s] . There are three theoretical limitations in the models with PIT-ASR loss in Eq <ref type="bibr" target="#b23">(24)</ref>. First, the number of output branches is the same as S, the number of the speakers. After the model is trained, it cannot handle the overlapping speech with more speakers. Second, the training cost is O(S 3 ) using the Hungarian algorithm, which prevents it from being applied to scenarios with a large number of speakers. Third, there is possible leakage with duplicated decoding hypothesis between output branches because the outputs in different branches do not have a direct dependency. To address these limitations, Kanda et al. proposed a simple but effective serialized output training (SOT) method <ref type="bibr" target="#b237">[238]</ref> which uses a single AED model to predict the merged label sequence Ψ(1, ..., S) from all speakers as in Figure <ref type="figure" target="#fig_3">15b</ref>. A sc symbol is inserted between the reference label sequences of speakers to represent speaker change. Furthermore, the reference label sequences are (</p><formula xml:id="formula_27">)<label>25</label></formula><p>Given the large demand of streaming ASR in the industry, the backbone model of multi-talker ASR is also moving from the non-streaming AED model to the streaming RNN-T model. Although RNN-T was used in <ref type="bibr" target="#b238">[239]</ref> for multitalker ASR, it's encoder is a bi-directional LSTM which is a non-streaming setting. In <ref type="bibr" target="#b239">[240,</ref><ref type="bibr" target="#b240">241]</ref>, the Streaming Unmixing and Recognition Transducer (SURT) was proposed as in Figure <ref type="figure" target="#fig_3">15c</ref>. A mask-based unmixing module is used to estimate masks in order to separate the speech input into two branches for recognition with RNN-T. Although PIT ASR loss in Eq. ( <ref type="formula" target="#formula_26">24</ref>) can be used, it will introduce large computational cost due to the label permutation. Therefore, Heuristic Error Assignment Training (HEAT) <ref type="bibr" target="#b239">[240]</ref> was proposed by ordering the label sequences based on the utterance start time into the set Ω(1, ..., S). The loss function can be written as</p><formula xml:id="formula_28">L HEAT = S s=1 CE(y s , r ω[s] ),<label>(26)</label></formula><p>where ω[s] stands for the s-th element of Ω(1, ..., S).</p><p>HEAT clearly reduces the training cost without losing accuracy <ref type="bibr" target="#b239">[240]</ref>. It is even more important to use Eq. ( <ref type="formula" target="#formula_28">26</ref>) in the continuous streaming setup where it is formidable to have all the permutations in a long conversation <ref type="bibr" target="#b241">[242]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E) Multi-channel Models</head><p>Beamforming is a standard technique for improving distant ASR system accuracy using microphone arrays with multichannel inputs <ref type="bibr" target="#b242">[243,</ref><ref type="bibr" target="#b243">244]</ref>. The most popular beamforming technology for ASR is signal processing based superdirective beamforming, while there is a trend to replace it with the neural beamforming for joint optimization with the backend hybrid models <ref type="bibr" target="#b244">[245]</ref><ref type="bibr" target="#b245">[246]</ref><ref type="bibr" target="#b246">[247]</ref><ref type="bibr" target="#b247">[248]</ref>. The joint optimization is more direct when the backend model is an E2E model <ref type="bibr" target="#b248">[249,</ref><ref type="bibr" target="#b249">250]</ref>, and can be applied with even more front end components <ref type="bibr" target="#b250">[251,</ref><ref type="bibr" target="#b251">252]</ref>. While all these methods still work on good neural beamforming modules, some recent studies try to bypass the beamforming module design by using a single E2E network to preform ASR directly on multi-channel inputs. Thanks to the power and flexibility of Transformer, the multi-channel Transformer ASR works <ref type="bibr" target="#b252">[253]</ref><ref type="bibr" target="#b253">[254]</ref><ref type="bibr" target="#b254">[255]</ref> replace the well-established beamformer with a multi-channel encoder which consumes the multi-channel inputs from microphone arrays. Figure <ref type="figure" target="#fig_0">16</ref> shows an example of a multi-channel encoder which consists of multiple blocks of cascaded within channel-wise self attention layer and cross-channel attention layer. The channel-wise self attention layer models the correlation across time within a channel while the cross-channel attention layer tries to learn the relationship across channels. The multi-channel encoder can then be plugged into any E2E model. Such Fig. <ref type="figure" target="#fig_0">16</ref>.: An example of multi-channel encoder <ref type="bibr" target="#b252">[253]</ref> E2E models do not need the conventional knowledge of beamformer design and were reported to perform better than the standard beamforming method followed by an ASR model <ref type="bibr" target="#b252">[253]</ref><ref type="bibr" target="#b253">[254]</ref><ref type="bibr" target="#b254">[255]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. MISCELLANEOUS TOPICS</head><p>E2E modeling is a huge research topic. Due to the limit of space, we can only cover the most important areas we think about from the industry point of view. There are more areas worth mentioning. First of all, great toolkits <ref type="bibr" target="#b255">[256]</ref><ref type="bibr" target="#b256">[257]</ref><ref type="bibr" target="#b257">[258]</ref><ref type="bibr" target="#b258">[259]</ref><ref type="bibr" target="#b259">[260]</ref><ref type="bibr" target="#b260">[261]</ref> are indispensable to the fast development of E2E ASR technologies. Among them, ESPnet <ref type="bibr" target="#b256">[257]</ref> is the most popular E2E toolkit widely used in the speech community although some companies have their own in-house E2E model training tools. We will continue to witness the growth of E2E toolkits which benefit the whole speech community. Model unit selection is a very important area in E2E modeling. While most units (characters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, word-piece <ref type="bibr" target="#b261">[262]</ref><ref type="bibr" target="#b262">[263]</ref><ref type="bibr" target="#b263">[264]</ref><ref type="bibr" target="#b264">[265]</ref><ref type="bibr" target="#b265">[266]</ref>, words <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b266">267]</ref>, or even phrases <ref type="bibr" target="#b267">[268]</ref>) are purely derived based on text and word-piece unit is the dominating one, there are also studies working on phoneme units <ref type="bibr" target="#b268">[269]</ref><ref type="bibr" target="#b269">[270]</ref><ref type="bibr" target="#b270">[271]</ref><ref type="bibr" target="#b271">[272]</ref>. As ASR is unique with the underlying acoustic signal, it should be better to build units bridging between the text and phoneme worlds. To that end, the pronunciation guided unit <ref type="bibr" target="#b272">[273]</ref><ref type="bibr" target="#b273">[274]</ref><ref type="bibr" target="#b274">[275]</ref> is worth studying.</p><p>Trained with paired speech-text data, E2E models are more data hungry than hybrid models. If the training data size is small, the performance of E2E models drops significantly. There are lots of ways to address the challenge of building E2E models for low-resource languages. First, regularization technologies such as dropout <ref type="bibr" target="#b275">[276]</ref> can be used to prevent E2E models from overfitting to limited training data <ref type="bibr" target="#b276">[277]</ref>. Data augmentation methods such as SpecAugment <ref type="bibr" target="#b277">[278]</ref> and speed perturbation <ref type="bibr" target="#b278">[279]</ref> are also very helpful. In addition, there are also methods such as adversarial learning <ref type="bibr" target="#b279">[280]</ref> and meta-learning <ref type="bibr" target="#b280">[281]</ref>. The most popular solution is to first pre-train E2E models either with multilingual data or with self-supervised learning (SSL), and then fine-tune with the low-resource labeled data. A multilingual E2E model already captures lots of information across languages, which makes the transfer learning using the target language data very effective <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b281">[282]</ref><ref type="bibr" target="#b282">[283]</ref><ref type="bibr" target="#b283">[284]</ref><ref type="bibr" target="#b284">[285]</ref>. SSL is even more powerful because it does not need any labeled data for pre-training, naturally solving the low-resource challenge. Therefore, SSL is becoming a new trend which especially works very well for ASR on resource limited languages <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b285">[286]</ref><ref type="bibr" target="#b286">[287]</ref><ref type="bibr" target="#b287">[288]</ref><ref type="bibr" target="#b288">[289]</ref>, with representative technologies such as wav2vec 2.0 <ref type="bibr" target="#b27">[28]</ref>, autoregressive predictive coding <ref type="bibr" target="#b286">[287]</ref>, and HuBERT <ref type="bibr" target="#b30">[31]</ref>. While most SSL studies focus on very limited supervised training data (e.g., 1000 hours), there are also recent studies showing promising results on industry-scale tens of thousand hours supervised training data <ref type="bibr" target="#b289">[290,</ref><ref type="bibr" target="#b290">291]</ref>.</p><p>When training E2E ASR models, separate utterances are usually presented to the trainer. Therefore most ASR models are designed to recognize independent utterances. When the model is used to recognize a long conversation, a common practice is to segment the long conversation into utterances and then recognize them independently. However, the context information from previous utterances is useful to recognize the current utterance. In <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b291">292,</ref><ref type="bibr" target="#b292">293]</ref>, audio and decoded hypotheses from previous utterances are concatenated as the additional input to the model when processing the current utterance.</p><p>When deploying E2E models to production, it is important to have an efficient decoding strategy, which was explored in <ref type="bibr" target="#b293">[294,</ref><ref type="bibr" target="#b294">295]</ref>. Because the prediction network does not fully function as an LM as discussed in Section VI.B, the LSTM/Transformer in the RNN-T prediction network was recently replaced with a simple and cheap embedding with very limited context, which can be used to significantly reduce decoding cost and model size <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b295">296]</ref>. When deployed to small-footprint devices, model compression <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b296">297]</ref>, quantization <ref type="bibr" target="#b297">[298,</ref><ref type="bibr" target="#b298">299]</ref>, and the combination of multiple technologies <ref type="bibr" target="#b299">[300]</ref> should be considered. Confidence measure and word timing are sometimes required for practical E2E systems. For example, in a video transcription system, confidence score is used to indicate which word may be recognized wrongly and then the user can listen to the video corresponding to that word using its timestamp. Examples of confidence measure work for E2E systems are <ref type="bibr" target="#b188">[189,</ref><ref type="bibr" target="#b300">[301]</ref><ref type="bibr" target="#b301">[302]</ref><ref type="bibr" target="#b302">[303]</ref>, and examples of word timing work for E2E systems are <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b188">189]</ref>.</p><p>Almost all E2E models take Mel filter bank feature which is extracted from speech waveform as the input. In order to do the recognition really from end to end, speech waveform as the input to E2E models was studied in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b303">[304]</ref><ref type="bibr" target="#b304">[305]</ref><ref type="bibr" target="#b305">[306]</ref><ref type="bibr" target="#b306">[307]</ref>, especially in recent influential wav2vec series work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b285">286]</ref>. Because both E2E models and hybrid models have their own advantages and different error patterns, some works try to combine them together via rescoring <ref type="bibr" target="#b307">[308]</ref>, minimum Bayes' risk combination <ref type="bibr" target="#b308">[309]</ref>, or two-pass modeling <ref type="bibr" target="#b309">[310]</ref>. It is also very easy for E2E models to integrate inputs with multi-modality especially audio and visual signal together. There are plenty of works showing the benefits of visual signal for E2E ASR <ref type="bibr" target="#b310">[311]</ref><ref type="bibr" target="#b311">[312]</ref><ref type="bibr" target="#b312">[313]</ref><ref type="bibr" target="#b313">[314]</ref><ref type="bibr" target="#b314">[315]</ref><ref type="bibr" target="#b315">[316]</ref><ref type="bibr" target="#b316">[317]</ref><ref type="bibr" target="#b317">[318]</ref>.  <ref type="bibr" target="#b326">[327]</ref> 2019 AED Transformer 2.1/4.9 Transformer-T, SpecAugment <ref type="bibr" target="#b88">[89]</ref> 2020 RNN-T Transformer 2.0/4.6 Conformer-T, SpecAugment <ref type="bibr" target="#b96">[97]</ref> 2020 RNN-T Conformer 1.9/3.9 wav2vec 2.0: SSL with unlabeled data, DataAugment <ref type="bibr" target="#b27">[28]</ref> 2020 CTC Transformer 1.8/3.3 internal LM prior correction, EOS modeling <ref type="bibr" target="#b327">[328]</ref> 2021 RNN-T BLSTM 2.2/5.6 w2v-BERT: SSL with unlabeled data, SpecAugment <ref type="bibr" target="#b328">[329]</ref> 2021 RNN-T Conformer 1.4/2.5 Robustness is always an important topic to ASR <ref type="bibr" target="#b318">[319]</ref>. E2E models tend to fit training data and should have even severe robustness challenge due to the mismatch between training and testing. However, there is not too much activity in addressing such mismatch <ref type="bibr" target="#b319">[320,</ref><ref type="bibr" target="#b320">321]</ref>. T/S learning was used to adapt a clean-trained E2E model to a noisy environment <ref type="bibr" target="#b116">[117]</ref>. Noise-invariant feature was learned to improve robustness in <ref type="bibr" target="#b321">[322,</ref><ref type="bibr" target="#b322">323]</ref>. Data augmentation <ref type="bibr" target="#b323">[324]</ref> is another effective way to expose more testing environments to E2E models during training.</p><p>Last but not least, it is always beneficial to examine technologies using a public database. There are lots of E2E works evaluated on Librispeech <ref type="bibr" target="#b331">[332]</ref>, pushing the stateof-the-art (SOTA) forward. Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> give some representative works on Librispeech with non-streaming and streaming E2E models, respectively 2 . It is amazing to see how significantly WER is reduced within only a few years, thanks to fast developing technologies. There are two clear trends: from CTC to AED/RNN-T in the model aspect and from LSTM to Transformer in the encoder aspect. As achieving SOTA results on Librispeech is the goal of most works, there are much more non-streaming E2E studies. Because Librispeech only has 960 hours of labeled data, methods helpful to limited resource scenarios such as SSL and data augmentation are essential to boost the ASR accuracy on Librispeech. 2 Factors such as model size and latency are not listed in the Tables. They all affect the final word error rate (WER) of E2E models, especially streaming E2E models. Therefore, a method obtaining lower WER than another does not always mean that method is superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this paper, we gave a detailed overview of E2E models and practical technologies that enable E2E models to not only surpass hybrid models in academic tasks but also potentially replace hybrid models in the industry. Although CTC is revived with advanced encoder structures, the most popular E2E models are AED and RNN-T. Because of the streaming nature, research has been gradually shifted from AED to RNN-T. The encoder is the most important module in E2E models, and attracts most research. The trend is also very clear -the encoder structure has shifted from LSTM to Transformer and its variants. Masking strategy is used to design efficient transformer encoders with low latency and small computational costs for industry application. In order to serve multilingual users well, multilingual E2E models are trained by pooling all the language data together. However, there is still a clear accuracy gap between the multilingual models with and without language ID. A configurable multilingual model can fill in the gap by training the model once and being configured based on the language selection by any multilingual user. Adaptation may be the most important area to work on in order to enable E2E models to replace hybrid models in the industry because of the huge accuracy improvement with adaptation for new speakers and new domains. Specifically, there is more research on domain adaptation and customization than speaker adaptation. The successful domain adaptation technologies usually have better use of text only data from the new domain while the successful customization methods can deal with the challenge of a very large context biasing list. In addition to the standard E2E model training criterion, T/S training is used to either learn a small student model approaching the performance of a large teacher model or learn a streaming student model approaching the performance of a non-streaming teacher model. MWER training is to ensure E2E models are optimized with the training criterion consistent with ASR evaluation metrics. Recently, there is a trend of developing advanced models such as non-autoregressive models which have much fast decoding speed by doing inference with one shot; unified models which are trained once but can be flexibly configured to meet different runtime requirements; and two-pass models which can leverage the advantages of both streaming and non-streaming models.</p><p>A few years ago, there were very few people who believed E2E models would replace hybrid models which have lots of features designed for commercial ASR products. Within a very short time, we have witnessed that E2E models not only surpass hybrid models in terms of accuracy but also catch up with the practical features in commercial ASR systems. Because of the power of E2E modeling, the trend is not only building E2E models to replace traditional ASR models but also unifying speech processing modules such as speech separation, signal processing, and speaker identification into a single E2E ASR model. Multi-talker E2E models and multi-channel E2E models are examples of such a direction. The multi-talker E2E modeling was further extended with identifying speakers <ref type="bibr" target="#b332">[333,</ref><ref type="bibr" target="#b333">334]</ref>. There are also works of joint ASR and speaker diarization using E2E models by inserting speaker category symbols into ASR transcription <ref type="bibr" target="#b334">[335]</ref><ref type="bibr" target="#b335">[336]</ref><ref type="bibr" target="#b336">[337]</ref>. As E2E ASR models directly map speech signals into target word sequences, they can be extended to E2E speech translation models when the target word sequence is from another language <ref type="bibr" target="#b337">[338]</ref><ref type="bibr" target="#b338">[339]</ref><ref type="bibr" target="#b339">[340]</ref>. The major challenge for E2E speech translation is how to get enough training data with paired speech in the source language and text in the foreign language. The amount of such speech translation data is much smaller than the amount of ASR training data. Another challenge is how to handle the word reordering <ref type="bibr" target="#b340">[341]</ref>.</p><p>Although E2E modeling has already become the dominating ASR technology, there are still lots of challenges to be addressed before E2E models fully replace traditional hybrid models in both academic and industry. First, in hybrid modeling, paired speech-text data is used to build an acoustic model while a very large amount of text only data is used to build LM. In contrast, general E2E models are only built with paired speech-text data. How to leverage the text only data to improve the accuracy of E2E models instead of simply doing LM fusion is a future direction to explore. In <ref type="bibr" target="#b341">[342]</ref>, a multi-modal data augmentation method was proposed by having two separate encoders: one for acoustic input and the other for text input, sharing the same attention and decoder modules. There are some cycle-consistency works using unpaired speech-text data to improve ASR accuracy <ref type="bibr" target="#b342">[343,</ref><ref type="bibr" target="#b343">344]</ref>. Because the amount of text only data is much larger than that of the paired speech-text data, the cost is formidable to generate TTS audio from such large scale text only data for industry application although it is doable for small scale. TTS audio sometimes also degrades the recognition accuracy on real speech <ref type="bibr" target="#b186">[187]</ref>. Therefore, although using TTS audio for domain adaptation is a good practice, it may not be a good way to build an industry-scale E2E model by synthesizing TTS audio from the text data used for LM training. In <ref type="bibr" target="#b344">[345,</ref><ref type="bibr" target="#b345">346]</ref>, a joint acoustic and text decoder was proposed to leverage the text data by synthesizing TTS audio from the text to improve the decoder in AED. This was simplified in <ref type="bibr" target="#b346">[347]</ref> by using multitask training with text only data. In <ref type="bibr" target="#b347">[348,</ref><ref type="bibr" target="#b348">349]</ref>, an LM trained on large scale text is used as a teacher model to generate soft labels as the regularization of the AED model training. One promising way is the factorized neural transducer work <ref type="bibr" target="#b194">[195]</ref> in Figure <ref type="figure">9</ref> which has a standalone block working as a neural LM. Therefore, that LM-function block can be trained with large-scale text data efficiently.</p><p>Another challenge is how to integrate knowledge into a single E2E model. For example, it is very easy for an E2E model with display format output to generate "5:45" when a user says "five forty five". However, it is very hard for an E2E model to output "5:45" when a user says "a quarter to six" because standard E2E models do not have the knowledge unless the training has seen such examples.</p><p>In hybrid models, it is very easy to continuously add any new word not seen during training. However, this is very challenging for E2E models. The customization work described in Section VI.C provides a way to bias the ASR results towards a contextual phrase list. This is different from expanding the E2E models' capability of recognizing new words without biasing the ASR results. Although there are few works that allow injecting any out-of-vocabulary word into the vocabulary of acoustic-toword E2E models during inference via subword-to-word embedding <ref type="bibr" target="#b349">[350,</ref><ref type="bibr" target="#b350">351]</ref>, this problem is far from being solved.</p><p>Finally, it is always challenging to train good E2E models with low-resource languages. Given, the recent success of SSL which does not need any labeled data to pre-train a representation for downstream tasks, we predict that SSL will be closely coupled with E2E models in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The author would like to thank the anonymous reviewers, Xie Chen, Yashesh Gaur, Yanzhang He, Yan Huang, Naoyuki Kanda, Zhong Meng, Yangyang Shi, Eric Sun, Xiaoqiang Wang, Yu Wu, Guoli Ye, and Rui Zhao, for providing valuable inputs to improve the quality of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.: Architectures of three popular end-to-end techniques<ref type="bibr" target="#b16">[17]</ref> </figDesc><graphic url="image-3.png" coords="2,101.34,233.33,144.58,157.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.: Example CTC paths for the word "team". A blank label b is inserted between every character. The blue path is ( b , b , t, b , e, a, m, m). The green path is ( b , t, e, a, a, b , m, b ). The red path is (t, b , e, b , b , a, b , m).</figDesc><graphic url="image-4.png" coords="2,316.42,49.11,210.48,116.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.: Attention methods for AED. The time step goes from left to right, and the label sequence goes from bottom to top. Every circle represents an attention weight. The darker the circle is, the larger the attention weight is.</figDesc><graphic url="image-7.png" coords="3,304.72,140.21,106.01,67.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.: The structures of Transformer and Conformer</figDesc><graphic url="image-10.png" coords="5,304.72,63.04,86.74,177.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.6.: Attention for different computational cost and latency configuration when predicting the output for x 10 . In every sub figure, the left side is the reception field, and the right side is the attention mask matrix.</figDesc><graphic url="image-15.png" coords="6,304.72,316.74,240.95,71.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7.: Diagram of configurable multilingual model (CMM). Uni denotes a universal multilingual module, and Li denotes the ith language.</figDesc><graphic url="image-16.png" coords="8,74.23,49.12,198.77,166.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8.: An example of the spliced data method. It generates "Cortana close door" from 3 utterances from the source training data by extracting the audio segments of "Cortana", "close", and "door" out and then concatenating them.</figDesc><graphic url="image-17.png" coords="10,56.69,49.11,233.87,98.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 13.:The training and decoding of dynamic encoder<ref type="bibr" target="#b216">[217]</ref> </figDesc><graphic url="image-22.png" coords="12,304.72,51.44,72.28,89.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. 15.: Architecture of E2E models to recognize overlapping speech</figDesc><graphic url="image-28.png" coords="14,56.69,479.86,240.95,202.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Representative non-streaming E2E works on Librispeech</figDesc><table><row><cell>JINYU LI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>work with key technologies</cell><cell cols="2">year model</cell><cell>encoder</cell><cell>test-clean/other WER</cell></row><row><cell>Deep Speech 2: more labeled data, curriculum learning [325]</cell><cell>2016</cell><cell>CTC</cell><cell>bi-RNN</cell><cell>5.3/13.2</cell></row><row><cell>policy learning, joint training [326]</cell><cell>2018</cell><cell>CTC</cell><cell>CNN+bi-GRU</cell><cell>5.4/14.7</cell></row><row><cell>Shallow fusion, BPE, and pre-training [33]</cell><cell>2018</cell><cell>AED</cell><cell>BLSTM</cell><cell>3.8/12.8</cell></row><row><cell cols="2">ESPRESSO recipe: lookahead word LM, EOS thresholding [258] 2019</cell><cell>AED</cell><cell>CNN+BLSTM</cell><cell>2.8/8.7</cell></row><row><cell>SpecAugment [278]</cell><cell>2019</cell><cell>AED</cell><cell>CNN+BLSTM</cell><cell>2.5/5.8</cell></row><row><cell>ESPnet recipe: SpecAugment, dropout [86]</cell><cell>2019</cell><cell>AED</cell><cell>Transformer</cell><cell>2.6/5.7</cell></row><row><cell>Semantic mask</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Representative streaming E2E works on Librispeech</figDesc><table><row><cell>work with key technologies</cell><cell cols="2">year model</cell><cell>encoder</cell><cell>test-clean/other WER</cell></row><row><cell>stable MoChA, truncated CTC prefix probability [330]</cell><cell>2019</cell><cell>AED</cell><cell>LC-BLSTM</cell><cell>6.0/16.7</cell></row><row><cell>triggered attention [48]</cell><cell>2019</cell><cell>AED</cell><cell>time-delayed LSTM</cell><cell>5.9/16.8</cell></row><row><cell cols="2">triggered attention, restricted self-attention, SpecAugment [49] 2020</cell><cell>AED</cell><cell>Transformer</cell><cell>2.8/7.2</cell></row><row><cell>Transformer-T, restricted self-attention, SpecAugment [89]</cell><cell cols="2">2020 RNN-T</cell><cell>Transformer</cell><cell>2.7/6.6</cell></row><row><cell>scout network, chunk self-attention, SpecAugment [52]</cell><cell>2020</cell><cell>AED</cell><cell>Transformer</cell><cell>2.7/6.4</cell></row><row><cell>dual casual/non-casual self-attention, SpecAugment [331]</cell><cell>2021</cell><cell>AED</cell><cell>Conformer</cell><cell>2.5/6.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note this mapping is different from the CTC mapping B in Eq. (2).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent NN: First results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wav2letter: an endto-end convnet-based speech recognition system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A streaming on-device end-to-end model surpassing server-side conventional model quality and latency</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-Y. Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6059" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Developing RNN-T models surpassing high-performance hybrid models with customization capability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3590" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6381" to="6385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advances in all-neural speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4805" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CTC in the context of generalized full-sum HMM training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="944" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advancing acoustic-toword CTC model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5794" to="5798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building competitive direct acoustics-to-word models for English conversational speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4759" to="4763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Advancing connectionist temporal classification with attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4769" to="4773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-attention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7115" to="7119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mask CTC: Non-autoregressive end-to-end ASR with CTC and mask predict</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3655" to="3659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative pre-training for speech with autoregressive predictive coding</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3497" to="3501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HuBERT: How much can a bad teacher benefit ASR pre-training?</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6533" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5060" to="5064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<biblScope unit="page" from="949" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multitask learning with low-level auxiliary tasks for encoder-decoder based speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3532" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Acousticto-word attention-based model complemented with character-level CTC-based model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5804" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auxiliary feature based adaptation of end-to-end ASR systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2444" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial training of endto-end speech recognition using a criticizing language model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>-Y. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6176" to="6180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention based on-device streaming speech recognition with large speech corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="956" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1408" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online and linear-time attention by enforcing monotonic alignments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2837" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monotonic chunkwise attention</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An online attentionbased model for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4390" to="4394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monotonic infinite lookback attention for simultaneous machine translation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1313" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Triggered attention for endto-end speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5666" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition with joint CTCattention based models</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Streaming automatic speech recognition with the transformer model</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6074" to="6078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Advanced longcontext end-to-end speech recognition using context-expanded transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2097" to="2101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Minimum latency training strategies for streaming sequence-to-sequence ASR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6064" to="6068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Low latency end-to-end streaming speech recognition with a scout network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2112" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comparison of streaming models and data augmentation methods for robust speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE, 2021</title>
				<meeting>ASRU. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="989" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A comparison of end-to-end models for long-form speech recognition</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recognizing long-form speech using streaming end-to-end models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="920" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with adaptive computation steps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Masanori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6246" to="6250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">CIF: Continuous integrate-and-fire for endto-end speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6079" to="6083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transformerbased online CTC/attention end-to-end speech recognition architecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6084" to="6088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Synchronous transformers for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7884" to="7888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Enhancing monotonic multihead attention for streaming ASR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2137" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transformer-based online speech recognition with decoder-end adaptive computation steps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zorilȃ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Streaming transformer ASR with blockwise synchronous beam search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to count words in fluent speech enables online speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sterpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Gaussian kernelized self-attention for long sequence data and its application to CTCbased speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6214" to="6218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Improving RNN transducer modeling for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Benchmarking LF-MMI, CTC and RNN-T criteria for streaming ASR</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Advancing RNN transducer technology for speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5654" to="5658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Joint ASR and language identification using RNN-T: An efficient approach to dynamic language switching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Punjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arsikere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Raeesy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="7218" to="7222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficient implementation of recurrent neural network transducer in tensorflow</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="506" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">An efficient streaming non-recurrent on-device end-to-end model with improvements to rare-word modeling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Botros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1777" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dissecting user-perceived latency of on-device E2E speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4553" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Joint endpointing and decoding with end-to-end models</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Simko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5626" to="5630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Towards fast and accurate streaming end-to-end ASR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-Y. Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6069" to="6073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Emitting word timings with end-to-end models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3615" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Alignment restricted streaming recurrent neural network transducer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Monotonic recurrent neural network transducer and decoding strategies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="944" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fastemit: Lowlatency streaming ASR with sequence-level emission regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-Y. Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6004" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Reducing streaming ASR model delay with self alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3440" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">End-to-end neural segmental models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1254" to="1264" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Recurrent neural aligner: An encoder-decoder neural network model for sequence to sequence mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1298" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Highway long short-term memory RNNs for distant speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Improving layer trajectory LSTM with future context frames</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6550" to="6554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A comparison of transformer and LSTM encoder decoder models for ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A comparative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the comparison of popular end-to-end models for large scale speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7829" to="7833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Transformertransducer: End-to-end speech recognition with self-attention</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12977</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Developing real-time streaming transformer transducer for speech recognition on largescale dataset</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5904" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Transformertransducers for code-switched speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ronanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5859" to="5863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Recent developments on ESPnet toolkit boosted by conformer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6783" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Transformer ASR with contextual block processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kumakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="427" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Streaming chunk-aware multihead attention for online end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2142" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Conv-transformer transducer: Low latency, low frame rate, streamable end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5001" to="5005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Selfattentional acoustic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3723" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning small-size DNN with output-distribution-based criteria</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1910" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Developing far-field speaker system via teacher-student learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5699" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Compression of end-to-end models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Knowledge distillation using output errors for self-attention end-to-end models</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6181" to="6185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Efficient knowledge distillation for RNNtransducer models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5639" to="5643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Improved training for online end-to-end speech recognition systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2913" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Guiding CTC posterior spike timings for improved posterior fusion and knowledge distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1616" to="1620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Distilling attention weights for CTC-based ASR systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ashihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CASSP. IEEE</title>
				<meeting>CASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6894" to="6898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Knowledge distillation from offline to streaming RNN transducer for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2117" to="2121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Knowledge distillation for streaming transformer transducer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2841" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Improving streaming automatic speech recognition with non-streaming model distillation on unsupervised data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Doutre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6558" to="6562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Domain adaptation via teacher-student learning for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Knowledge distillation for endto-end monaural multi-talker ASR system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2633" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Dynamic sparsity neural networks for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6014" to="6018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Dual-mode ASR: unify and improve streaming ASR with full-context modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">CoDERT: Distilling encoder representations with co-learning for transducer-based speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4543" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Collaborative training of acoustic encoders for speech recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4573" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Optimizing expected word error rate via sampling for speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3537" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4839" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Improving attention-based end-to-end ASR systems with sequence-based loss functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Minimum Bayes risk training of RNN-transducer for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="966" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Efficient minimum word error rate training of RNN-transducer for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Segbroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2807" to="2811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">On minimum word error rate training of the hybrid autoregressive transducer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3435" to="3439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Multilingual training of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7319" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Language independent end-to-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Towards language-universal end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4914" to="4918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4904" to="4908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Multilingual sequence-tosequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5621" to="5625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Multi-dialect speech recognition with a single sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4749" to="4753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Large-scale multilingual speech recognition with a streaming end-to-end model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with self-attention structured parameterization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4741" to="4745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Massively multilingual ASR: 50 languages, 1 model, 1 billion parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4751" to="4755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Improving multilingual transformer transducer models by reducing language confusions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3470" to="3474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Leveraging language ID in multilingual end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="928" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Streaming end-to-end bilingual ASR systems with joint language identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Punjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arsikere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Raeesy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garimella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03900</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Multilingual people</title>
		<ptr target="http://ilanguages.org/bilingual.php" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A configurable multilingual model is all you need to recognize all languages</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Scaling end-to-end models for large-scale multilingual ASR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE, 2021</title>
				<meeting>ASRU. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1011" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Multi-dialect speech recognition in English using attention on ensemble of experts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6244" to="6248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Mixture of informed experts for multilingual speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6234" to="6238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">An end-to-end language-tracking speech recognizer for mixedlanguage speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4919" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Towards code-switching ASR for end-to-end CTC models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6076" to="6080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Constrained output embeddings for end-to-end codeswitching speech recognition with only monolingual data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Khassanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2160" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">End-to-end codeswitching ASR for low-resourced language pairs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="972" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Bi-encoder transformer network for Mandarin-English code-switching speech recognition using mixture of experts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4766" to="4770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Towards context-aware end-to-end code-switching speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4776" to="4780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Investigation of methods to improve the recognition performance of Tamil-English codeswitched data in transformer framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Nj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7889" to="7893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">RNN-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCSLP. IEEE, 2021</title>
				<meeting>ISCSLP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">MUCS 2021: Multilingual and code-switching ASR challenges for low resource Indian languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vaideeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Unni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajpuria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2446" to="2450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Multi-encoderdecoder transformer for code-switching speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1042" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Adaptation algorithms for neural network-based speech recognition: An overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fainberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="33" to="66" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Speaker adaptation for end-to-end CTC models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="542" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Multiple-hypothesis CTCbased semi-supervised adaptation of end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6978" to="6982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Listen, attend, spell and adapt: Speaker adapted sequence-to-sequence ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andrés-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3805" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Speaker adaptation for attention-based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Personalization of end-to-end speech recognition on mobile devices for named entities</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lucassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zadrazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Rapid RNN-T adaptation using personalized speech synthesis and neural language generator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1256" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Rapid speaker adaptation for conformer transducer: Attention and bias are all you need</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1309" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Unsupervised speaker adaptation using attention-based speaker memory for end-to-end ASR</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sarı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7384" to="7388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Speech transformer with speaker aware persistent memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1261" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7893" to="7897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Maximum a posteriori adaptation of network parameters in deep models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1076" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">An investigation into ondevice personalization of end-to-end automatic speech recognition models</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zadrazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="774" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Robust continuous on-device personalization for automatic speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandorkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1284" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="387" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">A comparison of techniques for language model integration in encoder-decoder speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Maximum a posteriori based decoding for CTC acoustic models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1868" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Maximum-a-posteriori-based decoding for end-to-end acoustic models</title>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1023" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">A density ratio approach to language model fusion in end-to-end automatic speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Hybrid autoregressive transducer (HAT)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6139" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Internal language model estimation for domain-adaptive end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Internal language model training for domain-adaptive end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="7338" to="7342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Improving tail performance of a deliberation E2E ASR model using a large text corpus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mavandadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4921" to="4925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Minimum word error rate training with language model fusion for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2596" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Improving performance of end-to-end ASR on numeric sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2185" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end ASR systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunceler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5674" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Semi-supervised training for end-to-end models via weak distillation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2837" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Improving customization of neural transducers by mitigating acoustic mismatch of synthesized audio</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2027" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">On addressing practical challenges for RNN-transducer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE, 2021</title>
				<meeting>ASRU. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="526" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Semantic data augmentation for end-to-end Mandarin speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1269" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">On-thefly aligned data augmentation for sequence-to-sequence ASR</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1299" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Fast text-only domain adaptation of RNNtransducer prediction network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pylkkönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kilpikoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heikinheimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1882" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Internal language model adaptation with text-only data for end-toend speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05354</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">RNNtransducer with stateless prediction network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7049" to="7053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Factorized neural transducer for efficient language model adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01500</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Towards lifelong learning of end-to-end ASR</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>-Y. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2551" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Streaming small-footprint keyword spotting using sequence-tosequence models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="474" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Deep context: end-to-end contextual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Instant one-shot word-learning for context-specific neural sequence-to-sequence speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE, 2021</title>
				<meeting>ASRU. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Phoebe: Pronunciation-aware contextualization for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6171" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Joint grapheme and phoneme embeddings for contextual end-to-end ASR</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3490" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Contextual RNN-T for open domain ASR</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Shallow-fusion end-to-end contextual biasing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1418" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Deep shallow fusion for RNN-T personalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Class LM and word mapping for contextual biasing in end-to-end ASR</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4348" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1772" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Contextual density ratio for language model biasing of sequence to sequence ASR systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andrés-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albesano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2007" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">A light-weight contextual spelling correction model for customizing transducer-based speech recognition systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1982" to="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">A spelling correction model for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5651" to="5655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Listen attentively, and spell once: Whole sentence generation via a nonautoregressive architecture for low-latency speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3381" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Spiketriggered non-autoregressive transformer for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5026" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Nonautoregressive transformer for speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Żelasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="121" to="125" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Nonautoregressive transformer ASR with CTC-enhanced decoder input</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5894" to="5898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">CASS-NAT: CTC alignment-based single step non-autoregressive transformer for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5889" to="5893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Improved mask-CTC for non-autoregressive end-to-end ASR</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="8363" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Mixture model attention: Flexible streaming and non-streaming automatic speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021, 2021</date>
			<biblScope unit="page" from="1812" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Dynamic encoder transducer: A flexible solution for trading off accuracy for latency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2042" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Amortized neural networks for low-latency speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macoskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4558" to="4562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<title level="m" type="main">Transformer transducer: One model unifying streaming and non-streaming speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03192</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Flexi-transducer: Optimizing latency, accuracy and compute for multi-domain on-device scenarios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2107" to="2111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Multimode transformer transducer with stochastic future context</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1827" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Investigation of transformer based spelling correction model for CTC-based end-to-end Mandarin speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2180" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Cascade RNN-transducer: Syllable based streaming on-device mandarin speech recognition with a syllable-to-character converter</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Two-pass endto-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2773" to="2777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Deliberation model based two-pass end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7799" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Transformer based deliberation for two-pass speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Cascaded encoders for unifying streaming and non-streaming ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5629" to="5633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Low latency speech recognition using end-to-end prefetching</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1962" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">A better and faster endto-end model for streaming ASR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5634" to="5638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Advances in online audio-visual meeting transcription</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abramovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aksoylar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurvich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Recognizing multi-talker speech with permutation invariant training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2456" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Endto-end multi-speaker speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4819" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">End-to-end monaural multi-speaker ASR system without pretraining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6256" to="6260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="237" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">End-to-end multi-speaker speech recognition with transformer</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6134" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Serialized output training for end-to-end overlapped speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2797" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">End-to-end multi-talker overlapping speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6129" to="6133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Streaming end-to-end multitalker speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="803" to="807" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Streaming multi-speaker ASR with RNN-T</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sklyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6903" to="6907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Continuous streaming multi-talker ASR with dual-path transducers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Speech recognition with microphone arrays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microphone arrays</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="331" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Microphone array processing for distant speech recognition: From close-talking microphones to far-field sensors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Deep beamforming networks for multi-channel speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5745" to="5749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Multichannel signal processing with deep neural networks for automatic speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="965" to="979" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Frequency domain multi-channel acoustic modeling for distant speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Minhua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6640" to="6644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Multichannel end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2632" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Exploring end-to-end multi-channel ASR with bias information for meeting transcription</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">End-to-end far-field speech recognition with unified dereverberation and beamforming</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="324" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">End-to-end dereverberation, beamforming, and speech recognition with improved numerical stability and advanced frontend</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6898" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">End-to-end multi-channel transformer for speech recognition</title>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Multichannel transformer transducer for speech recognition</title>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="296" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goderre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Milanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3840" to="3844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Returnn: the RWTH extensible training framework for universal recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5345" to="5349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Espresso: A fast end-to-end neural speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<monogr>
		<title level="m" type="main">Lingvo: a modular and scalable framework for sequence-to-sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08295</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title level="m" type="main">Speechbrain: A general-purpose speech toolkit</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4054" to="4058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Stateof-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Subword and crossword units for CTC acoustic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="396" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Hybrid CTC-attention based end-to-end speech recognition using subword units</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCSLP. IEEE</title>
				<meeting>ISCSLP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Exploring model units and training strategies for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Acoustic-to-word model without OOV</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Acoustic-to-phrase models for speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2240" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">No need for a lexicon? evaluating the value of the pronunciation lexica in end-to-end models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schogol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5859" to="5863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<monogr>
		<title level="m" type="main">Model unit exploration for sequence-to-sequence speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01955</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">An investigation of phone-based subword units for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1778" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Phoneme based neural transducer for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5644" to="5648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with pronunciation-assisted sub-word modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7110" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Acoustic data-driven subword modeling for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeineldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2886" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Phonetically induced subwords for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papadourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1992" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<title level="m" type="main">Improved regularization techniques for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07108</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Combining end-to-end and adversarial training for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Drexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Meta learning for end-to-end low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7844" to="7848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition and keyword search on low-resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5280" to="5284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Sequencebased multi-lingual low resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4909" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Transfer learning of language-independent end-to-end ASR with language model fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6096" to="6100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Transfer learning approaches for streaming end-to-end speech recognition system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2152" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Unispeech: Unified speech representation learning with labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">947</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<title level="m" type="main">Unispeech at scale: An empirical study of pre-training method on large-scale speech recognition dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<title level="m" type="main">BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13226</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Transformer-based long-context end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5011" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Improving RNN-T ASR accuracy using context audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sklyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1792" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Alignment-length synchronous decoding for RNN transducer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7804" to="7808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Less is more: Improved RNN-T decoding using limited label context and path merging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="5659" to="5663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Tied &amp; reduced RNN-T decoder</title>
		<author>
			<persName><forename type="first">R</forename><surname>Botros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4563" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Iterative compression of end-to-end ASR model using AutoML</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>-Y. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vipperla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishtiaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G C</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3361" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Quantization aware training with absolute-cosine regularization for automatic speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alexandridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3366" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">4-bit quantization of LSTM-based speech recognition models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fasoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2586" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Optimizing speech recognition for the edge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLSys On-device Intelligence Workshop</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">An evaluation of word-level confidence estimation for end-to-end automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caranica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cucu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Confidence estimation for attention-based sequenceto-sequence models for speech recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6388" to="6392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">Learning word-level confidence for subword end-to-end ASR</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6393" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition from the raw waveform</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="781" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">E2E-sincnet: Toward fully end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Linares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7714" to="7718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Raw waveform encoder with multi-scale globally attentive locally recurrent networks for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="316" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Integrating sourcechannel and attention-based sequence-to-sequence models for speech recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Combination of end-to-end and hybrid models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1783" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Have best of both worlds: Two-pass hybrid and E2E cascading framework for speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition with a hybrid CTC/attention architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Attention-based audio-visual fusion for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sterpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="111" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Modality attention for end-to-end audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6565" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition system with multitask learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Listen, look and deliberate: Visual context-aware speech recognition using pre-trained text-video representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE, 2021</title>
				<meeting>SLT. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="621" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">End-to-end audio-visual speech recognition with conformers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="7613" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">An overview of noise-robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="745" to="777" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Towards a competitive end-to-end speech recognition for CHiME-6 dinner party transcription</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrusenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="319" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">An investigation of endto-end models for robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Velmurugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE, 2021</title>
				<meeting>ICASSP. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6893" to="6897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Learning noise-invariant representations for robust speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Learning noise invariant features through transfer learning for robust end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7024" to="7028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">Data augmentation methods for end-to-end speech recognition on distant-talk scenarios</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Narisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="301" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with policy learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5819" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<monogr>
		<title level="m" type="main">Semantic mask for transformer based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03010</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b327">
	<monogr>
		<title level="m" type="main">Librispeech transducer model with internal language model prior correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merboldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03006</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">W2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE, 2021</title>
				<meeting>ASRU. IEEE, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="244" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">Online hybrid CTC/Attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2623" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Dual causal/non-causal selfattention for streaming end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1822" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Joint speaker counting, speech recognition, and speaker identification for overlapped speech of any number of speakers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Streaming multi-talker speech recognition with joint speaker identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1782" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Joint speech recognition and speaker diarization via sequence transduction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="396" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Speech recognition and multi-speaker diarization of long conversations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="691" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">Understanding medical conversations: Rich transcription, confidence scores &amp; information extraction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Shafey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4418" to="4422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Adapting transformer to end-to-end spoken language translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Di Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1133" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Multilingual end-to-end speech translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="570" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<monogr>
		<title level="m" type="main">Investigating the reordering capability in CTC-based non-autoregressive end-to-end speech translation</title>
		<author>
			<persName><forename type="first">S.-P</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04840</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Multimodal data augmentation for end-to-end ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2394" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">Listening while speaking: Speech chain by deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence-to-sequence ASR using unpaired speech and text</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3790" to="3794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">An attention-based joint acoustic and text on-device end-to-end model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7039" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">A deliberationbased joint acoustic and text decoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mavandadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
				<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2057" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Multitask training with text data for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021">2021, 2021</date>
			<biblScope unit="page" from="2566" to="2570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">Learn spelling from teachers: Transferring knowledge from language models to sequence-to-sequence speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3795" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Distilling the knowledge of BERT for sequence-tosequence ASR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Futami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3635" to="3639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Acoustically grounded word embeddings for improved acoustics-to-word speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5641" to="5645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Word-level speech recognition with a letter to word encoder</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2100" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
