<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational auto-encoding of protein sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-03">3 Jan 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sam</forename><surname>Sinai</surname></persName>
							<email>samsinai@g.harvard.edu</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Program for Evolutionary Dynamics</orgName>
								<orgName type="department" key="dep2">Department of Organismic and Evolutionary Biology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Kelsic</surname></persName>
							<email>kelsic@hms.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Harvard</forename><forename type="middle">Medical</forename><surname>School</surname></persName>
						</author>
						<author>
							<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Genetics ¶ Department of Mathematics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
							<email>martinnowak@harvard.edu</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Program for Evolutionary Dynamics</orgName>
								<orgName type="department" key="dep2">Department of Organismic and Evolutionary Biology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational auto-encoding of protein sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-03">3 Jan 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1712.03346v3[q-bio.QM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proteins are responsible for the most diverse set of functions in biology. The ability to extract information from protein sequences and to predict the effects of mutations is extremely valuable in many domains of biology and medicine. However the mapping between protein sequence and function is complex and poorly understood. Here we present an embedding of natural protein sequences using a Variational Auto-Encoder and use it to predict how mutations affect protein function. We use this unsupervised approach to cluster natural variants and learn interactions between sets of positions within a protein. This approach generally performs better than baseline methods that consider no interactions within sequences, and in some cases better than the state-of-the-art approaches that use the inverse-Potts model. This generative model can be used to computationally guide exploration of protein sequence space and to better inform rational and automatic protein design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Protein engineering is of increasing importance in modern therapeutics. Designing novel proteins that perform a particular function is challenging as the number of functional proteins compared to all possible protein sequences is miniscule. This renders naive experimental search for desirable variants intractable. Hence, a computational heuristic that can narrow the experimental search space (virtual screening) is extremely valuable.</p><p>While a variety of energy-based models for protein folding have been used in the past decades, recent advances in machine learning, particularly in the domain of generative models, have opened up new avenues for computational protein design. Rich databases of protein sequences that document functional proteins found in living organisms provide us with ample training data. The majority of these datasets lack labels (indicators of their performance) however, which prompts for an unsupervised learning approach. As these sequences arise from closely related living organisms, it is reasonable to assume that they are functional (and also similar in their functionality).</p><p>Given the sparse, unstructured, and discrete space that protein sequences exist in, it is prudent to anchor the search for functional sequences on a known protein with the desired functionality. Starting from that sequence of interest, we can search public databases of sequence variants from related organisms and align them. This alignment of sequences constitutes an evolutionary cluster of nearby variants known as a multiple sequence alignment (MSA).</p><p>We are interested in using MSA data in an unsupervised manner to train models that can inform us about protein function. We hope to then use these models to find good candidate sequences (absent in our training), that can function similarly or better than those that we have already observed. Generative models have two appealing properties for this purpose: (i) They can be trained on sequence data alone, (ii) They can produce new candidate sequences that are similar to those present in our dataset, but not exactly the same.</p><p>Variational auto-encoders (henceforth VAE) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> are one type of such unsupervised generative models that aim to reconstruct their input through a compressed and continuous latent domain. Traditional auto-encoders are neural networks that reconstruct their input imperfectly. VAEs incorporate variational Bayesian methods to impose a lower bound on the probability of an input (which also serves as a built-in regularization method), allowing for a probabilistic interpretation of results.</p><p>A protein sequence x with length L lives on a L dimensional space, each with 20 possible values. The number of sequences within these 20 L possibilities that perform a particular function is very small. What we hope to achieve is to compress this large space into a lower dimensional continuous embedding of latent variables that explain the differences between protein sequences. For protein design purposes, we can then traverse this space to find new functional proteins. Additionally, we would hope that this compression would teach us about key properties that affect protein function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Over the past few years generative graphical models have been used on sequence alignments to predict protein structure and function <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. These models learn correlations between amino acids in different positions, and then try to approximate the effects of changing one amino-acid to another in a given position. The most successful applications of these methods have used Potts models as their core modeling approach. These models incorporate independent and pairwise interactions along the sequence. The technical details are explained in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and their application for a large set of data has been recently published <ref type="bibr" target="#b4">[5]</ref>. The methods show that harnessing correlations between pairs of amino acids at different positions provides significant power for protein folding and function prediction. Recently, variational-auto encoders have been used for continuous representation of chemical compounds, which allowed for optimization of the process of chemical design <ref type="bibr" target="#b5">[6]</ref>. Additionally, variational inference on graphical models (akin to those presented above as Potts models) were shown to hold promise in predicting protein structure <ref type="bibr" target="#b6">[7]</ref>. Here we show that VAEs also hold promise for protein design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our model needs to learn the joint probability p(x, z) = p(z)p(x|z) where z ∈ Z are latent variables, and x ∈ X are the observed data. If we can learn a good distribution for the latent variables, we can then generate new data points like those in X that we haven't observed in our dataset by sampling z from p(z) and sampling new points x from p(x|z). Computing p(z|x) from our observed data requires us to compute the evidence term for each data point in X:</p><formula xml:id="formula_0">p(x) = p(x|z)p(z)dz<label>(1)</label></formula><p>A good model would maximize the probability of our data. However, direct computation of this integral is intractable. Instead it can be approximated using variational inference. Specifically, we can approximate p(x) by using the Evidence Lower BOund (ELBO):</p><formula xml:id="formula_1">log p(x) ≥ E q [log p(x|z)] − D KL (q(z|x)||p(z))<label>(2)</label></formula><p>Where in the above formula, q is a family of normal distributions (a standard assumption of these models), approximating p(z|x), and D KL is the Kullback-Leibler divergence. VAEs learn the parameters θ and φ for the distributions q(z|x) and p(x|z) simultaneously through gradient descent.</p><p>In the language of neural networks, q θ specifies the encoder and p φ specifies the decoder. By maximizing the lower bound on the evidence through gradient ascent, we get an approximation for the maximum likelihood of the data. Notably, we also use the standard assumption that the prior p(z) ∼ N (0, 1).</p><p>Once we have build a generative model that produce sequences like those in our dataset with high probability, we use it to generate novel but similar sequences x, or evaluate the likelihood of sequences that the model hasn't seen before.</p><p>We treat the one-hot-encoded sequences from the MSA as training data, and train our model to reconstruct these sequences. Once the model is trained, the probability of each input sequence (from training or test set) can be estimated as follows:</p><formula xml:id="formula_2">log p(x|z) ∝ log(tr(H T P ))<label>(3)</label></formula><p>Where H is an m × n matrix representing the one-hot encoding of the sequence of interest, m is the number of amino-acids and n the number of positions considered in the protein alignment. P , with identical dimensions as H, is the probability weight matrix generated by feeding the network a sequence. P can be generated in multiple ways, but the simplest procedure is to compute it by reconstructing the same sequence that was represented by H. Alternatively, P can be computed by an average reconstruction of multiple neighboring sequences, or the reconstruction of the wild-type sequence. We found that these approaches result in similar performance for function prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Validation</head><p>We validate our model by feeding the network sequences of single and double mutants (with respect to the reference sequence) and calculate their probability. We then compare, through rank correlation, this probability with the experimental fitness measurements. Neither the test sequences, nor the fitness measurements are passed through the network during training. We report the outcomes from training the model using the procedure described above on 5 protein families for which fitness measurements are publicly available <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture</head><p>We use the following architecture for the VAE. The encoder and the decoder network have three dense layers of 250 exponential linear units "ELU" <ref type="bibr" target="#b12">[13]</ref>. The encoder and decoder both include a dropout layer. This architecture was selected by grid search on the hyper-parameters including number of dense hidden layers (1-4), number of units per layer (50-350), inclusion/exclusion of dropout and batch normalization layers between each hidden layer. The final decoder layer uses sigmoid neurons. We use Keras <ref type="bibr" target="#b13">[14]</ref> to implement our VAE model and train our model using the ADAM optimizer <ref type="bibr" target="#b14">[15]</ref>. Empirically, networks with dense layers trained faster and performed comparably or better than convolutional layers. For protein function prediction, we use 5 latent variables. For lower dimensional representation and visualization, we use 2 or 3 latent variables. This pruning of latent variables slightly weakens the predictive power (by 0−5% depending on dataset), but provides more easily interpretable representations in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our results can be summarized by three main observations: (i) The probability estimates calculated by the network correlate well with protein functions measured in the experiments (Fig. <ref type="figure" target="#fig_0">1a,b</ref>) (ii) The embedding in 2D separates the variants in minimum edit-distance clusters when a 2D latent space is used (Fig. <ref type="figure" target="#fig_0">1c</ref>). (iii) The VAE learns pairwise and higher-level interactions between loci on the protein (Fig. <ref type="figure" target="#fig_0">1d</ref>)</p><p>We show that VAE's are a viable approach to predict sequence functionality from unlabeled sequences (Fig. <ref type="figure" target="#fig_0">1</ref>). Overall, the VAE performs better than the baseline in the 5 datasets tested (Fig. <ref type="figure" target="#fig_0">1b</ref>), suggesting that it captures relevant information about protein structure. These datasets were selected because their MSA were presumed to be large and sufficiently diverse for training and because they were used by previous approaches that aimed to predict protein function. We expect that proteins with small MSA size relative to their length and low natural diversity are less suitable for this approach. In line with this expectation, our VAE model performs better than the inverse Potts approach for PABP (for both single and double mutants), which has the largest MSA size relative to its length.</p><p>Our observations indicate that these models can generate candidate sequences that have a high likelihood of performing a particular function comparable to sequences in the training set. Unlike the Inverse Potts model (which it performs closely to), here the latent layer of the VAE provides a continuous representation of the protein sequence. As it has been argued for chemical molecules <ref type="bibr" target="#b5">[6]</ref>, the continuous representation of the protein may be used together with gradient-based optimization to achieve a desirable property. As we show in Fig. <ref type="figure" target="#fig_0">1c</ref>, the latent space encodes phylogenetic data (distance clusters), and possibly other features about the protein. The ability to continuously traverse the latent space provided by this approach should yield new opportunities for informed protein design that are qualitatively different than present-day methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current limitations and future work</head><p>This study serves as a proof-of-concept of the utility of VAEs in representing protein families, as well as their ability to predict the effects of mutations. Our work can be improved in certain dimensions. Despite longer training times, we expect that some recurrent or convolutional architectures may outperform our model, hence a more exhaustive search of such architectures would be prudent. The predicted effects of pairwise and higher order interactions can also be validated by projecting them onto protein tertiary structures. Additionally, our method could be adjusted to use sample weights as is standard in other approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. However we found empirically that reweighing did not consistently help the performance across datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of results. (a) Comparison of VAE predictions vs. fitness measurements for double mutations of a reference sequence (wildtype) (Experimental measurements are yeast PABP mutants, see<ref type="bibr" target="#b7">[8]</ref>). Red line shows the hypothetical perfect rank correlation. (b) Comparison of the VAE model's prediction ability with baseline (independent) and pairwise models (Inverse Potts, see<ref type="bibr" target="#b4">[5]</ref>). The size of the dataset is provided for reference. (c) Projection of training data on the 2D latent space. The red square is the latent coordinates for the reference sequence. Points are colored by k-means clustering of sequences, showing that the "branches" in the star-like latent structure correspond to close-by sequences. This is further confirmed by the fact that the experimental data, single (green) and double (purple) mutants, fall very close to the reference (shown as inset). (d) Top: An example of a change in the input sequence represented as one-hot matrix H. Red corresponds to wild-type (reference) and yellow to a mutant (and white is shared positions). These sequences are separately fed into the network, and the difference of the reconstruction matrices P mut − P wt is shown on the bottom panel. Bottom: A single mutation results in updates of probability in many other locations on the sequence, thus at least some pairwise and higher-order interactions (not shown explicitly) are captured. The wild-type sequence is denoted by dark spots and the mutation is marked by x (gold).</figDesc><graphic url="image-3.png" coords="4,108.55,237.47,171.36,158.72" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Pierce Ogden, Surojit Biswas, Gleb Kuznetsov, and Jeffery Gerold for helpful comments. We would also like to thank Debora Marks, John Ingraham, and Adam Riesselman for their feedback on this project as they have independently pursued a similar research objective <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>Example code to reproduce the analysis from this manuscript on PABP data can be found here: https://github.com/samsinai/VAE_protein_function</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protein 3d structure computed from evolutionary sequence variation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e28766</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: using pseudolikelihoods to infer potts models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lövkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aurell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12707</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mutation effects predicted from sequence co-variation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Poelwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schärfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02415</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational inference for sparse and undirected models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep mutational scanning of an rrm domain of the saccharomyces cerevisiae poly (a)-binding protein</title>
		<author>
			<persName><forename type="first">D</forename><surname>Melamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Gamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rna</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1537" to="1551" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dissecting enzyme function with microfluidicbased deep mutational scanning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Abate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="7159" to="7164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activity-enhancing mutations in an e3 ubiquitin ligase identified by high-throughput mutagenesis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Starita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Pruneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hiatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shendure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Brzovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Klevit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="E1263" to="E1272" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolvability as a function of purifying selection in tem-1 β-lactamase</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Stiffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="882" to="892" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Systematic mapping of protein mutational space by prolonged drift reveals the deleterious effects of seemingly neutral mutations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rockah-Shmuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Tóth-Petróczy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Tawfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e1004421</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep generative models of genetic variation capture mutation effects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06527</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
