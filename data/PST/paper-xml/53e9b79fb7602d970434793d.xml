<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Memory Bank-Level Parallelism in the Presence of Prefetching</title>
				<funder>
					<orgName type="full">Cockrell Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chang</forename><forename type="middle">Joo</forename><surname>Lee</surname></persName>
							<email>cjlee@ece.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Veynu</forename><surname>Narasiman</surname></persName>
							<email>narasima@ece.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
							<email>patt@ece.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">?Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">?Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Memory Bank-Level Parallelism in the Presence of Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.1.0 [Processor Architectures]: General</term>
					<term>C.5.3 [Microcomputers]: Microprocessors</term>
					<term>Design, Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DRAM systems achieve high performance when all DRAM banks are busy servicing useful memory requests. The degree to which DRAM banks are busy is called DRAM Bank-Level Parallelism (BLP). This paper proposes two new cost-effective mechanisms to maximize DRAM BLP. BLP-Aware Prefetch Issue (BAPI) issues prefetches into the on-chip Miss Status Holding Registers (MSHRs) associated with each core in a multi-core system such that the requests can be serviced in parallel in different DRAM banks. BLP-Preserving Multi-core Request Issue (BPMRI) does the actual loading of the DRAM controller's request buffers so that requests from the same core can be serviced in parallel, minimizing the serialization of each core's concurrent requests. When combined, BAPI and BPMRI improve system performance by 11.7% on a 4-core CMP system for a wide variety of multiprogrammed workloads. BAPI and BPMRI also complement various existing DRAM scheduling and prefetching algorithms, and can be used in conjunction with them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Modern DRAM chips consist of multiple banks that can be accessed independently. Requests to different DRAM banks can proceed concurrently. As a result, their access latencies can be overlapped and DRAM throughput can improve leading to high system performance. The notion of servicing multiple requests in parallel in different DRAM banks is called DRAM Bank-Level Parallelism (BLP).</p><p>Many sophisticated performance improvement techniques such as prefetching, out-of-order execution, and runahead execution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> are designed to amortize the cost of long memory latencies by generating multiple outstanding memory requests with the hope of exploiting Memory-Level Parallelism (MLP) <ref type="bibr" target="#b7">[8]</ref>. The effectiveness of these techniques critically depends on whether the application's outstanding memory requests are actually serviced in parallel in different DRAM banks. If the requests in the DRAM controller's buffers (which we call DRAM request buffers) are NOT to different banks, the amount of BLP exploited will be very low, thereby reducing the effectiveness of such techniques.</p><p>This paper shows that the issue policy of memory requests into Miss Status Holding Registers (MSHRs) and DRAM request buffers significantly affects the level of BLP exploited by show that the proposed techniques complement parallelism-and prefetch-aware DRAM scheduling policies. Figure <ref type="figure" target="#fig_0">1</ref> illustrates our baseline CMP system design that consists of multiple cores and multiple DRAM channels. Each core has its own hardware prefetcher that monitors its L2 demand access stream to generate prefetch requests. Once generated, prefetch requests are buffered in a FIFO (First-In First-Out) buffer which we call the prefetch request buffer. This buffer is similar to the prefetch buffer for the L1 cache in the Intel Core processor <ref type="bibr" target="#b2">[3]</ref>. 1 The oldest prefetch in the prefetch request buffer is chosen to be sent to the MSHR allocator every processor cycle. The MSHR allocator decides whether an L1 instruction/data miss or a prefetch request is allocated. It prioritizes demands over prefetches since delaying the service of demands can hurt performance. Before an MSHR entry is allocated, all existing MSHRs are searched for a matching entry. If no matching entry is found, a free MSHR entry is allocated and the request is sent to the L2 access buffer. If this request is a prefetch, it is invalidated from the prefetch request buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline CMP Memory System Design</head><note type="other">Prefetcher</note><p>When an L2 access (either a demand or prefetch) turns out to be an L2 miss, it is sent to the L2 miss buffer where it waits until it is sent to the DRAM request buffer in the corresponding DRAM channel. The destination (i.e., which DRAM request buffer in which DRAM channel) is predetermined based on the physical address of the request. L2 miss requests in the L2 access buffer from each core contend for DRAM request buffers. The L2-to-DRAM Controller (L2-to-DC) request issuer performs this arbitration in a round-robin fashion among cores. The DRAM controller in each DRAM channel examines all requests in its DRAM request buffers and decides which request will be serviced by the DRAM system. Once a request is serviced, the data from DRAM is sent to the L2 cache through the L2 fill buffer and the corresponding MSHR entry is freed.</p><p>The total number of outstanding demand/prefetch requests in the system cannot be more than the total number of MSHR entries. Also, the size of the DRAM request buffers limit the scope of the DRAM controller for DRAM access scheduling. Therefore, the order we send requests to the MSHRs and DRAM request buffers can significantly affect the amount of DRAM BLP exploited by the DRAM controller. In this paper, we focus on the issue policies for entrance into these two buffers. The parts of the baseline design we modify are highlighted in Figure <ref type="figure" target="#fig_0">1</ref>. 1 The FIFO queue in Intel's processor sends the oldest request to the L1 cache as long as a Fill Buffer entry (MSHR entry) is available. If the prefetch FIFO is full in Intel Core, a new prefetch overrides the oldest prefetch. Our prefetch request buffer is also a FIFO, but is connected to the L2 cache (instead of L1) and does not allow a new prefetch to override the oldest one (instead we just stall the prefetcher) since we found that overriding hurts performance by removing many useful prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hardware Prefetchers</head><p>A hardware prefetcher speculates on an application's memory access patterns and sends memory requests to the memory system earlier than the application demands the data. If done well, the prefetched data is installed in the cache and future demand accesses that would have otherwise missed now hit in the cache. If the prefetcher generates a large number of useful prefetches, then significant performance improvement can be achieved.</p><p>We use a stream prefetcher <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref> similar to the one in IBM's POWER 4 <ref type="bibr" target="#b24">[25]</ref> for most of our experiments. Stream prefetchers are commonly used in many processors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9]</ref> since they do not require significant hardware cost and work well for many applications. Our implementation of the stream prefetcher is bestperforming on average among a variety of prefetchers we evaluated for the 55 SPEC CPU 2000/2006 benchmarks. We also evaluate our mechanisms with other prefetchers in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prefetching: Increasing Potential for DRAM Bank-Level Parallelism</head><p>Hardware prefetchers can increase the potential for DRAM BLP because they generate multiple memory requests within a short period of time. With prefetching enabled, demand requests and potential future requests (useful prefetches) are both in the memory system at the same time. This increase in concurrent requests provides more potential to exploit DRAM BLP as shown in the following example.</p><p>Figure <ref type="figure" target="#fig_2">2</ref>(a) shows a code example from libquantum where a significant number of useful prefetches are generated by the stream prefetcher. For ease of understanding, we abstract away many details of the DRAM system (also for Figures <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_5">5</ref>). Figure <ref type="figure" target="#fig_2">2</ref>(b) shows the memory accesses generated when the code is executed both with and without a prefetcher. We assume that the first two accesses (to cache line addresses A, and A+1) are mapped to the same DRAM bank and that the two subsequent accesses (to A+2, and A+3) are mapped to a different bank.  Figure <ref type="figure" target="#fig_2">2</ref>(c) shows the DRAM service time when the code is executed without prefetching. Due to the lookahead provided by the processor's instruction window, accesses to A+1 and A+2 are slightly overlapped. On the other hand, with the prefetcher enabled, if the prefetches reach the memory system (DRAM request buffers) quickly such that the DRAM controller can see all these requests, the DRAM service time of the prefetches significantly overlap as shown in Figure <ref type="figure" target="#fig_2">2(d)</ref>. Therefore, overall DRAM service time is significantly improved compared to no prefetching (shown as "Saved cycles" in the figure). As shown in the example, a hardware prefetcher can increase the potential for improving DRAM bank-level parallelism. However, we found that this potential is NOT always fully exposed to the DRAM system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">What Can Limit Prefetching's Benefits?</head><p>If an on-chip memory system design does not take DRAM BLP into account, it may limit the benefits of prefetching. For example, the FIFO buffer (which we call prefetch request buffer) in the Intel Core design <ref type="bibr" target="#b2">[3]</ref> buffers prefetch requests until they can be sent to the memory system. This FIFO structure will always send the oldest prefetch request to the memory system provided that the memory system has room for an additional request. This design choice can limit the amount of DRAM BLP exploited when servicing the prefetch requests since the oldest request in the buffer is always sent first regardless of whether or not it can be serviced in parallel with other requests. A more intelligent policy would consider DRAM BLP when sending prefetch requests to the memory system.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> illustrates this problem. Figure <ref type="figure" target="#fig_3">3</ref>(a) shows the initial state of the prefetch request buffer, MSHRs (three entries), and DRAM request buffers (three entries per DRAM bank). There is only one outstanding demand request (request 1 in the figure). This request is mapped to bank 0 and just about to be scheduled to access DRAM. There are five prefetches in the prefetch request buffer. The first two prefetches will access DRAM bank 0 and the three subsequent prefetches will access DRAM bank 1. For this example we assume that all the prefetches are useful and therefore will be required by the program soon.  shows the DRAM service timeline when prefetches are issued into MSHRs in a FIFO fashion. In this case, the demand request and the two prefetch requests to bank 0 fill up the MSHRs and therefore the first prefetch to bank 1 will not be issued until the demand request gets serviced by DRAM and its MSHR entry is freed. As a result, BLP is low.</p><p>A DRAM BLP-aware issue policy would send a prefetch to bank 1 first, followed by a prefetch to bank 0. In other words, we can alternately issue prefetches to bank 1 and bank 0. Using this issue policy, the service of prefetches to bank 1 can start earlier and overlap with accesses to bank 0 as shown in Figure <ref type="figure" target="#fig_3">3(c</ref>). Therefore, BLP increases and overall DRAM service time improves (shown as "Saved cycles" in the figure ).</p><p>This example provides two insights. First, simply increasing the number of outstanding requests in the memory system does not necessarily mean that their latencies will overlap. A BLPunaware prefetch issue policy (to MSHRs) can severely limit the BLP exploited by the DRAM controller. Second, a simple prefetch issue policy that is aware of which bank a memory request will access can improve DRAM service time by prioritizing prefetches to different banks over prefetches to the same bank.</p><p>So far we assumed that all prefetches are useful. However, if prefetches are useless, the BLP-aware prefetch issue policy will not be helpful. It may increase DRAM throughput but only for useless requests. We address this issue in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MECHANISM</head><p>Our mechanism consists of two techniques to increase DRAM BLP. One is BLP-Aware Prefetch Issue (BAPI) which attempts to increase BLP for prefetch requests on each core. The other is BLP-Preserving Multi-core Request Issue (BPMRI) which tries to minimize the destructive interference in the BLP of each application when multiple applications run together on a CMP system. We describe these two techniques in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BLP-Aware Prefetch Issue</head><p>BLP-Aware Prefetch Issue (BAPI) tries to send prefetches from the prefetch request buffer to the MSHRs such that the number of different DRAM banks the requests access is maximized rather than sending the prefetches based on FIFO order. To achieve this, the FIFO prefetch request buffer is modified into the structures shown in Figure <ref type="figure" target="#fig_4">4</ref>. Instead of having one unified FIFO buffer for buffering new prefetch requests before they enter MSHRs, BAPI contains multiple FIFOs (one per DRAM bank) that buffer new prefetch requests. However, to keep the number of supported new prefetch requests the same as the baseline and also to minimize the total storage cost dedicated to prefetch requests, we use multiple index buffers (one per DRAM bank) and a single, unified prefetch request storage structure. An index buffer stores indexes (i.e., pointers) into the prefetch request storage structure. The prefetch request storage structure is a regular memory array that stores prefetch addresses generated by the prefetcher. Last, there is a free list that keeps track of free indexes in the prefetch request storage structure. The index buffers and free list are all FIFO buffers and all of the buffers have the same number of entries as the baseline unified FIFO.  When the prefetcher generates a request, the free list is consulted. If a free index exists, the request address is inserted into the prefetch request storage structure at the index allocated to it. At the same time, that index is also inserted into the appropriate index buffer corresponding to the bank the prefetch is mapped to. BAPI selects one index among the oldest indexes from each index buffer every processor cycle. Then, the corresponding prefetch request (i.e., prefetch address) is obtained from the prefetch request storage and sent to the MSHR allocator. If the MSHR allocator successfully allocates an entry for the prefetch request, the selected index is inserted into the free list and also removed from the index buffer.</p><p>Prefetch Issue Policy: BAPI, shown in Figure <ref type="figure" target="#fig_4">4</ref>, decides which prefetch to send to the MSHR allocator among the prefetch indexes from each index buffer. It makes its decision based on the DRAM BLP currently exposed in the memory system. To monitor the DRAM BLP of requests, the processor keeps track of the number of outstanding requests (both demands and prefetches) in the MSHRs separately for each DRAM bank. To accomplish this, we use a counter for each DRAM bank, called MSHR bank occupancy counter, which keeps track of how many requests to that bank are currently present in the MSHRs. When a demand/prefetch request is allocated an MSHR entry, its corresponding bank occupancy counter is incremented. When a request is serviced and its MSHR is freed, the corresponding bank occupancy counter is decremented.</p><p>The key idea of BAPI is to select the next prefetch to place into the MSHRs by examining MSHR bank occupancy counters such that the selected request improves the potential DRAM BLP. To do so, one would choose a prefetch request to the bank whose MSHR bank occupancy counter is the smallest. However, we found that this policy alone is not enough to expose more BLP to the DRAM controller for all applications. There are a large number of applications for which a prefetcher generates many prefetches to just a single bank but almost no prefetches to the other banks during a phase of execution (especially for streaming applications). For such applications, the issue policy based on MSHR occupancy alone still ends up filling the MSHRs with requests to only one bank. This results in two problems. First, it results in no BLP improvement because the prefetches/demands to other banks that are soon generated cannot be sent to the memory system because the MSHRs are already full. Second, the MSHRs can be filled up with prefetches and thus demands that need MSHR entries can be delayed.</p><p>To prevent this problem, BAPI uses a threshold, pref etch send threshold to limit the maximum number of requests to a single bank that can be outstanding in the MSHRs. This policy reserves room in the MSHRs for requests to other banks when most requests being generated are biased to just a few banks. Because many applications exploit row buffer locality in DRAM banks (since the access latency to the same row accessed last time is relatively low), having too low a threshold can hurt performance by preventing many of the useful prefetches to the same row from being row-hits (because the row may be closed before the remaining prefetch requests arrive). On the other hand, having too high a threshold will result in no BLP improvement as the MSHRs may get filled with accesses to only a few banks. Therefore, balancing the threshold is important for high performance. We empirically found that a value of 27 (when the total number of MSHR entries is 32) for pref etch send threshold provides a good trade-off for SPEC benchmarks by exploiting BLP without constraining the row-buffer locality of requests.</p><p>Rule 1 summarizes our prefetch issue policy to MSHRs.</p><p>Rule 1 BLP-Aware Prefetch Issue policy (BAPI) for each issue cycle do 1. Make the oldest prefetch to each bank valid only if the corresponding M SHR bank occupancy counter value is less than pref etch send threshold.</p><p>2. Among those valid prefetches, select the request to the bank whose M SHR bank occupancy counter value is least. end for Adaptive thresholding based on prefetch accuracy estimation: Prefetching may not work well for all applications or all phases of a single application. In such cases, performance improvement is low (or may even degrade) since useless prefetches will eventually be serviced, resulting in artificially high BLP and wasted DRAM bandwidth. A good prefetch issue policy should prohibit or allow sending prefetches to the memory system based on how accurate the prefetcher is. Our BLP-aware adaptive prefetch issue policy does exactly that: it limits the number of prefetches allowed in the MSHRs by dynamically adjusting pref etch send threshold based on the run-time accuracy of the prefetcher. This naturally limits the number of prefetches sent to memory when prefetch accuracy is low. This improves performance for two main reasons: 1) It reserves more room in the MSHRs for demands, thereby reducing contention between demand requests and useless prefetches and 2) It effectively stalls the prefetcher from generating more useless prefetches since the prefetch request buffer will quickly become full.</p><p>To implement this, we need to measure the run-time accuracy of the prefetcher <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. Therefore, we add an extra prefetch bit per L2 cache line and MSHR entry which indicates whether a line was brought in by a prefetch. Using this bit, each core keeps track of the number of useful prefetches in a counter, useful prefetch counter. It also counts the total number of prefetches in another counter, prefetch sent counter. Each core's prefetch accuracy is calculated by division of its two counters and the result is stored in its prefetch accuracy register. The two counters are reset over predetermined intervals so that the accuracy measurement adapts to the phases of an application.</p><p>BAPI dynamically adjusts prefetch send threshold for each core based on the estimated prefetch accuracy. If the estimated accuracy is very low for an interval, a low prefetch send threshold value is used which severely limits the number of useless prefetches sent to each bank. We empirically found that three levels of prefetch send threshold work well for SPEC workloads. The threshold values used in our system are shown in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BLP Preserving Multi-Core Request Issue</head><p>BLP-Aware Prefetch Issue (BAPI) increases the potential of DRAM BLP for individual applications on each core. In order for the DRAM controller to exploit this potential, the increased BLP should be exposed to the DRAM request buffers. However, in CMP systems, multiple cores share parts of the on-chip memory system. In our CMP system of Figure <ref type="figure" target="#fig_0">1</ref>, the structures above the L2 miss buffers are shared by all cores. Therefore, requests from different cores contend for the shared DRAM request buffers in the DRAM controller. Due to this contention, a BLP-unaware L2-to-DRAM Controller (L2-to-DC) request issue policy can destroy the BLP of an individual application.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> describes this problem. Figure <ref type="figure" target="#fig_5">5</ref>(a) shows the initial state of the L2 miss buffers of two cores (A and B) and the DRAM request buffers for two banks. Each core has potential to benefit from BLP in that one request of each core goes to bank 0 and the other goes to bank 1. The L2-to-DC request issuer chooses a single request from the L2 miss buffers to be placed in the corresponding DRAM request buffer every cycle.  Motivation: Existing systems use a round-robin policy in the L2-to-DC request issuer. Each cycle, a request from a different core is issued into DRAM request buffers and the cores are prioritized in a round-robin order. If such a policy is used as shown in Figure <ref type="figure" target="#fig_5">5</ref>(b), core A's request to bank 0 is sent to the DRAM request buffers the first cycle and core B's request to bank 1 is sent the next cycle. The DRAM controller (based on the first-come first-served principle used in many existing systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>) would service these requests (A0 and B1) from different cores concurrently because they are the oldest in each DRAM bank request buffer. This results in the destruction of the BLP potential of each core because requests from the same core are serviced serially instead of in parallel. Hence, the full latency of each request is exposed to each core and therefore each core stalls for approximately two DRAM bank access latencies.</p><p>On the other hand, a BLP-preserving L2-to-DC request issue policy would send all the requests from one core first as shown in Figure <ref type="figure" target="#fig_5">5(c</ref>). Therefore, the DRAM controller will service core A's requests (A0 and A1) concurrently since they are the oldest in each bank. The requests from core B will also be serviced in parallel, after A's requests are complete. In this case, the BLP potential of each core is realized by the DRAM controller. The service of core A's requests finishes much earlier compared to the round-robin policy because core A's requests are overlapped. Core A stalls for approximately a single DRAM bank access latency instead of two and core B's stall time does not change much. Therefore, overall system performance improves because core A can make faster progress instead of stalling.</p><p>This example shows that a round-robin-based L2-to-DC request issue policy can destroy the BLP within an application by consecutively placing requests from different cores into the DRAM request buffers. As such, the DRAM controller may not be able to exploit the BLP potential of each application, which ultimately results in performance degradation. To ensure that each application makes fast progress with its DRAM requests serviced in parallel instead of serially, the L2-to-DC request issuer should preserve the BLP of requests from each core.</p><p>Mechanism: BLP Preserving Multi-core Request Issue (BPMRI) tries to minimize the destructive interference in the BLP of each application on a CMP system. The basic idea is to consecutively send many memory requests from one core to the DRAM request buffers so that the BLP of that core (or application) can be preserved in the DRAM request buffers for DRAM scheduling. If requests from a single core arrive consecutively (back-to-back) into the DRAM request buffers, they will be serviced concurrently as long as the requests span multiple DRAM banks, thereby preserving the BLP within the individual application. Note that our first technique, BAPI, already increases the likelihood that outstanding memory requests of a core are to different banks; hence, BAPI and BPMRI are synergistic.</p><p>BPMRI continues issuing memory requests from a single core into DRAM request buffers until the number of consecutive requests sent reaches a threshold, request send threshold, or there are no more requests in that core's L2 miss buffer. When this termination condition is met, BPMRI chooses another core and repeats the process. BPMRI selects the next core based on how memory intensive each application is. It prioritizes the core (application) that is the least memory intensive. To do this, BPMRI monitors the number of requests that come into the L2 miss buffer during predetermined intervals using a counter, L2 miss counter, for each core. At the start of an interval, BPMRI ranks each core based on the accumulated L2 miss counters (computed during the previous interval) and records the rank in a register, rank register, for each core. The core with the lowest value in its L2 miss counter is ranked the highest. The rank determined for each core is used to select the next core (upon meeting a termination condition) during that interval. The L2 miss counters are reset each interval to adapt to the phase behavior of applications. Rule 2 summarizes the BPMRI policy. We choose to limit the maximum number of consecutive requests sent and also choose to prioritize memory non-intensive applications since an uncontrolled "one core-first policy" can lead to the starvation of memory non-intensive applications. If a memory-intensive application continuously generates many requests, once those requests start to be issued into the DRAM request buffers, requests from other applications may not get a chance to enter the DRAM request buffers. Limiting the maximum number of requests consecutively sent from a single core alleviates this problem. In addition, the performance impact of delaying requests from a memory non-intensive application is more significant than delaying requests from a memory-intensive application. Therefore, prioritizing requests from memory nonintensive applications (ranking) leads to better overall system performance. Note that this approach is similar to the shortestjob-first policy in that it prioritizes shorter jobs (memory nonintensive cores that spend less time in the memory system) from the point of view of the memory system. The shortest-job-first policy was shown to lead to optimal system throughput <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Model</head><p>We use a cycle accurate x86 CMP simulator for our evaluations. Our simulator faithfully models all microarchitectural details such as bank conflicts, port contention, and buffer/queuing delays. The baseline on-chip memory system is modeled as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The baseline configuration of each core is shown in Table <ref type="table" target="#tab_4">1</ref> and the shared resource configuration for single, 4, and 8-core systems is shown in Table <ref type="table" target="#tab_5">2</ref>. Our simulator also models a DDR3 DRAM system in detail and Table <ref type="table" target="#tab_6">3</ref> shows the DDR3 DRAM timing specifications used for our evaluations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>To measure CMP system performance, we use Individual Speedup (IS), Weighted Speedup (WS) <ref type="bibr" target="#b21">[22]</ref>, and Harmonic mean of Speedups (HS) <ref type="bibr" target="#b12">[13]</ref>. WS corresponds to system throughput and HS corresponds to the inverse of job turnaround time <ref type="bibr" target="#b5">[6]</ref>. In the following equations, N is the number of cores in the CMP system. IP C alone i is the IPC when application i runs alone on one core of the CMP system (other cores are idle). IP C together i is the IPC when application i runs on one core and other applications run on the other cores of the CMP system.</p><formula xml:id="formula_0">ISi = IP C together i IP C alone i , W S = N X i IP C together i IP C alone i , HS = N N X i IP C alone i IP C together i</formula><p>To evaluate how our mechanisms improve the performance of prefetching, we define prefetch-related metrics. Bus traffic is the number of cache lines transferred over the bus during the  We define DRAM BLP as the average number of DRAM banks which are busy (servicing a request) when at least one bank is busy. To evaluate the effect of DRAM throughput improvement on each core, we define instruction window Stall cycles Per Load instruction (SPL) which indicates on average how much time the processor spends idly waiting for DRAM service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SP L =</head><p>T otal number of window stall cycles T otal number of load instructions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Workloads</head><p>We use the SPEC CPU 2000/2006 benchmarks for experimental evaluation. Each benchmark was compiled using ICC (Intel C Compiler) or IFORT (Intel Fortran Compiler) with the -O3 option. We ran each benchmark with the reference input set for 200 million x86 instructions selected by Pinpoints <ref type="bibr" target="#b18">[19]</ref> as a representative portion of each benchmark.</p><p>The characteristics of the 14 most memory-intensive SPEC benchmarks with and without a stream prefetcher are shown in Table <ref type="table" target="#tab_8">4</ref>. To evaluate our mechanism on CMP systems, we formed combinations of multiprogrammed workloads from all the 55 SPEC 2000/2006 benchmarks. We ran 30 and 15 pseudorandomly chosen workload combinations<ref type="foot" target="#foot_2">3</ref> for our 4 and 8-core CMP configurations respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation and Hardware Cost</head><p>For evaluations of BAPI, we use pref etch send threshold values based on the run-time prefetcher accuracy as shown in Table 5. We use a value of 10 for request send threshold for BPMRI. The estimation of prefetch accuracy and rank recording is performed every 100K processor cycles. These values were empirically determined by simulations. Table <ref type="table">5</ref>: Dynamic pref etch send threshold values for BAPI Table <ref type="table" target="#tab_10">6</ref> shows the storage cost for our implementation of BAPI and BPMRI. The total storage cost for the 4-core system described in Tables <ref type="table" target="#tab_4">1</ref> and<ref type="table" target="#tab_5">2</ref> is 94,440 bits (?11.5KB), which is equivalent to only 0.6% of the L2 cache data storage. Note that the additional FIFOs (for index buffers and free lists) and prefetch bits account for 99% of the total storage. FIFOs are made of regular memory arrays and index registers (pointers to the head/tail) and therefore the actual design cost/effort is not expensive. Prefetch bits are already used in many proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref> to indicate whether or not a cache line (or request) was brought in (or made) by the prefetcher.</p><p>None of the issuing logic for BAPI or BPMRI is on the critical path of execution. Therefore, we believe that our mechanism is easy to implement with low design cost/effort.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-Core Results</head><p>We evaluate BLP-Aware Prefetch Issue (BAPI) in this section. Recall that BAPI aims to increase the BLP potential of a single application whether the application is running alone on a single core machine or running together with other applications on a CMP system. To eliminate the effects of inter-application interference, we first evaluate BAPI on our single core system.</p><p>Figures <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_10">7</ref> show IPC, DRAM BLP, stall cycles per load instruction (SPL), and bus traffic for the 14 most memory intensive benchmarks when we use 1) no prefetching, 2) the baseline with stream prefetching (using the FIFO prefetch issue policy), 3) BAPI with a static threshold (BAPI-static), and 4) BAPI (with adaptive thresholding; BAPI-dynamic or simply BAPI). BAPI-static uses a single constant value for prefetch send threshold which is set to 27 empirically, whereas BAPI-dynamic varies this threshold based on the accuracy of the prefetcher (as shown in Table <ref type="table">5</ref>). IPC is normalized to prefetching with the baseline issue policies.</p><p>On average, BAPI-dynamic improves performance over the baseline by 8.5%. This improvement is due to two major factors: 1) increased DRAM BLP of prefetches in phases where the prefetcher works well, and 2) limiting the issue of prefetches for applications or phases where the prefetcher is inaccurate. These two factors are analyzed in detail below.</p><p>Analysis: Both BAPI-static and dynamic improve performance for the nine leftmost benchmarks shown in Figure <ref type="figure" target="#fig_8">6(a)</ref>. These benchmarks are all prefetch friendly as can be seen in Figure <ref type="figure" target="#fig_10">7</ref>: most of the prefetches are useful (high prefetch accu-  racy) and these useful prefetches cover a majority of the total bus traffic (high prefetch coverage). BAPI increases performance over baseline prefetching by exposing more DRAM BLP of prefetches to the DRAM controller. As shown in Figure <ref type="figure" target="#fig_8">6</ref>(b), BAPI increases BLP for these nine applications and therefore improves DRAM throughput. This leads to significant reductions in stall cycles per load (SPL) as shown in Figure <ref type="figure" target="#fig_8">6(c)</ref>. DRAM throughput improvement also leads to high prefetch coverage. Since MSHR entries are freed sooner due to better DRAM throughput, more prefetches are able to enter the memory system which improves prefetcher coverage. This is best illustrated by the increase in useful prefetches with BAPI for swim and lbm as shown in Figure <ref type="figure" target="#fig_10">7</ref>.</p><p>Note that for lbm, baseline prefetching with FIFO issue degrades DRAM BLP while improving performance by 10.9% compared to no prefetching. Ibm consists of multiple sequential memory access streams in a loop and therefore it exploits DRAM BLP even without prefetching. The stream prefetcher is beneficial by bringing in many cache lines earlier than needed; hence, it improves performance. However, this is done in a BLP inefficient way due to the FIFO prefetch issue policy as described in Section 2.1. In other words, the FIFO prefetch issue policy significantly limits the DRAM BLP potential for lbm by filling up the MSHRs with prefetch requests that span just a few banks even though there are many younger prefetches to other free DRAM banks waiting in the prefetch request buffer. As a result, the prefetcher's performance improvement is relatively small compared to the other prefetch friendly benchmarks. BAPI mitigates this problem by prioritizing prefetches to different banks, thereby improving DRAM BLP by 15.1% and overall performance by 27.9% compared to the FIFO issue policy.  Adaptivity to Usefulness of Prefetches: On the other hand, for the five rightmost benchmarks, BAPI-static does not improve performance over the baseline. As shown in Figure <ref type="figure" target="#fig_10">7</ref>, the stream prefetcher does not work well for these benchmarks: it generates a large number of useless prefetches which unnecessarily consume on-chip buffer/cache resources and DRAM bandwidth. As shown in Figure <ref type="figure" target="#fig_8">6</ref>(a), prefetching degrades performance for galgel, art, and milc. BAPI-static does not help these benchmarks either since the useless prefetches are still serviced.</p><p>In fact, for galgel, art, and milc, BAPI-static increases the number of useless prefetches due to increased DRAM throughput as shown in Figure <ref type="figure" target="#fig_10">7</ref>. Thus, BLP-aware prefetch issue alone does not help performance when prefetch accuracy is low.</p><p>BAPI-dynamic alleviates the problem of useless prefetches by limiting the number of prefetches issued into the MSHRs when the prefetcher generates a large number of useless prefetches. As a result, MSHR entries do not quickly fill up with useless prefetches and thus can be used by demand requests. This mechanism causes the prefetch request buffer to fill up, thereby stalling the prefetcher. As shown in Figure <ref type="figure" target="#fig_10">7</ref>, BAPI-dynamic eliminates a large number of useless prefetches and reduces total bus traffic by 5.2% on average. BAPI-dynamic almost recovers the performance loss due to useless prefetches for galgel and art, and improves performance for both milc and omnetpp by 6.6%.</p><p>Adaptivity to Phase Behavior: BAPI-dynamic adapts to the phase behavior of lucas, leslie3d, soplex, GemsFDTD, and lbm.</p><p>While most of the time the prefetcher generates useful requests, in certain phases of these applications it generates many useless prefetches. BAPI-dynamic improves performance for these benchmarks by adaptively adjusting pref etch send threshold which removes many of the useless prefetches while keeping the useful ones as shown in Figure <ref type="figure" target="#fig_10">7</ref>.</p><p>We conclude that BAPI significantly improves performance (by 8.5%) by increasing DRAM BLP (by 11.7%) while also reducing memory bus traffic (by 5.2%) in the single-core system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Sensitivity to MSHR Size</head><p>Thus far we have assumed that each core has a limited number of MSHR entries (32) because MSHRs are not scalable since they require associative search <ref type="bibr" target="#b25">[26]</ref>. In this section, we study the effect of our techniques with various MSHR sizes. We varied the total number of MSHR entries from 8 to 256 and measured the average IPC (gmean) for the 14 most memory-intensive benchmarks as shown in Table <ref type="table" target="#tab_12">7</ref>. To isolate the effect of limited MSHRs, we assume that there is an unlimited number of DRAM request buffer entries for this experiment (this is why the IPC improvement of BAPI with a 32-entry MSHR is different from that shown in Section 5.1). The values of pref etch send threshold are empirically determined for both BAPI-static and BAPI separately for each MSHR size to provide the best performance.  We make three major observations. First, as the number of MSHR entries increases, the performance of baseline prefetching increases since more BLP is exposed in DRAM request buffers.   The performance improvement saturates at 128 entries because the DRAM system itself becomes the performance bottleneck when a high level of BLP is exposed. In fact, increasing the MSHR size from 128 to 256 entries slightly degrades performance because more useless prefetches of some applications (especially, art and milc) enter the memory system (due to the large number of MSHR entries) causing interference with demand requests both in DRAM and in caches.</p><p>Second, both BAPI-static and BAPI (with dynamic thresholding) continue to improve performance up to 64-entry MSHRs since they expose more BLP of prefetches to DRAM request buffers. Even though BAPI-static's performance saturates at 64 MSHR entries, BAPI improves performance with 128 and 256-entry MSHRs because it continues to expose higher levels of useful BLP without filling the memory system with useless prefetches. Its ability to adaptively expose useful BLP to the memory system and thereby more efficiently utilize the MSHR entries makes BAPI best-performing regardless of MSHR size.</p><p>Finally, BAPI with a smaller MSHR achieves the benefits of a significantly larger MSHR without the associated cost of building one: BAPI with 32-entry MSHRs performs as well as the baseline with 128-entry MSHRs. Similarly, BAPI with 16entry MSHRs performs within 1% of the baseline with 64-entry MSHRs. Note that BAPI requires very simple extra logic and FIFO structures (?2KB storage cost for the single-core system) whereas increasing the number of MSHR entries is more costly in terms of both latency and area due to two reasons <ref type="bibr" target="#b25">[26]</ref>: 1) MSHRs require associative search, 2) MSHRs require the storage of cache line data. We conclude that BAPI is a cost-effective mechanism that efficiently uses MSHRs and therefore provides higher levels of BLP without the cost of large MSHRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Core Results</head><p>In this section, we evaluate BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI) when employed together in CMP systems. To provide insight into how our mechanisms work, we begin with a case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Case Study on the 4-Core System</head><p>We evaluate a workload consisting of four prefetch-friendly (high prefetch accuracy and coverage) applications to show how our mechanisms further improve the benefits of prefetching and thus system performance by improving and preserving DRAM BLP. Figure <ref type="figure" target="#fig_12">8</ref> shows performance metrics when libquantum, lucas, soplex, and GemsFDTD run together on the 4-core system.</p><p>As shown in Figure <ref type="figure" target="#fig_12">8</ref>(c), prefetching with the baseline issue policies (FIFO prefetch issue and round-robin L2-to-DC request issue) improves WS by 23.5% compared to no prefetching. This increase is due to the performance improvement of libquantum, soplex, and GemsFDTD. The performance of lucas actually degrades even though baseline prefetching improves performance for lucas on the single-core system. There are two reasons for this. First, the baseline round-robin L2-to-DC issue policy destroys the BLP of requests for lucas the most among the four applications. Since lucas is the least memory intensive (as shown in Table <ref type="table" target="#tab_8">4</ref>) of the four applications, the issue of lucas's requests to DRAM request buffers is relatively infrequent compared to the others. As a result, 1) lucas's requests starve behind more intensive applications' requests in the L2 miss buffer and 2) lucas's BLP is more easily destroyed because requests from other applications intervene between lucas's requests when a round-robin issue policy is used. Second, although amenable to prefetching in general, the prefetch accuracy of lucas is not as good compared to the other applications, and therefore lucas suffers the most from useless prefetches (as shown in Section 5.1).</p><p>BPMRI alleviates the first problem as shown in Figures <ref type="figure" target="#fig_12">8(a</ref>) and (b). BPMRI ranks lucas's requests highest because lucas is the least memory intensive application among the four. Whenever BPMRI needs to choose the next core to issue requests from, lucas gets prioritized and its requests are issued consecutively into the DRAM request buffers. Therefore, lucas's starvation is mitigated and its BLP is preserved. BPMRI regains the performance lost due to baseline prefetching as shown in Figure <ref type="figure" target="#fig_12">8(a)</ref>. BPMRI also significantly improves the performance of the other three benchmarks by preserving the BLP of each application, thereby improving WS and HS by 12.0% and 11.3% respectively compared to the baseline.</p><p>BAPI mitigates the second problem of lucas. As discussed in Section 5.1, BAPI adapts to the phase behavior of lucas: when the prefetcher generates many useless prefetches, BAPI limits the issue of prefetches thereby reducing many of the negative effects of prefetching. On the other hand, BAPI exposes more BLP of prefetches to the memory system when the prefetcher is accurate. Therefore, BAPI increases performance for lucas as well as the other three applications, improving WS and HS by 9.4% and 7.9% compared to baseline prefetching.</p><p>When BPMRI and BAPI are combined, the performance of each application further improves as each application's SPL is reduced as shown in Figure <ref type="figure" target="#fig_12">8(b)</ref>. BAPI increases each application's BLP potential and BPMRI preserves this BLP thereby allowing the DRAM controller to exploit it. As a result, WS and HS improve by 19.4% and 17.4% respectively compared to the baseline BLP-unaware request issue policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Overall Performance on the 4-Core System</head><p>Figure <ref type="figure" target="#fig_13">9</ref> shows the average system performance and bus traffic for all 30 workloads. When employed alone, BAPI improves average performance (WS) by 9.1%, BPMRI by 4.6% compared to the baseline. Combined together, BAPI and BPMRI improve WS and HS by 11.7% and 13.8% respectively, showing that the two techniques are complementary. Bus traffic is also reduced by 5.3%. The performance gain of the two mechanisms are due to 1) increased DRAM BLP provided by intelligent memory issue policies, 2) reduced waste in DRAM bandwidth and on-chip cache space due to limiting the number of useless prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Overall Performance on the 8-Core System</head><p>Figure <ref type="figure" target="#fig_15">10</ref> shows the average system performance and bus traffic for the 15 workloads we examined on the 8-core system. BAPI and BPMRI are still very effective and significantly improve sys-tem performance. Combined together, they improve WS and HS by 10.9% and 13.6%, while reducing bus traffic by 2.9%. In contrast to the 4-core system where BAPI alone provided higher performance than BPMRI alone, BPMRI alone improves performance more than BAPI alone. This is because as the number of cores increases, destructive interference in each application's BLP also increases, and reducing this interference becomes a lot more important.  We conclude that the proposed techniques are effective in terms of both performance and bandwidth-efficiency for a wide variety of multiprogrammed workloads on both 4-core and 8-core systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect on Other Prefetchers</head><p>We evaluate our mechanisms on two different types of prefetchers: GHB (Global History Buffer)-based CZone Delta Correlation (C/DC) <ref type="bibr" target="#b17">[18]</ref> and PC-based stride <ref type="bibr" target="#b0">[1]</ref>. Both the C/DC and stride prefetchers accurately capture a substantial number of memory accesses that are mapped to different DRAM banks, just as the stream prefetcher does. Therefore, BAPI and BPMRI improve system performance compared to the baseline (WS: 10.9% and 5.4%, for C/DC and stride respectively). Our techniques also reduce bus traffic by 4.7% and 2.9% for C/DC and stride respectively. To conclude, our proposal is effective for a variety of state-of-the-art prefetching algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with Parallelism-Aware Batch DRAM Scheduling</head><p>Parallelism-Aware Batch Scheduling (PAR-BS) <ref type="bibr" target="#b14">[15]</ref> aims to improve performance and fairness in DRAM request scheduling. It tries to service memory requests in the DRAM request buffers from the same core concurrently so that the DRAM BLP of each application is preserved in DRAM scheduling. Therefore the amount of BLP exploited by PAR-BS is limited by the number of requests to different banks in DRAM request buffers.</p><p>BAPI complements PAR-BS: it increases the number of prefetches to different banks and PAR-BS can exploit this increased level of BLP to improve performance further. BPMRI also complements PAR-BS even though their benefits partially overlap. If an application's requests to different banks are not all in the DRAM request buffers, PAR-BS cannot exploit the full BLP of each application. BPMRI, by consecutively issuing an application's requests from the L2 miss buffer to the DRAM request buffers, increases the probability that each application's requests to different banks are all in the DRAM request buffers. Hence, BPMRI increases the potential of each application's BLP that can be exploited by PAR-BS.</p><p>In addition, by consecutively issuing requests from a core back-to-back into the DRAM request buffers, BPMRI enables any DRAM controller to service those requests in parallel. Hence, a first-come-first-serve based DRAM controller combined with BPMRI can preserve each application's BLP without requiring the DRAM controller to be BLP-aware.  To verify this, we implemented PAR-BS tuned for best performance for our 4-core workloads. Figure <ref type="figure" target="#fig_19">12</ref> shows the performance of 1) baseline prefetching with the FR-FCFS DRAM scheduling policy which exploits rowbuffer locality <ref type="bibr" target="#b19">[20]</ref>, 2) PAR-BS, 3) BPMRI, 4) PAR-BS with BPMRI, 5) PAR-BS with BAPI, 6) PAR-BS with BAPI and BPMRI, and 7) BAPI and BPMRI. BPMRI's performance gain is equivalent to that of PAR-BS (with the round-robin L2-to-DC issue policy) since it successfully preserves the BLP of each application and makes the simple FR-FCFS DRAM scheduling policy behave similarly to PAR-BS. When combined with PAR-BS, BPMRI improves WS and HS by an additional 1.9% and 1.4% by better preserving the BLP of requests from each application. BAPI along with PAR-BS significantly improves the performance of PAR-BS (WS and HS improve by 7.1% and 7.3% respectively) because BAPI exposes more BLP potential of each application in the DRAM requests buffers for PAR-BS to exploit. To conclude, our mechanisms 1) complement PAR-BS, and 2) enable parallelism-unaware DRAM controllers to achieve similar performance as PAR-BS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with Prefetch-Aware DRAM Controllers</head><p>Prefetch-Aware DRAM Controllers (PADC) <ref type="bibr" target="#b11">[12]</ref> was proposed to maximize DRAM row buffer hits for useful requests (demands and useful prefetches). PADC also delays and drops useless prefetches to reduce waste in on-chip buffer resources and DRAM bandwidth. PADC aims to minimize DRAM latency of useful requests by prioritizing useful row-hit requests over others to the same bank. In other words, the main goal of PADC is to exploit row buffer locality in each bank in a useful manner. Our goal is orthogonal: BAPI and BPMRI aim to maximize DRAM bank-level parallelism so that more requests from an application can be serviced in different DRAM banks in parallel.  Figure <ref type="figure" target="#fig_21">13</ref> shows the performance of PADC alone and PADC combined with our mechanisms for the 4core workloads. PADC significantly improves WS and HS by 14.1% and 16.3% respectively compared to the baseline. When combined with PADC, BAPI and BPMRI improve WS and HS by 20.6% and 22.5%. We conclude that our mechanisms complement PADC and thus significantly improve system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DRAM Access Scheduling</head><p>A number of DRAM scheduling policies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed. Although these proposals have the similar goal of improving performance by increasing DRAM throughput, they do so by improving the DRAM controller's scheduling policy. Therefore, their scope is limited by the number and composition of requests in the DRAM request buffers. If the requests in the DRAM request buffers are not to different banks, BLP will be low regardless of the DRAM scheduling policy. Our mechanism solves this problem by issuing requests into on-chip buffers in a BLP-aware manner. It exposes more BLP to the DRAM scheduler, enabling it to provide higher DRAM throughput. As such, our techniques are orthogonal to DRAM scheduling policies. As shown in Sections 5.4 and 5.5, our mechanisms complement parallelism-aware and prefetch-aware DRAM scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Memory-Level Parallelism</head><p>Many proposals have explored increasing Memory-Level Parallelism (MLP) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref>. These works define MLP as the average number of outstanding memory requests when there is at least one outstanding request to memory. They implicitly assume that the DRAM latency of outstanding requests to memory will overlap. In contrast, we show that simply having a large number of outstanding requests does not necessarily mean that their latencies will overlap. In order to overlap, the requests should span multiple banks and be in the DRAM controller concurrently, which our mechanism enables. Hence, our proposal is orthogonal to and improves the effectiveness of techniques that improve MLP. As we quantitatively showed in this paper, our proposal provides significant benefits over two MLP-improving techniques, prefetching and out-of-order execution, by enabling them to better exploit BLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Prefetch Handling</head><p>Adaptive prefetch handling techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref> aim to reduce the interference between prefetch and demand requests in the memory system. In contrast, our work focuses on increasing and preserving the DRAM BLP of useful requests (demands and useful prefetches) and therefore is orthogonal to prefetch handling mechanisms. As we discuss in Section 5.5, our mechanisms are complementary to prefetch-aware DRAM controllers <ref type="bibr" target="#b11">[12]</ref> which employ an adaptive prefetch handling technique that is reported to outperform feedback-directed prefetching <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We showed that uncontrolled memory request issue policies to resource-limited on-chip buffers limit the level of DRAM banklevel parallelism (BLP) that can be exploited by the DRAM controller, thereby limiting system performance. To overcome this limitation, we proposed new cost-effective on-chip memory request issue mechanisms. Our evaluations show that the mechanisms 1) work synergistically and significantly improve both system performance and bandwidth-efficiency, 2) work well with various types of prefetchers, and 3) complement various DRAM scheduling policies. We conclude that our proposed mechanisms improve system performance and bandwidth efficiency for both single-core and multi-core systems and can be an enabler for achieving and enhancing the benefits of a multitude of techniques that are designed to exploit memory-level parallelism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CMP memory system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Demand to address x Pref x: Prefetch to address x Dem A, Pref A+1, Pref A+2, Pref A+3 Dem A, Dem A+1, Dem A+2, Dem A+3 With prefetcher (for reg-&gt;node[].state) Without prefetcher (for reg-&gt;node[].state) { for(i=0; i&lt;reg-&gt;size; i++) reg-&gt;node[i].state ^= ((MAX_UNSIGNED) 1 &lt;&lt; target); } (a) Code example (b) Memory access stream</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: How prefetching can increase DRAM BLP (libquantum)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FIFO vs. DRAM BLP-aware prefetch issue policyFigure 3(b)shows the DRAM service timeline when prefetches are issued into MSHRs in a FIFO fashion. In this case, the demand request and the two prefetch requests to bank 0 fill up the MSHRs and therefore the first prefetch to bank 1 will not be issued until the demand request gets serviced by DRAM and its MSHR entry is freed. As a result, BLP is low.A DRAM BLP-aware issue policy would send a prefetch to bank 1 first, followed by a prefetch to bank 0. In other words, we can alternately issue prefetches to bank 1 and bank 0. Using this issue policy, the service of prefetches to bank 1 can start earlier and overlap with accesses to bank 0 as shown in Figure3(c). Therefore, BLP increases and overall DRAM service time improves (shown as "Saved cycles" in the figure).This example provides two insights. First, simply increasing the number of outstanding requests in the memory system does not necessarily mean that their latencies will overlap. A BLPunaware prefetch issue policy (to MSHRs) can severely limit the BLP exploited by the DRAM controller. Second, a simple prefetch issue policy that is aware of which bank a memory request will access can improve DRAM service time by prioritizing prefetches to different banks over prefetches to the same bank.So far we assumed that all prefetches are useful. However, if prefetches are useless, the BLP-aware prefetch issue policy will not be helpful. It may increase DRAM throughput but only for useless requests. We address this issue in Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BLP-Aware Prefetch Issue</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Round-robin vs. BLP-preserving request issue policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Stall cycles per load instruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance, BLP, and SPL of BAPI on single-core system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Bus traffic of BAPI on single-core system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Case study on the 4-core system (libquantum, lucas, soplex, and GemsFDTD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance on 4-core CMP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance for 8-core CMP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: BAPI and BPMRI with stride and C/DC prefetchers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison with PAR-BS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison with PADC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>aware Saved cycles</head><label></label><figDesc></figDesc><table><row><cell cols="2">DRAM controller</cell><cell></cell><cell>Overlapped service time</cell></row><row><cell cols="3">Prefetch request buffer Bank 1 Bank 0 Bank 0 MSHRs DRAM request buffers 1 Bank 1</cell><cell cols="2">Prefetch issue order to MSHRs: 2, 3, 4, 5, 6 Time 2.Pref B0 3.Pref B0 4.Pref B1 5.Pref B1 1.Dem B0 6.Pref B1 } FIFO</cell></row><row><cell>1.Dem B0</cell><cell>6.Pref B1</cell><cell></cell><cell cols="2">(b) DRAM service time for FIFO prefetch issue</cell></row><row><cell></cell><cell>5.Pref B1</cell><cell></cell><cell>Overlapped service time</cell></row><row><cell></cell><cell>4.Pref B1 3.Pref B0 2.Pref B0</cell><cell>Bank 1 Bank 0</cell><cell>1. DemB0 2. PrefB0 3. PrefB0 5.Pref B1 6.Pref B1 4.Pref B1</cell><cell>} BLP-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prefetch issue order to MSHRs: 4, 2, 5, 3, 6</cell></row><row><cell cols="5">(a) Initial state of memory buffers (c) DRAM service time for DRAM BLP-aware prefetch issue</cell></row></table><note><p><p>Older</p>Dem Bx: Demand to DRAM bank x Pref Bx: Prefetch to DRAM bank x</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>storage Prefetch request generated Pref addr From prefetcher Free list Free index Index buffer BLP-aware prefetch issuer Index buffer MSHR bank occupancy Prefetch accuracy for bank N-1 for bank 0 Index selected</head><label></label><figDesc></figDesc><table><row><cell>Index</cell><cell>Index</cell><cell>Index</cell></row><row><cell>Index</cell><cell>Index</cell><cell>Index</cell></row><row><cell>Pref addr</cell><cell></cell><cell></cell></row><row><cell>Pref addr</cell><cell></cell><cell></cell></row><row><cell>Prefetch selected</cell><cell></cell><cell></cell></row><row><cell>MSHR allocator</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Rule 2 BLP-Preserving Multi-core Request Issue policy (BPMRI)A valid request is a request in a core's L2 miss buffer that has a free entry in the corresponding bank's DRAM request buffer.</figDesc><table><row><cell>for each issue cycle do</cell></row><row><cell>next core ? previous core</cell></row><row><cell>cond1 ? no valid requests in next core's L2 miss buffer</cell></row><row><cell>cond2 ? consecutive requests from next core &gt;= threshold</cell></row><row><cell>if cond1 OR cond2 then</cell></row><row><cell>next core ? highest ranked core with valid request</cell></row><row><cell>end if</cell></row><row><cell>issue oldest valid request from next core</cell></row><row><cell>end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Baseline configuration of each core</figDesc><table><row><cell></cell><cell>Out of order; decode/retire up to 4 instructions,</cell></row><row><cell cols="2">Execution core issue/execute up to 8 microinstructions; 15 stages</cell></row><row><cell></cell><cell>256-entry reorder buffer; 32-entry MSHRs</cell></row><row><cell>Front end</cell><cell>Fetch up to 2 branches; 4K-entry BTB; 64K-entry gshare/PAs hybrid branch predictor</cell></row><row><cell></cell><cell>L1 I and D: 32KB, 4-way, 2-cycle, 1 read/write ports;</cell></row><row><cell cols="2">On-chip caches Unified L2: 512KB (1MB for 1-core), 8-way, 8-bank,</cell></row><row><cell></cell><cell>15-cycle, 1 read/write port; 64B line size for all caches</cell></row><row><cell></cell><cell>Stream prefetcher: 32 stream entries,</cell></row><row><cell>Prefetcher</cell><cell>prefetch degree of 4, prefetch distance of 64 [25, 23],</cell></row><row><cell></cell><cell>128-entry prefetch request buffer</cell></row><row><cell></cell><cell>800MHz DRAM bus cycle, DDR3 1600MHz [14],</cell></row><row><cell cols="2">8 to 1 core to DRAM bus frequency ratio; DRAM and bus 8B-wide data bus per channel, BL = 8; 1 rank,</cell></row><row><cell></cell><cell>8 banks per channel, 8KB row buffer per bank;</cell></row><row><cell cols="2">On-chip, open-row, demand-first [12] FR-FCFS [20] DRAM controllers 1, 2, 4 channels for 1, 4, 8-core CMPs;</cell></row><row><cell cols="2">64-entry (8 ? 8 banks) for single-core processor DRAM request 256 and 512-entry (16 ? 8 banks per channel) buffers for 4 and 8-core CMPs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Baseline shared resource configuration</figDesc><table><row><cell cols="6">Param DRAM cycles Param DRAM cycles Param DRAM cycles</cell></row><row><cell>t RP</cell><cell>11</cell><cell>t RCD</cell><cell>11</cell><cell>CL</cell><cell>11</cell></row><row><cell>CWL</cell><cell>8</cell><cell>AL</cell><cell>0</cell><cell>t BL</cell><cell>4</cell></row><row><cell>t RC</cell><cell>39</cell><cell>t RAS</cell><cell>28</cell><cell>t RT P</cell><cell>4</cell></row><row><cell>t BL</cell><cell>4</cell><cell>t CCD</cell><cell>4</cell><cell>t RRD</cell><cell>4</cell></row><row><cell>t F AW</cell><cell>24</cell><cell>t W T R</cell><cell>4</cell><cell>t W R</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>DRAM timing specifications</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Characteristics for 14 memory-intensive SPEC benchmarks with/without stream prefetcher: IPC, MPKI (L2 misses Per 1K Instructions), DRAM BLP, ACC (prefetch accuracy), and COV (prefetch coverage)</figDesc><table><row><cell cols="4">execution of a workload. It comprises the cache lines brought in</cell></row><row><cell cols="4">from demand, useful prefetch, and useless prefetch requests. We</cell></row><row><cell cols="4">define Prefetch accuracy (ACC) and coverage (COV) as follows:</cell></row><row><cell></cell><cell>ACC =</cell><cell>Number of usef ul pref etches N umber of pref etches sent</cell><cell>,</cell></row><row><cell>COV =</cell><cell cols="3">N umber of usef ul prefetches N umber of demand requests + Number of usef ul pref etches</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Hardware storage cost of BAPI and BPMRI (N line , Ncore, NMSHR, N buf f er , N channel , N</figDesc><table /><note><p>bank : number of L2 cache lines, cores, MSHR entries, prefetch request buffer entries, DRAM channels, DRAM banks per channel)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Average IPC performance with various MSHR sizes</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: ETH BIBLIOTHEK ZURICH. Downloaded on December 05,2020 at 10:22:37 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The L2-to-DC request issuer need not run at the processor clock frequency since L2 misses are not frequent. In our evaluations, it runs at one-fourth the processor clock frequency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We imposed the requirement that each of the multiprogrammed workloads have at least one memory-intensive application since these applications are most relevant to our study. We consider an application to be memory-intensive if its L2 Misses Per 1K Instructions (MPKI) is greater than 5.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank Khubaib, other HPS members, and the reviewers for their comments. <rs type="person">Chang Joo Lee</rs> was supported by an <rs type="funder">IBM</rs> scholarship during this work. We also gratefully acknowledge the support of the <rs type="funder">Cockrell Foundation</rs> and <rs type="funder">Intel Corporation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-31</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Inside Intel Core microarchitecture and smart memory access. Intel Technical White Paper</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving data cache performance by pre-executing instructions under a cache miss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-11</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-15</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Buffer block prefetching method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gindele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Technical Disclosure Bulletin</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1977-07">July 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MLP yes! ILP no! In ASPLOS Wild and Crazy Idea Session</title>
		<author>
			<persName><forename type="first">A</forename><surname>Glew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The microarchitecture of the Pentium 4 processor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-39</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-8</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prefetch-aware DRAM controllers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-41</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Balancing throughput and fairness in SMT processors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gummaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ISPASS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<idno>Micron. 2Gb DDR3 SDRAM</idno>
		<ptr target="http://download.micron.com/pdf/datasheets/dram/ddr3/" />
		<title level="m">MT41J512M4 -64 Meg x 4 x 8 banks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallelism-aware batch scheduling: Enhancing both performance and fairness of shared DRAM systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-35</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-9</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fair queuing memory systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-39</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AC/DC: An adaptive data cache prefetcher</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT-13</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pinpointing representative portions of large Intel Itanium programs with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karunanidhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In MICRO-37</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memory access scheduling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">J</forename><surname>Kapasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-27</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Various optimizers for single stage production</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Symbiotic job scheduling for a simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS-9</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-13</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">Inc. OpenSPARC (T M ) T1 Microarchitecture Specification</title>
		<imprint/>
		<respStmt>
			<orgName>Sun Microsystems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">POWER4 system microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>IBM Technical White Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scalable cache miss handling for high memory-level parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-39</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhancing memory level parallelism via recovery-free value prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-17</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A performance comparison of DRAM memory system optimizations for SMT processors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-11</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing cache pollution via dynamic data prefetch filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
