<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multiscale Active Facial Patches for Expression Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Lin</forename><surname>Zhong</surname></persName>
							<email>linzhong@cs.rutgers.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zafeiriou</forename><forename type="middle">L</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08873</postCode>
									<settlement>Somerset</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76019</postCode>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multiscale Active Facial Patches for Expression Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8600A6C098662AD1C3C4A26E1BAB2EF0</idno>
					<idno type="DOI">10.1109/TCYB.2014.2354351</idno>
					<note type="submission">received August 13, 2013; revised June 1, 2014 and August 14, 2014; accepted August 18, 2014. Date of publication September 29, 2014; date of current version July 15, 2015. This paper was</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cohn-Kanade</term>
					<term>facial expression</term>
					<term>GEMEP-FERA</term>
					<term>group sparsity</term>
					<term>MMI</term>
					<term>multiscale facial patch selection</term>
					<term>multitask learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new idea to analyze facial expression by exploring some common and specific information among different expressions. Inspired by the observation that only a few facial parts are active in expression disclosure (e.g., around mouth, eye), we try to discover the common and specific patches which are important to discriminate all the expressions and only a particular expression, respectively. A two-stage multitask sparse learning (MTSL) framework is proposed to efficiently locate those discriminative patches. In the first stage MTSL, expression recognition tasks are combined to located common patches. Each of the tasks aims to find dominant patches for each expression. Secondly, two related tasks, facial expression recognition and face verification tasks, are coupled to learn specific facial patches for individual expression. The two-stage patch learning is performed on patches sampled by multiscale strategy. Extensive experiments validate the existence and significance of common and specific patches. Utilizing these learned patches, we achieve superior performances on expression recognition compared to the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACIAL expressions play significant roles in our daily communication, due to their abilities to reflect human emotions, and social interaction. In the past three decades, automatic facial expressions recognition has become an increasingly fascinating topic in the computer vision and pattern recognition communities for their extensive applications, such as human-computer interface, multimedia, and security <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>. However, as the basis of expression recognition, the exploration of the functional facial features is still an open problem.</p><p>Studies in psychology show that facial features of expressions are located around mouth, nose, and eyes, and their locations are essential for explaining and categorizing facial expressions. Through electrical muscle stimulation, References <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b17">[18]</ref> found that most expressions are invoked by a small number of facial muscles around the mouth, nose and eyes [see Fig. <ref type="figure" target="#fig_0">1(a)</ref>]. This indicates that most of the descriptive regions for each expression are located around certain face parts. Moreover, expressions can be generally categorized into six popular "basic" or "universal" expressions <ref type="bibr" target="#b25">[26]</ref>: anger, disgust, fear, happiness, sadness, and surprise. (Of course, there are a lot of complex expressions which are not basic expressions). These expressions seems to be universal across different ethnicities and cultures <ref type="bibr" target="#b20">[21]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, each of these basic expressions can be further decomposed into a set of several related action units (AUs) <ref type="bibr" target="#b18">[19]</ref>, e.g., happiness can be roughly decomposed to cheek raiser and lip corner puller. However, few existing methods statistically utilize these prior knowledge about facial muscle and AUs to aid facial expression analysis in computer vision community.</p><p>Previous expression recognition methods can be generally categorized into two groups: AU-based methods and message and sign judgement methods. AU-based methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref> recognize expressions by detecting AUs, which have more descriptive power, but these methods suffer from the difficulties of AU detection. Message and sign judgement methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b57">[58]</ref> reveal the differences among expressions by facial appearance variations, which has been proved to be more reliable on single still images. However, these methods treat different facial parts equally or assign different weights to them empirically, thus lacking statistical support for the weight settings. This motivates us to fully make use of the prior knowledge from facial muscles and AU studies to extract the most discriminative regions, which can further assist expression analysis.</p><p>Inspired by the locations of AUs, we divide human face into nonoverlapping patches on different scale levels, and then conceptually group these patches into three categories: common facial patches, specific facial patches, and the rest. Common facial patches are active ones for all expressions. Specific facial patches are only active for one particular expression. Therefore, the most important facial patches are the common ones shared by all expressions; specific patches are only a few and only useful to discriminate a particular expression; the rest of the patches are of less help to expression recognition. The effective facial patches corresponding to different facial muscles may have different sizes, and the optimal size of patches is hard to be determined and it may vary among different areas of the face, we employ three different scale patches in this paper to cover effective facial parts with different sizes.  face image is divided into more patches. An example of face division (8 Ã— 8) is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a).</head><p>A two-stage multitask sparse learning (MTSL) framework is proposed to explore common and specific patches statistically on each scale level, respectively. In the first stage, the binary classification problem for each expression is treated as an individual task (see Fig. <ref type="figure" target="#fig_3">4</ref>), then a MTSL model is built based on these related tasks to extract the common facial patches. In the second stage, the face verification task (see Fig. <ref type="figure" target="#fig_5">5</ref>) is designed to be coupled with the previous classification task for one expression. In this way, another MTSL model can be constructed to find out the specific patches for this particular expression. Similarly, the specific patches for all the expressions can be figured out separately. After the common and specific patches on different scales are learned by the twostage MTSL model, they are combined to boost the facial expression recognition accuracy.</p><p>For all the scales, the common and specific patches, found by extensive experiments on the Cohn-Kanade database <ref type="bibr" target="#b27">[28]</ref> and the MMI database <ref type="bibr" target="#b51">[52]</ref>, not only confirm the psychology discoveries of the facial muscles and AUs, but also provide more accurate appearance locations. Moreover, these common and specific patches at the same scale can be used to boost the performance of expression recognition. The learned patches are shown to be effective across different databases, e.g., Cohn-Kanade database <ref type="bibr" target="#b27">[28]</ref>, GEMEP-FERA <ref type="bibr" target="#b32">[33]</ref>. Only using relatively small number of patches (âˆ¼ 1/3 of the face), our method still outperforms other methods in expression recognition. Finally, the learned effective patches at different scales can be also combined to further improve the expression recognition performance.</p><p>Our contributions are as follows.</p><p>1) We provide a solid validation for an important psychology discovery, that only partial area of the face (corresponding to underlying facial muscles) are discriminative for expression recognition. 2) A two-stage MTSL framework is proposed to formulate the commonalities among expressions, and to find out the locations of common and specific patches for expressions. 3) Multiscale image division strategy is utilized to generate patches of different size for facial expression analysis.</p><p>More convincing conclusion about facial parts (muscles) could be achieved, since they are of different sizes.</p><p>4) Extensive experiments with three different scales on three public databases demonstrate that these active patches are effective in recognizing expressions. The common and specific patches can be combined to improve the performances of state-of-the-arts. Patches across different scales can also been fused to further boost the performance. The rest of this paper is organized as follows. Section II reviews related work of facial expression analysis and MTSL. Section III presents our framework to learn common and specific patches based on MTSL. These effective patches on all different scales are learned in this section. The experimental results on three public databases are shown in Section IV. We conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Facial Expression Analysis</head><p>Most facial expression analysis methods generally follow the aforementioned two categories: AU-based and message and sign judgement methods. Although this paper belongs to the latter category, it is still necessary to give a completed review on related works on expression analysis.</p><p>1) AU-based facial expression analysis inspired by the well-known study on facial activity, facial action coding system (FACS) <ref type="bibr" target="#b18">[19]</ref>. In this system, the subtle changes in facial appearance are encoded into 32 AUs with individual linguistic description. Since each basic expressions can be decomposed into several related AUs, the expression recognition problem can be transferred to AUs detection problem. Bartlett et al. <ref type="bibr" target="#b5">[6]</ref> recognized six single upper face AUs, but no simultaneous AUs are considered in combination. Tian et al. <ref type="bibr" target="#b46">[47]</ref> detected 16 AUs from face image sequences using lip tracking, template matching and neural networks. More works have been done on spontaneous facial expression data by automatic recognition of AUs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Some differences between spontaneous and deliberate facial behavior are also studied by <ref type="bibr" target="#b16">[17]</ref>. Recently, AU detection and AU-based expression recognition methods make a lot of significant progresses. Tong et al. <ref type="bibr" target="#b48">[49]</ref> explore the dynamic and semantic relationships of facial AUs to improve their recognition performance. A dynamic Bayesian network is built by Tong et al. <ref type="bibr" target="#b47">[48]</ref> for better facial activity understanding. Senechal et al. <ref type="bibr" target="#b41">[42]</ref> combine different types of features using SimpleMKL learning algorithm to extract geometric and appearance information simultaneously. Sandbach et al. Fig. <ref type="figure">2</ref>. Discovering the common patches across six expressions using MTSL. Each single expression task is the binary classification task for one expression (see Fig. <ref type="figure" target="#fig_3">4</ref>). Expression tasks are combined in a MTSL model to select out the common patches under the group sparsity constraint.</p><p>2) Message and sign judgement facial expression analysis methods generally consist of the two main steps: facial representation and expression recognition. a) Facial representation derives a set of features from original facial images to effectively represent all faces. Different features have been applied to either the whole-face or specific face regions to extract the facial appearance changes, such as Gabor <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, haar-like features <ref type="bibr" target="#b57">[58]</ref>, local binary patterns (LBPs) <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Zafeiriou and Pitas <ref type="bibr" target="#b63">[64]</ref> explored the graph structures with landmarks to represent the variations among different expressions. In <ref type="bibr" target="#b43">[44]</ref>, facial images are equally divided into small regions, and then LBP features are extracted from these empirically weighted sub-regions to represent the facial appearance. The LBP features are shown to be effective in expression recognition, so this paper will also utilize the LBP features with the same sub-region division strategy. Different from their work, we will focus on learning the effective sub-regions statistically. b) Expression recognition aims to correctly categorize different facial representations. Support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b60">[61]</ref> is the most popular and effective learning method in facial expression recognition. Reference <ref type="bibr" target="#b43">[44]</ref> is the most similar work to ours, so it will be considered as the baseline. For fair comparison, this paper will also employ SVM as the classification algorithm. Besides these works, there are also some works utilizing the geometric features <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b68">[69]</ref>, such as the location of facial feature points (corners of the eyes, mouth, etc.). Some methods perform facial expression analysis based on 3-D face models <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b61">[62]</ref>. More works on fusion of audio and visual information can be found in <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MTSL</head><p>Sparsity methods have attracted much attention in computer vision, multimedia and medical image communities, and have been employed in many applications, such as face recognition <ref type="bibr" target="#b58">[59]</ref>, background substraction <ref type="bibr" target="#b24">[25]</ref>, image annotation and retrieval <ref type="bibr" target="#b66">[67]</ref>, and shape prior-based segmentation <ref type="bibr" target="#b67">[68]</ref>. Many algorithm are proposed to solve these problems of sparsity priors, such as greedy methods [basis pursuit (BP) <ref type="bibr" target="#b12">[13]</ref>, matching pursuit <ref type="bibr" target="#b31">[32]</ref>, orthogonal matching pursuit (OMP) <ref type="bibr" target="#b49">[50]</ref>], or L1 norm relaxation and convex optimization <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>MTSL is an inductive transfer machine learning approach. It aims to learn a problem together with some related problems for better performance <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>. MTSL is then designed in <ref type="bibr" target="#b2">[3]</ref> for feature selection, through encouraging multiple predictors from different tasks to share similar parameter sparsity patterns. MTSL also obtained a rewarding performance on handwritten character recognition in <ref type="bibr" target="#b22">[23]</ref>. Yuan et al. <ref type="bibr" target="#b62">[63]</ref> developed a visual classification algorithm by learning the shared parts among different representation tasks. Recently, Chen et al. <ref type="bibr" target="#b11">[12]</ref> provided a faster solution to MTSL problems.</p><p>Suppose there are T related tasks, and (x t i , y t i ), i = 1, 2, . . . , N t is the training set of task t, where each sample is represented by K-dimensional features, x t i âˆˆ R K , and y t i âˆˆ {-1, 1} indexes x t i is negative or positive. w t is a K-dimensional vector of representation coefficients for task t. All the w t s are the rows of the matrix W = [w t k ] t,k , while every column of the matrix W is a T-dimensional vector that means the representation coefficients from the kth feature across different tasks,</p><formula xml:id="formula_0">w k = [w 1 k , w 2 k , . . . , w T k ]</formula><p>. MTSL aims to learn the shared sparse information among all the tasks. The formulation with L 1 /L 2 mixed-norm regularization is as follows:</p><formula xml:id="formula_1">arg min W T t=1 1 N t N t i=1 J t w t , x t i , y t i + Î» K k=1 w k 2 (1)</formula><p>where J t (w t , x t i , y t i ) is the cost function of the tth task, Î» is a constant to balance the sparsity, and is the mathematic format for L 1 norm. The regularization term encourages most columns of matrix W to be zero, and the remaining nonzero columns indicate the corresponding features are shared features across all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED WORK</head><p>In this section, we first introduce the multiscale facial appearance representation strategy, and then the learning procedures of common and specific patches at each scale level are illustrated. Finally, we design the classifier with these learned effective patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiscale Appearance Representation</head><p>Facial expressions are usually manifested by local facial appearance variations. However, it is not easy to automatically localize these local active areas on a facial image. A facial image is divided into p local patches, and then LBP features are used to represent the local appearance of the patch. These features have been proven to be a powerful descriptor in expression recognition <ref type="bibr" target="#b43">[44]</ref> and face verification <ref type="bibr" target="#b56">[57]</ref>. Since the facial parts corresponding to facial muscles do not have equal size, multiscale division strategy should be applied to facial images to generate facial patches with different sizes. These patches could offer a more complete coverage for the effective facial parts than the generated patches if only one single scale is applied. The facial patches should have reasonable size, which can not be too big to cover too much facial parts or too small to have no physic meaning. In this paper, we adopt three different scale sizes, we set p = 8 Ã— 8, 6 Ã— 6, 4 Ã— 4 in the experiments with the image size of 96 Ã— 96. We denote these three scales as S8, S6, and S4, respectively. An example of division for S8 is shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. For each patch, the uniform LBP features are extracted with the LBP operator LBP 8,1 , as shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>, and mapped to a m-dimensional histogram (m = 59 in this paper).</p><p>Based on these local patches, the common patches across all expressions on each scale level are learned for expression recognition. Then, some specific patches for each expression are explored to enhance the performance. Finally, these learned patches on different scales are fused to further boost the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Common Patches Across Expressions</head><p>Discovering the common patches across all the expressions is actually equivalent to learning the shared discriminative patches for all the expressions. Since MTSL can learn common representations among multiple related tasks <ref type="bibr" target="#b2">[3]</ref>, our problem can be transferred into a MTSL problem. T related tasks are defined as T discriminative patch learners for T facial expressions, respectively (we set T = 6 for six basic expressions). Supposing each image has p patches, it can be represented by (p Ã— m)-dimensional LBP-based histogram features. Let K = p Ã— m. However, (1) cannot directly model our problem. Different from the MTSL model described in (1), we focus on the selection of common patches instead of individual features. Since a group of consecutive features stand for one patch, and the number of common patches are not large, group sparsity prior can be assumed <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Our problem is modeled as the following MTSL problem, in which the regularization term of ( <ref type="formula">1</ref>) is modified to a patch level sparse constraint:</p><formula xml:id="formula_2">arg min W T t=1 1 N t N t i=1 J t w t , x t i , y t i + Î» p j=1 w G j 2 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Here, w G j is a sub-matrix of matrix W, where G j denotes the jth patch, as shown in Fig. <ref type="figure">2</ref>. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates how to set up each task. In each task, images of one particular expression are considered as positive samples, while others are negative samples. This regularization term encourages the representation coefficients of the features in most patches to be zero, and then the remaining nonzero patches indicate the shared important representation for all the expressions. The cost function of J t is defined as a logistic loss function</p><formula xml:id="formula_4">J t w t , x t i , y t i = ln 1 + exp -y t i x t i â€¢ w t . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>To solve this patch-based MTSL, the proposed algorithm is based on the accelerated gradient method proposed in <ref type="bibr" target="#b59">[60]</ref>. The algorithm comprises two main steps: the generalized gradient mapping step and the aggregation step. The two steps alternately update two matrices in each iteration, i.e., a weight matrix sequence W s and an aggregation matrix sequence V s , s is the iteration index number. The updating of W s+1 is the generalized gradient mapping step, which uses the current aggregation matrix V s to update matrix W s . During this updating, we heuristically enforce the group sparsity prior which makes the representation coefficients of the features in one patch to be zeros under the condition in steps 5-9 of Algorithm 1. The updating of V is the aggregation step, in which we construct a linear combination of W s+1 and W s to update V s+1 (step 11 in Algorithm 1). The detailed problem solving procedure are summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning Specific Patches for Individual Expression</head><p>Although learned common patches can discriminate all facial expressions, the performance could not be the best, because each expression also has its special properties besides the common properties. Here, we aim to explore some specific Algorithm 1 Algorithm for Learning Common Patches Initialize: W 0 takes equal weights, V 0 = W 0 and a 0 = 1.</p><p>Tuning parameter Î» and step size Î·. 3: for s = 0...S do 4:</p><formula xml:id="formula_6">w t s+1 = v t s - Î·[ 1 1+exp(-(Y t ) X t v t s ) exp(-(Y t ) X t v t s )(-(X t ) Y t )] 5:</formula><p>if w G j ,s+1 2 â‰¥ Î»Î· then  </p><formula xml:id="formula_7">a s+1 = 2 s+3 , Î´ s+1 = W s+1 -W s 11: V s+1 = W s+1 +</formula><formula xml:id="formula_8">w G j = w t k ,</formula><p>where w G j is the weight for patch j, and w t k âˆˆ G j 18: Output: order w G j decreasingly, and output the top patches as the common patches for all expressions.</p><p>facial patches for each expression with the help of face verification, and then they are used to further boost the performance of common facial patches. The motivation to employ the face verification task is that those special facial patches are important face regions, which are not only useful for recognize this expression, but also very significant for identifying the subjects. Take an expression e for example. Recognition e task will prefer to select out those patches which are useful only to recognizing the expression. Since we have the assumption that those patches should be the important face regions, and thus they are also very discriminative to face verification task, a MTSL model can be used to couple these two tasks and select out those shared important patches between these two tasks more robustly. The learned patches should embed some specific signatures of the face identity.</p><p>This MTSL framework for specific patches is the same as the framework of learning the common patches except the different task design. The individual expression analysis task is organized in the same way as in Fig. <ref type="figure" target="#fig_3">4</ref>. Fig. <ref type="figure" target="#fig_5">5</ref> illustrates how to organize the task of face verification. For face verification, we need to compare two images and label them as the same person or not, so we organize the training data of this task by the feature difference between two images. Assuming (x 2 i , y 2 i )</p><formula xml:id="formula_9">N 2 i=1</formula><p>is the training set, x 2 i is the feature difference between two images in ith image pair. y 2 i âˆˆ {-1, 1} indicates whether the two images in ith pair come from one subject or not. N 2 is the number of image pairs. The superscript 2 means this task is the second task in the MTSL model. The procedure for solving this problem is the same with Algorithm 1. Because there are six expressions, six MTSL models are needed to be built to learn their specific patches, respectively.</p><p>The specific patches have overlap with the learned common patches. Since the common patches will be used for all expressions, the overlapped patches are removed from the specific patches. The rest patches are considered as the final specific patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classifier Design</head><p>With the extracted common and specific patch features based on the training data, classifiers are then built based on these features for testing data. MTSL model can directly give out classification results <ref type="bibr" target="#b62">[63]</ref>. However, to fairly compare with <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b43">[44]</ref>, SVM is adopted to learn the expression classifiers and the one-against-all strategy is employed to decompose the six class problem into multiple binary classification problems. Each binary classification will output a confidence value of the test sample belonging to this class. The class label with the highest confidence will be the final classification result of this sample. The performances of common patches and the combination of common and specific patches are evaluated, respectively. For expression e, denotes the common patches as P c , and the specific patches as {P e s } 6 e=1 . When both common and specific patches are investigated, the features from P c and P e s are concatenated to represent facial images, and train the SVM classifiers. While only use the features of P c when common patches are tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate the learned common and specific patches for facial expression recognition. All methods are compared on three datasets, the Cohn-Kanade database <ref type="bibr" target="#b27">[28]</ref>, the MMI database <ref type="bibr" target="#b51">[52]</ref> and the GEMEP-FERA <ref type="bibr" target="#b32">[33]</ref>, which are widely used for facial expression recognition algorithms. Our methods are denoted as CPL and CSPL, respectively (see Table <ref type="table">I</ref>). To efficiently evaluate the performance of our proposed methods, they are compared with <ref type="bibr" target="#b43">[44]</ref>, which is the most recent comprehensive study on expression recognition with remarkable </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I METHOD ABBREVIATIONS</head><p>results. In <ref type="bibr" target="#b43">[44]</ref>, two methods are evaluated, denoted as ADL and AFL, respectively. ADL uses Adaboost to select important patches and then performs SVM on the extracted LBP features of these patches. AFL uses all the patches to train the classifier without feature selection. MCPL, MCSPL, MADL, and MAFL are the methods using multiscale patches for CPL, CSPL, ADL, and AFL, respectively. For fair comparison, all the methods are based on the same patch (sub-region) division strategy, same feature representation, and the same classification method (SVM). The only difference among the methods is the patches they use. All method abbreviations are listed in Table <ref type="table">I</ref>. Ten folds cross-validation is employed for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on the Cohn-Kanade Database</head><p>The Cohn-Kanade database consists of 100 university students aged from 18 to 30 years old, of which 65% were female, 15% were African-American, and 3% were Asian or Latino. Subjects were instructed to perform a series of 23 facial displays, six of which were based on description of prototypic emotions. For our experiments, image sequences are selected out from 96 subjects, whose sequences could be labeled as one of the six basic emotions. For each sequence, we only use the three peak frames with the most expressions. The faces are detected automatically by Viola's face detector <ref type="bibr" target="#b54">[55]</ref>, and then they are normalized to 96Ã—96 as in <ref type="bibr" target="#b44">[45]</ref> based on the location of the eyes. Fig. <ref type="figure" target="#fig_7">7</ref> shows some normalized samples with all expressions.</p><p>To better demonstrate the patch selection strategy and the physical meaning of the selected patches, we first apply our method to only one patch scale (S8). The performance of the recognition can be further boosted by combining the patch selection across all different scales.</p><p>1) Analysis of Common Patches: As described in Section II-A, the proposed MTSL aims to select the shared patches instead of the shared features, so we apply the L 1 /L 2 norm regularization on the patch level to obtain patch-based group sparsity. Fig. <ref type="figure" target="#fig_4">6</ref> reports the representation coefficient results for six expression tasks. We can see that the representation coefficients of features are sparse, and show the property of patch-based group sparsity. It is also clear to see the index correspondences for nonzero values across six expressions, which indicates the commonalities among them. So, this result demonstrates the effectiveness of our proposed algorithm in learning the shared common patches for expressions.</p><p>Before evaluating the recognition performance of the common patches, we want to inspect the performance when a different number of common patches is selected. Fig. <ref type="figure" target="#fig_8">8</ref> reports the results with a different number of the common patches. We can see that the recognition rate increases quickly with the first leading common patches, and when the number of the selected patches reaches around 20, it will get a recognition rate of 88.42%. If too many common patches are selected, the performance goes down slightly and fluctuates. It means that only some common patches are discriminative for all the expressions. When some patches with little importance are selected   as the common patches, they will introduce some noises and influence the discriminative power of the common patches. We set the number of the common patches to be 20 in the following experiments. Fig. <ref type="figure" target="#fig_9">9</ref> shows the superimposing effect of the selected common patches over the ten fold experiments. There are great overlaps between different fold experiments. It indicates that our algorithm is robust to the selection of the training set. The selected common patches are basically around the areas of mouth, eye, and eyebrows, which are consistent with AU-based analysis in FACS <ref type="bibr" target="#b18">[19]</ref>.</p><p>Table <ref type="table" target="#tab_5">IV</ref> reports the detailed recognition performance of the common patches on each expression, where the expressions of   anger, disgust, fear, happiness, sadness, surprise are denoted as ag, dg, fa, hp, sd, and sp for simplicity. Promising recognition rates are obtained on all the expressions except anger. Anger is often misclassified as sadness. This is because these two expressions have similar appearance variations on the common patches. This problem can be alleviated by adding some specific patches, which will be discussed next.</p><p>2) Analysis of Specific Patches: Although a rewarding recognition result can be obtained by only using the common patches, the performance can be further improved by integrating some specific patches of each expression. Fig. <ref type="figure" target="#fig_10">10</ref> shows the top three learned specific patches for each expression based on the proposed multitask learning. We can see the locations of these patches are highly related to expression types. Take surprise for example. The selected specific patches show the characteristics of surprise expression, in which special appearance changes are distributed in opened mouth, on stared eye, and raised eyebrow. In CSPL, the common patches and the specific patches are integrated together, and the experimental results are reported in Table <ref type="table" target="#tab_6">V</ref>. Compared to the results of CPL (Table <ref type="table" target="#tab_5">IV</ref>), we can see that adding specific patches can further improve the performance of the common patches.</p><p>3) Comparisons With Other Methods: To further evaluate the proposed CPL and CSPL, we compare them to ADL and AFL developed in <ref type="bibr" target="#b43">[44]</ref>. Table VI lists the F1 measure for every expression and the overall recognition rates of these four methods. AFL gets the recognition rate of 86.94%, which is much worse than our methods. It shows the importance of selecting discriminative patches. The confusion matrixes of methods, AFL and ADL, can also be found in Tables <ref type="table" target="#tab_3">II</ref> and<ref type="table" target="#tab_4">III</ref>. Although ADL also uses Adaboost to select the patches, it does not take the commonalities among all the expressions into account. ADL gets a recognition rate of 82.26% with the selected patches (highest rate with 20 Â± 3 patches), while the recognition rates of CPL and CSPL are 88.42% and 89.89%, respectively. It demonstrates that the learned common and specific patches by our proposed twostage MTSL can really improve the performance of expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Analysis of Multiscale Patches:</head><p>The selected patches are shown to be effective in expression recognition, even the number of them is limited <ref type="bibr" target="#b69">[70]</ref>. However, no prior of the optimal patch size is given. Besides, patches with different scales may represent different information for recognition. So, we can employ the patch selection strategy on different patch sizes, and then combine the selected patches with different scales together to further improve the recognition performance. In Fig. <ref type="figure" target="#fig_11">11</ref>, we show the selected common patches for all the scales for one of the ten-fold experiments. For each scale, only 1/3 patches are selected out. The results show that for different scales, the effective patches are around the mouth and the eyes. As shown in Table <ref type="table" target="#tab_7">VI</ref>, the performance of these multiscale patches can achieve better performance (90.34%) than only using the common patches from a single scale S8 (88.42%). The F1 measures for each expression also validate the usefulness of multiscale strategy. The specific patches are selected out at different scale individually, following the way we already shown for the scale S8. Incorporating selected specific patches, we get the best performance (91.53%). Thus, the experiment results show that multiscale patches could contain more useful statistic information for recognition than one scale with a single division strategy. Our patches selection are effective in selecting the discriminative patches across all patch scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on the MMI Database</head><p>The MMI database includes 30 students and research staff members aged from 19 to 62, of whom 44% are female, having either a European, Asian, or South American ethnic background. In this database, 213 sequences have been labeled with six basic expressions, in which 205 sequences are with frontal face. Different from <ref type="bibr" target="#b43">[44]</ref>, in which only the experimental data are collected from 99 selected sequences, we conduct our experiments on the data from all the 205 sequences. As in <ref type="bibr" target="#b43">[44]</ref>, the apex images are extracted from the sequences as the experimental data. Facial image are cropped based on locations of eyes, and resize it to 96 Ã— 96, same as on Cohn-Kanade database.</p><p>MMI is a more challenging database than the Cohn-Kanade database. First, the subjects make expressions nonuniformly. Different people make the same expression in different ways. Second, some subjects wear accessories, such as glasses, headcloth, or moustache. Additionally, in some sequences, the apex frames are not with high expression intensity. All these factors will greatly degrade the recognition performance.</p><p>We first investigate the performance of the common patches with different patch number, and Fig. <ref type="figure" target="#fig_0">12</ref> shows the results. It can be seen that the results are similar to the results on the Cohn-Kanade database. About 20 common patches are discriminative for all the expressions, so we set the number of the common patches as 20 on this database too. Table <ref type="table" target="#tab_10">IX</ref> lists the F1 measures for each expression and the overall recognition rates of CPL, CSPL, AFL, ADL, AFL, and their corresponding multiscale methods, respectively. Same as on the Cohn-Kanade data, CPL, and CSPL are superior to AFL and ADL. However, the performances of all four methods are much lower than that of the Cohn-Kanade database, because this database has several challenging factors as mentioned above. CSPL obtains much better performance than CPL. This is because each expression has a very big variance due to the diversity of the subjects in this database, but the common patches cannot describe these specific variations. Although a much better result of 86.7% is reported in <ref type="bibr" target="#b43">[44]</ref>, their experimental data are carefully chosen 99 sequences, while we perform the experiments on all the 205 sequences. Besides, they adopt sliding and multiscale windows to extract much more patches. We only divide the facial image into 64 patches, and we also obtain a recognition rate of 73.53% on more than double the size of the data than in <ref type="bibr" target="#b43">[44]</ref>. Tables VII and VIII list the confusion matrix of CPL and CSPL, respectively.   We further explore the common and specific patches selection for different patch sizes. The accuracy rate of the baseline method MAFL and MADL increases with more multiscale patches. The recognition rate of MCPL and MCSPL using more multiscale patches both achieve better performance than CPL and CSPL with single patch scale. The detail results are also shown in Table <ref type="table" target="#tab_10">IX</ref>.</p><p>The experimental results indicate the location of learned common and specific patches, which confirms the previous knowledge about active facial parts in psychology. The rewarding performances of these patches in facial expression recognition provide a solid basis for patches selection and weight setting in similar applications. This paper opens the road for the researches of utilizing the prior knowledge of facial muscles in psychology, and further improves the performances of existing methods in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on the GEMEP-FERA2011 Database</head><p>The Geneva multimodal emotion portrayals (GEMEP) is a collection of audio and video recordings <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. It consists   set <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. For each video in training or testing, five frames are evenly extracted from the video. We use the landmark detection from <ref type="bibr" target="#b70">[71]</ref> to localize the position of the eyes, based on which we align and crop out the face images. The landmark detection result and cropped images are shown in Fig. <ref type="figure" target="#fig_12">13</ref>. The emotion recognition challenge involves the classification of the following five emotions: anger, fear, joy, relief, and sadness. When determining the label of the test video, we first classify the extracted frames individually using a five-way forced strategy, then the emotion class which obtains the highest score will be the winner of the sequence.</p><p>Since the subjects in training part of database are quite few, it is quite difficult to learn the common patches and the specific patches on this database itself. In this experiment, we utilize the knowledge of the learned patches from the CK database. It also shows the capability of generalization across different databases. We follow the training/testing partition in the database, and the performances of our methods are compared with some related works in Table X. Our method MCSPL achieves much better recognition rate than the baseline work (i.e., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>). These results that the patches selected out by our algorithm are discriminative for expression recognition and are also robust across databases. Compared to the 1st prize of 2011 FERA competition (i.e., <ref type="bibr" target="#b41">[42]</ref>), which gets 83.5% accuracy, our method MCSPL achieves 80.0%. The detailed confusion matrix of the result is shown in Table <ref type="table">XI</ref>. Senechal et al. <ref type="bibr" target="#b41">[42]</ref> utilize a lot of features, such as LGPB histograms, 2.5 D Active Appearance model to combine appearance and geometry information. It also employs complex classification algorithm, such as, multikernels SVMs, temporal filtering. However, our method just uses the LBP feature and naive SVM. Considering these factors, our patch learning method is proved to be effective, and the results of our method are promising. More detailed comparisons of using different patches are given in Table XII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a new method to analyze facial expressions is proposed. Different from previous work, we aimed at exploring the commonalities among the expressions by discovering the common and specific patches. A two-stage sparse learning model is proposed to learn the locations of these patches based on the prior knowledge of facial muscles and AUs. A multiscale face division strategy is employed to obtain facial patches with different coverage area and eliminate the side effects from fixed patch size. The effectiveness of these patches are evaluated by facial expression recognition. Extensive experiments show that common patches can generally discriminate all the expressions, and the recognition performance can be further improved by integrating specific patches. More comprehensive patches can also be selected out to achieve better performance by using multiscale patch division strategy. The learned location information of these patches also confirms the location knowledge of facial muscles in psychology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Illustration of facial muscles distribution [18]. (b) Major AUs for six expressions. The arrows represent AUs.</figDesc><graphic coords="2,74.99,53.48,199.44,129.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b38">[39]</ref>,<ref type="bibr" target="#b39">[40]</ref> exploit 3-D motion-based features between frames of 3-D facial geometry sequences for dynamic AU detection and further expression recognition. AU-based methods decompose the facial expressions into different individual muscle activities, and then infer the expression categories based on the AU detection results. These methods can have great representation power, but AU detection itself is quite difficult and it is still an open problem to the community.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Cropped facial image is divided into 64 patches. (b) LBP feature example (LBP P,R refers to a neighborhood size of P equally spaced pixels on a circle of radius R that form a circularly symmetric neighbor set. P = 8, R = 1 for this example).</figDesc><graphic coords="4,87.49,53.44,174.24,95.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of one single expression task. Each task is a binary expression classification problem. Take expression task of happiness for example here.</figDesc><graphic coords="4,326.00,53.48,222.96,141.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 :</head><label>6</label><figDesc>Set w G j ,s+1 = (1 -Î»Î· w G j ,s+1 2 )w G j ,s+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Design of face verification task. Image pairs from the same subject are considered as positive samples. Otherwise, as negative samples.</figDesc><graphic coords="5,327.01,52.92,220.56,152.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results for six expressions in the coefficient matrix after MTSL for learning the common patches. x-axis corresponds to the feature index in the coefficient matrix, where features index are ordered consecutively as group by patches. y-axis is the weight values for features in each task after MTSL. The nonzeros parts are grouped, and matches across all tasks. (a) Anger. (b) Disgust. (c) Fear. (d) Happiness. (e) Sadness. (f) Surprise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Example of six basic expressions from the Cohn-Kanade database (anger, disgust, fear, happiness, sadness, and surprise).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Expression recognition rate with a different number of common patches. (a) Recognition result with selected common patches for the scale S8. The patch number for the three faces images marked with selected common patches are 10, 20, and 40, respectively. (b) Results for S6. Patch number are 11 and 24, respectively. (c) Results for S4. Patch number are 5 and 10, respectively. All results show the most effective patches are around the mouth and the eyes, and using only one third of all the patches can achieve satisfied performance.</figDesc><graphic coords="7,74.49,53.56,462.72,104.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Distribution of selected common patches on faces. The darker the red color is, the more times (shown as numbers) the patch has been selected as common patches in ten-fold experiments.</figDesc><graphic coords="7,116.99,224.64,115.44,115.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Top three specific patches for six expressions after eliminating the shared patches on the Cohn-Kanade database.</figDesc><graphic coords="7,66.99,390.41,214.56,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Distribution of selected common patches with different scales on faces. The darker the red color is, the more times the patch has been selected. To make the results visibly clearer, only one result of the ten-fold experiments is shown. The selected patches for all the scales are around the mouth and the eyes.TABLE VII CONFUSION MATRIX OF CPL ON MMI DATABASE (MEASURED BY RECOGNITION RATE: %)</figDesc><graphic coords="9,123.99,215.63,101.28,101.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (a) Landmark detection result of [71]. (b) and (c) Aligned and cropped face image examples from the GEMEP-FERA2011 Database.</figDesc><graphic coords="9,341.00,513.49,192.96,75.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,64.00,53.00,483.60,121.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>]. t indicates the task index, and t = 1, . . . , T. j is the group index, and j = 1, . . . , p.</figDesc><table /><note><p>1: Input: Training data {(x t i , y t i ), i = 1, . . . , N t }, define X t = [x t 1 ; . . . ; x t N t ], Y t = [y t 1 ; . . . ; y t N t ], V = [v 1 ; . . . ; v T 2:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II CONFUSION</head><label>II</label><figDesc>MATRIX OF AFL ON COHN-KANADE DATABASE (MEASURED BY RECOGNITION RATE: %)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III CONFUSION</head><label>III</label><figDesc></figDesc><table /><note><p>MATRIX OF ADL ON COHN-KANADE DATABASE (MEASURED BY RECOGNITION RATE: %)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV CONFUSION</head><label>IV</label><figDesc>MATRIX OF CPL ON THE COHN-KANADE DATABASE (MEASURED BY RECOGNITION RATE: %)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V CONFUSION</head><label>V</label><figDesc>MATRIX OF CSPL ON THE COHN-KANADE DATABASE (MEASURED BY RECOGNITION RATE: %)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI RECOGNITION</head><label>VI</label><figDesc>PERFORMANCES AND F1 MEASURES PER EXPRESSION FOR ALL COMPARED METHODS (I.E., AFL, ADL, CPL, CSPL, MAFL, MADL, MCPL, MCSPL) ON THE COHN-KANADE DATABASE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII CONFUSION</head><label>VII</label><figDesc>MATRIX OF CPL ON MMI DATABASE (MEASURED</figDesc><table /><note><p>BY RECOGNITION RATE: %)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII CONFUSION</head><label>VIII</label><figDesc>MATRIX OF CSPL ON THE MMI DATABASE (MEASURED BY</figDesc><table /><note><p>RECOGNITION RATE: %) Fig. 12. Recognition rate with different common patch number. Result of one fold experiment is shown.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX F1</head><label>IX</label><figDesc>MEASURES PER EXPRESSION AND RECOGNITION PERFORMANCES FOR ALL COMPARED METHODS (I.E., AFL, ADL, CPL, CSPL, MAFL, MADL, MCPL, MCSPL) ON THE MMI DATABASE TABLE X CLASSIFICATION RATE FOR EMOTION DETECTION ON THE GEMEP-FERA2011 DATABASE TABLE XII F1 MEASURES PER EXPRESSION AND RECOGNITION PERFORMANCES FOR ALL COMPARED METHODS (I.E., AFL, ADL, CPL, CSPL, MAFL, MADL, MCPL, MCSPL) ON THE GEMEP-FERA2011 DATABASETABLE XI CONFUSION MATRIX OF MCSPL FOR EMOTION RECOGNITION ON THE OVERALL TEST SET OF GEMEP-FERA2011 DATABASE</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Facial expression</title>
		<ptr target="http://en.wikipedia.org/wiki/Facial_expression" />
		<imprint>
			<date type="published" when="2008">2013. Oct. 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Special issue on inductive transfer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997-07">Jul. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introducing the Geneva multimodal expression corpus for experimental research on emotion perception</title>
		<author>
			<persName><forename type="first">T</forename><surname>BÃ¤nziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1161" to="1179" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing the Geneva multimodal emotion portrayal (GEMEP) corpus</title>
		<author>
			<persName><forename type="first">T</forename><surname>BÃ¤nziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blueprint for Affective Computing</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Sourcebook</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Scherer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Banziger</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roesch</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="271" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring facial expressions by computer image analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: Machine learning and application to spontaneous behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="568" to="573" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully automatic facial action recognition in spontaneous behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>7th Int. Conf. Autom. Face Gesture Recognit<address><addrLine>Southampton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Manifold based analysis of facial expression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="605" to="614" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning incoherent sparse and low-rank patterns from multiple tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno>ID. 22</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Dis. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person-independent facial expression detection using constrained local models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops</title>
		<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="915" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Foundations of human computing: Facial expression and emotion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimodal Interfaces</title>
		<meeting>Int. Conf. Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic analysis and recognition of brow actions and head motion in spontaneous facial behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moriyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst., Man, Cybern</title>
		<meeting>IEEE Int. Conf. Syst., Man, Cybern</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="610" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The timing of facial motion in posed and spontaneous smiles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Wavelets Multiresolut. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mecanisme de la Physionomie Humaine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duchenne</surname></persName>
		</author>
		<imprint>
			<publisher>Renouard</publisher>
			<biblScope unit="page">1862</biblScope>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Facial Action Coding System: A Technique for the Measurement of Facial Movement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A vision system for observing and extracting facial action parameters</title>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task feature selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Berkeley, Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Statistics, Univ. California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from examples in the small sample case-Face expression recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="488" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with dynamic group sparsity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Izard</surname></persName>
		</author>
		<title level="m">The Face of Emotion</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Appleton-Century-Crofts</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops</title>
		<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>4th IEEE Int. Conf. Autom. Face Gesture Recognit<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An interiorpoint method for large-scale l1-regularized least squares</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression extracted automatically from video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The first facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="921" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metaanalysis of the first facial expression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression: Recognition of facial actions and their temporal segments form face profile image seqences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Case-based reasoning for userprofiled recognition of emotions from face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="391" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automated facial expression recognition system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 43rd Annu. Int. Carnahan Conf. Security Technol</title>
		<meeting>43rd Annu. Int. Carnahan Conf. Security Technol<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="172" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Binary pattern analysis for 3D facial action unit detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf<address><addrLine>Surrey, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognition of 3D facial expression dynamics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="762" to="773" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Authentic facial expression analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>6th IEEE Int. Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial action recognition combining heterogeneous features via multikernel learning</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="993" to="1005" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Smile detection by boosting pixel differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="431" to="436" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluation of face resolution for expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognizing upper face action units for facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recognizing action unites for facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for spontaneous facial action modeling and understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="258" to="273" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Facial action unit recognition by exploiting their dynamic and semantic relationships</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1699" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Signal recovery from random measurements via orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully automatic facial action unit detection and temporal analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">149</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Induced disgust, happiness and surprise: An addition to the MMI facial expression database</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Lang. Resour. Eval</title>
		<meeting>Int. Conf. Lang. Resour. Eval</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Foundations of human computing: Facial expression and emotion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimodal Interfaces</title>
		<meeting>Int. Conf. Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Social signal processing: Survey of an emerging domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1743" to="1759" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automated drowsiness detection for improved driver safety comprehensive databases for facial expression analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vural</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Technol</title>
		<meeting>Int. Conf. Autom. Technol<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Boosted multi-task learning for face verification with applications to web image and video search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Haar features for FACS AU recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>Int. Conf. Autom. Face Gesture Recognit<address><addrLine>Southampton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="97" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Accelerated gradient method for multi-task sparse learning problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Data Min</title>
		<meeting>Int. Conf. Data Min</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploring facial expressions with compositional features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2638" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>Int. Conf. Autom. Face Gesture Recognit<address><addrLine>Southampton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual classification with multi-task joint sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3493" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Discriminant graph structures for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1528" to="1540" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automatic image annotation using group sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3312" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic image annotation and retrieval using group sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="838" to="849" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards robust and effective shape modeling: Sparse shape compositon</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="277" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Active and dynamic information fusion for facial expression understanding from image sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="714" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Workshops (ICCVW)<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Authors&apos; photographs and biographies not available at the time of publication</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
