<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Visual Turing Test for Scene Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riley</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Visual Turing Test for Scene Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DE933FFCB3F764D12546EFDDBF5ED33</idno>
					<idno type="DOI">10.1109/3DV.2013.12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the first large scale system for capturing and rendering relightable scene reconstructions from massive unstructured photo collections taken under different illumination conditions and viewpoints. We combine photos taken from many sources, Flickr-based ground-level imagery, oblique aerial views, and streetview, to recover models that are significantly more complete and detailed than previously demonstrated. We demonstrate the ability to match both the viewpoint and illumination of arbitrary input photos, enabling a Visual Turing Test in which photo and rendering are viewed side-by-side and the observer has to guess which is which. While we cannot yet fool human perception, the gap is closing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The last few years have seen dramatic progress in the area of 3D reconstruction from photographs, to the point that much of the world has been reconstructed and can be browsed in tools like Microsoft's Photosynth, Google's Photo Tours, and Apple's Flyover 3D Maps.</p><p>Yet, we are still far from being able to generate 3D geometric models that look just like the real thing. Far from it; even the best-of-breed vision-based 3D reconstruction techniques are not good enough to support most computer graphics applications (games, films, virtual tourism, etc), and instead require extensive manual editing (in the case of 3D maps) or image-based rendering (e.g., Photosynth) to compensate for deficiencies in the reconstructed geometry.</p><p>But what exactly does it mean to look just like the real thing? One definition is that people should be unable to tell apart photos from renderings. For any photo, I can produce a scene rendering (for the same viewpoint and illumination conditions) that appears so realistic that you can't tell which one is real. This is a grand challenge problem; we call it the Visual Turing Test for Scene Reconstruction.</p><p>While we are still far from being able to pass the Visual Turing Test, it defines a useful benchmark, and a goal to strive for in 3D reconstruction research. Achieving this goal also necessitates two new capabilities that have not previously been demonstrated. First, we have to match any photo. This requires building a model that leverages all avail-able imagery, from the ground, the air, the walking paths, and the streets. To this end, we are the first to demonstrate models derived from Flickr photos, Streetview, and aerial imagery, merged into one reconstruction. We employ an unusually large number of photos (100K+) to create our models, to ensure that they are as complete as possible. Second, we must be able to render the scene to match the viewpoint and illumination in any photo. The latter requires estimating not only geometry, but surface reflectance, as well as the lighting in that particular photo. We present the first large scale results on reflectance estimation and lighting matching for Internet Photo Collections. To our best knowledge, we are also the first to conduct a large scale Visual Turing Test which consists of 100 randomly selected renderings, each of which is shown in 4 resolution scales, and 142 human test subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past few years, researchers succeeded in developing very large-scale 3D reconstruction pipelines for Internet photo collections <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> that can scale to millions of photos. These models are detailed but incomplete, containing holes in areas of sparse coverage. In contrast, aerial-based reconstructions in products like Google Earth and Apple's 3D Maps provide more uniform scene coverage, but lack the detail and resolution of ground-based models. We seek to achieve the best of both worlds, also leveraging Google Streetview imagery to fill gaps in coverage.</p><p>There is a large literature on the topic of reflectance and illumination modeling in the graphics and vision communities, going back multiple decades. The vast majority of prior work, however, assumed a controlled laboratory environment or imposed restrictions such as fixed lighting, or materials that do not vary over the surface. Most closely related to our work are methods that operate outside, "in the wild," with widely varying, unknown viewpoints, illumination, and surface material variation.</p><p>A few researchers have reconstructed depthmaps from time-lapse webcam videos of outdoor scenes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, where the fixed viewpoint and strong directional sun-light allow application of photometric stereo techniques.</p><p>Internet photo collections pose additional challenges, as both the viewpoint and illumination are unknown and vary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Google Earth</head><p>Building Rome in a Day <ref type="bibr" target="#b3">[4]</ref> PMVS Our reconstruction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference photo</head><p>Rendered view Relit at sunset Sunset reference photo in each photo. Indeed, the only previous work to attempt relighting for Internet photo collections is <ref type="bibr" target="#b7">[8]</ref> and they provided results for only one dataset (Statue of Liberty) consisting of six photos. While their lighting and reflectance model is more sophisticated than ours, it is not scalable-it took three hours to process the six image dataset. By using a more streamlined illumination and reflectance model, we are able to process tens of thousands of images, while achieving high quality visual results. Also related is work on multi-view intrinsic image decomposition <ref type="bibr" target="#b9">[10]</ref>. They recover a PMVS point cloud, which is then used to estimate per-point, per-view illumination (a single color value to represent the combined illumination and shading) which can be spread smoothly across each view; they leverage multi-view constraints on the (Lambertian) reflectance during estimation. This approach enables transferring illumination from one image to another that has many PMVS points in common, again after smoothly spreading the illumination across the second image. However, their datasets are fairly small, typically tens of images. Further, the method does not support general relighting, instead copying sparse illumination (a per-point color) from one image to another image, and both images must cover roughly the same portion of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preprocessing</head><p>Our pre-process consists of collecting images, recovering camera poses with structure-from-motion (SfM), recovering a point cloud with multi-view stereo (MVS), and recovering a mesh with per-vertex visibility to sets of images.</p><p>Given a landmark (e.g., the Colosseum), we download ground-level images from Flickr <ref type="bibr" target="#b0">[1]</ref> and obtain aerial images from Google. We augment this set with Google Streetview images in regions that are poorly covered by Flickr photos; these images are capture from the in-browser Streetview rendering. We also invert the sRGB function  typically applied to photographic imagery to put the pixel values in a linear space.</p><p>We then recover a triangle mesh for the landmark using freely available software. In particular, we employ Visu-alSFM <ref type="bibr" target="#b12">[13]</ref> to estimate camera poses, PMVS/CMVS <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> to recover a dense, oriented point cloud, and Poisson Surface Reconstruction <ref type="bibr" target="#b8">[9]</ref> to reconstruct a triangle mesh. We remove large (typically inaccurate) "hole-fill" triangles from the Poisson reconstruction; specifically, we filter out all triangles with average edge length greater than 20 times the average edge length of the entire mesh. Finally, we estimate a set of visible images per vertex, i.e., a set of images in which the vertex is visible. Below, we describe the SfM and visible image estimation steps in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SfM Reconstruction</head><p>For some datasets, the number of available images is quite large (e.g., 140K Flickr images of the Colosseum) with a considerable amount of overlap. Matching all the images to each other would be quite slow. Further, photos are typically concentrated around a small number of popular viewpoints <ref type="bibr" target="#b3">[4]</ref>, heavily oversampling those particular regions and needlessly slowing the entire pipeline. We take a two-step SfM approach that limits the number of images used and encourages good coverage around the landmark.</p><p>In the first step, we randomly subsample K images (we use K = 1000) to give an initial subset I K . The remaining images form a set I R . We apply VisualSFM to I K to recover camera poses and image matches; images match if they have common SfM features. The resulting set I 1 contains images that were matched successfully by VisualSFM.</p><p>In the second step, we augment I 1 with another subset I A taken from I R and then re-run VisualSFM. To add images to I A (which is initially empty), we match each of I 1 's images against the images in I R , where a match must have a large number of features in common (≥ 300) and be geometrically consistent with the initial SfM reconstruction. To encourage good coverage, we process the images in I 1 in order, starting with the images that have the fewest number of matches, thus giving priority to sparsely covered areas. Further, we do not consider images in I 1 that are already wellmatched (having ≥ 30 matches). To promote high quality reconstruction and to control the total number of images, we sort the images in I R according to image resolution and iterate through them (highest resolution first) when matching to an image in I 1 , stopping after finding a fixed number of matches (we set the number to 10). After an image from I R is matched, it is removed from I R and added to I A . When finished building I A , we re-run a second pass of VisualSFM on I 1 ∪ I A yielding a set I 2 containing images that matched successfully during reconstruction. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the results on the Colosseum after the two SfM steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visibility Estimation</head><p>After reconstructing a Poisson mesh and automatically trimming out large hole-fill triangles, we estimate a set of images in I 2 that can see each vertex, i.e., one visibility set per vertex. The original PMVS points already have a conservative visibility set per oriented point, a set comprised of images that matched well at that point; we use the PMVS points and visibility sets to bootstrap the process of estimating per-vertex visibilty.</p><p>Specifically, for each vertex v in the trimmed Poisson mesh, we collect the 30 nearest PMVS points and their visibility sets. We then select the 9 images that appear most frequently in those visibility sets. Next, we project all vertices in the 7-ring neighborhood of v (i.e., vertices within 7 edge hops from v) into the selected images and compute an average color at each of those vertices. We then consider each image I in I 2 . If v is facing away from I or if a ray cast from I to v hits another part of the Poisson model first, then I is eliminated from consideration. Otherwise, the 7ring neighborhood is projected into I, and the resampled colors from I are compared against the average colors of the vertices, using Normalized Cross Correlation (NCC) as the metric. If the NCC score is higher than a threshold (0.8), then I is added to v's visibility set. To accelerate the process, we find nearest PMVS neighbors using FLANN <ref type="bibr" target="#b10">[11]</ref> (set for exact neighbor-finding), and we use pre-computed z-buffers instead of ray casting for occlusion testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Lighting and Reflectance Estimation</head><p>Given the images with recovered poses and the reconstructed mesh with per-vertex visibility sets, we estimate lighting parameters for each image and reflectance parameters for each vertex. In the remainder of the section, we present our shading model and objective function, how to detect cloudy images (useful for bootstrapping the optimization), how we optimize for the shading model parameters, and finally some implementation notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shading model and objective function</head><p>For the outdoor scenes we are reconstructing, we adopt a simple but effective representation for illumination and materials. In particular, we assume the lighting is comprised of uniform, hemispherical sky illumination plus directional sunlight, and we assume all materials are diffuse.</p><p>Given a point P i (a vertex in the mesh), an image I j , and their associated shading parameters, the rendered pixel intensity R i,j of P i in I j is calculated with an ambient+diffuse shading model as follows:</p><formula xml:id="formula_0">R i,j (Θ) = a i f (N i )k sky j + max[0, L j • N i ]k sun j δ i,j ,<label>(1)</label></formula><formula xml:id="formula_1">Θ = {N i , a i , L j , k sky j , k sun j , δ i,j }.<label>(2)</label></formula><p>a i and N i are the surface albedo and normal at P i , respectively. k sky j and k sun j are the skylight (ambient) and sunlight (diffuse) intensities, respectively. L j is the lighting direction, parameterized in spherical coordinates. Please refer to the supplementary file for additional constraints on these variables. Note that we have not explicitly modeled camera exposure; instead, this is a scale factor that is implicitly pre-multiplied into the light intensities. δ i,j models sunlight visibility and is</p><formula xml:id="formula_2">1 if P i is in sunlight in image I j ,</formula><p>else it is 0. f (N i ) models the ambient (skylight) occlusion, that is, how much of the hemisphere is visible from, and hence, illuminates the point. In principle, we can use the input mesh model to take into account occlusions caused by the surrounding structure as in <ref type="bibr" target="#b7">[8]</ref>. For efficiency, we just use the normal N i to determine hemispherical sky visibility, ignoring occluders. f <ref type="bibr" target="#b11">[12]</ref>) where U is the unit-length "up" direction in the scene. Though we expect that adding sky occlusion due to surrounding geometry could improve results, we found that our simplified model works well in practice.</p><formula xml:id="formula_3">(N i ) = (1 -N i • U )/2 (derived in</formula><p>We assume, for the moment, that our images, albedos, and lighting are grayscale; we discuss color in Section 4.4.</p><p>Based on our shading equation ( <ref type="formula" target="#formula_0">1</ref>), the lighting and reflectance estimation problem can be formulated as follows:</p><formula xml:id="formula_4">argmin Θ i j∈Vi Ri,j R i,j (Θ) -Ri,j<label>2 2 .</label></formula><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>Ri,j is the observed pixel intensity of point P i in image I j , and V i is the list of image indexes in which P i is visible, where i and j are indexes to points and images. Note that the objective is simply the sum of squared differences of image intensities between the observation and what is predicted by our shading model, weighted by the squared root of the observed intensity. The weight is intended to give less weight to points that may be in shadow. This weighing scheme has proven effective, particularly in the early stage of the optimization, where {δ i,j } are all initialized to 1, i.e., all the points are assumed to be sunlit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Identifying Cloudy Images</head><p>Solving (3) on a large mesh with thousands of images is a very challenging problem because optimizing it has a high computational cost, exacerbated by the non-linearity of the functional which gives rise to numerous local minima. To improve both the computational efficiency and to avoid local minima, we make use of cloudy images, which have negligible sunlight intensity and can directly lead to estimates of skylight intensities and surface albedos for points visible in those images. (In our experiments, around 15% of photos are taken under cloudy weather and 40% of all the 3D points are visible in at least one of the cloudy images. This section describes how we identify cloudy images.</p><p>An image is identified as cloudy, if it passes at least one of the following three tests.</p><p>• The first test is on the camera shot setting stored inside the EXIF tag. We compute the exposure value as {exposure-time}{ISO-value} {F-number} 2</p><p>, and identify the image as cloudy, if the value is modest, that is, within the range [0.05, 5.0]. A small value typically indicates a sunny day with strong illumination, while a large value indicates a night-time shot.</p><p>• The second test is on the skyness at the top portion of an image, as inspired by Ackermann et al. <ref type="bibr" target="#b2">[3]</ref>. Given an image, we compute the average intensities in the RGB channels over the top 3% of the image region, and identify the image as cloudy, if (2B avg -R avg -G avg &lt; 100) holds.</p><p>• The last test is on the ratio between the skylight and sunlight intensities (k sky j /k sun j ) after lighting estimation (i.e., during optimzation). An image is identified as cloudy if the ratio is more than 10.</p><p>As described in Sec. 4.3, the first two tests are initially used to identify cloudy images. After the first lighting estimation, we include the third test to update cloudy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Algorithm</head><p>The core estimation algorithm consists of three steps: 1) partial albedo estimation from cloudy images; 2) lighting estimation; and 3) per-point albedo estimation (See Fig. <ref type="figure" target="#fig_1">2</ref> for the entire algorithm flow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skylight and partial albedo from cloudy images</head><p>For cloudy images, the shading equation ( <ref type="formula" target="#formula_0">1</ref>) does not have a sunlight component and is simplified to</p><formula xml:id="formula_6">R C i,j (N i , a i , k sky j ) = a i f (N i )k sky j .<label>(4)</label></formula><p>Let I C denote a set of cloudy images. We collect a set of points P C that are visible in at least three cloudy images, where estimation becomes reliable. The optimization problem (3) can similarly be reduced as follows:</p><formula xml:id="formula_7">argmin {ai,k sky j } = i∈P C j∈Vi∩I C Ri,j R C i,j (a i , k sky j ) -Ri,j<label>2 2 .</label></formula><p>(5) Note that the surface normal N i is technically a variable in <ref type="bibr" target="#b4">(5)</ref>. However, we instead use the Poisson normal, because normal estimation is unreliable without the directional lighting component, and the input mesh model has already fairly accurate surface normal estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lighting estimation</head><p>Even with partial surface albedo estimates, it is very expensive to solve (3) by using all the points, which could number in the tens of millions. We observe that not all the points are necessary to estimate lighting parameters; thus, we first focus on solving lighting parameters for each image, while operating on a small but effective set of points.</p><p>For lighting estimation, we would like to select a subset of points (vertices) that are visible in many images, but also achieve coverage by ensuring each image contains at least m(= 1000) such points. After initializing the set P L with 2000 points that have the most number of visible images, we pick an image that has less than m visible points, and add 100 points from that image to P L . The 100 points are randomly sampled, where the sampling probability is proportional to the number of visible images for each point, so that points with more visible images are more likely to be added. The process repeats until all the images have more than m points or no more points can be added. Now, we finally solve (3), but with two modifications. First, we use the subsampled point set. Second, we add a damping term to bias our solution to the surface albedo estimate ãi from (5) and the normal Ñi in the input mesh, giving a new objective:</p><formula xml:id="formula_8">argmin Θ i∈P L j∈Vi Ri,j R i,j (Θ) -Ri,j 2 2 +λ 1 i∈P C a i f (N i ) -ãi f ( Ñi ) 2 2 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>λ 1 = 1 is used in our experiments. Note that the damping term is added for points P C that are visible in some cloudy images and has estimates from (5).</p><p>After solving (6), we update the cloudy image set P C by using the estimated lighting parameters as in Section 4.2. Then, we solve ( <ref type="formula">5</ref>) and ( <ref type="formula" target="#formula_8">6</ref>) in exactly the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-point albedo and normal estimation</head><p>The final step is to fix the lighting parameters {L j , k sky j , k sun j }, then solve for the remaining parameters {N i , a i , δ i,j }, which can be optimized for each point independently:</p><formula xml:id="formula_10">argmin {Ni,ai,δi,j } j∈Vi Ri,j R i,j (Θ) -Ri,j 2 2 +λ 1 i∈P C a i f (N i ) -ãi f ( Ñi ) 2 2 + w i N i -Ñi 2 2 . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>The last term arises from the observation that when a point is visible only in a few images, normal and albedo estimation become very noisy. In this case, since the surface normal estimation from the input mesh model is fairly accurate, we add a damping term on the surface normal itself, while adaptively weighing the term based on the amount of available image information for that point. Note that Ñi is the surface normal in the input mesh. The definition of the per-point weight w i is given in the supplementary material. Note that our system optimizes surface normals. It contributes a modest improvement in capturing the lighting variation in the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation details</head><p>Here we describe several implementation details. First, we employ the Matlab function lsqnonlin to solve all of the the optimization problems.</p><p>Second, δ i,j is a binary variable and cannot be optimized easily. δ i,j is initialized to be 1 at the beginning. When δ i,j is a free variable in an optimization problem, we solve it in three steps: 1) Fix δ i,j and solve the other parameters with lsqnonlin; 2) Solve δ i,j while fixing the others for each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colosseum</head><p>San Marco Square , are all color values. In practice, when sunlight direction L j is not a free variable, we simply solve the optimization problem in each color channel independently. When L j is a free variable in an optimization, we first map the colors to grayscale (luminance) to solve the problem, then solve the same problem again in each color channel independently while fixing the lighting direction.</p><p>Poisson Surface Reconstruction <ref type="bibr" target="#b8">[9]</ref> converts a PMVS point cloud into a triangle mesh, where the output mesh resolution can be controlled by a parameter (depth). We noticed that increasing this parameter (and hence resolution) too much introduces surface artifacts, however, we still want to match the texture resolution of the mesh to that of the input images for optimal rendering. Therefore, we use a modest parameter for depth, in particular, 12 for the San Marcos Square and 13 for the Colosseum. Then, we simply apply the triangle subdivision -split a triangle into four smaller ones -to increase the mesh resolution, where the surface subdivision is adopted in two ways in our system. First, we subdivide the entire mesh once uniformly to increase its resolution. Second, regions of interest, which are specified by drawing rectangles on images, are subdivided by three times to increase resolution locally.</p><p>Lastly, inaccurate geometry at the top of the structure often projects to sky pixels in the input images. Since sky pixels are usually much brighter, the estimated albedos become very high and cause visible artifacts in the rendering. To address this, we adopt a simple thresholding method that removes mesh vertices that have near upward normals and have high albedo values. More concretely, we drop a mesh vertex, if the associated surface normal is within 9 degrees from the up-direction, and the estimated albedo value is more than 255 × 2 in any of the three color channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>This section presents the first large scale 3D reconstructions with lighting and reflectance models from community photo collections mixed with aerial and streetview imagery. Figure <ref type="figure" target="#fig_3">4</ref> shows the two datasets used in our experiments, where some statistics are given in Table <ref type="table" target="#tab_0">1</ref>. The computational time is collected by running the system on a cluster of 80 cores. The visibility and the per-point albedo estimation processes distribute workload to all 80 cores while the lighting estimation process runs on a single core. Figure <ref type="figure" target="#fig_3">4</ref> shows albedo renderings. Note that in the Colosseum model rendering, the points on the Colosseum are mostly reconstructed from ground level images, thus are much denser than the points on the rest of the city. The rest of the city is mostly created from aerial images that are taken within a short period of time. Since there is not much lighting variation in the aerial images, shadows on the ground are baked into the albedo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Visual Turing Test</head><p>We conducted a series of Visual Turing Tests to evaluate realism of our renderings using Amazon Mechanical Turk. We present a pair of images, one real and one rendered, from the same viewpoint and illumination condition, then ask the subject to specify which is "more realistic".</p><p>The results of the Visual Turing Tests depend on the image resolution. Simply put, the higher the image resolution, the easier it is to detect small imperfections in the reconstructed model. Therefore, we conducted tests for four different image resolutions, in particular, when the longer side of an image is 100, 200, 400, and 600 pixels in length.</p><p>We chose one hundred randomly selected Flickr photos as reference views. Each view is presented in four resolution levels; hence, there are in total four hundred image pairs in the study. Each image pair is shown to twenty test subjects. Low resolution images are sent to workers prior to high resolution images for the same viewpoint, to avoid having the high-res results (which are easier for subjects to get right) pollute the low-res tests. Some of the image pairs are shown in Figure <ref type="figure" target="#fig_5">5</ref>, where the real photos are on the left and our rendered images are on the right. Note that we don't feed the estimated shadow map into the rendering process as it contains shadows from foreground occluders. The sky is rendered with a simple sky dome texture mapping.</p><p>Visual Turing Test results are provided in Figure <ref type="figure" target="#fig_4">6a</ref>, where the x-axis corresponds to the hundred examples, and the y-axis is the probability, in which our renderings succeeded on the test, that is, fooled the subjects. Examples are sorted along the x axis in the ascending order of the success rate in the 100 pixel resolution. Clearly, the probability of picking a rendered image is higher at lower resolu- tion. Indeed, a handful of low-res rendered images actually passed the Visual Turing Test, meaning that the majority of the subjects believed our renderings are more realistic than the photos. The average success rate at the four resolutions are 0.3455, 0.16, 0.0735, and 0.034, respectively. Moreover, 30% of the subjects were fooled on almost half of the low-res tests, which suggests that passing the low-res Visual Turing Test is perhaps a goal within reach for 3D reconstruction research. Interestingly, there are a couple viewpoints in which subjects had trouble identifying real photos even for the highest resolution. Figure <ref type="figure" target="#fig_4">6b</ref> summarizes the statistics of the success rates (y-axis) over the hundred examples for the four resolution levels (x-axis). For each resolution, the five horizontal markings correspond to the minimum, lower quartile, median, upper quartile, and the maxium of the success rates.</p><p>Figure <ref type="figure" target="#fig_4">6c</ref> illustrates how many tests (out of four hundreds) are completed by each of the 142 subjects. Note that one worker participates in multiple tests, but cannot do the same test more than once (same photo, same resolution).</p><p>Finally, there are a number of other factors (apart from resolution) that appear to be correlated with success or failure on the Visual Turing Test. One is the presence of people (Figure <ref type="figure" target="#fig_6">7</ref>). Subjects are much more likely to pick a photo as more realistic if the photo contains people, which suggests that adding people to the renderings could improve realism. Visual artifacts can also give our results away. For the right-most result in Figure <ref type="figure" target="#fig_6">7</ref>, the viewpoint is located right behind some geometry fragments which occlude the object of interest and cause severe artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">More Evaluations</head><p>To further validate the accuracy of our lighting estimation, we render images with and without shadow effects  (Figure <ref type="figure">8</ref>). The shadows (highlighted with green ellipses) rendered with our estimated lighting configurations match those in the input images. The rightmost column shows the renderings from an aerial viewpoint, illustrating the presence of large shadows cast by the tower.</p><p>An alternative solution for reproducing lighting effects without estimating lighting and albedo is to compute average/median colors over the mesh from visible images and applying a histogram matching to ground truth images. However, as illustrated in Figure <ref type="figure" target="#fig_7">9</ref>, such an approach does not produce any directional lighting effects, which is crucial to visual fidelity. It also suffers from inconsistent colorization, because it does not properly handle widely varying viewpoints and illumination conditions that are present in Internet photo collections.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> illustrates the importance of the visibility test in our system, which removes the influences of foreground occluders that are often present in community photo collections. Our system can reveal structure behind occluders as if they are lit under the same illumination conditions. Figure <ref type="figure" target="#fig_0">10c</ref> shows a close-up view of structure where the subdivision scheme was used to increase the mesh resolution, which illustrates the fidelity of our reconstruction even at an inch-scale in a city-scale 3D model. Please see the supplementary material for more results and details on the Visual Turing Test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Limitations</head><p>We present a system to capture and render relightable scene reconstructions from massive unstructured photo collections consisting of Flickr photos, streetview and aerial images. Our system captures a wide range of lighting variations and scene reflectance, and recovers fine grain texture details. The evaluation on a large scale Visual Turing Test demonstrates the effectiveness of our system.</p><p>As a step towards solving the grand challenge of Visual Turing Test, our system has notable limitations and thus a number of areas for future work. We have not modeled ambient occlusion which is an important lighting effect. There is one coupled scale ambiguity between lighting colors and albedo values. Simple geometry might not provide enough information for light estimation, which further introduced ambiguity. Our system models outdoor environments under the sun and sky illuminations. It would be interesting to extend the framework to more complicated illumination models, e.g., night-time shots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison with state of the art (top row) and results for rendering and relighting of one viewpoint (bottom row)</figDesc><graphic coords="2,66.00,170.18,125.51,70.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Workflow overview. We highlight the key technical contributions of the proposed system.</figDesc><graphic coords="2,313.72,471.21,116.13,75.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Ensuring a uniform SfM reconstruction. (a) SfM model from a randomly subsampled image set. (b) The final SfM model after augmenting the image set.</figDesc><graphic coords="2,432.25,471.45,115.86,75.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Two datasets used in our experiments, where the reconstructions are rendered from aerial viewpoints with albedo colors without additional lighting effects. Top: Colosseum. Bottom: San Marco Square.</figDesc><graphic coords="5,314.16,72.82,132.87,85.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Statistics of the Visual Turing Test. Please zoom in for a better visualization of the plot. (a) Per-image average. (b) Per resolution level statistics. (c) Worker plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visual Turing test. In each image pair, the ground truth image is on the left and our result is on the right.</figDesc><graphic coords="7,174.02,441.09,79.69,59.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Typical failure cases. Bad geometry and people are two major causes for our method to fail the Visual Turing Test. More than 90% of test subjects pick the reference photos (left) as more realistic in every resolution level for these examples.</figDesc><graphic coords="7,94.12,441.09,79.68,59.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Rendering with median color and histogram matching. Left: input ground truth images; middle: our results; right: images rendered from average pixel color and applying histogram matching to the ground truth image. Note the lack of directional lighting effects and color noise in red ellipses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .Figure 10 .</head><label>810</label><figDesc>Figure 8. Validating the accuracy of our lighting estimation.</figDesc><graphic coords="8,150.44,256.73,88.45,66.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of our datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Flickr # Input Images Street-view Aerial</cell><cell># Images after SfM</cell><cell># MVS points</cell><cell># Mesh vertices</cell><cell cols="3">Running time [hour] Visibility estimation Lighting estimation Per-point albedo estimation</cell></row><row><cell>Colosseum</cell><cell>140k</cell><cell>77</cell><cell>14</cell><cell>3,267</cell><cell>10m</cell><cell>27m</cell><cell>20</cell><cell>~ 1</cell><cell>3</cell></row><row><cell>San Marco</cell><cell>14k</cell><cell>33</cell><cell>0</cell><cell>2,687</cell><cell>23m</cell><cell>34m</cell><cell>23</cell><cell>~ 1</cell><cell>4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-5067-1/13 $26.00 © 2013 IEEE DOI 10.1109/3DV.2013.12</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by funding from National Science Foundation grant IIS-0963657, Google, Intel, Microsoft, and the UW Animation Research Labs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Flickr</surname></persName>
		</author>
		<ptr target="http://www.flickr.com.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heliometric stereo: Shape from sun position</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photometric stereo for outdoor webcams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building rome on a cloudless day</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Jen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards internet-scale multi-view stereo</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relighting objects from image collections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bekaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coherent intrinsic images from photo collections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Area light sources for real-time graphics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Snyder</surname></persName>
		</author>
		<idno>MSR-TR-96-11</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://homes.cs.washington.edu/˜ccwu/vsfm" />
		<title level="m">VisualSFM : A visual structure from motion system</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
