<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEPA: Self-Supervised Audio Embedding for Depression Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-28">28 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pingyue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mengyue</forename><surname>Wu</surname></persName>
							<email>mengyuewu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Heinrich</forename><surname>Dinkel</surname></persName>
							<email>heinrich.dinkel@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<email>kai.yu@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence X-LANCE Lab</orgName>
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence X-LANCE Lab</orgName>
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence X-LANCE Lab</orgName>
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence X-LANCE Lab</orgName>
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEPA: Self-Supervised Audio Embedding for Depression Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-28">28 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3479236</idno>
					<idno type="arXiv">arXiv:1910.13028v3[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies â†’ Neural networks</term>
					<term>Supervised learning by classification</term>
					<term>Supervised learning by regression</term>
					<term>Multi-task learning Deep neural networks</term>
					<term>automatic depression detection</term>
					<term>self-supervised learning</term>
					<term>feature embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depression detection research has increased over the last few decades, one major bottleneck of which is the limited data availability and representation learning. Recently, self-supervised learning has seen success in pretraining text embeddings and has been applied broadly on related tasks with sparse data, while pretrained audio embeddings based on self-supervised learning are rarely investigated. This paper proposes DEPA, a self-supervised, pretrained depression audio embedding method for depression detection. An encoderdecoder network is used to extract DEPA on in-domain depressed datasets (DAIC and MDD) and out-domain (Switchboard, Alzheimer's) datasets. With DEPA as the audio embedding extracted at responselevel, a significant performance gain is achieved on downstream tasks, evaluated on both sparse datasets like DAIC and large major depression disorder dataset (MDD). This paper not only exhibits itself as a novel embedding extracting method capturing responselevel representation for depression detection but more significantly, is an exploration of self-supervised learning in a specific task within audio processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Depression, a disease of considerable attention, has been affecting more than 300 million people worldwide. An increasing amount of research has been conducted on automatic depression detection and severity prediction, in particular, from conversational speech, which has embedded crucial information about one's mental state. Despite recent advances in deep learning, automatic depression detection from speech remains a challenging task.</p><p>Since depression is a complicated mental disorder consisting of various symptoms, utilizing traditional feature extraction methods for emotion recognition might lack precision in asserting each individual's mental state. Previous exploration has covered commonlyused emotion-related features such as COVAREP <ref type="bibr" target="#b4">[5]</ref>, general-purpose audio features including log-Mel spectrogram (LMS), and combination of Short-Time Fourier Transform (STFT) and Mel-Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b19">[20]</ref>, and speaker-related audio embedding like i-vector <ref type="bibr" target="#b3">[4]</ref>. However, as these features are not tailored for application in assessing mental disorders, thus could be less efficient towards such a task with high specificity.</p><p>Another characteristic of depression is that usually only one single label (diagnosis results) is provided for a multi-turn interview. Specifically, during a session with a doctor, it would be impossible to give a specific label ğ‘¦ ğ‘¡ âˆˆ {0, 1}, representing the mental state (depressed or healthy) for each time step ğ‘¡. Here ğ‘¡ can be chosen on any arbitrary level, such as phone-, word-or sentence-level. Those long sequences subsequently influence depression detection performance. Previous work has hinted that extracting embeddings on segment-level (e.g., sentence, response) might benefit performance <ref type="bibr" target="#b19">[20]</ref>, while modeling depression via a stationary, time-step independent representation is likely to fail <ref type="bibr" target="#b0">[1]</ref>. Hence, a successful audio-embedding for depression detection needs to be extracted on sequence-level (e.g., spoken sentence/utterance), to capture rich, long-term spoken context as well as emotional development within an interview.</p><p>The last important problem is that models so far are heavily restricted by the limited amount of depression data. Hence even with the recent advances of deep learning, this data sparsity has caused difficulty in model performance enhancement and reproduction. One potential solution to the aforementioned data sparsity problem is to pretrain a model on large data and then leverage the model's knowledge to a downstream task. However, pretraining on supervised tasks (e.g., speech recognition) is time-intensive and costly with manual labeling. Self-supervised training, which utilizes the original property of the data, can potentially remove the dependency on manual labels, thus being able to easily scale with data. Contribution. This paper proposes DEPA, a self-supervised, pretrained depression audio embedding method for automatic depression detection (see Figure <ref type="figure" target="#fig_0">1</ref>). To our knowledge, this is the first time a self-supervised neural network pretraining is performed on a depression detection task.</p><p>â€¢ We achieved the highest classification performance and lowest regression errors on the benchmark depression detection dataset by modeling each patient's response via the sequence of his/her uttered speech and realize the extraction of response-level representation with DEPA. â€¢ To highlight the necessity of using sentence-level representations for tasks like depression detection, we compared with previously-used audio features, including general-purpose features, emotion-related representations, and x-vector speaker embeddings. Results suggest a significant performance gain with the use of DEPA and the efficiency of sequence-level representations. We also design several experiments to further illustrate performance enhancement by using sentence-level representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, related work on depression detection and selfsupervised learning will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Depression detection</head><p>Various methods have been proposed for automatic depression detection. Representation learning and classifier selection are the two major research areas within depression detection. Deep learning methods have been employed to extract high-level feature representations <ref type="bibr" target="#b0">[1]</ref>. In particular <ref type="bibr" target="#b10">[11]</ref> utilized causal convolutional neural networks (C-CNN) to enable sequence-level feature extraction and achieved a high performance by combining visual, audio, and textual modalities. Results indicate that sequence-level representation outperforms frame-level ones with respect to depression detection. Notable work on pretraining audio features for depression detection includes <ref type="bibr" target="#b21">[22]</ref>, which trained an audio word-book in unsupervised fashion using Gaussian mixture models to extract segment-level features and uses a BLSTM model with max-temporal pooling as a depression classifier. <ref type="bibr" target="#b19">[20]</ref> investigated a knowledge transfer from emotion recognition to depression detection by firstly pretraining a recurrent neural network on a fully-labeled emotion recognition dataset. Their results suggest that emotion is a possible marker for automatic depression detection and that transfer learning enhances performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised learning</head><p>Self-supervised learning is a technique where training data is autonomously labeled, yet the training procedure is supervised. A classic example of self-supervised learning is auto-encoders <ref type="bibr" target="#b7">[8]</ref>, aiming to reconstruct a given input from a hidden representation. Learning representations with self-supervised training has lead to remarkable improvement in several fields, including textual, visual, audio, and multimodal processing. In natural language processing (NLP), self-supervised text embedding pretraining can be seen as a major breakthrough, with methods such as GloVe <ref type="bibr" target="#b15">[16]</ref>, BERT <ref type="bibr" target="#b6">[7]</ref>, and ELMo <ref type="bibr" target="#b16">[17]</ref>. Self-supervised pretraining from audio-visual signals, such as SoundNet <ref type="bibr" target="#b1">[2]</ref>, have been found to outperform traditional spectrogram-based features in acoustic environment classification.</p><p>In fact, much research has focused on self-supervised audio-visual segmentation and feature extraction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Recently, many researches in Computer Vision field use contrastive learning as their self-supervised learning method. SimCLR <ref type="bibr" target="#b2">[3]</ref>, MoCo <ref type="bibr" target="#b11">[12]</ref>, and CoCLR <ref type="bibr" target="#b9">[10]</ref> all use contrastive learning with some variations to retain visual representations.</p><p>In particular, pretrained approaches such as EmoAudioNet <ref type="bibr" target="#b18">[19]</ref> have been applied in depression detection, however, its pretraining process requires large 1000h (Librispeech), gender-labeled training data to be successful, which requires extensive manual labor. Our main inspiration for this work stems from Audio2Vec <ref type="bibr" target="#b24">[25]</ref>, where a self-supervised approach was proposed, the objective of which is to extract general-purpose audio representations for mobile devices.</p><p>Relatively, little research has been conducted on self-supervised audio representation learning in depression detection, or such medical applications. The reasons could include: 1) Content-rich audio contains undesirable information, such as environmental sounds, interfering speech, and noise. 2) Features are typically low-level and extracted within a short time-scale (e.g., 40 ms), each containing little information about high-level concepts (e.g., a single phoneme contains little information about a sentence). 3) Due to the nature of depression detection, interviews are comparatively long (many minutes), which, combined with fine-scale features, means that a classifier needs to remember very long sequences while filtering out unimportant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEPA: SELF-SUPERVISED AUDIO EMBEDDING</head><p>This paper proposes DEPA, an auditory feature extracted via a neural network to summarize spoken language. Our proposed method consists of a self-supervised convolutional encoder-decoder network, where the encoder is later used as DEPA embedding extractor from spectrograms. Given a spectrogram of a specific audio clip (e.g., a spoken sentence) X âˆˆ R ğ‘†Ã—ğ· , where ğ‘† is the number of frames and ğ· the data dimension (e.g., frequency bins). We proceed to slice X into</p><formula xml:id="formula_0">ğ‘† (1+ğ›¼) (2ğ‘˜+1) â€¢ğ‘‡ non-overlapping samples X ğ‘– âˆˆ R ( (2ğ‘˜+1) â€¢ğ‘‡ )Ã—ğ·</formula><p>, where ğ‘‡ is the number of frames in one sub-spectrogram which will be explained below, ğ‘˜ is the hyperparameter which controls number of such sub-spectrograms in one sample, and ğ›¼ â‰¥ 0 is the gap parameter such that each sample being ğ‘”ğ‘ğ‘ âˆˆ R (ğ›¼ â€¢(2ğ‘˜+1) â€¢ğ‘‡ )Ã—ğ· apart. The gap between two segments avoids that the self-supervised model exploits spectral leakage to shortcut and easily solve the task.</p><formula xml:id="formula_1">X = [X 0 , ğ‘”ğ‘ğ‘, X 1 , ğ‘”ğ‘ğ‘, â€¢ â€¢ â€¢ , X ğ‘– , â€¢ â€¢ â€¢ , ] Each sample X ğ‘– is sliced into 2ğ‘˜ + 1 sub-spectrograms M ğ‘— âˆˆ R ğ‘‡ Ã—ğ· . Each X ğ‘–</formula><p>is therefore the concatenation of a center M 0 subspectrogram and its ğ‘˜ adjacent left-right context:</p><formula xml:id="formula_2">X ğ‘– = [M âˆ’ğ‘˜ , â€¢ â€¢ â€¢ , M âˆ’1 , M 0 , M 1 , â€¢ â€¢ â€¢ , M ğ‘˜ ] ,</formula><p>If X is shorter than (2ğ‘˜ +1)ğ‘‡ frames, we pad zeros to fill (2ğ‘˜ +1)ğ‘‡ frames.</p><p>Our self-supervised learning uses a generative strategy. The training process treats the center spectrogram M 0 as the target, taking its surrounding spectrograms M ğ‘— , ( ğ‘— â‰  0) to re-generate the center spectrogram and computes the embedding loss (Equation ( <ref type="formula" target="#formula_3">1</ref>)). Figure <ref type="figure" target="#fig_1">2</ref> shows the slicing process, and the detailed pretraining process is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gap</head><formula xml:id="formula_3">L ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ = 1 ğ‘‡ ğ¹ ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ğ¹ âˆ‘ï¸ ğ‘‘=1 (M 0 ğ‘¡,ğ‘‘ âˆ’ M â€² 0 ğ‘¡,ğ‘‘ ) 2 . (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>Encoder architecture. The encoder architecture contains three downsampling blocks, followed by an extra convolution layer as well as an adaptive pooling layer. Each block consists of a convolution, average pooling, batch-normalization, and rectified linear unit (ReLU) activation layer. The time-axis 2ğ‘˜ğ‘‡ is subsampled to 2ğ‘˜ğ‘‡ 64 before being average pooled in time and frequency dimension.</p><p>Decoder architecture. The decoder upsamples the encoder output v via four transposed convolutional upsampling blocks and predicts the center spectrogram M â€² 0 âˆˆ R ğ‘‡ Ã—ğ· . The encoder-decoder architecture is shown in Figure <ref type="figure" target="#fig_3">3</ref>.  After pretraining the encoder-decoder network, DEPA is extracted via feeding a variable-length audio segment R (here on patient response-level) into the encoder model and obtaining a single |v| = 256-dimensional embedding per segment. The sequence of DEPA embeddings is then further fed into a depression detection network, which can be seen in Figure <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DOWNSTREAM TASK: DEPRESSION DETECTION</head><p>In this section, we detail our approach to the downstream task of depression detection on two datasets: DAIC, a small dataset used as depression detection benchmark; MDD, a large dataset focused specifically on female patients with major depression detection (see section 5 for a detailed introduction).</p><p>Small Benchmark Data with Two Label Sets. Depression state and severity score is provided in the DAIC dataset, hence, we propose a multi-task scheme, combining depression state classification and depression score prediction. This approach models a patients' depression sequentially, meaning that only the patients' responses are utilized. Due to the recent success of LSTM networks in this field <ref type="bibr" target="#b0">[1]</ref>, our depression prediction structure follows a bidirectional LSTM (BLSTM) approach with four layers of size 128.</p><p>The model outputs for response ğ‘Ÿ a two dimensional vector </p><formula xml:id="formula_5">â„“ ğ‘ğ‘ğ‘’ (ğ‘¦ â€² ğ‘ , ğ‘¦ ğ‘ ) = -[ğ‘¦ ğ‘ â€¢ log ğ‘¦ â€² ğ‘ + (1 âˆ’ ğ‘¦ ğ‘ ) log(1 âˆ’ ğ‘¦ â€² ğ‘ )]<label>(2)</label></formula><formula xml:id="formula_6">â„“ â„ğ‘¢ğ‘ (ğ‘¦ â€² ğ‘Ÿ , ğ‘¦ ğ‘Ÿ ) = 0.5(ğ‘¦ ğ‘Ÿ âˆ’ ğ‘¦ â€² ğ‘Ÿ ) 2 , if |ğ‘¦ ğ‘Ÿ âˆ’ ğ‘¦ â€² ğ‘Ÿ | &lt; 1 |ğ‘¦ ğ‘Ÿ âˆ’ ğ‘¦ â€² ğ‘Ÿ | âˆ’ 0.5, otherwise<label>(3)</label></formula><formula xml:id="formula_7">â„“ (ğ‘¦ â€² ğ‘ , ğ‘¦ ğ‘ , ğ‘¦ â€² ğ‘Ÿ , ğ‘¦ ğ‘Ÿ ) = â„“ ğ‘ğ‘ğ‘’ (ğœ (ğ‘¦ â€² ğ‘ ), ğ‘¦ ğ‘ ) + â„“ â„ğ‘¢ğ‘ (ğ‘¦ â€² ğ‘Ÿ , ğ‘¦ ğ‘Ÿ )<label>(4)</label></formula><p>Two outputs are constructed, one directly predicts the binary outcome of a participant being depressed, the other outputs the estimated PHQ-8 score. We opt to use a combination of binary cross entropy (BCE, for binary classification, Equation ( <ref type="formula" target="#formula_5">2</ref>)) and Huber loss (for regression, Equation (3)). ğ‘¦ ğ‘ , ğ‘¦ ğ‘Ÿ are the ground truth binary and PHQ-8 score, respectively, while ğœ is the sigmoid function. In this way, our model considers the internal relationship between binary classification and PHQ-8 score regression, where a higher PHQ-8 score commonly indicates a probability of being classified as depressed.</p><p>Large Data with One Classification Label. MDD, a privately collected large depression dataset, is also applied in our downstream detection task. For this dataset, we merely predict the depression state, which is the only label provided. Therefore, the utilized method is similar to the one above with minor changes: the BLSTM only output one scaler: ğ‘¦ â€² ğ‘ , and only a binary cross-entropy loss â„“ ğ‘ğ‘ğ‘’ is used. Similarly, we model the patient's response in a sequential manner.  In addition, a large conversational dataset (MDD) for major depression disorder detection under collection has now consisted of 1000 hours of speech conversation between interviewers and subjects, with a balanced proportion of healthy and depressed participants (722 depressed and 527 healthy). We split the dataset into a training set (70%), a development set (15%), and a test set (15%). Unlike the fully-transcribed DAIC dataset, no annotation is provided in MDD. We hence applied the X-vector-based speaker diarization tool provided by the Kaldi Toolkit <ref type="bibr" target="#b17">[18]</ref> to extract all patient's speaking segments from the audio. MDD is incorporated to highlight the benefit of summarizing long sequences using DEPA. Detailed statistics regarding the proportion of depressed/healthy subjects, the number of patient responses, and their average duration is displayed in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETUP</head><p>Pretraining Data. We aim to compare DEPA in regards to pretraining on related, e.g., in-domain (depression detection) and outdomain (e.g., speech recognition) datasets.</p><p>Regarding in-domain data, we utilized the aforementioned DAIC and MDD datasets (we take a subset of 411 hours) for in-domain pretraining in order to compare DEPA to traditional audio feature approaches. In order to ascertain DEPAs' usability, we further used the mature Switchboard (SWB) <ref type="bibr" target="#b8">[9]</ref> dataset, containing 300 hours of English telephone speech. Lastly, we utilized the Alzheimer's disease (AD) dataset, collected by Shanghai Mental Clinic Center <ref type="bibr" target="#b12">[13]</ref>, containing about 400 hours (questions and answers) of Mandarin interview recordings from elderly participants. The four datasets are described in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Feature Selection. Regarding front-end features, our work investigates common LMS and log-power STFT features. Due to different sample rates across the datasets, we resample each dataset's audio to 22050 Hz. All following features are extracted as default with a hop length (ğœ” â„ğ‘œğ‘ ) of 5ğ‘šğ‘  and a Hann window length (ğœ” ğ‘¤ğ‘–ğ‘› ) of four times ğœ” â„ğ‘œğ‘ (e.g., 20ğ‘šğ‘ ). 128 dimensional LMS and 512-dimensional STFT features were chosen as the default signal-processing frontend. In order to compare DEPA against non-self-supervised approaches, 553-dimensional higher-order (mean, median, variance, min, max, skewness, kurtosis) COVAREP <ref type="bibr" target="#b4">[5]</ref> (HCVP) features were extracted on response-level. HCVP can be seen as a traditional high-level representation, which is an ensemble of lower-level descriptors, such as MFCC, pitch, glottal flow, and other features.</p><p>Lastly, we also extracted 256-dimensional x-vectors a Resnet34 structure <ref type="bibr" target="#b22">[23]</ref>, for comparison purposes. X-vectors, which are a state-of-the-art method within speaker recognition, have been seen to outperform traditional i-vectors, reported as markers for depression and some other mental diseases <ref type="bibr" target="#b3">[4]</ref>. Moreover, the model is trained for 25 epochs using Adam optimization with a starting learning rate of 0.004, and a batch size of 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEPA Pretraining</head><p>Depression Detection Training Process. As mentioned, for the DAIC dataset, we used a multi-task learning strategy to output both binary classification and PHQ-8 scores with a BLSTM network structure. Regarding the MDD dataset, the BLSTM only output classification prediction. Data standardization was applied by calculating a global mean and variance on the training set and using those on the development set. A dropout of 0.1 was applied after each BLSTM layer to prevent overfitting. Adam optimization with a starting learning rate of 4e âˆ’5 and a batch size of 1 was used.</p><p>Metrics. Following previous work <ref type="bibr" target="#b0">[1]</ref>, results are reported in terms of mean average error (MAE) and root mean square deviation (RMSE) for regression and macro-averaged (class-wise) precision, recall, and their harmonic mean (F1) score for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>Results on the two different datasets are provided respectively: DAIC, a benchmark dataset for depression detection, is used to compare against previously methods and demonstrate how DEPA can help boost performance on sparse data scenarios; MDD, by contrast, provides insight on how DEPA compares with raw features, along with a different number of input responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DAIC Results</head><p>Our results using the proposed BLSTM approach with and without DEPA pretraining are compared to previous attempts in Table <ref type="table" target="#tab_3">3</ref>. The results are analyzed on multiple levels.</p><p>Feature Level Comparison. The results in Table <ref type="table" target="#tab_3">3</ref> are in line with our initial assumption, that frame-level audio-features are indeed underperforming compared to response-level ones, especially for the BLSTM model. This is likely due to the models' inherent incapability to remember very long sequences (&gt;10000 frames) for an abstract task such as depression detection, which is also commonly seen within other audio processing tasks where long sequences are harder to predict. Regarding the classification results, it can be seen that traditional HCVP features outperform LMS, STFT, and x-vector approaches. Specifically, with respect to regression, HCVP achieves a score of 4.95, much lower than any other frame-level feature approach (LMS, STFT, dMFCC-VT, CVP). The sub-optimal performance of the x-vector system is likely due to the short response durations in this dataset, being on average â‰ˆ 2 seconds long. The performance of x-vector systems generally decreases for utterances shorter than 3 seconds <ref type="bibr" target="#b22">[23]</ref>. Furthermore, response-level features are likely to contain more context-related information, while framelevel features tend to isolate information between frames.</p><p>Feature Comparison. Even though a multitude of features are compared (MFCC, LMS, LLD, CVP, STFT), no clear trend can be established between feature and final performance. Regarding our BLSTM approach, STFT features consistently underperform against LMS and HCVP features in terms of MAE. This is likely due to the increased amount of parameters needed to be estimated by the BLSTM model (input layer increases from 128 to 512) in conjunction with the limited available training data. This can partially be improved by either reducing the feature size (e.g., utilize LMS, MFCC features) or the number of samples per speaker (e.g., use the response averaged HCVP features). By contrast, when experimented with DEPA features, extracting DEPA from STFT features constantly outperforms DEPA from LMS features.</p><p>Pretraining Datasets Comparison. DEPA pretraining on the same DAIC dataset can be seen to enhance performance for LMS (F1 0.61 â†’ 0.68) and especially STFT features (F1 0.64 â†’ 0.90). This, in turn, reinforces our initial assumption that response-level features are much more useful for depression detection. Pretraining on large datasets (MDD, SWB, and AD) outperformed DAIC in terms of binary classification as well as regression. Further, pretraining on AD resulted in the best performance in terms of all metrics. Larger datasets (DAIC &lt;SWB &lt;MDD = AD) for DEPA pretraining generally improve performance for STFT features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MDD Results</head><p>We further evaluated DEPA performance on large depression dataset MDD and reported the result on the test set, with the best setting observed from the DAIC experiments. We compared the difference of using raw STFT features and DEPA STFT pretrained on MDD, along with a different number of patient's speech responses as the model input. As seen in Figure <ref type="figure" target="#fig_7">5</ref>, DEPA largely outperforms raw features without DEPA pretraining, regardless of the input response numbers. We have not conducted experiments of raw STFT using more than 400 responses because the sequences are too long. More input responses increase the performance and the ğ¹ 1 score soars to 0.84 when we input all responses with DEPA. On the contrary, when using raw features the performance is negatively correlated with the input responses. The average duration of one response is around 0.7ğ‘  (Table <ref type="table">1</ref>) and raw STFT is extracted with a hop length ğœ” â„ğ‘œğ‘ = 5ğ‘šğ‘ . Hence one response approximately corresponds to a 140 Ã— 512 raw STFT feature, which is 140 times longer than DEPA. This suggests DEPA's superior capability of summarizing sequences, leading to a performance enhancement with increasing amount of input length.</p><p>The results demonstrate that, for medical tasks where a summarizing label is predicted based on a long, multi-turn conversation, a sentence-level representation is necessary while traditional framelevel feature extraction methods might fail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation</head><p>We first design an experiment to demonstrate the importance of response-level representations for depression detection. Then a series of ablation studies are conducted to analyze possible factors which may influence the pretraining model. Lastly, additional generative strategies are compared with the current center spectrogram generation strategy.</p><p>Segment length. In order to validate the importance of extracting response-level features, we compare the performance of DEPA extracted from different segment lengths (number of frames) in Table <ref type="table" target="#tab_5">4</ref>. In this experiment, each response is first split into subsegments of length ğ¿ ğ‘ ğ‘’ğ‘” âˆˆ {150, 300, 500} ms, whereas for each ğ¿ ğ‘ ğ‘’ğ‘” a single DEPA feature is extracted. Segments are extracted without overlap, and zero-padding is used for the last segment. Each segment length can be interpreted as being on a word (150 ms), short sentence (300 ms), and sentence (500 ms) level.</p><p>The results indicate an increase in segment length for DEPA extraction commensurates with an increase in binary classification performance. More significant amounts of frames increase the performance, whereas the performance peaks when DEPA is extracted at the response level (the default). Note that larger chunks represent a shorter amount of input features the BLSTM needs to process. This subsequently benefits the learning mechanism of the BLSTM. The larger the chunks are, the more information is summarized; hence fewer independent features are extracted. Interestingly, even though the binary classification performance is greatly enhanced, the MAE score is less influenced. This might indicate that DEPA is an excellent tool for summarizing interviewee sentences yet falls short in extracting meaningful depression-related information. However, note that an MAE of â‰ˆ 5.5 is in line with most results in previous works (see Feature frame-shift. Experiments comparing differences in frameshift (ğœ” â„ğ‘œğ‘ ) for DEPA extraction in regards to different values of ğ‘˜ can be seen in Table <ref type="table" target="#tab_6">5</ref>. Note that for all experiments, the window size ğœ” ğ‘¤ğ‘–ğ‘› is set to four times the frame-shift.</p><p>The results indicate that using a smaller ğœ” â„ğ‘œğ‘ enhances performance (compare ğœ” â„ğ‘œğ‘ = 5, 10). A short hop-size is potentially beneficial to detect slight variations in speech, leading to better performance.</p><p>A similar improvement in binary classification performance can be observed when increasing the number of adjacent spectrograms fed into the network ğ‘˜, culminating at ğ‘˜ = 5. Generative Strategy. In addition to generating the center spectrogram given the context, we conduct experiments to compare the performance of two different generative strategies: forward and backward. In the forward strategy (Figure <ref type="figure" target="#fig_9">6a</ref>), we predict the last spectrogram M ğ‘˜ with all previous ones M ğ‘– , ğ‘– = [âˆ’ğ‘˜, âˆ’ğ‘˜+1, â€¢ â€¢ â€¢ , ğ‘˜âˆ’1] in the sample, while in the backward strategy (Figure <ref type="figure" target="#fig_9">6b</ref>), the first spectrogram M âˆ’ğ‘˜ is treated as prediction target. Results in Table <ref type="table" target="#tab_7">6</ref> indicate that generating the center spectrogram performs the best among three different patterns. It's probably because that the center strategy combines both forward and backward ones and benefits from it. Furthermore, the backward strategy outperforms the forward one, probably because it's harder to figure out the cause (backward) than the result (forward), and a harder pretraining process is beneficial to the downstream task depression detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This work proposes DEPA, an audio embedding pretraining method for automatic depression detection. An encoder-decoder model is trained in a self-supervised fashion to predict and reconstruct a center spectrogram given a spectrogram context. Then, DEPA is extracted from a trained encoder model and fed into a depression detection BLSTM network. DEPA exhibits an excellent performance compared to traditional LMS, STFT, HCVP, x-vector, and other commonly used features. DEPA pretrained on In-domain DAIC suggests a significantly better result on detection presence detection using STFT features (DAIC F1 0.90, MDD F1 0.94) compared to LMS features (DAIC F1 0.68, MDD F1 0.71) as well as other approaches without DEPA. Pretraining on large datasets, e.g. DEPA on AD reached F1 0.94 &amp; MAE 4.75, further shows that additional outdomain data is beneficial to depression detection research. Results validated on MDD also illustrate performance enhancement with DEPA's superior capability of summarizing sequences. DEPA can be a generalized method for similar tasks that need to summarize long sequences given a single output label, for e.g. most medical  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The DEPA pretraining framework. The training objective follows the estimation of a middle spectrogram within a sequence of 2ğ‘˜ + 1 spectrograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Slicing an audio clip spectrogram (X) into samples X ğ‘– and sub-spectrograms M ğ‘— for DEPA training. The gap avoids spectral leakage of a sub-spectrogram to its neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DEPA pretraining encoder-decoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(ğ‘¦ â€² ğ‘ (ğ‘Ÿ ), ğ‘¦ â€² ğ‘Ÿ (ğ‘Ÿ )), representing the estimated binary patient state (ğ‘¦ â€² ğ‘ (ğ‘Ÿ )) as well as the PHQ-8 score (ğ‘¦ â€² ğ‘Ÿ (ğ‘Ÿ ), a numerical metric to evaluate depression extent). Finally, first timestep pooling is applied to compile all responses of a patient to a single vector (ğ‘¦ â€² ğ‘ (0), ğ‘¦ â€² ğ‘Ÿ (0)). The architecture is shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Depression detection with DEPA on DAIC with multi-task training scheme. The encoder from the proposed encoder-decoder model provides the BLSTM network with high-level auditory features. In this figure, DEPA is extracted on response-level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Process. Our encoder-decoder training utilizes LMS and STFT front-end features, with hyper-parameters ğ‘˜ = 5,ğ‘‡ = 96, ğ›¼ = 0.1, extracting a |v| = 256 dimensional DEPA embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Depression detection w and w/o DEPA pretraining on MDD, experimented with different number of patient's responses incorporated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two different strategies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>In-and out-domain datasets used for DEPA pretraining.</figDesc><table><row><cell cols="3">Domain Dataset Duration (h) Language</cell></row><row><cell>In</cell><cell>DAIC MDD</cell><cell>13 411 Mandarin English</cell></row><row><cell>Out</cell><cell>SWB AD</cell><cell>300 400 Mandarin English</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between DEPA and other audio-based depression detection methods on the DAIC development set.</figDesc><table><row><cell>Classification</cell><cell>Regression</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 )</head><label>3</label><figDesc>.</figDesc><table><row><cell></cell><cell>Classification</cell><cell></cell><cell cols="2">Regression</cell></row><row><cell>Duration</cell><cell>Pre Rec</cell><cell cols="3">F1 MAE RMSE</cell></row><row><cell>150 ms</cell><cell cols="2">0.58 0.54 0.56</cell><cell>5.51</cell><cell>6.51</cell></row><row><cell>300 ms</cell><cell cols="2">0.68 0.69 0.69</cell><cell>5.63</cell><cell>6.37</cell></row><row><cell>500 ms</cell><cell cols="2">0.78 0.76 0.77</cell><cell>5.57</cell><cell>6.43</cell></row><row><cell cols="4">response 0.93 0.96 0.94 4.75</cell><cell>5.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison on performance of DEPA extracted from different segment lengths. The encoder-decoder has been trained on the AD dataset with hyperparameters ğ‘˜ = 5, ğœ” â„ğ‘œğ‘ = 5ğ‘šğ‘ .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison between different configurations of ğ‘˜ and the hop size ğœ” â„ğ‘œğ‘ during pretraining on the AD dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Classification</cell><cell></cell><cell>Regression</cell></row><row><cell cols="2">ğ‘˜ ğœ” â„ğ‘œğ‘</cell><cell>Pre Rec</cell><cell cols="2">F1 MAE RMSE</cell></row><row><cell>3</cell><cell cols="3">5 ms 10 ms 0.62 0.62 0.62 0.84 0.87 0.86</cell><cell>4.99 5.49</cell><cell>6.24 6.47</cell></row><row><cell>4</cell><cell cols="3">5 ms 10 ms 0.64 0.66 0.65 0.91 0.89 0.90</cell><cell>4.86 5.48</cell><cell>6.02 6.42</cell></row><row><cell>5</cell><cell cols="4">5 ms 0.93 0.96 0.94 4.75 10 ms 0.72 0.68 0.70 5.43</cell><cell>5.73 6.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results of different generating patterns</figDesc><table><row><cell>Regression</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>This work has been supported by National Natural Science Foundaof China (No.61901265), Shanghai Pujiang Program (No.19PJ1406300), State Key Laboratory of Media Convergence Production Technology and Systems Project (No.SKLMCPTS2020003) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102). We thank our collaboration with Bio-X Institute, Shanghai Jiao Tong University for the permission to use MDD dataset. Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University. Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting Depression with Audio/Text Sequence Modeling of Interviews</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Tuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-2522</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-2522" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2018. 1716-1720</title>
				<meeting>Interspeech 2018. 1716-1720</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International conference on machine learning. PMLR</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variability compensation in small data: Oversampled extraction of i-vectors for the classification of depressed speech</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhyasaharan</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarek</forename><surname>Krajewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="970" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">COVAREP-A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 ieee international conference on acoustics, speech and signal processing (icassp)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SimSensei Kiosk: A Virtual Human Interviewer for Healthcare Decision Support</title>
		<author>
			<persName><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Benn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alesia</forename><surname>Gainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arno</forename><surname>Hartholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaux</forename><surname>Lhommet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gale</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Morbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apar</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2615731.2617415" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems</title>
				<meeting>the 2014 International Conference on Autonomous Agents and Multi-agent Systems<address><addrLine>Paris, France; Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Assessing Alzheimer&apos;s Disease from Speech Using the i-vector Approach</title>
		<author>
			<persName><forename type="first">JosÃ©</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egas</forename><surname>LÃ³pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ¡szlÃ³</forename><surname>TÃ³th</surname></persName>
		</author>
		<author>
			<persName><forename type="first">IldikÃ³</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¡nos</forename><surname>KÃ¡lmÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdolna</forename><surname>PÃ¡kÃ¡ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GÃ¡bor</forename><surname>Gosztolya</surname></persName>
		</author>
		<editor>Speech and Computer, Albert Ali Salah, Alexey Karpov, and Rodmonga Potapova</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="289" to="298" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SWITCHBOARD: Telephone speech corpus for research and development</title>
		<author>
			<persName><forename type="first">John J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>ICASSP- 92</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08592</idno>
		<title level="m">Measuring Depression Symptom Severity from Spoken Language and 3D Facial Expressions</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<title level="m">Building A Continuous Dementia Management Model In Communities Of Shanghai</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depaudionet: An efficient deep model for audio based depression classification</title>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
				<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
				<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Clinical Depression and Affect Recognition with EmoAudioNet</title>
		<author>
			<persName><forename type="first">Emna</forename><surname>Rejaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoud</forename><surname>Kadoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Bentounes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Othmani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Emna</forename><surname>Rejaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Komaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Said</forename><surname>Agrebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Othmani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07208</idno>
		<title level="m">MFCC-based Recurrent Neural Network for Automatic Clinical Depression Recognition and Assessment from Speech</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised audio-visual co-segmentation</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2357" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A weakly supervised learning framework for detecting social anxiety and depression</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Salekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><forename type="middle">A</forename><surname>Teachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Stankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies</title>
				<meeting>the ACM on interactive, mobile, wearable and ubiquitous technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depression severity estimation from multiple modalities</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Evgeny A Stepanov</surname></persName>
		</author>
		<author>
			<persName><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Absar</forename><surname>Shammur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu-LaurenÅ£iu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Vieriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 20th International Conference on e-Health Networking, Applications and Services (Healthcom)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised audio representation learning for mobile devices</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Gfeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11796</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>FÃ©lix de Chaumont Quitry, and Dominik Roblek</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1145/2988257.2988258</idno>
		<ptr target="https://doi.org/10.1145/2988257.2988258" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
				<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge<address><addrLine>Amsterdam, The Netherlands; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
	<note>AVEC &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting depression using vocal, facial and semantic communication cues</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>James R Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrianne</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooya</forename><surname>Schwarzentruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjune</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Tsung</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Dagli</surname></persName>
		</author>
		<author>
			<persName><surname>Quatieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
				<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Sound of Pixels</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03160</idno>
		<ptr target="http://arxiv.org/abs/1804.03160" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
