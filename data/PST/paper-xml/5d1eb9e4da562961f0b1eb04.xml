<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 GNN-FILM: GRAPH NEURAL NETWORKS WITH FEATURE-WISE LINEAR MODULATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 GNN-FILM: GRAPH NEURAL NETWORKS WITH FEATURE-WISE LINEAR MODULATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new Graph Neural Network (GNN) type using feature-wise linear modulation (FiLM). Many standard GNN variants propagate information along the edges of a graph by computing "messages" based only on the representation of the source of each edge. In GNN-FiLM, the representation of the target node of an edge is additionally used to compute a transformation that can be applied to all incoming messages, allowing feature-wise modulation of the passed information.</p><p>Results of experiments comparing different GNN architectures on three tasks from the literature are presented, based on re-implementations of baseline methods. Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between baseline models are smaller than reported in the literature. Nonetheless, GNN-FiLM outperforms baseline methods on a regression task on molecular graphs and performs competitively on other tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning from graph-structured data has seen explosive growth over the last few years, as graphs are a convenient formalism to model the broad class of data that has objects (treated as vertices) with some known relationships (treated as edges). Example usages include reasoning about physical and biological systems, knowledge bases, computer programs, and relational reasoning in computer vision tasks. This graph construction is a highly complex form of feature engineering, mapping the knowledge of a domain expert into a graph structure which can be consumed and exploited by high-capacity neural network models.</p><p>Many neural graph learning methods can be summarised as neural message passing <ref type="bibr" target="#b5">(Gilmer et al., 2017)</ref>: nodes are initialised with some representation and then exchange information by transforming their current state (in practice with a single linear layer) and sending it as a message to all neighbours in the graph. At each node, messages are aggregated in some way and then used to update the associated node representation. In this setting, the message is entirely determined by the source node (and potentially the edge type) and the target node is not taken into consideration. A (partial) exception to this is the family of Graph Attention Networks <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, where the agreement between source and target representation of an edge is used to determine the weight of the message in an attention architecture. However, this weight is applied to all dimensions of the message at the same time.</p><p>A simple consequence of this observation may be to simply compute messages from the pair of source and target node state. However, the linear layer commonly used to compute messages would only allow additive interactions between the representations of source and target nodes. More complex transformation functions are often impractical, as computation in GNN implementations is dominated by the message transformation function.</p><p>However, this need for non-trivial interaction between different information sources is a common problem in neural network design. A recent trend has been the use of hypernetworks <ref type="bibr" target="#b6">(Ha et al., 2017)</ref>, neural networks that compute the weights of other networks. In this setting, interaction between two signal sources is achieved by using one of them as the input to a hypernetwork and the other as input to the computed network. While an intellectually pleasing approach, it is often impractical because the prediction of weights of non-trivial neural networks is computationally expensive.</p><p>Approaches to mitigate this exist (e.g., <ref type="bibr" target="#b17">Wu et al. (2019)</ref> handle this in natural language processing), but are often domain-specific.</p><p>A more general mitigation method is to restrict the structure of the computed network. Recently, "feature-wise linear modulations" (FiLM) were introduced in the visual question answering domain <ref type="bibr" target="#b10">(Perez et al., 2017)</ref>. Here, the hypernetwork is fed with an encoding of a question and produces an element-wise affine function that is applied to the features extracted from a picture. This can be adapted to the graph message passing domain by using the representation of the target node to compute the affine function. This compromise between expressiveness and computational feasibility has been very effective in some domains and the results presented in this article indicate that it is also a good fit for the graph domain.</p><p>This article explores the use of hypernetworks in learning on graphs. Sect. 2 first reviews existing GNN models from the related work to identify commonalities and differences. This involves generalising a number of existing formalisms to new formulations that are able to handle graphs with different types of edges, which are often used to model different relationship between vertices. Then, two new formalisms are introduced: Relational Graph Dynamic Convolutional Networks (RGDCN), which dynamically compute the neural message passing function as a linear layer, and Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM), which combine learned message passing functions with dynamically computed element-wise affine transformations. In Sect. 3, a range of baselines are compared in extensive experiments on three tasks from the literature, spanning classification, regression and ranking tasks on small and large graphs. Experiments were performed on re-implementations of existing model architectures in the same framework and hyperparameter setting searches were performed with the same computational budgets across all architectures. The results show that differences between baselines are smaller than the literature suggests and that the new FiLM model performs well on a number of interesting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>Notation. Let L be a finite (usually small) set of edge types. Then, a directed graph G = (V, E) has nodes V and typed edges E ? V ? L ? V, where (u, , v) ? E denotes an edge from node u to node v of type , usually written as u ? v.</p><p>Graph Neural Networks. As discussed above, Graph Neural Networks operate by propagating information along the edges of a given graph. Concretely, each node v is associated with an initial representation h (0)  v (for example obtained from the label of that node, or by some other model component). Then, a GNN layer updates the node representations using the node representations of its neighbours in the graph, yielding representations h (1)  v . This process can be unrolled through time by repeatedly applying the same update function, yielding representations h (2)  v . . . h (T ) v . Alternatively, several GNN layers can be stacked, which is intuitively similar to unrolling through time, but increases the GNN capacity by using different parameters for each timestep.</p><p>In Gated Graph Neural Networks (GGNN) <ref type="bibr" target="#b8">(Li et al., 2016)</ref>, the update rule uses one linear layer W per edge type to compute messages and combines the aggregated messages with the current representation of a node using a recurrent unit r (e.g., GRU or LSTM cells), yielding the following definition.</p><formula xml:id="formula_0">h (t+1) v = r h (t) v , u ?v?E W h (t) u ; ? r (1)</formula><p>The learnable parameters of the model are the edge-type-dependent weights W and the recurrent cell parameters ? r .</p><p>In Relational Graph Convolutional Networks (R-GCN) <ref type="bibr" target="#b12">(Schlichtkrull et al., 2018)</ref>, the gated unit is replaced by a simple non-linearity ? (e.g., the hyperbolic tangent).</p><formula xml:id="formula_1">h (t+1) v = ? u ?v?E 1 c v, ? W h (t) u (2)</formula><p>Here, c v, is a normalisation factor usually set to the number of edges of type ending in v. The learnable parameters of the model are the edge-type-dependent weights W . It is important to note that in this setting, the edge type set L is assumed to contain a special edge type 0 for self-loops v 0 ? v, allowing state associated with a node to be kept.</p><p>In Graph Attention Networks (GAT) <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, new node representations are computed from a weighted sum of neighbouring node representations. The model can be generalised from the original definitional to support different edge types as follows (we will call this R-GAT below).<ref type="foot" target="#foot_0">1</ref> </p><formula xml:id="formula_2">e u, ,v = LeakyReLU(? ? (W h (t) u W h (t) v )) a v = softmax(e u, ,v | u ? v ? E) h (t+1) v = ? u ?v?E (a v ) u ?v ? W h (t) u (3)</formula><p>Here, ? is a learnable row vector used to weigh different feature dimensions in the computation of an attention ("relevance") score of the node representations, x y is the concatenation of vectors x and y, and (a v ) u ?v refers to the weight computed by the softmax for that edge. The learnable parameters of the model are the edge-type-dependent weights W and the attention parameters ? . In practice, GATs usually employ several attention heads that independently implement the mechanism above in parallel, using separate learnable parameters. The results of the different attention heads are then concatenated after each propagation round to yield the value of h (t+1) v .</p><p>More recently, <ref type="bibr" target="#b18">Xu et al. (2019)</ref> analysed the expressiveness of different GNN types, comparing their ability to distinguish similar graphs with the Weisfeiler-Lehman (WL) graph isomorphism test. Their results show that GCNs and the GraphSAGE model <ref type="bibr" target="#b7">Hamilton et al. (2017)</ref> are strictly weaker than the WL test and hence they developed Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b18">(Xu et al., 2019)</ref>, which are indeed as powerful as the WL test. While the GIN definition is limited to a single edge type, Corollary 6 of <ref type="bibr" target="#b18">Xu et al. (2019)</ref> shows that using the definition</p><formula xml:id="formula_3">h (t+1) v = ? (1 + ) ? f (h (t) v ) + u?v?E f (h (t) u ) ,</formula><p>there are choices for , ? and f such that the node representation update is sufficient for the overall network to be as powerful as the WL test. In the setting of different edge types, the function f in the sum over neighbouring nodes needs to reflect different edge types to distinguish graphs such as v 1 ? u 2 ? w and v 2 ? u 1 ? w from each other. Using different functions f for different edge types makes it possible to unify the use of the current node representation h (t)  v with the use of neighbouring node representations by again using a fresh edge type 0 for self-loops v 0 ? v. In that setting, the factor (1 + ) can be integrated into f 0 . Finally, following an argument similar to <ref type="bibr" target="#b18">Xu et al. (2019)</ref>, ? and f at subsequent layers can be "merged" into a single function which can be approximated by a multilayer perceptron (MLP), yielding the final R-GIN definition</p><formula xml:id="formula_4">h (t+1) v = ? u ?v?E MLP (h (t) u ; ? ) .<label>(4)</label></formula><p>The learnable parameters here are the edge-specific weights ? . Note that Eq. ( <ref type="formula" target="#formula_4">4</ref>) is very similar to the definition of R-GCNs (Eq. ( <ref type="formula">2</ref>)), only dropping the normalisation factor 1 c v, and replacing linear layers by an MLP.</p><p>While many more GNN variants exist, the four formalisms above are broadly representative of general trends. It is notable that in all of these models, the information passed from one node to another is based on the learned weights and the representation of the source of an edge. In contrast, the representation of the target of an edge is only updated (in the GGNN case Eq. (1)), treated as another incoming message (in the R-GCN case Eq. (2) and the R-GIN case Eq. ( <ref type="formula" target="#formula_4">4</ref>)), or used to weight the relevance of an edge (in the R-GAT case Eq. (3)). Sometimes unnamed GNN variants of the above are used (e.g., by <ref type="bibr" target="#b13">Selsam et al. (2019)</ref>; <ref type="bibr" target="#b9">Paliwal et al. (2019)</ref>), replacing the linear layers to compute the messages for each edge by MLPs applied to the concatenation of the representations of source and target nodes. In the experiments, this will be called GNN-MLP, formally defined as follows.<ref type="foot" target="#foot_1">2</ref> </p><formula xml:id="formula_5">h (t+1) v = ? u ?v?E 1 c v, ? M LP h (t) u h (t) v ; ?<label>(5)</label></formula><p>Below, we will instantiate the M LP with a single linear layer to obtain what we call GNN-MLP0, which only differs from R-GCNs (Eq. ( <ref type="formula">2</ref>)) in that the message passing function is applied to the concatenation of source and target state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRAPH HYPERNETWORKS</head><p>Hypernetworks (i.e., neural networks computing the parameters of another neural network) <ref type="bibr" target="#b6">(Ha et al., 2017)</ref> have been successfully applied to a number of different tasks; naturally raising the question if they are also applicable in the graph domain.</p><p>Intuitively, a hypernetwork corresponds to a higher-order function, i.e., it can be viewed as a function computing another function. Hence, a natural idea would be to use the target of a message propagation step to compute the function computing the message; essentially allowing it to focus on features that are especially relevant for the update of the target node representation.</p><p>Relational Graph Dynamic Convolutional Networks (RGDCN) A first attempt would be to adapt (2) to replace the learnable message transformation W by the result of some learnable function f that operates on the target representation:</p><formula xml:id="formula_6">h (t+1) v = ? u ?v?E f (h (t) v ; ? f, )h (t) u</formula><p>However, for a representation size D, f would need to produce a matrix of size D 2 from D inputs. Hence, if implemented as a simple linear layer, f would have on the order of O(D 3 ) parameters, quickly making it impractical in most contexts.</p><p>This can be somewhat mitigated by splitting the node representations</p><formula xml:id="formula_7">h (t) v into C "chunks" h (t) v,c of dimension K = D C : W ,t,v,c = f (h (t) v ; ? f, ,c ) h (t+1) v = 1?c?C ? u ?v?E W ,t,v,c h (t) u,c<label>(6)</label></formula><p>The number of parameters of the model can now be reduced by tying the value of some instances of ? f, ,c . For example, the update function for a chunk c can be computed using only the corresponding chunk of the node representation h (t) v,c , or the same update function can be applied to all "chunks" by setting ? f, ,1 = . . . = ? f, ,C . The learnable parameters of the model are only the hypernetwork parameters ? f, ,c . This is somewhat less desirable than the related idea of <ref type="bibr" target="#b17">Wu et al. (2019)</ref>, which operates on sequences, where sharing between neighbouring elements of the sequence has an intuitive interpretation that is not applicable in the general graph setting.</p><p>Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM) In (6), the message passing layer is a linear transformation conditioned on the target node representation, focusing on separate chunks of the node representation at a time. In the extreme case in which the dimension of each chunk is 1, this method coincides with the ideas of <ref type="bibr" target="#b10">Perez et al. (2017)</ref>, who propose to use layers of element-wise affine transformations to modulate feature maps in the visual question answering setting; there, a natural language question is the input used to compute the affine transformation applied to the features extracted from a picture.</p><p>In the graph setting, we can use each node's representation as an input that determines an elementwise affine transformation of incoming messages, allowing the model to dynamically up-weight and down-weight features based on the information present at the target node of an edge. This yields the following update rule, using a learnable function g to compute the parameters of the affine transformation.</p><formula xml:id="formula_8">? (t) ,v , ? (t) ,v = g(h (t) v ; ? g, ) h (t+1) v = ? u ?v?E ? (t) ,v W h (t) u + ? (t) ,v<label>(7)</label></formula><p>The learnable parameters of the model are both the hypernetwork parameters ? g, and the weights W . In practice, implementing g as a single linear layer works well.</p><p>In the case of using a single linear layer, the resulting message passing function is bilinear in source and target node representation, as the message computation is centred around</p><formula xml:id="formula_9">(W g h (t) v ) (W h (t) u )</formula><p>. This is the core difference to the (linear) interaction of source and target node representations in models that use W (h (t)  u h (t) v ). A simple toy example may illustrate the usefulness of such a mechanism: assuming a graph of nodes V A and V B and edge types 1 and 2, a task may involve counting the number of 1-neighbours of V A nodes and of 2-neighbours of V B nodes. By setting</p><formula xml:id="formula_10">? 1,va = 1, ? 2,va = 0 for v a ? V A and ? 1,v b = 0, ? 2,v b = 1 for v b ? V B , GNN-FiLM can</formula><p>solve this in a single layer. Simpler approaches can solve this by counting A/1, A/2, B/1 and B/2 neighbours separately in one layer and then projecting to the correct counter, but require more feature dimensions and layers for this. As this toy example illustrates, a core capability of GNN-FiLM is to learn to ignore graph edges based on the representation of target nodes.</p><p>Note that the featurewise modulation can also be viewed of an extension of the gating mechanism of GRU or LSTM cells used in GGNNs. Concretely, the "forgetting" of memories in a GRU/LSTM is similar to down-weighting messages computed for the self-loop edges and the gating of the cell input is similar to the modulation of other incoming messages. However, GGNNs apply this gating to the sum of all incoming messages (cf. Eq. (1), wheras in GNN-FiLM the modulation additionally depends on the edge type, allowing for a more fine-grained gating mechanism.</p><p>Finally, a small implementation bug brought focus to the fact that applying the non-linearity ? after summing up messages from neighbouring nodes can make it harder to perform tasks such as counting the number of neighbours with a certain feature. In experiments, applying the non-linearity before aggregation as in the following update rule improved performance.</p><formula xml:id="formula_11">h (t+1) v = l u ?v?E ? ? (t) ,v W h (t) u + ? (t) ,v ; ? l<label>(8)</label></formula><p>However, this means that the magnitude of node representations is now dependent on the degree of nodes in the handled graph. This can sometimes lead to instability during training, which can in turn be controlled by adding an additional layer l after message passing, which can be a simple bounded nonlinearity (e.g. tanh), a fully connected layer, or layer normalisation <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, or any combination of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GNN BENCHMARK TASKS</head><p>Due to the versatile nature of the GNN modelling formalism, many fundamentally different tasks are studied in the research area and it should be noted that good results on one task often do not transfer over to other tasks. This is due to the widely varying requirements of different tasks, as the following summary of tasks from the literature should illustrate.</p><p>? Cora/Citeseer/Pubmed <ref type="bibr" target="#b14">(Sen et al., 2008)</ref>: Each task consists of a single graph of ? 10 000 nodes corresponding to documents and undirected (sic!) edges corresponding to references. The sparse ? 1 000 node features are a bag of words representation of the corresponding documents. The goal is to assign a subset of nodes to a small number of classes. State of the art performance on these tasks is achieved with two propagation steps along graph edges.</p><p>? PPI <ref type="bibr" target="#b19">(Zitnik &amp; Leskovec, 2017)</ref>: A protein-protein interaction dataset consisting of 24 graphs of ? 2 500 nodes corresponding to different human tissues. Each node has 50 features selected by domain experts and the goal is node-level classification, where each node may belong to several of the 121 classes. State of the art performance on this task requires three propagation steps. ? QM9 property prediction <ref type="bibr" target="#b11">(Ramakrishnan et al., 2014)</ref> Hence, tasks differ in the complexity of edges (from undirected and untyped to directed and manytyped), the size of the considered graphs, the size of the dataset, the importance of node-level vs. graph-level representations, and the number of required propagation steps.</p><p>This article includes results on the PPI, QM9 and VarMisuse tasks. Preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by <ref type="bibr" target="#b15">Shchur et al. (2018)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IMPLEMENTATION</head><p>To allow for a wider comparison, the implementation of GNN-FiLM is accompanied by implementations of a range of baseline methods. These include GGNN <ref type="bibr" target="#b8">(Li et al., 2016</ref>) (see Eq. (1)), R-GCN <ref type="bibr" target="#b12">(Schlichtkrull et al., 2018</ref>) (see Eq. ( <ref type="formula">2</ref>)), R-GAT <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref> (see Eq. (3)), and R-GIN (Hamilton et al., 2017) (see Eq. (4))<ref type="foot" target="#foot_2">3</ref> . Additionally, GNN-MLP0 is a variant of R-GCN using a single linear layer to compute the edge message from both source and target state (i.e., Eq.</p><p>(5) instantiated with an "MLP" without hidden layers), and GNN-MLP1 is the same with a single hidden layer. The baseline methods were re-implemented in TensorFlow and individually tested to reach performance equivalent to results reported in their respective source papers. All code for the implementation of these GNNs is released on https://revealed/after/double/blind/ lifted, together with implementations of all tasks and scripts necessary to reproduce the results reported in this paper. This includes the hyperparameter settings found by search, which are stored in tasks/default hypers/ and are selected by default on the respective tasks. The code is designed to facilitate testing new GNN types on existing tasks and easily adding new tasks, allowing for rapid evaluation of new architectures.</p><p>Early on in the experiments, it became clear that the RGDCN approach (Eq. ( <ref type="formula" target="#formula_7">6</ref>)) as presented is infeasible. It is extremely sensitive to the parameter initialisation and hence changes to the random seed lead to wild swings in the target metrics. Hence, no experimental results are reported for it in the following. It is nonetheless included in the article (and the implementation) to show the thought process leading to GNN-FiLM, as well as to allow other researchers to build upon this.</p><p>In the following, GNN-FiLM refers to the formulation of Eq. (8), which performed better than the variant of Eq. ( <ref type="formula" target="#formula_8">7</ref>) across all experiments. Somewhat surprisingly, the same trick (of moving the non-linearity before the message aggregation step) did not help the other GNN types. For all models, using each layer only for a single propagation step performed better than using fewer layers with several propagation steps.</p><p>In all experiments, models were trained until the target metric did not improve anymore for some additional epochs (25 for PPI and QM9, 5 for VarMisuse). The reported results on the held-out test data are averaged across the results of a number of training runs, each starting from different random parameter initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">PROTEIN-PROTEIN INTERACTIONS (PPI)</head><p>The models are first evaluated on the node-level classification PPI task <ref type="bibr" target="#b19">(Zitnik &amp; Leskovec, 2017)</ref>, following the dataset split from earlier papers. Training hence used a set of 20 graphs and validation and test sets of two separate graphs each. The graphs use two edge types: the dataset-provided untyped edges as well as a fresh "self-loop" edge type to allows nodes to keep state across propagation steps.</p><p>Hyperparameters for all models were selected based on results from earlier papers and a small grid search of a number of author-selected hyperparameter ranges (see App. A for details). This resulted in three (R-GAT), four (GGNN, GNN-FiLM, GNN-MLP1, R-GCN), or five (GNN-MLP0, R-GIN) layers (propagation steps) and a node representation size of 256 (GNN-MLP0, R-GIN) or 320 (all others). All models use dropout on the node representations before all GNN layers, with a keep ratio of 0.9. After selecting hyperparameters, all models were trained ten times with different random seeds on a NVidia V100. Tab. 1 shows the micro-averaged F1 score on the classification task on the test graphs, with standard deviations and training times in seconds computed over the ten runs. The results for all re-implemented models are better than the results reported by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref> for the GAT model (without edge types). A cursory exploration of the reasons yielded three factors. First, the generalisation to different edge types (cf. Eq. ( <ref type="formula">3</ref>)) and the subsequent use of a special self-loop edge type helps R-GAT (and all other models) significantly. Second, using dropout between layers significantly improved the results. Third, the larger node representation sizes (compared to 256 used by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>) improved the results again. However, the new GNN-FiLM improves slightly over these four baselines from the literature, while converging substantially faster than all baselines, mainly because it converges in significantly fewer training steps (approx. 240 epochs compared to 400-600 epochs for the other models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">QUANTUM CHEMISTRY (QM9)</head><p>All models were additionally evaluated on graph-level regression tasks on the QM9 molecule data set <ref type="bibr" target="#b11">(Ramakrishnan et al., 2014)</ref>, considering thirteen different quantum chemical properties. The ?130k molecular graphs in the dataset were split into training, validation and test data by randomly selecting 10 000 graphs for the latter two sets. Additionally, another data split without a test set was used for the hyperparameter search (see below). The graphs use five edge types: the datasetprovided typed edges (single, double, triple and aromatic bonds between atoms) as well as a fresh "self-loop" edge type that allows nodes to keep state across propagation steps. The evaluation differs from the setting reported by <ref type="bibr" target="#b5">Gilmer et al. (2017)</ref>, as no additional molecular information is encoded as edge features, nor are the graphs augmented by master nodes or additional edges. <ref type="foot" target="#foot_3">4</ref>Hyperparameters for all models were found using a staged search process. First, 500 hyperparameter configurations were sampled from an author-provided search space (see App. A for details) and run on the first three regression tasks. The top three configurations for each of these three tasks were then run on all thirteen tasks and the final configuration was chosen as the one with the lowest average mean absolute error across all properties, as evaluated on the validation data of that dataset split. This process led to eight layers / propagation steps for all models but GGNN and R-GIN, which showed best performance with six layers. Furthermore, all models used residual connections connecting every second layer and GGNN, R-GCN, GNN-FiLM and GNN-MLP0 additionally used layer normalisation (as in Eq. ( <ref type="formula" target="#formula_11">8</ref>)).  Each model was trained for each of the properties separately five times using different random seeds on compute nodes with NVidia P100 cards. The average results of the five runs are reported in Tab. 2, with their respective standard deviations. <ref type="foot" target="#foot_4">5</ref> The results indicate that the new GNN-FiLM model outperforms the standard baselines on all tasks and the usually not considered GNN-MLP variants on the majority of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">VARIABLE USAGE IN PROGRAMS (VARMISUSE)</head><p>Finally, the models were evaluated on the VarMisuse task of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. This task requires to process a graph representing an abstraction of a program fragment and then select one of a few candidate nodes (representing program variables) based on the representation of another node (representing the location to use a variable in). The experiments are performed using the released split of the dataset, which contains ? 130k training graphs, ? 20k validation graphs and two test sets: SEENPROJTEST, which contains ? 55k graphs extracted from open source projects that also contributed data to the training and validation sets, and UNSEENPROJTEST, which contains ? 30k graphs extracted from completely unseen projects.</p><p>Due to the inherent cost of training models on this dataset <ref type="bibr" target="#b2">(Balog et al. (2019)</ref> provide an in-depth performance analysis), a limited hyperparameter grid search was performed, with only ? 30 candidate configurations for each model (see App. A for details). For each model, the configuration yielding the best results on the validation data set fold was selected. This led to six layers for GGNN and R-GIN, eight layers for R-GAT and GNN-MLP0, and ten layers for the remaining models. Graph node hidden sizes were 128 for all models but GGNN and R-GAT, which performed better with 96 dimensions.</p><p>The results, shown in Tab. 3, are somewhat surprising, as they indicate a different ranking of model architectures as the results on PPI and QM9, with R-GCN performing best. All re-implemented baselines beat the results reported by <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>, who also reported that R-GCN and GGNN show very similar performance. This is in spite of a simpler implementation of the task than in the original paper, as it only uses the string labels of nodes for the representation and does not use the additional type information provided in the dataset. However, the re-implementation of the task uses the insights from <ref type="bibr" target="#b4">Cvitkovic et al. (2019)</ref>, who use character CNNs to encode node labels and furthermore introduce extra nodes for subtokens appearing in labels of different nodes, connecting them to their sources (e.g., nodes labelled openWullfrax and closeWullfrax are both connected to a fresh Wullfrax node).</p><p>A deeper investigation results showed that the more complex models seem to suffer from significant overfitting to the training data, as can be seen in the results for training and validation accuracy reported in Tab. 3. A brief exploration of more aggressive regularisation methods (more dropout, weight decay) showed no improvement and a deeper understanding of the cause of these results remains for future work.</p><p>Furthermore, the large variance in results on the validation set (especially for R-GCN) makes it likely that the hyperparameter grid search with only one training run per configuration did not yield the best configuration for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION &amp; CONCLUSIONS</head><p>After a review of existing graph neural network architectures, the idea of using hypernetworkinspired models in the graph setting was explored. This led to two models, Graph Dynamic Convolutional Networks and GNNs with feature-wise linear modulation, were presented. While GDCNs seem to be impractical to train, experiments show that GNN-FiLM is competitive with or improving on baseline models on three tasks from the literature.</p><p>The extensive experiments also show that a number of results from the literature could benefit from more substantial hyperparameter search and are often missing comparisons to a number of obvious baselines:</p><p>? The results in Tab. 1 indicate that GATs have no advantage over GGNNs or R-GCNs on the PPI task, which does not match the findings by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>. ? The results in Tab. 3 indicate that R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. ? The GNN-MLP models are obvious extensions that are often alluded to, but are not part of the usually considered set of baseline models. Nonetheless, experiments across all three tasks have shown that these methods outperform better-published techniques such as GGNNs, R-GCNs and GATs, without a substantial runtime penalty.</p><p>These results indicate that there is substantial value in independent reproducibility efforts and comparisons that include "obvious" baselines, matching the experiences from other areas of machine learning as well as earlier work by <ref type="bibr" target="#b15">Shchur et al. (2018)</ref> on reproducing experimental results for GNNs on citation network tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>: ? 130 000 graphs of ? 8 nodes represent molecules, where nodes are heavy atoms and undirected, typed edges are bonds between these atoms, different edge types indicating single/double/etc. bonds. The goal is to regress each graph to a number of quantum chemical properties. State of the art performance on these tasks requires at least four propagation steps. ? VarMisuse (Allamanis et al., 2018): ? 235 000 graphs of ? 2500 nodes each represent program fragments, where nodes are tokens in the program text and different edge types represent the program's abstract syntax tree, data flow between variables, etc. The goal is to select one of a set of candidate nodes per graph. State of the art performance requires at least six propagation steps.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>GNN results on PPI task. GAT * result taken from<ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">Avg. Micro-F1 Time (s)</cell></row><row><cell>GAT  *</cell><cell>0.973 ?0.002</cell><cell>n/a</cell></row><row><cell>GGNN</cell><cell>0.990 ?0.001</cell><cell>432.6</cell></row><row><cell>R-GCN</cell><cell>0.989 ?0.000</cell><cell>759.0</cell></row><row><cell>R-GAT</cell><cell>0.989 ?0.001</cell><cell>782.3</cell></row><row><cell>R-GIN</cell><cell>0.991 ?0.001</cell><cell>704.8</cell></row><row><cell cols="2">GNN-MLP0 0.992?0.000</cell><cell>556.9</cell></row><row><cell cols="2">GNN-MLP1 0.992?0.001</cell><cell>479.2</cell></row><row><cell cols="2">GNN-FiLM 0.992?0.000</cell><cell>308.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GNN average error rates and standard deviations on QM9 target values.</figDesc><table><row><cell>Property</cell><cell>GGNN</cell><cell>R-GCN</cell><cell>R-GAT</cell><cell>R-GIN</cell><cell cols="3">GNN-MLP0 GNN-MLP1 GNN-FiLM</cell></row><row><cell>mu</cell><cell>3.85 ?0.16</cell><cell>3.21 ?0.06</cell><cell>2.68 ?0.06</cell><cell>2.64 ?0.11</cell><cell>2.36 ?0.04</cell><cell>2.44 ?0.12</cell><cell>2.38 ?0.13</cell></row><row><cell>alpha</cell><cell>5.22 ?0.86</cell><cell>4.22 ?0.45</cell><cell>4.65 ?0.44</cell><cell>4.67 ?0.52</cell><cell>4.27 ?0.36</cell><cell>4.63 ?0.54</cell><cell>3.75 ?0.11</cell></row><row><cell>HOMO</cell><cell>1.67 ?0.07</cell><cell>1.45 ?0.01</cell><cell>1.48 ?0.03</cell><cell>1.42 ?0.01</cell><cell>1.25 ?0.04</cell><cell>1.29 ?0.06</cell><cell>1.22 ?0.07</cell></row><row><cell>LUMO</cell><cell>1.74 ?0.06</cell><cell>1.62 ?0.04</cell><cell>1.53 ?0.07</cell><cell>1.50 ?0.09</cell><cell>1.35 ?0.04</cell><cell>1.50 ?0.19</cell><cell>1.30 ?0.05</cell></row><row><cell>gap</cell><cell>2.60 ?0.06</cell><cell>2.42 ?0.14</cell><cell>2.31 ?0.06</cell><cell>2.27 ?0.09</cell><cell>2.04 ?0.05</cell><cell>2.06 ?0.10</cell><cell>1.96 ?0.06</cell></row><row><cell>R2</cell><cell cols="4">35.94 ?35.68 16.38 ?0.49 52.39 ?42.58 15.63 ?1.40</cell><cell cols="3">14.86 ?1.62 15.81 ?1.42 15.59 ?1.38</cell></row><row><cell>ZPVE</cell><cell>17.84 ?3.61</cell><cell cols="2">17.40 ?3.56 14.87 ?2.88</cell><cell>12.93 ?1.81</cell><cell cols="3">12.00 ?1.66 14.12 ?1.10 11.00 ?0.74</cell></row><row><cell>U0</cell><cell>8.65 ?2.46</cell><cell>7.82 ?0.80</cell><cell>7.61 ?0.46</cell><cell>5.88 ?1.01</cell><cell>5.55 ?0.38</cell><cell>6.94 ?0.64</cell><cell>5.43 ?0.96</cell></row><row><cell>U</cell><cell>9.24 ?2.26</cell><cell>8.24 ?1.25</cell><cell>6.86 ?0.53</cell><cell>18.71 ?23.36</cell><cell>6.20 ?0.88</cell><cell>7.00 ?1.06</cell><cell>5.95 ?0.46</cell></row><row><cell>H</cell><cell>9.35 ?0.96</cell><cell>9.05 ?1.21</cell><cell>7.64 ?0.92</cell><cell>5.62 ?0.81</cell><cell>5.96 ?0.45</cell><cell>7.98 ?0.88</cell><cell>5.59 ?0.57</cell></row><row><cell>G</cell><cell>7.14 ?1.15</cell><cell>7.00 ?1.51</cell><cell>6.54 ?0.36</cell><cell>5.38 ?0.75</cell><cell>5.09 ?0.57</cell><cell>7.14 ?0.51</cell><cell>5.17 ?1.13</cell></row><row><cell>Cv</cell><cell>8.86 ?9.07</cell><cell>3.93 ?0.48</cell><cell>4.11 ?0.27</cell><cell>3.53 ?0.37</cell><cell>3.38 ?0.20</cell><cell>4.60 ?0.74</cell><cell>3.46 ?0.21</cell></row><row><cell>Omega</cell><cell>1.57 ?0.53</cell><cell>1.02 ?0.05</cell><cell>1.48 ?0.87</cell><cell>1.05 ?0.11</cell><cell>0.84 ?0.02</cell><cell>5.60 ?8.82</cell><cell>0.98 ?0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on VarMisuse task. GGNN * result taken from appendix of<ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell><cell>TRAIN</cell><cell>VALID</cell><cell cols="3">SEENPROJTEST UNSEENPROJTEST</cell></row><row><cell>GGNN  *</cell><cell>n/a</cell><cell>n/a</cell><cell>84.0 n/a</cell><cell>74.1</cell><cell>n/a</cell></row><row><cell>GGNN</cell><cell>87.5 ?1.8%</cell><cell>82.1 ?0.9%</cell><cell>85.7 ?0.5%</cell><cell cols="2">79.3 ?1.2%</cell></row><row><cell>R-GCN</cell><cell>88.7 ?3.1%</cell><cell>85.7 ?1.6%</cell><cell>87.2 ?1.5%</cell><cell cols="2">81.4 ?2.3%</cell></row><row><cell>R-GAT</cell><cell>90.4 ?3.9%</cell><cell>84.2 ?1.0%</cell><cell>86.9 ?0.7%</cell><cell cols="2">81.2 ?0.9%</cell></row><row><cell>R-GIN</cell><cell>93.4 ?1.8%</cell><cell>84.2 ?1.0%</cell><cell>87.1 ?0.1%</cell><cell cols="2">81.1 ?0.9%</cell></row><row><cell>GNN-MLP0</cell><cell>95.3 ?2.4%</cell><cell>83.4 ?0.3%</cell><cell>86.5 ?0.2%</cell><cell cols="2">80.5 ?1.4%</cell></row><row><cell>GNN-MLP1</cell><cell>94.7 ?1.2%</cell><cell>84.4 ?0.4%</cell><cell>86.9 ?0.3%</cell><cell cols="2">81.4 ?0.7%</cell></row><row><cell>GNN-FiLM</cell><cell>94.3 ?1.0%</cell><cell>84.6 ?0.6%</cell><cell>87.0 ?0.2%</cell><cell cols="2">81.3 ?0.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that this is similar to the ARGAT model presented by<ref type="bibr" target="#b3">Busbridge et al. (2019)</ref>, but unlike the models studied there (and like the original GATs) uses a single linear layer to compute attention scores e u, ,v , instead of simpler additive or multiplicative variants.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>These are similar to R-GIN, but apply an MLP to the concatenation of source and target state for each message.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that Eq. (3) and Eq. (4) define generalisations to different edge types not present in the original papers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Adding these features is straightforward, but orthogonal to the comparison of different GNN variants.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that training sometimes did not converge (as visible in the large standard deviation). Removing these outliers, GGNN achieved 18.11(?1.62) and R-GAT achieved 17.66(?1.23) on R2; R-GIN has an average error rate of 7.04(?1.41) on U, and GNN-MLP1's result on the Omega task is 1.19(?0.08).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETER SEARCH SPACES</head><p>A.1 PPI For all models, a full grid search considering all combinations of the following parameters was performed:</p><p>? hidden size ? {192, 256, 320} -size of per-node representations.</p><p>? graph num layers ? {2, 3, 4, 5} -number of propagation steps / layers.</p><p>? graph layer input dropout keep prob ? {0.8, 0.9, 1.0} -dropout applied before propagation steps.</p><p>A.2 QM9 For all models, 500 configurations were considered, sampling hyperparameter settings uniformly from the following options:</p><p>? hidden size ? {64, 96, 128} -size of per-node representations.</p><p>? graph num layers ? {4, 6, 8} -number of propagation steps / layers.</p><p>? graph layer input dropout keep prob ? {0.8, 0.9, 1.0} -dropout applied before propagation steps. ? layer norm ? {True, F alse} -decided if layer norm is applied after each propagation step.</p><p>? dense layers ? {1, 2, 32} -insert a fully connected layer applied to node representations between every dense layers propagation steps. (32 effectively turns this off) ? res connection ? {1, 2, 32} -insert a residual connection between every res connection propagation steps. (32 effectively turns this off) ? graph activation function ? {relu, leaky relu, elu, gelu, tanh} -non-linearity applied after message passing. ? optimizer ? {RMSProp, Adam} -optimizer used (with TF 1.13.1 default parameters).</p><p>? lr ? [0.0005, 0.001] -learning rate.</p><p>? cell ? {RNN , GRU , LSTM } -gated cell used for GGNN (only part of search space for GGNN). ? num heads ? {4, 8, 16} -number of attention heads used for R-GAT (only part of search space for R-GAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 VARMISUSE</head><p>For all models, a full grid search considering all combinations of the following parameters was performed:</p><p>? hidden size ? {64, 96, 128} -size of per-node representations.</p><p>? graph num layers ? {6, 8, 10} -number of propagation steps / layers.</p><p>? graph layer input dropout keep prob ? {0.8, 0.9, 1.0} -dropout applied before propagation steps. ? cell ? {GRU , LSTM } -gated cell used for GGNN (only part of search space for GGNN). ? num heads ? {4, 8} -number of attention heads used for R-GAT (only part of search space for R-GAT).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast training of sparse graph neural networks on dense hardware</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906">1906.11786, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Relational graph attention networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dane</forename><surname>Sherburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<idno>CoRR, abs/1904.05811</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open vocabulary learning on source code with a graph-structured cache</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graph representations for higher-order logic and theorem proving</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1905.10006</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional network</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference (ESWC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a SAT solver from single-bit supervision</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>B?nz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
