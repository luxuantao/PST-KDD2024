<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Information Extraction as Unified Semantic Matching</title>
				<funder ref="#_E6UZnNS #_SaST9TS #_4r4gDds">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">CCF-Baidu Open Fund</orgName>
				</funder>
				<funder ref="#_mWvvZzD">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Lou</surname></persName>
							<email>loujie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
							<email>luyaojie@iscas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
							<email>daidai@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jia</surname></persName>
							<email>jiawei07@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">State Key Laboratory of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Le Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">State Key Laboratory of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Information Extraction as Unified Semantic Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The challenge of information extraction (IE) lies in the diversity of label schemas and the heterogeneity of structures. Traditional methods require task-specific model design and rely heavily on expensive supervision, making them difficult to generalize to new schemas. In this paper, we decouple IE into two basic abilities, structuring and conceptualizing, which are shared by different tasks and schemas. Based on this paradigm, we propose to universally model various IE tasks with Unified Semantic Matching (USM) framework, which introduces three unified token linking operations to model the abilities of structuring and conceptualizing. In this way, USM can jointly encode schema and input text, uniformly extract substructures in parallel, and controllably decode target structures on demand. Empirical evaluation on 4 IE tasks shows that the proposed method achieves state-of-the-art performance under the supervised experiments and shows strong generalization ability in zero/few-shot transfer settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Information extraction aims to extract various information structures from texts <ref type="bibr" target="#b1">(Andersen et al. 1992;</ref><ref type="bibr" target="#b8">Grishman 2019)</ref>. For example, given the sentence "Monet was born in Paris, the capital of France", an IE system needs to extract various task structures such as entities, relations, events, or sentiments in the sentence. It is challenging because the target structures have diversified label schemas (person, work for, positive sentiment, etc.) and heterogeneous structures <ref type="bibr">(span, triplet, etc.)</ref>.</p><p>Traditional IE model leverages task-and schemaspecialized architecture, which is commonly specific to different target structures and label schemas. The expensive annotation leads to limited predefined categories and small data size in general domains for information extraction tasks. From another perspective, task-specific model design makes it challenging to migrate learned knowledge between different tasks and extraction frameworks. The above problems lead to the poor performance of IE models in lowresource settings or facing new label schema, which greatly restricts the application of IE in real scenarios. Very recently, <ref type="bibr" target="#b22">Lu et al. (2022)</ref> proposed the concept of universal information extraction (UIE), which aims to resolve multiple IE tasks using one universal model. To this end, they proposed a sequence-to-sequence generation model, which takes flattened schema and text as input, and directly generates diversified target information structures. Unfortunately, all associations between information pieces and schemas are implicitly formulated due to the black-box nature of sequence-to-sequence models (Alvarez-Melis and Jaakkola 2017). Consequently, it is difficult to identify what kind of abilities and knowledge are learned to transfer across different tasks and schemas. Therefore we have no way of diagnosing under what circumstances such transfer learning across tasks or schemas would fail. For the above reasons, it is necessary to explicitly model and learn transferable knowledge to obtain effective, robust, and explainable transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USM</head><p>We find that, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, even with diversified tasks and extraction targets, all IE tasks can be fundamentally decoupled into the following two critical operations: 1) Structuring, which proposes label-agnostic basic substructures of the target structure from the text. For example, proposing the utterance structure "Monet" for entity mention and "born in" for event mention, the associated pair structure ("Monet", "Paris") for relation mention, and ("born in", "Paris") for event argument mention. 2) Conceptualiz- ing, which generalizes utterance and paired substructures to corresponding target semantic concepts. More importantly, these two operations can be explicitly reformulated using a semantic matching paradigm when given a target extraction schema. Specifically, structuring operations can be viewed as building specific kinds of semantic associations between utterances in the input text, while conceptualizing operations can be regarded as matching between target semantic labels and the given utterances or substructures. Consequently, if we universally transform information extraction into combinations of a series of structuring and conceptualizing, reformulate all these operations with the semantic matching between structures and schemas, and jointly learn all IE tasks under the same paradigm, we can easily conduct various kinds of IE tasks with one universal architecture and share knowledge across different tasks and schemas.</p><p>Unfortunately, directly conducting semantic matching between structures and schemas is impractical for universal information extraction. First, sentences have many substructures, resulting in a large number of potential matching candidates and a large scale of matching, which makes the computational efficiency of the model unacceptable. Second, the schema of IE is structural and hard to match with the plain text. In this paper, we propose directed token linking for universal IE. The main idea is to transform the structuring and conceptualizing into a series of directed token linking operations, which can be reverted to semantic matching between utterances and schema.</p><p>Based on the above observation, we propose USM, a unified semantic matching framework for universal information extraction (UIE), which decomposes structures and verbalizes label types for sharing structuring and conceptualizing abilities. Specifically, we design a set of directed token linking operations (token-token linking, label-token linking, and token-label linking) to decouple task-specific IE tasks into two extraction abilities. To learn the common extraction abilities, we pre-train USM by leveraging heterogeneous supervision from linguistic resources. Compared to previous works, USM is a new transferable, controllable, efficient end-to-end framework for UIE, which jointly encodes extraction schema and input text, uniformly extracts substructures, and controllably decodes target structures on demand.</p><p>We conduct experiments on four main IE tasks under the supervised, multi-task, and zero/few-shot transfer settings.</p><p>The proposed USM framework achieves state-of-the-art results in all settings and solves massive tasks using a single multi-task model. Under the zero/few-shot transfer settings, USM shows a strong cross-type transfer ability due to the shared structuring and conceptualizing obtained by pretraining.</p><p>In summary, the main contributions of this paper are: 1. We propose an end-to-end framework for universal information extraction -USM, which can jointly model schema and text, uniformly extract substructures, and controllably generate the target structure on demand. 2. We design three unified token linking operations to decouple various IE tasks, sharing extraction capabilities across different target structures and semantic schemas and achieving "one model for solving all tasks" by multitask learning. 3. We pre-train a universal foundation model with largescale heterogeneous supervisions, which can benefit future research on IE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified Semantic Matching via Directed Token Linking</head><p>Information extraction is structuring the text's information and elevating it into specific semantic categories. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, USM takes the arbitrary extraction label schema l and the raw text t as input and directly outputs the structure according to the given schema. For example, given the text "Monet was born in Paris, the capital of France", USM needs to extract ("France", capital, "Paris") for the relation type capital and (person, "Monet")/(country, "France") for the entity type person and country. The main challenges here are: 1) how to unifiedly extract heterogeneous structures using the shared structuring ability; 2) how to uniformly represent different extraction tasks under diversified label schemas to share the common conceptualizing ability.</p><p>In this section, we describe how to end-to-end extract the information structures from the text using USM. Specifically, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, USM first verbalizes all label schemas <ref type="bibr" target="#b15">(Levy et al. 2017;</ref><ref type="bibr" target="#b16">Li et al. 2020;</ref><ref type="bibr" target="#b22">Lu et al. 2022</ref>) and learns the schema-text joint embedding to build a shared label text semantic space. Then we describe three basic token</p><formula xml:id="formula_0">[L] person [L] country [L] birth place [L] capital [T] Monet ? Paris ? France.</formula><p>Token-Token Linking for Structuring Label-Token Linking for Utterance Conceptualizing Token-Label Linking for Pairing Conceptualizing  In practice, we employ different label symbols "[L]" for utterance conceptualizing: "[LM]" for the label of single mention, such as entity types and event trigger types; "[LP]" for the predicate of association pair, such as relation types and event argument types.</p><formula xml:id="formula_1">[L] person [L] country [L] birth place [L] capital [T] Monet ? Paris ? France. label ? mention (subject) [L] person [L] country [L] birth place [L] capital [T] Monet was born in Paris, the capital of France. [L] person [L] country [L] birth place [L] capital [T] Monet ? Paris ? France. subject ? label Schema-constraint Decoding head ? tail Monet subject ? object [L] person [L] country [L] birth place [L] capital [T] Monet ? Paris ? France. label ? mention (object)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directed Token Linking</head><p>linking operations and how to structure and conceptualize information from text using these three operations. Finally, we introduce how to decode the final results using schemaconstraint decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Schema-Text Joint Embedding</head><p>To capture the interaction between label schema and text, USM first learns the joint contextualized embeddings of schema labels and text tokens. Concretely, USM first verbalizes the extraction schema s as token sequence l = {l 1 , l 2 , ..., l |l| } following the structural schema instructor <ref type="bibr" target="#b22">(Lu et al. 2022)</ref>, then concatenates schema sequence l and text tokens t = {t 1 , t 2 , ..., t |t| } as input, and finally computes the joint label-text embeddings H = [h 1 , h 2 , ..., h |l|+|t| ] as follow:</p><formula xml:id="formula_2">H = Encoder(l 1 , l 2 , ..., l |l| , t 1 , t 2 , ..., t |t| , M)<label>(1)</label></formula><p>where Encoder(?) is a transformer encoder, and M ? R (|l|+|t|)?(|l|+|t|) is the mask matrix that determines whether a pair of tokens can be attended to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token-Token Linking for Structuring</head><p>After obtaining the joint label-text embeddings</p><formula xml:id="formula_3">H = [h l 1 , ..., h l |l| , h t 1 , ..., h t |t| ],</formula><p>USM structures all valid substructures using Token-Token Linking (TTL) operations:</p><p>1. Utterance: a continuous token sequence in the input text, e.g., entity mention "Monet" or event trigger "born in".</p><p>We extract a single utterance with inner span head-totail (H2T) linking, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. For example, to extract the span "Monet" and "born in" as valid substructures, USM utilizes H2T to link "Monet" to itself and link "born" to "in". 2. Association pair: a basic related pair unit extracted from the text, e.g., relation subject-object pair ("Monet", "Paris") or event trigger-argument ("born in", "Paris"). We extract span pairs with head-to-head (H2H) and tailto-tail (T2T) linking operations. For example, to extract the subject-object pair "Monet" and "Paris" as a valid substructure, USM links "Monet" and "Paris" using H2H as well as links "Monet" and "Paris" using T2T. For the above three token-to-token linking (H2T, H2H, T2T) operations, USM respectively calculates the token-to-token linking score s TTL (t i , t j ) over all valid token pair candidates ?t i , t j ?. For each token pair ?t i , t j ?, the linking score s TTL (t i , t j ) is calculated as:</p><formula xml:id="formula_4">s TTL (t i , t j ) = FFNN l TTL (h t i ) T R j-i FFNN r TTL (h t j ) (2)</formula><p>where FFNN l/r are feed-forward layers with output size d. R j-i ? R d?d is the rotary position embedding <ref type="bibr" target="#b39">(Su et al. 2021</ref><ref type="bibr" target="#b40">(Su et al. , 2022</ref>) that can effectively inject relative position information into the valid structure mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label-Token Linking for Utterance Conceptualizing</head><p>Given label token embeddings h l 1 , ..., h l |l| and text token embeddings h t 1 , ..., h t |t| , USM conceptualizes valid utterance structures with label-token linking (LTL) operations. The output of LTL is a pair of label name and text mention, e,g., (person, "Monet"), (country, "France"), and (born, "born in"). There are two types of utterance conceptualizing: the first one is the type of mention, which indicates assigning the label types to every single mention, such as entity type person for entity mention "Monet"; the second one is the predicate of object, which assigns the predicate type to each object candidate, such as relation type birth place for "Paris" and event argument type place for "Paris".</p><p>We conceptualize the type of mention and the predicate of object with the same label-to-token linking operation, thus enabling the two label semantics to reinforce each other. Following the head-tail span extraction style, we name each substructure with label-to-head (L2H) and label-to-tail (L2T) linking operations. For the pair of label name birth place and text span Paris, USM links the head of the label birth with the head of text span "Paris" and links the tail of label place with the tail of text span "Paris".</p><p>For the above two label-to-token linking (L2H, L2T) operations, USM respectively calculates the label-to-token linking score s LTL (l i , t j ) over all valid label and text token pair candidates ?l i , t j ?:</p><formula xml:id="formula_5">s LTL (l i , t j ) = FFNN label LTL (h l i ) T R j-i FFNN text LTL (h t j ) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token-Label Linking for Pairing Conceptualizing</head><p>To conceptualize the association pair, USM links the subject of the association pair to the label name using Token-Label Linking (TLL). Precisely, TLL operation links the subject of triplet and the predicate type with head-to-label (H2L) and tail-to-label (T2L) operations. For instance, TLL links the head of text span "Monet" and the head of the label birth with H2L and links the tail of text span "Monet" and the tail of the label place with T2L following the head-tail span extraction style. For the above two token-label linking (H2L, T2L) operations, the linking score s TLL (t i , l j ) is computed as:</p><formula xml:id="formula_6">s TLL (t i , l j ) = FFNN text TLL (h l i ) T R j-i FFNN label TLL (h t j ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Schema-constraint Decoding for Structure Composing</head><p>USM decodes the final structures using a schema-constraint decoding algorithm, given substructures extracted by unified token linking operations. During the decoding stage, we separate types for different tasks according to the schema definition. For instance, in the joint entity and relation extraction task, we uniformly encode entity types and relation types as labels to utilize the common structuring and conceptualizing ability but compose the final result by separating the entity or relation types from input types. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, USM 1) first decodes mentions and subject-object unit extracted by token-token linking operation: {"Monet", "Paris", "France", ("Monet", "Paris"), ("France", "Paris")}; 2) and then decodes labelmention pairs by label-token linking operation: {(person, "Monet"), (country, "France"), (birth place, "Paris"), (capital, "Paris")}; 3) and finally decodes label-association pairs using token-label linking operation: ("Monet", birth place), ("France", capital). The above three token linking operations do not affect each other; hence the extraction operations are fully non-autoregressive and highly parallel.</p><p>Finally, we separate the entity types country and person, relation types birth place, and capital from input types according to the schema definition. Based on the result from token-label linking ("Monet", birth place), ("France", capital), we can consistently obtain the full structure ("Monet", birth place, "Paris") and ("France", capital, "Paris").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning from Heterogeneous Supervision</head><p>This section introduces how to leverage heterogeneous supervised resources to learn the common structuring and conceptualizing abilities for unified token linking. Specifically, with the help of verbalized label representation and unified token linking, we unify heterogeneous supervision signals into &lt;text, token pairs&gt; for pre-training. We first pre-train the USM on the heterogeneous resources, which contain three different supervised signals, including task annotation signals (e.g., IE datasets), distant signals (e.g., distant supervision datasets), and indirect signals (e.g., question answering datasets), then adopt the pre-trained USM model to specific downstream information extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>USM uniformly encodes label schema and text in the shared semantic representation and employs unified token linking to structure and conceptualize information from text. To help USM to learn the common structuring and conceptualizing abilities, we collect three different supervised signals from existing linguistic sources for the pre-training of USM:</p><p>D task is the task annotation dataset, where each instance has a gold annotation for information extraction. We use Ontonotes <ref type="bibr" target="#b28">(Pradhan et al. 2013)</ref>, widely used in the field of information extraction as gold annotation, which contains 18 entity types. D task is used as in-task supervision signals to learn task-specific structuring and conceptualizing abilities.</p><p>D distant is the distant supervision dataset, where each instance is aligned by text and knowledge base. Distant supervision is a common practice to obtain large-scale training data for information extraction <ref type="bibr" target="#b24">(Mintz et al. 2009;</ref><ref type="bibr" target="#b31">Riedel et al. 2013)</ref>. We employ NYT <ref type="bibr" target="#b31">(Riedel et al. 2013)</ref> and Rebel (Huguet Cabot and Navigli 2021) as our distant supervision datasets, which are obtained by aligning text with Freebase and Wikidata, respectively. Rebel dataset has a large label schema, and all verbalized schemas are too long to be concatenated with input text and fed to the pre-trained transformer encoder. We sample negative label schema to construct meta schema <ref type="bibr" target="#b22">(Lu et al. 2022)</ref> as label schema for pretraining.</p><p>D indirect is the indirect supervision dataset, where each instance is derived from other related NLP tasks <ref type="bibr" target="#b46">(Wang, Ning, and Roth 2020;</ref><ref type="bibr" target="#b4">Chen et al. 2022)</ref>. We utilize reading comprehension datasets from MRQA <ref type="bibr" target="#b7">(Fisch et al. 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Function</head><p>For pre-training, fine-tuning and multi-task learning, we unify all datasets as {(x i , y i )}, where x i is text and y i is linking annotation of each token linking pair (TTM, LTM, TLM). We use the same learning function for all settings with the homogenized data format.</p><p>The main challenge of USM learning is the sparsity of linked token pairs. The linked ratio only occupies less than 1% of all valid token pair candidates. To overcome the extreme sparsity of linking instances, we optimize class imbalance loss <ref type="bibr" target="#b40">(Su et al. 2022)</ref> for each instance as follows:</p><formula xml:id="formula_7">L = m?M log ? ? 1 + (i,j)?m + e -sm(i,j) ? ? + log ? ? 1 + (i,j)?m - e sm(i,j) ? ? (5)</formula><p>where M denotes linking types of USM, m + indicates the linked pairs, m -indicates the non-linked pairs, and s m (i, j) is the predicate linking score for the linking operation m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>This section conducts massive experiments under supervised settings and transfer settings to demonstrate the effectiveness of the proposed unified semantic matching framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Supervised Settings</head><p>We conduct supervised experiments on extensive information extraction tasks, including 4 tasks and 13 datasets (entity extraction, relation extraction, event extraction, sentiment extraction) and their combinations (e.g., joint entityrelation extraction). The used datasets includes ACE04 <ref type="bibr" target="#b25">(Mitchell et al. 2005)</ref>, ACE05 <ref type="bibr" target="#b42">(Walker et al. 2006);</ref><ref type="bibr">CoNLL03 (Tjong Kim Sang and De Meulder 2003)</ref>, CoNLL04 <ref type="bibr">(Roth and</ref><ref type="bibr">Yih 2004), SciERC (Luan et al. 2018)</ref>, NYT <ref type="bibr" target="#b30">(Riedel, Yao, and McCallum 2010)</ref>, CASIE <ref type="bibr" target="#b35">(Satyapanich, Ferraro, and Finin 2020</ref><ref type="bibr">), SemEval-14/15/16 (Pontiki et al. 2014</ref><ref type="bibr" target="#b27">, 2015</ref><ref type="bibr">, 2016)</ref>. We employ the same end-toend settings and evaluation metrics as <ref type="bibr" target="#b22">Lu et al. (2022)</ref>.</p><p>We compare the proposed USM framework with the taskspecific state-of-the-art methods and the unified structure generation method -UIE <ref type="bibr" target="#b22">(Lu et al. 2022)</ref>. For our approach, we show three different settings:</p><p>? USM is the pre-trained model which learned unified token linking ability from heterogeneous supervision; ? USM Roberta is the initial model of the pre-trained USM, which employs RoBERTa-Large <ref type="bibr" target="#b19">(Liu et al. 2019)</ref> as the pre-trained transformer encoder; ? USM Unify is initialized by the pre-trained USM and conducts multi-task learning with all datasets but ignores overlapped datasets: ACE05-Ent/Rel and 15/16-res. For the USM Roberta and USM settings, we fine-tune them on each specific task separately. We run each experiment with three seeds and report their average performance.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the overall performance of USM and other baselines on the 13 datasets, where AVE-unify indicates the average performance of non-overlapped datasets, and AVEtotal indicates the average performance of all datasets. We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Zero-shot Transfer Settings</head><p>We conduct zero-shot cross-type transfer experiments on 9 datasets across various domains to verify the transferable conceptualization learned by USM. In this setting, we directly employ the pre-trained USM to conduct extraction on new datasets.  For entity extraction, the cross-type extraction datasets include Movie (MIT-Movie), Restaurant (MIT-Restaurant) <ref type="bibr" target="#b18">(Liu et al. 2013)</ref>, Social (WNUT-16) <ref type="bibr" target="#b38">(Strauss et al. 2016)</ref>, and AI/Literature/Music/Politics/Science from CrossNER <ref type="bibr" target="#b20">(Liu et al. 2021)</ref>. We investigate the effect of different supervised signals in the zero-shot entity extraction setting. D task indicates we first train USM on the common entity extraction dataset -Ontonotes, then directly conduct extraction on the new types, which emulates the most common label transfer method used in real-world scenarios. To be consistent with the real scenario, we select the best checkpoint according to the F1 score on the dev set of D task .</p><p>For zero-shot relation extraction, we compare USM with the following strong baselines:</p><p>? <ref type="bibr">GPT-3 175B (Brown et al. 2020</ref>) is a large-scale, generative pre-trained model, which can extract entity and relation by formulating the task as a question answering problem through prompting <ref type="bibr">(Wang et al. 2022a</ref>). ? DEEPSTRUCT 10B is a structured prediction model pretrained on six large-scale entity, relation, and triple datasets <ref type="bibr">(Wang et al. 2022a)</ref>.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the entity extraction performance on the unseen label subset, in which types are not appearing in the pre-training dataset. And Table <ref type="table">3</ref> shows the performance of zero-shot relation extraction on CoNLL04. From Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table">3</ref>, we can see that: 1) USM has a strong zero-shot transferability across labels. USM shows good migration performance on Movie, Literature, and Music domains even when learning from D task with limited entity types. For relation extraction, USM (356M) outperforms the strong zeroshot baseline GPT-3 (175B) and DEEPSTRUCTURE (10B) with a smaller model size. 2) Heterogeneous supervision boosts USM with unified label semantics and outperforms the task annotation baseline by a large margin. Compared to the task annotation baseline (D task ), USM significantly and consistently improves the performance on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Few-shot Transfer Settings</head><p>To further investigate the effects of verbalized label semantics, we conduct few-shot transfer experiments on four IE tasks and compare USM with the following baselines:</p><p>? UIE-large* is the pre-trained sequence-to-structure model for effective low-resource IE tasks, which injects label semantics by generating labels and words in structured extraction language synchronously and guiding the generation with a structural schema instructor. ? USM Roberta is the initial model of USM, which directly use Roberta-large as the pre-trained encoder; ? USM Symbolic replaces the names of labels with symbolic representation (meaning-less labels, e.g., label1, label2, ...) during the fine-tuning stage of USM, which is used to verify the effect of verbalized label semantics. For few-shot transfer experiments, we follow the data splits and settings with the previous work <ref type="bibr" target="#b22">(Lu et al. 2022)</ref> and repeat each experiment 10 times to avoid the influence of random sampling <ref type="bibr">(Huang et al. 2021)</ref>. Table <ref type="table" target="#tab_4">4</ref> shows the performance on 4 IE tasks under the few-shot settings, where AVE-S is the average performance of 1/5/10-shot experiments. We can see that: 1) By modeling IE tasks via unified semantic matching, USM exceeds the few-shot stateof-the-art UIE-large 5.11 on average. Although UIE also adopts verbalized label representation, this structure generation method needs to learn to generate the novel schema word in the target structure during transfer learning. In contrast, USM only needs to learn to match them, providing a better inductive bias and leading to a much smaller decoding search space. The pre-trained unified token linking ability boosts the USM in all settings. 2) It is crucial to verbalize label schemas rather than meaningless symbols, especially for complex extraction tasks. USM Symbolic , which uses symbolic labels instead of verbalized labels, drastically reduces performance on all tasks. For tasks with more semantic types, such as event extraction with 33 types, the performance drops significantly, even lower than that of USM Roberta initialized directly with Roberta-large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In the past decade, due to powerful representation ability, deep learning methods <ref type="bibr" target="#b2">(Bengio et al. 2003;</ref><ref type="bibr" target="#b5">Collobert et al. 2011)</ref> have made amazing achievements in information extraction tasks. Most of these methods decompose extraction into multiple sub-tasks and follow the classical neural classifier method (Krizhevsky, Sutskever, and Hinton 2012) to model each sub-task, such as entity extraction, relation classification, event trigger detection, event argument classification, etc. And several architectures are proposed to model the extraction, such as sequence tagging <ref type="bibr" target="#b14">(Lample et al. 2016;</ref><ref type="bibr" target="#b50">Zheng et al. 2017)</ref>, span classification <ref type="bibr" target="#b36">(Sohrab and Miwa 2018;</ref><ref type="bibr" target="#b37">Song et al. 2019;</ref><ref type="bibr" target="#b41">Wadden et al. 2019)</ref>, table filling <ref type="bibr" target="#b9">(Gupta, Sch?tze, and Andrassy 2016;</ref><ref type="bibr" target="#b45">Wang and Lu 2020)</ref>, question answering <ref type="bibr" target="#b15">(Levy et al. 2017;</ref><ref type="bibr" target="#b16">Li et al. 2020)</ref>, and token pair <ref type="bibr">(Wang et al. 2020;</ref><ref type="bibr" target="#b50">Yu et al. 2021)</ref>.</p><p>Recently, to solve various IE tasks with a single architecture, UIE employs unified structure generation, models the various IE tasks with structured extraction language, and pre-trains the ability of structure generation using distant text-structure supervision <ref type="bibr" target="#b22">(Lu et al. 2022)</ref>. Unlike the generation-based approach, we model universal information extraction as unified token linking, which reduces the search space during decoding and leads to better generalization performance. Beyond distant supervision, we further introduce indirect supervision from related NLP tasks to learn the unified token linking ability.</p><p>Similar to USM in this paper, matching-based IE approaches aim to verbalize the label schema and structure candidate to achieve better generalization <ref type="bibr">(Liu et al. 2022)</ref>. Such methods usually use pre-extracted syntactic structures <ref type="bibr">(Wang et al. 2021a</ref>) and semantic structures <ref type="bibr" target="#b11">(Huang et al. 2018)</ref> as candidate structures, then model the extraction as text entailment <ref type="bibr" target="#b26">(Obamuyide and Vlachos 2018;</ref><ref type="bibr" target="#b34">Sainz et al. 2021;</ref><ref type="bibr" target="#b23">Lyu et al. 2021;</ref><ref type="bibr" target="#b33">Sainz et al. 2022</ref>) and semantic structure mapping <ref type="bibr" target="#b3">(Chen and Li 2021;</ref><ref type="bibr" target="#b6">Dong, Pan, and Luo 2021)</ref>. Different from the pre-extraction and matching style, this paper decouples various IE tasks to unified token linking operations and designs a one-pass end-to-end information extraction framework for modeling all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a unified semantic matching framework -USM, which jointly encodes extraction schema and input text, uniformly extracts substructures in parallel, and controllably decodes target structures on demand. Experimental results show that USM achieves state-of-the-art performance under the supervised experiments and shows strong generalization ability under zero/few-shot transfer settings, which verifies USM is a novel, transferable, controllable, and efficient framework. For future work, we want to extend USM to NLU tasks, e.g., text classification, and investigate more indirect supervision signals for IE, e.g., text entailment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The USM framework for UIE. USM takes label schema and text as input and directly outputs the target structure through the Structuring and Conceptualizing operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall framework of Unified Semantic Matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Illustrations of Directed Token Linking. Token-Token Linking structures utterance and association pair substructures from the text, Label-Token Linking conceptualizes the utterance, and Token-Label Linking conceptualizes the association pair. In practice, we employ different label symbols "[L]" for utterance conceptualizing: "[LM]" for the label of single mention, such as entity types and event trigger types; "[LP]" for the predicate of association pair, such as relation types and event argument types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>as our Overall results of USM on different datasets. AVE-unify indicates the average performance of non-overlapped datasets (except ACE05-Rel/Evt and 15/16-res), and AVE-total indicates the average performance of all datasets.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>UIE</cell><cell>Task-specific SOTA Methods</cell><cell></cell><cell cols="3">USM Roberta USM USM Unify</cell></row><row><cell>ACE04</cell><cell>Entity F1</cell><cell>86.89</cell><cell>(Lou, Yang, and Tu 2022)</cell><cell>87.90</cell><cell>87.79</cell><cell>87.62</cell><cell>87.34</cell></row><row><cell>ACE05-Ent</cell><cell>Entity F1</cell><cell>85.78</cell><cell>(Lou, Yang, and Tu 2022)</cell><cell>86.91</cell><cell>86.98</cell><cell>87.14</cell><cell>-</cell></row><row><cell>CoNLL03</cell><cell>Entity F1</cell><cell>92.99</cell><cell>(Wang et al. 2021b)</cell><cell>93.21</cell><cell>92.76</cell><cell>93.16</cell><cell>92.97</cell></row><row><cell>ACE05-Rel</cell><cell>Relation Strict F1</cell><cell>66.06</cell><cell>(Yan et al. 2021)</cell><cell>66.80</cell><cell>66.54</cell><cell>67.88</cell><cell>-</cell></row><row><cell>CoNLL04</cell><cell>Relation Strict F1</cell><cell cols="3">75.00 (Huguet Cabot and Navigli 2021) 75.40</cell><cell>75.86</cell><cell>78.84</cell><cell>77.12</cell></row><row><cell>NYT</cell><cell cols="4">Relation Boundary F1 93.54 (Huguet Cabot and Navigli 2021) 93.40</cell><cell>93.96</cell><cell>94.07</cell><cell>94.01</cell></row><row><cell>SciERC</cell><cell>Relation Strict F1</cell><cell>36.53</cell><cell>(Yan et al. 2021)</cell><cell>38.40</cell><cell>37.05</cell><cell>37.36</cell><cell>37.42</cell></row><row><cell>ACE05-Evt</cell><cell>Event Trigger F1</cell><cell>73.36</cell><cell>(Wang et al. 2022b)</cell><cell>73.60</cell><cell>71.68</cell><cell>72.41</cell><cell>72.31</cell></row><row><cell>ACE05-Evt</cell><cell>Event Argument F1</cell><cell>54.79</cell><cell>(Wang et al. 2022b)</cell><cell>55.10</cell><cell>55.37</cell><cell>55.83</cell><cell>53.57</cell></row><row><cell>CASIE</cell><cell>Event Trigger F1</cell><cell>69.33</cell><cell>(Lu et al. 2021)</cell><cell>68.98</cell><cell>70.77</cell><cell>71.73</cell><cell>71.56</cell></row><row><cell>CASIE</cell><cell>Event Argument F1</cell><cell>61.30</cell><cell>(Lu et al. 2021)</cell><cell>60.37</cell><cell>63.05</cell><cell>63.26</cell><cell>63.00</cell></row><row><cell>14-res</cell><cell cols="2">Sentiment Triplet F1 74.52</cell><cell>(Lu et al. 2022)</cell><cell>74.52</cell><cell>76.35</cell><cell>77.26</cell><cell>77.29</cell></row><row><cell>14-lap</cell><cell cols="2">Sentiment Triplet F1 63.88</cell><cell>(Lu et al. 2022)</cell><cell>63.88</cell><cell>65.46</cell><cell>65.51</cell><cell>66.60</cell></row><row><cell>15-res</cell><cell cols="2">Sentiment Triplet F1 67.15</cell><cell>(Lu et al. 2022)</cell><cell>67.15</cell><cell>68.80</cell><cell>69.86</cell><cell>-</cell></row><row><cell>16-res</cell><cell cols="2">Sentiment Triplet F1 75.07</cell><cell>(Lu et al. 2022)</cell><cell>75.07</cell><cell>76.73</cell><cell>78.25</cell><cell>-</cell></row><row><cell>AVE-unify</cell><cell>-</cell><cell>71.10</cell><cell>-</cell><cell>71.34</cell><cell>71.83</cell><cell>72.46</cell><cell>72.11</cell></row><row><cell>AVE-total</cell><cell>-</cell><cell>71.75</cell><cell>-</cell><cell>72.05</cell><cell>72.61</cell><cell>73.35</cell><cell>-</cell></row><row><cell cols="4">indirect supervision datasets: HotpotQA (Yang et al. 2018),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Natural Questions (Kwiatkowski et al. 2019), NewsQA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(Trischler et al. 2017), SQuAD (Rajpurkar et al. 2016) and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">TriviaQA (Joshi et al. 2017). Compared with limited entity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>types in D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>task and relation types D distant , diversified question expressions can provide richer label semantic information for learning conceptualizing. For each (question, context, answer) instance in D indirect , we take the question as label schema, the context as input text, and the answer as mention. It captures structuring and conceptualizing ability in the pretraining stage by learning token-token and label-token linking operations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of Zero-shot transfer settings on unseen entity label subset with different supervision signals. Unseen indicates label types that do not appear in the pre-training dataset. ? indicates the improvement of pre-training using extra supervision signals (D distant and D indirect ).</figDesc><table><row><cell></cell><cell cols="3">Movie Restaurant Social</cell><cell>AI</cell><cell cols="4">Literature Music Politics Science</cell><cell>Ave</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Performance on Unseen Label Subset of D t and D i</cell><cell></cell><cell></cell></row><row><cell>#Unseen/#All</cell><cell>12/12</cell><cell>7/8</cell><cell>7/10</cell><cell>10/14</cell><cell>8/12</cell><cell>9/13</cell><cell>5/9</cell><cell>13/17</cell><cell>-</cell></row><row><cell>D task</cell><cell>25.07</cell><cell>2.50</cell><cell cols="2">22.54 10.82</cell><cell>50.74</cell><cell>44.11</cell><cell>9.75</cell><cell>13.98</cell><cell>22.44</cell></row><row><cell>D task + D indirect</cell><cell>37.73</cell><cell>14.73</cell><cell cols="2">29.34 28.18</cell><cell>56.00</cell><cell>44.93</cell><cell>36.10</cell><cell>44.09</cell><cell>36.39</cell></row><row><cell></cell><cell></cell><cell cols="7">Performance on Unseen Label Subset of Pre-training Dataset</cell><cell></cell></row><row><cell>#Unseen/#All</cell><cell>10/12</cell><cell>7/8</cell><cell>6/10</cell><cell>8/14</cell><cell>7/12</cell><cell>8/13</cell><cell>4/9</cell><cell>12/17</cell><cell>-</cell></row><row><cell>D task</cell><cell>32.10</cell><cell>2.50</cell><cell>1.64</cell><cell>10.68</cell><cell>52.42</cell><cell>45.93</cell><cell>11.16</cell><cell>14.12</cell><cell>21.32</cell></row><row><cell>D task + D indirect</cell><cell>39.76</cell><cell>14.73</cell><cell cols="2">20.62 24.12</cell><cell>56.24</cell><cell>44.21</cell><cell>32.92</cell><cell>44.25</cell><cell>34.61</cell></row><row><cell>D task + D distant</cell><cell>35.35</cell><cell>21.10</cell><cell cols="2">40.64 27.57</cell><cell>56.97</cell><cell>49.29</cell><cell>43.72</cell><cell>44.05</cell><cell>39.84</cell></row><row><cell cols="2">D task + D distant + D indirect 42.11</cell><cell>26.01</cell><cell cols="2">44.37 34.91</cell><cell>65.69</cell><cell>60.07</cell><cell>56.65</cell><cell>55.26</cell><cell>48.13</cell></row><row><cell>?</cell><cell>10.01</cell><cell>23.51</cell><cell cols="2">42.73 24.23</cell><cell>13.27</cell><cell>14.14</cell><cell>45.49</cell><cell>41.14</cell><cell>26.82</cell></row><row><cell></cell><cell cols="2">CoNLL04 Model Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-3</cell><cell>18.10</cell><cell>175B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DEEPSTRUCT</cell><cell>25.80</cell><cell>10B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>USM</cell><cell>25.95</cell><cell>356M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 3: Performance of Zero-shot transfer settings on re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">lation extraction. * GPT-3 175B indicates formulating the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">extraction task as a question answering problem through</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">prompting, and DEEPSTRUCT 10B is a pre-trained language</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">model for structure prediction (Wang et al. 2022a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">can observe that: 1) By verbalizing labels and modeling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">all IE tasks as unified token linking, USM provides a novel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and effective framework for IE. USM achieves state-of-the-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">art performance and outperforms the strong task-specific</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">methods by 1.30 in AVE-total. Even without pre-training,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">USM Roberta also shows strong performance, which indicates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the strong portability and generalization ability of unified to-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ken linking. 2) Heterogeneous supervision provides a better</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">foundation for structuring and conceptualizing information</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">extraction. Compared to the initial model USM Roberta and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the pre-trained model USM, the heterogeneous pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">achieved an average 0.74 improvement across all datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">3) By homogenizing diversified label schemas and hetero-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">geneous target structures into the unified token sequence,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">USM Unify can solve massive IE tasks with a single multi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">task model. USM Unify outperforms task-specific state-of-the-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">art methods with different model architectures and encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">backbones in average, providing an efficient solution for ap-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>plication and deployment.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Few-shot results on end-to-end IE tasks.</figDesc><table><row><cell>For a fair</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We sincerely thank the reviewers for their insightful comments and valuable suggestions. This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No.<rs type="grantNumber">2020AAA0109400</rs>) and the <rs type="funder">Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62122077</rs>, <rs type="grantNumber">61876223</rs>, and <rs type="grantNumber">62106251</rs>). Hongyu Lin is sponsored by <rs type="funder">CCF-Baidu Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mWvvZzD">
					<idno type="grant-number">2020AAA0109400</idno>
				</org>
				<org type="funding" xml:id="_E6UZnNS">
					<idno type="grant-number">62122077</idno>
				</org>
				<org type="funding" xml:id="_SaST9TS">
					<idno type="grant-number">61876223</idno>
				</org>
				<org type="funding" xml:id="_4r4gDds">
					<idno type="grant-number">62106251</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A causal framework for explaining the predictions of black-box sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Facts from Press Releases to Generate News Stories</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Huettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Schmandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Nirenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ANLP</title>
		<meeting>of ANLP</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2020</date>
		</imprint>
	</monogr>
	<note>Language Models are Few-Shot Learners</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Learning</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">New Frontiers of Information Extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Processing (Almost) from Scratch. J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MRQA</title>
		<meeting>of MRQA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twenty-five years of information extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-Shot Named Entity Recognition: An Empirical Baseline Study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">REBEL: Relation Extraction By End-to-end Language generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; P.-L</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP Findings</title>
		<meeting>of EMNLP Findings</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2021</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2012">2017. 2012</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-Shot Relation Extraction via Reading Comprehension</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Unified MRC Framework for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pretraining to Match for Unified Low-shot Relation Extraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asgard: A portable architecture for multilingual dialogue systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>A Robustly Optimized BERT Pretraining Approach. CoRR</publisher>
			<pubPlace>RoBERTa</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI<address><addrLine>Lou, C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2022. 2018</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zeroshot Event Extraction via Transfer Learning: Challenges and Insights</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ACE 2004 Multilingual Training Corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zakhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LDC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot Relation Classification as Textual Entailment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Obamuyide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jim?nez-Zafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval</title>
		<meeting>of Se-mEval</meeting>
		<imprint>
			<date type="published" when="2016">2018. 2016</date>
		</imprint>
	</monogr>
	<note>Proc. of FEVER.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval. Pontiki, M.; Galanis</title>
		<meeting>of SemEval. Pontiki, M.; Galanis</meeting>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
		</imprint>
	</monogr>
	<note>Proc. of SemEval</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards Robust Linguistic Analysis using OntoNotes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling Relations and Their Mentions without Labeled Text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation Extraction with Matrix Factorization and Universal Schemas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Linear Programming Formulation for Global Inference in Natural Language Tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Textual Entailment for Event Argument Extraction: Zero-and Few-Shot with Multi-Source Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Findings</title>
		<meeting>of ACL Findings</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CASIE: Extracting Cybersecurity Event Information from Text</title>
		<author>
			<persName><forename type="first">T</forename><surname>Satyapanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Exhaustive Model for Nested Named Entity Recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging Dependency Forest for Neural Medical Relation Extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Results of the WNUT16 Named Entity Recognition Shared Task</title>
		<author>
			<persName><forename type="first">B</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WNUT</title>
		<meeting>of WNUT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">RoFormer: Enhanced Transformer with Rotary Position Embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RepL4NLP</title>
		<editor>
			<persName><forename type="middle">Tjong</forename><surname>Corr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Kim Sang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</editor>
		<meeting>of RepL4NLP</meeting>
		<imprint>
			<date type="published" when="2003">2022. 2003. 2017</date>
		</imprint>
	</monogr>
	<note>Proc. of CoNLL</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Entity, Relation, and Event Extraction with Contextualized Span Representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">ACE 2005 Multilingual Training Corpus</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>LDC</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2021a. Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2022a. DeepStruct: Pretraining of Language Models for Structure Prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Findings</title>
		<meeting>of ACL Findings</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learnability with Indirect Supervision Signals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Findings</title>
		<meeting>of ACL Findings</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COL-ING</title>
		<meeting>of COL-ING</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2021. 2017</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
