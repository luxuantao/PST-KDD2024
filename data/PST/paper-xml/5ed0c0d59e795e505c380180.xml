<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Based Vehicle Trajectory Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaouther</forename><surname>Messaoud</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
						</author>
						<title level="a" type="main">Attention Based Vehicle Trajectory Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4388C613A2B00AE8CBAAAE0C8848CE19</idno>
					<idno type="DOI">10.1109/TIV.2020.2991952</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2020.2991952, IEEE Transactions on Intelligent Vehicles This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2020.2991952, IEEE Transactions on Intelligent Vehicles</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>trajectory prediction</term>
					<term>vehicles interactions</term>
					<term>recurrent networks</term>
					<term>multi-head attention</term>
					<term>multi-modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-driving vehicles need to continuously analyse the driving scene, understand the behavior of other road users and predict their future trajectories in order to plan a safe motion and reduce their reaction time. Motivated by this idea, this paper addresses the problem of vehicle trajectory prediction over an extended horizon. On highways, human drivers continuously adapt their speed and paths according to the behavior of their neighboring vehicles. Therefore, vehicles' trajectories are very correlated and considering vehicle interactions makes motion prediction possible even before the start of a clear maneuver pattern. To this end, we introduce and analyze trajectory prediction methods based on how they model the vehicles interactions. Inspired by human reasoning, we use an attention mechanism that explicitly highlights the importance of neighboring vehicles with respect to their future states. We go beyond pairwise vehicle interactions and model higher order interactions. Moreover, the existence of different goals and driving behaviors induces multiple potential futures. We exploit a combination of global and partial attention paid to surrounding vehicles to generate different possible trajectory. Experiments on highway datasets show that the proposed model outperforms the state-of-the-art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N order to navigate, self-driving vehicles need to under- stand the behavior of other traffic participants. As communications are not always possible, self-driving vehicles must perceive and anticipate the intentions of surrounding vehicles in order to plan comfortable proactive motions and avoid urgent reactive decisions and conflicts with others. In fact, motion prediction helps self-driving vehicles understand possible future situations and decide about a future behavior that minimizes the possible risks accordingly. Motion behavior may be inferred by considering the features that characterises it. Vehicles' past states give relevant information about the dynamics, the direction and the speed of the performed maneuver. However, the trajectory taken by each vehicle in the future is not only dependent on its own state history: even the vehicle class impacts the motion pattern. In addition, the presence and actions of the neighboring vehicles have a great influence on a vehicle's behavior as well. Therefore, in this work, we propose to model the interactions between all the neighboring vehicles to represent the most relevant information about the social context with a focus on learning to capture long-range relations. In our approach, we attempt to mimic human reasoning, which pays a selective attention to a subset of surrounding vehicles in order to extract 1 Inria Paris, 2 rue Simone Iff 75012 Paris FRANCE {kaouther.messaoud,anne.verroust,fawzi.nashashibi}@inria.fr 2 CReSTIC, Universit√© de Reims Champagne-Ardenne, Reims, FRANCE itheri.yahiaoui@univ-reims.fr the elements that most influence the target vehicle's future trajectories while paying less attention to other vehicles. For example, a vehicle performing a lane change maneuver will pay more attention to the vehicles in the target lane than those in the other lanes. Consequently, its future behavior could be more dependent on distant vehicles in the target lane than the close ones in the other lanes. This study is an extension of our previous work <ref type="bibr" target="#b0">[1]</ref>, which focuses on deploying multi-head attention in the task of trajectory prediction. We adopt the attention mechanism to derive the relative importance of surrounding vehicles with respect to their future motion: it selectively aggregates the features that model the interaction between the vehicles by a weighted sum of the features representing all the surrounding vehicles' trajectories and thus directly relates vehicles based on their correlation without regard to their distance. We also use multihead attention in order to extract different types of interactions and combine them to capture higher order relationships. This provides a better understanding of the scene. Drivers' behaviors are not deterministic. In similar driving situations, they can perform different maneuvers or even when doing the same maneuver, the execution can be different in terms of speed and pattern. Therefore, we propose a method that is able to predict a multi-modal finite set of trajectories that correspond to predicted trajectories conditioned on the degree of attention paid to the surrounding vehicles. Quantitative and qualitative experiments are conducted to show the contribution of the model, and quantitative comparisons with recent approaches show that the proposed approach outperforms state-of-the-art accuracy in highway driving trajectory prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH</head><p>The task of vehicle motion forecasting has been addressed in the literature from different perspectives. Therefore, numerous vehicle motion prediction methods have recently been proposed. Here, we give an overview of the deployed methods, focusing on deep learning pattern based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Motion Prediction Module</head><p>We follow Rudenko et al. <ref type="bibr" target="#b1">[2]</ref> who divide the motion prediction problem into three main components.</p><p>1) Stimuli: The features that influence and determine the future intention of the target vehicle are mainly composed of target vehicle cues and environment information. Target vehicle features. They enclose target vehicle past state observations (positions, velocities, etc.). Lenz et al. <ref type="bibr" target="#b2">[3]</ref> use as input to their model only the current state of a set of neighboring vehicles in order to achieve the Markov Property. Other existing studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref> use a sequence of past features to benefit from extra temporal information in the prediction task. Environment features. These are composed of: -Static elements including static obstacles and environment geometry.</p><p>-Dynamic elements representing the other traffic participants.</p><p>2) Modeling approach: Different representations of the motion model are used, which can be classified into: Physics-based methods, where the future trajectory is predicted by applying explicit, hand-crafted, physics-based dynamical models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. These approaches basically build upon the motion's low level properties. Consequently, they are restricted to short-term motion prediction. Pattern-based methods that learn the motion and behaviors of vehicles from data of observed trajectories. Aoude et al. <ref type="bibr" target="#b11">[12]</ref> combine a physics-based approach with Gaussian Processes based motion patterns to generate probabilistically weighted feasible motions of the surrounding vehicles. Other methods divide the vehicle trajectory into a finite set of typical patterns named maneuvers. Tran and Firl <ref type="bibr" target="#b12">[13]</ref> identify the vehicle maneuvers by comparing the likelihoods of the observed track for the constructed non-parametric regression models. Hermes et al. <ref type="bibr" target="#b13">[14]</ref> cluster the motion patterns with a rotationallyinvariant distance metric into maneuvers and predict vehicles trajectories by matching the observation data to the maneuvers. <ref type="bibr">Schlechtriemen et al. [15]</ref> deploy a Naive Bayes Classifier followed by a Hidden Markov Model (HMM), where each state of the HMM corresponds to one of the maneuvers extracted from the naturalistic driving data. Houenou et al. <ref type="bibr" target="#b15">[16]</ref> conceive a maneuvers recognition module, then, generate different continuous realizations of the predicted maneuver. The main limitation of these approaches is that they do not model the interactions between the neighboring vehicles on the future trajectory. Kafer et al. <ref type="bibr" target="#b16">[17]</ref> tackle the task of joint pairwise vehicle trajectory prediction at intersections. They compare the observed motion pattern to the database and extract, for each vehicle, possible predicted trajectories independently. Then, they jointly compute, for each pair, the probability of possible trajectories. Most recent studies deploy deep learning based methods. They will be detailed in the Section II-B. Planning-based methods reason on the motion intent of rational agents. Sierra Gonz√°lez et al. <ref type="bibr" target="#b17">[18]</ref> deploy Markov Decision Process (MDPs) to represent the driver decisionmaking strategy. They model a vehicle's trajectory by a sequence of states. Then, they build a cost function using a linear combination of static and dynamic features parameterizing each state. Inverse Reinforcement Learning (IRL), accounting for risk-aversive vehicles' interactions, operates to learn the cost function parameters from demonstrations. They use Dynamic Bayesian Networks, in <ref type="bibr" target="#b18">[19]</ref>, to model vehicles' interactions. Li et al. <ref type="bibr" target="#b19">[20]</ref> extend Generative Adversarial Imitation Learning (GAIL) <ref type="bibr" target="#b21">[21]</ref> and deploy it to predict the driver's future actions given an image and past states. The proposed method is able to imitate different types of human driving behavior in a simulated highway scenario. Rhinehart et al. <ref type="bibr" target="#b23">[22]</ref> use a deep imitative model to learn and predict desirable future autonomous behavior. They train their model with an expert human behaviors dataset, and use it to generate expert-like paths to each of the precomputed goals.</p><p>3) Prediction: Vehicle intent prediction is divided into two main aspects: maneuver <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[23]</ref> and trajectory prediction <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The former generates a high-level representation of the motion such as lane changing and lane keeping. The latter outputs the predicted state over time. Different forms of outputs are used in the motion prediction task. In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, the exact future positions are predicted. Others <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b5">[6]</ref> deploy a multimodal solution using Gaussian mixture models over predicted states. Ridel et al. <ref type="bibr" target="#b27">[26]</ref> generate the probability distributions over grids with multiple trajectory samples. Sampling generative models such as Generative Adversarial Networks (GANs) was used in <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning Pattern-based Motion Prediction</head><p>Motion prediction can be treated as a time series regression or classification problem. Recurrent Neural Networks (RNNs) are the main reason behind the significant advances in sequence modeling and generation. They have shown promising results in diverse domains such as natural language processing and speech recognition. Therefore, RNN-based approaches have been deployed as well in the tasks of maneuver and trajectory prediction. Long Short Term Memories (LSTMs) are a particular implementation of RNNs. They are characterised by their ability to extract long-term relations between features. In other word, unlike other neural networks, they consider sequential information and model the dependency in inputs. They act by performing the same operations for every input item of a sequence while taking into consideration the computation of the previous input item. LSTMs have been deployed, recently, for predicting driver future behaviors. Indeed, different LSTM-based models have been conceived going from simple LSTM with one or more layers in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[30]</ref> to different types of combinations and extensions: A dual LSTM architecture was adopted in <ref type="bibr" target="#b25">[24]</ref>: the first LSTM extracts high-level driver behavior succeeded by a second for continuous trajectory generation. LSTM encoder decoder based architectures were deployed in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b0">[1]</ref>. One of the most important parts in a driver intention prediction model is the surrounding vehicles' interaction extractor. It is also conceived differently in the state of the art. Some existing studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> implicitly infer the dependencies between vehicles. They feed a sequence of surrounding vehicles features as inputs to their model. Then, they accord to the LSTM the task of learning the influence of surrounding vehicles on the target vehicle's motion. Other approaches explicitly model the vehicles' interactions using several combinations of networks. Alahi et al. <ref type="bibr" target="#b33">[32]</ref> introduced the social LSTM concept for pedestrian trajectory prediction task. They encode the motion of each agent using an LSTM block. Then, they extract the interactions between agents by sharing the hidden states between all the LSTMs corresponding to a set of neighboring pedestrians. Hou et al <ref type="bibr" target="#b34">[33]</ref> use a structural-LSTM network to learn high-level dependencies between vehicles. Similar to social LSTM, they attribute one LSTM for each vehicle. Then, they use convolutional layers applying successive local operations followed by a maxpool layer. the spatial-neighboring LSTMs share their cell and hidden states by a radial connection. The output states of the LSTMs are treated recurrently in a deeper layer. The decoder generates all the predicted trajectories. Deo et al. <ref type="bibr" target="#b6">[7]</ref> extend the social pooling and deploy it for vehicle trajectory prediction task. They use an LSTM encoder to generate a representation of each vehicle trajectory. Then, they use convolutional layers applying successive local operations on the outputs from the encoders followed by a maxpool layer. Therefore, they generate a context vector that consists on a compact representation of the vehicles interactions. But successive local operations are not always sufficient. Furthermore, the generated context vector is independent of the target vehicle's state. Zhao et al. <ref type="bibr" target="#b28">[27]</ref> extend the convolutional social pooling to simultaneous multi-agents trajectory prediction. Multi-head attention mechanism was introduced by Vaswani et al. <ref type="bibr" target="#b35">[34]</ref> for natural language processing purposes. A relational recurrent network based on attention mechanism was deployed in <ref type="bibr" target="#b7">[8]</ref> for trajectory prediction. In <ref type="bibr" target="#b0">[1]</ref>, an attention-based nonlocal vehicle dependencies model that represents vehicles' interactions based on their importance to the target vehicle is introduced. The attention mechanism reduces the number of local operations by directly relating distant elements. The motion prediction results computed by this method on the NGSIM dataset <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref> improve those reported in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this article, we extend our previous approach <ref type="bibr" target="#b0">[1]</ref> to tackle the target vehicle trajectory prediction problem (cf. Section III) as follows:</p><p>‚Ä¢ We focus on studying non-local social pooling using a multi-head attention mechanism. Therefore, we remove the convolution layer used to extract local interactions in our previous method <ref type="bibr" target="#b0">[1]</ref>. ‚Ä¢ We expand our previous approach by exploiting additional information to boost our prediction. We follow <ref type="bibr" target="#b24">[23]</ref> and, in order to take into account the social effect of the surrounding vehicles on the prediction target based on relative dynamics, we include additional information (velocity, acceleration) in the vehicle state vectors. We also integrate the vehicle class information since the type of the vehicle characterises its motion pattern. ‚Ä¢ We investigate the interest of using multiple attention heads and we analyse the interactions extracted using each head. We also compare several ways of attention computation.</p><p>‚Ä¢ We augment our architecture to generate a multi-modal solution based on a combination of partial and global attentions paid to the surrounding vehicles.</p><p>Experimental evaluations presented in Section IV show the benefits of using attention mechanisms to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TARGET VEHICLE TRAJECTORY PREDICTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>The goal of this part is to predict the future trajectory of a target vehicle T , knowing its past tracks and the past tracks of its neighboring vehicles at observation time t obs . We have as input the past tracks of the target and its n neighboring vehicles. The input tracks of a vehicle i are defined as</p><formula xml:id="formula_0">X i = [x 1 i , . . . , x t obs i</formula><p>] where x t i = (x t i , y t i , v t i , a t i , class) is the state vector. We note X T the state of the target vehicle T . The coordinates of all the considered vehicles, are expressed in a stationary frame of reference where the origin is the position of the target vehicle at time t obs . The y -axis and x -axis point respectively to one direction of motion on the highway and to the direction perpendicular to it. Our model outputs the parameters characterizing a probability distribution over the predicted positions of the target vehicle.</p><formula xml:id="formula_1">Y pred = [y t obs +1</formula><p>pred , . . . , y</p><formula xml:id="formula_2">t obs +t f pred ]</formula><p>Where y t = (x t , y t ) is the predicted coordinates of the target vehicle.</p><p>Our model infers the conditional probability distribution P(Y|X). The distribution over the possible positions at time t ‚àà {t obs + 1, . . . , t obs + t f } can be presented as a bivariate Gaussian distribution with the parameters Œò t = (¬µ t , Œ£ t ) of the form:</p><formula xml:id="formula_3">y t ‚àº N (¬µ t , Œ£ t )</formula><p>Where ¬µ t is the mean vector and Œ£ t is the covariance matrix:</p><formula xml:id="formula_4">¬µ t = ¬µ t x ¬µ t y , Œ£ t = (œÉ t x ) 2 œÉ t x œÉ t y œÅ t œÉ t x œÉ t y œÅ t (œÉ t y ) 2</formula><p>We evaluate our model by considering the mean ¬µ t values as the predicted positions y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Model</head><p>It is crucial to understand the relationships and interactions that occur on the road to make realistic predictions about vehicle motions. Therefore, our model architecture is made up of three main components (cf. Fig. <ref type="figure">1</ref>):</p><p>‚Ä¢ Encoding layer, where the temporal evolution of the vehicle's trajectories and their motion properties are encoded by an LSTM encoder. The LSTM encoders, with shared weights, generate a vector encoding of each vehicle motion. The multi-head attention module models the interactions between the target (green car) and the neighboring vehicles based on their importance. The decoder receives the interaction vector and the target vehicle encoding and generates a distribution for the predicted trajectory. The blocks added in green are the extension of the multi-head attention method to Multi-Modal Trajectory Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Trajectory Encoder</head><p>This encoding layer encodes the trajectories of the vehicles belonging to a neighborhood of the target vehicle at time t = t obs . Unlike most of the previous studies that consider a restricted number of vehicles immediately around the target vehicle, we compute a grid over the surrounding area. This representation of the context has the following advantages:</p><p>‚Ä¢ It represents the drivable areas.</p><p>‚Ä¢ It enables us to consider all the vehicles present in the neighboring area without restriction.</p><p>Each state vector x t i of each vehicle i of the neighboring area is embedded using a fully connected layer to form an embedding vector e t i .</p><formula xml:id="formula_5">e t i = Œ®(x t i ; W emb )</formula><p>where Œ®() is a fully connected function with LeakyReLU non linearity, W emb is the learnt the embedding weights. T he LSTM encoder is fed by the embedding vectors of each vehicle i for time steps t = 1, . . . , t obs :</p><formula xml:id="formula_6">h t i = LST M (h t-1 i , e t i ; W encoder )</formula><p>h t i is the hidden state vector of the i th neighboring vehicle at time t. We note h t T the hidden state vector of the target vehicle at time t. W encoder are the LSTM encoders weights. Each LSTM encoder share the same weights W encoder . We built a 3D spatial grid H composed of the neighboring vehicles' hidden states at time t obs based on their positions at time t obs .</p><p>H(n, m, :) = Œ¥ nm (x t obs i , y t obs i )h t obs i ‚àÄi ‚àà A T Œ¥ nm (x, y) is an indicator function that equals 1 if (x, y) is in the cell (n, m) and 0 otherwise. A T consists of the set of surrounding vehicles present in the considered area. The columns correspond to the three lanes (M = 3). The considered spacial area corresponding to the grid is centered on the target vehicle position and sized of (N, M ). It covers a longitudinal distance of 90 m with a grid cell size of 4.5 m. We note C the dimension of the trajectory encoding vectors h t obs i and we reshape the grid H to (N M, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Vehicle Interaction Modules</head><p>As the behavior of vehicles on a highway could be highly correlated, it is important to consider the interactions between the vehicles when predicting their future motion. Attention is used to capture long-range spatio-temporal dependencies. The attention module explicitly models the interactions between the target vehicle and the other vehicles in the grid H and selects the surrounding vehicles to pay attention to when computing the future trajectory of the target vehicle. Instead of computing vehicle relationships at each time step, which is computationally expensive, we use the hidden states of the encoder LSTM computed at the observation time as inputs to the attention module. These hidden states are projected into a high-dimensional space, if we consider all the attention heads. The vehicles interactions can be exploited as follows:</p><p>‚Ä¢ The hidden state of the target vehicle is mapped to a query</p><formula xml:id="formula_7">Q l = Œ∏ l (h t obs T , W Œ∏ l ) ‚Ä¢ The grid is mapped to form the keys K l = œÜ l (H, W œÜ l )</formula><p>and the values V l = œÅ l (H, W œÅ l ). W Œ∏ l , W œÜ l and W œÅ l are the weight matrices that will be learned in each attention head l. An attention feature head l is then calculated as a weighted sum of values v lj , where the attention weights, Œ± lj , weight the effect of surrounding vehicles on the target vehicle future motions, based on their relative dynamics.</p><p>We investigate three possible ways to compute the attention weights Œ± l :</p><formula xml:id="formula_8">head l = N M j=1 Œ± lj v lj</formula><p>1) Œ±-Attention: Attention weights are computed from the encoding vectors of the surrounding vehicles independently of the target vehicle state. They are computed using a tanh function and a fully-connected layer.</p><formula xml:id="formula_9">Œ± l = sof tmax(w T l tanh(K l ))</formula><p>w l is a learned weight, and Œ± l ‚àà R 1√óN M is the l th attention.</p><p>2) Dot-Product Attention: The weights represent the effect of an interaction between a pair of vehicles based on their relative dynamics. They are the product of the query Q with keys K.</p><formula xml:id="formula_10">Œ± l = sof tmax( Q l K T l ‚àö d ) Q l K T l</formula><p>is matrix multiplication used to calculate dot product similarities. d is a scaling factor that equals to the dimensionality of the projection space.</p><p>3) Concatenation Attention: The pairwise relation can be also represented by concatenation operation, as in <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b39">[38]</ref>.</p><formula xml:id="formula_11">Œ± l = sof tmax(w T l concat(repeat(Q l ), K l ))</formula><p>One can notice that dot-product and concatenation attentions consider pairwise inter-relationships, whereas Œ±-attention does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. High Order Interaction</head><p>We deploy a higher order interaction extractor based on multi-head attention to retain different types of spatio-temporal relationships. The use of multi-head is inspired by the Transformer <ref type="bibr" target="#b35">[34]</ref> architecture. In fact, a single learned attention feature mainly focuses on one inter-related subgroup of vehicles that may represent a single aspect of the possible spatiotemporal relationships occurring in the neighborhood of the target vehicle. In order to extend the attention to higher order interactions, different queries, keys and values are generated n h times in parallel, in n h attention heads, with different learned linear projections</p><formula xml:id="formula_12">Q l , K l and V l , l ‚àà [1, n h ].</formula><p>The n h generated attention features represent n h subgroups of vehicles inter-related with the target vehicle. These representations are concatenated and dynamically weighted to extract complex interactions between the different subgroups.</p><formula xml:id="formula_13">z = Concat(head 1 , head nh )W O</formula><p>z is the compact context vector that combines interaction information of all the vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Trajectory Prediction</head><p>LSTM Decoder is fed by the context vector z, which contains the selected information about the vehicles interactions, and the motion encoding of the target vehicle: h dec = Concat(h t obs T , z). It generates the predicted parameters of the distributions over the target vehicle's estimated future positions for time steps t = t obs + 1,. . . , t obs + t f . Œò t = Œõ(LST M (h t-1 dec ; W dec )) where Œò t is the predicted parameters of the positions distribution at time t, Œõ() is a fully connected function followed by a LeakyReLU non linearity, W dec are the learnt weights of the LSTM decoder and h t-1 dec is the hidden state vector of the decoder at time t -1.</p><p>model is trained by minimizing the following negative log-likelihood loss function:</p><formula xml:id="formula_14">L nll (Y pred ) = - t obs+1 ‚â§t‚â§t obs +t f log(P Œò t (y t |X))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Multi-Modal Trajectory Prediction</head><p>Given the history of a vehicle's motion, there are many plausible future trajectories. Generating one trajectory for motion forecasting tends to be the average of the possible motions. When a driver decides to perform a specific motion, he directs his attention to a set of neighboring vehicles. For example, a driver exerting a lane change maneuver will mainly pay attention to the vehicles in the target lane. Therefore, from considered set of neighboring vehicles, we may derive a plausible future trajectory. To do so, we deploy a muti-head attention as described before and, we proceed as following (Figure <ref type="figure">1</ref>): The decoder receives n h encodings of the scene based on different attention heads.</p><formula xml:id="formula_15">h l dec = Concat(h t obs T , head l , z) l ‚àà [1,</formula><p>n h ] using each encoding, the decoder generates a plausible trajectory Y l pred . During the training, we compute only the loss L nll (Y l * pred ) corresponding to closest predicted trajectory to the groundtruth Y l * pred . Therefore, the position outputs are updated only for the minimum error.</p><p>augment the proposed architecture by a network composed of two fully connected layers separated by a non-linear function. It receives the outputs of all the attention heads and decides about the probability (p l , l ‚àà [1, n h ]) of each produced trajectory being the closest to the real one. This network outputs the likelihood of the n h predicted trajectories. For this purpose, we add to the loss function a second term, which is the classification cross-entropy loss with n h classes <ref type="bibr" target="#b26">[25]</ref>.</p><formula xml:id="formula_16">L Class = - n h l=1 Œ¥ l * (l)log(p l )</formula><p>where Œ¥ is function equal to 1 if l = l * and 0 otherwise. Therefore, the probability of the best matching trajectory p l * is trained to become closer to 1, and the probabilities of the others to 0. This makes the probability outputs updated for all the attention heads. During the evaluation, we compute the loss function by taking the selected trajectory Y s pred having the maximum probability p s (Note that Y s pred may be different from Y l * pred ). The proposed network causes each attention head to specialize in extracting interaction features characterizing a distinct class of driver behavior without requiring explicit labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATIONS</head><p>Evaluations have been performed on public driving datasets that are described in Section IV-A. The approach proposed in Section III is compared with state-of-the-art quantitatively. Qualitative results are also presented for further analysis.</p><p>A. Datasets 1) highD <ref type="bibr" target="#b40">[39]</ref>: captured in 2017 and 2018. It was recorded by camera-equipped drones from an aerial perspective of six different German highways at 25 Hz. It is composed of 60 recordings of about 17 minutes each, covering a segment of about 420m of two-way roads (Figure <ref type="figure">2</ref>). It consists of vehicle position measurements from six different Fig. <ref type="figure">2</ref>: Highway drone dataset highD <ref type="bibr" target="#b40">[39]</ref>: recordings cover about 420 m of German highways. highways with 110 000 vehicles (about 12 times as many vehicles as NGSIM) and a total driven distance of 45 000 km. This dataset is of great importance since it has 5 600 recorded complete lane changes and presents recent driver behaviors.</p><p>2) NGSIM <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref>: a publicly available large dataset captured in 2005 at 10Hz, widely studied and used in the literature, especially in the task of future intention prediction of vehicles <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. We use this dataset to compare our model with the state-of-the-art. We split each of the datasets into train (75%) and test (25%) sets. We split the trajectories into segments of 8s of the trajectories composed of a track history of 3s and a prediction horizon of 5s. We downsample each segment to get only 5 fps to reduce the complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Implementation Details</head><p>We deploy LSTM encoder with 64 units (C=64) and decoder with 128 units. The dimension of the embedding space is 32. We use different number of parallel attention operations applied on the projected vectors of size d=32. The batch size is 128 and the adopted optimizer is Adam <ref type="bibr" target="#b41">[40]</ref>. The model is implemented using PyTorch <ref type="bibr" target="#b42">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metric</head><p>In our evaluation, we use Root of the Mean Squared Error (RMSE) metric since it averages the distance between predicted trajectories and the ground truth.</p><formula xml:id="formula_17">L RM SE = 1 t f t obs +t f t=t obs+1 (x t T -x t pred ) 2 + (y t T -y t pred ) 2</formula><p>We use the means of the predicted distributions over the future trajectories to calculate the RMSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Models Compared</head><p>Evaluations have been performed on the following models that all consider the interactions between surrounding vehicles. They are fed with the track history of the target and the surrounding vehicles and output distributions over the future trajectory of the target vehicle.</p><p>‚Ä¢ Maneuver-LSTM (M-LSTM) <ref type="bibr" target="#b5">[6]</ref>: an encoder-decoder based model where the encoder encodes the trajectories of the target and surrounding vehicles. The encoding vector and maneuver encodings are fed to the decoder which generates multi-modal trajectory predictions. ‚Ä¢ Social LSTM (S-LSTM) <ref type="bibr" target="#b33">[32]</ref>: social encoder-decoder using fully connected pooling. ‚Ä¢ Convolutional Social Pooling (CS-LSTM) <ref type="bibr" target="#b6">[7]</ref>: social encoder-decoder using convolutional pooling.</p><p>(CS-LSTM(M)) generates multi-modal trajectory predictions based on six maneuvers (2 longitudinal and 3 lateral). ‚Ä¢ Multi-Agent Tensor Fusion (MATF GAN) <ref type="bibr" target="#b28">[27]</ref>: the model encodes the scene context and vehicles' past trajectories, then, deploys convolutional layers to capture interactions. Finally, the decoder generates the predicted trajectories, using adversarial loss. ‚Ä¢ Non-local Social Pooling (NLS-LSTM) <ref type="bibr" target="#b0">[1]</ref>: combines local and non local operations to generate an adapted context vector for social pooling. Five attention heads are used in this approach. ‚Ä¢ Multi-head Attention Social Pooling (MHA-LSTM): This is the model described in this paper using multihead dot product attention with x t i = (x t i , y t i ), i.e. without using velocity, acceleration and class information for each vehicle and with four attention heads.</p><p>‚Ä¢ Multi-head Attention Social Pooling (MHA-LSTM(+f)):</p><p>MHA-LSTM with additional input features (velocity, acceleration and class) and with three attention heads. E. Target vehicle trajectory prediction: Quantitative evaluation 1) Overall evaluation: Table <ref type="table" target="#tab_1">I</ref> shows the RMSE values for the models being compared on the NGSIM and highD datasets. Previous studies <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref> compare their results with independent prediction models to put emphasis on the importance of considering surrounding agents. In this work, we not only show that considering surrounding vehicles is a key factor to perform trajectory prediction but we also model their interactions in a more efficient way. To compare our model, we consider the results reported in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref> on the NGSIM dataset and we train S-LSTM and CS-LSTM on highD dataset as well. We train and test the approaches on the NGSIM and highD datasets separately and we notice that the RMSE values obtained on the NGSIM dataset are higher than the ones computed on the highD This may be due to the difference in size of the two datasets: highD contains about 12 times more vehicles than NGSIM. It can be also caused by annotation inaccuracies resulting in physically unrealistic vehicle behaviors in the NGSIM dataset, as observed by Coifman et al. <ref type="bibr" target="#b43">[42]</ref>. Anyway, examining the RMSE values for either NGSIM or highD datasets leads to the same order for the proposed methods. Our attention-based approaches (NLS-LSTM, MHA-LSTM and MHA-LSTM(+f)) perform better than the others. MHA-LSTM reduces the prediction error by about 10% compared to the CS-LSTM while having comparable execution time. With MHA-LSTM(+f), we investigate the use of additional features like the speed and acceleration. We notice that this leads to significant improvements in the motion prediction accuracy, as MHA-LSTM(+f) outperforms all the methods. This consolidates our assumption that the relation between is not only related to their positions but also to their dynamics. The class of transportation (truck or car) also characterizes the speed and pattern of the motion. Therefore, these results indicate that multi-head attention better models the interdependencies of vehicle motion than convolutional social pooling. Moreover, this suggests that considering the relative importance of surrounding vehicles using both positions and dynamics when encoding the context is better than focusing on local dependencies.</p><p>2) Effects of using multiple attention heads: In order to evaluate the influence of the number of attention heads on the prediction accuracy, let us examine the RMSE values obtained by MHA-LSTM on the highD dataset with 2, 3 4, 5 and 6 attention heads on Table <ref type="table" target="#tab_2">II</ref>. We notice that using several attention heads improves the prediction accuracy since each attention head represents a set of weights capturing one aspect of the effect of surrounding vehicles on the target vehicle. In addition, combining the attention vectors helps extract higher order relations. The best performance is reached with four We have conducted further experiments to evaluate the benefits of adding extra features, including explicit vehicle dynamics and class (MHA-LSTM(+f)). We observe that we outperform previous results when using different numbers of attention heads.</p><p>Considering the trade-off between the complexity of calculation and the MHA-LSTM(+f) RMSE corresponding to different numbers of attention heads, we choose to deploy three attention heads in the experiments that follow.</p><p>3) Comparison of attention methods: In Table <ref type="table" target="#tab_3">III</ref>, we show the performances of the three possible ways to compute the attention weights in MHA-LSTM(+f), named Œ±-attention, dot product attention, and concatenation attention presented in Section III-D. One can note that dot product and concatenation attentions outperform the Œ±-attention. Therefore, we conclude that both the dynamics of the surrounding vehicles and their relationships with the target vehicle are of great importance for trajectory prediction. 4) Error evaluation per lane change: In order to complete the evaluation of our approach, we use the trained model MHA-LSTM(+f) to estimate the lateral and longitudinal errors obtained while carrying out right (RLC), left (LLC) lane change maneuvers or lane Following (LF) in the test set of the highD dataset (cf. Table <ref type="table">3</ref>). One can notice that the observed lateral error is low even during lane changes maneuvers (5% of the test data). This demonstrates the effectiveness of our method in predicting lane changes. It may also be observed that the longitudinal and lateral errors are greater during the LLC maneuvers. This may be due to the fact that the vehicle often speeds up when it performs LLC, which is not the case for the other maneuvers (LF or RLC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis of Predictions</head><p>To understand which vehicles are taken into account by each attention head in our method, in Figure <ref type="figure" target="#fig_1">3</ref> we present the attention maps corresponding to lane change maneuvers carried out by the target vehicle. More precisely, a left lane change and a right lane change are shown and the attention maps are computed at times t obs = t lc -2s, t obs = t lc -1s and t obs = t lc where t lc is the time of crossing the lane mark during the lane change maneuver. Each attention map corresponds to an attention head. The target vehicle is shown in green in the center of the attention map, the grey rectangular region corresponds to the 2D drivable area described by the grid H and the colors of the other vehicles indicate the attention weight associated to them in the attention head (they are darker when their attention weight increases). We can remark that each attention head focuses on a subset of vehicles in the grid that are crucial to determining the future trajectory of the target vehicle. Moreover, like a human driver, most of the attention is directed to vehicles in front of the target vehicle, the vehicles behind it being less considered. We also notice that, in each example, one attention head considers all the vehicles in the grid equally (attention head 2 for the left lane change and attention head 1 for the right lane change). Moreover, at time t lc -2s, attention map 3 is such that the most important vehicles belong to the target lane even though other vehicles are closer to the target vehicle in another lane. This consolidates our assumption that the closest neighbors do not always have the strongest influence on the target vehicle. Some other factors like the speed and the vehicle's lane are also essential for correctly estimating the importance of a neighbor. To emphasise that aspect, we consider the relative speeds of the vehicles surrounding the target vehicle and belonging either to the same lane as the target vehicle or to the target lane in examples 1 and 2. Table <ref type="table" target="#tab_5">V</ref> summarizes the states of the considered interacting vehicles. ‚Ä¢ Preceding, following: a vehicle belonging to the same lane as the target vehicle and preceding or following it. ‚Ä¢ Lead, rear: a vehicle belonging to the target lane and positioned ahead or behind the target vehicle. ‚Ä¢ S, Sl, F: same speed, slower, faster than the target vehicle respectively. ‚Ä¢ -, +: decelerating, accelerating respectively. In example 1, the preceding vehicle is slower than the target vehicle. The latter has two possible maneuvers: either to continue in the same lane and decelerate, or to accelerate and make a left lane change. In the left lane, the lead vehicle is distant to the target vehicle and has comparable velocity. This makes the lane change maneuver more likely. In example 2, the preceding vehicle is accelerating and the following one is faster than the target vehicle. Therefore, the target vehicle has two options, either to accelerate or to make a right lane change. In these two examples, we notice that even 2 seconds before performing a lane change, the target vehicle focuses mainly on the vehicles that belong to the target lane and which may have an influence on its future speed. Indeed, in both cases, the target vehicle performs the lane change while accelerating or decelerating according to the situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Multi-Modal Trajectory Prediction</head><p>Using multi-Modal Trajectory Prediction, we model the uncertainties of the future and acknowledge the existence of multiple possible paths. Generating one solution trajectory tends to average all the possible trajectories which may lead to unrealistic predicted behaviors. To address this problem, we use each attention head to specialize for a distinct class of driver behavior. In the following experiment, we use a combination of each of the three attention head and the global attention to generate three different possible trajectories. Table <ref type="table" target="#tab_6">VI</ref> and Figure <ref type="figure" target="#fig_2">4</ref> show the RMSE in meters over a 5- second prediction horizon for the generated trajectories using three attention heads for the RMSE values obtained as follows:</p><p>‚Ä¢ Min and Max RMSE were computed by selecting at each instant the trajectory having respectively the minimum and maximum RMSE.   heads and having the maximum probability at t obs . We notice that one of the generated possible trajectories presents lower prediction error than the one solution trajectory by comparing the Min RMSE to the results in Table <ref type="table" target="#tab_6">VI</ref>. Moreover, choosing the trajectory that has the best probability of predicting the target trajectory gives better results than systematically selecting the trajectory computed by one attention head (either H 1 , H 2 or H 3 ). However, the network for trajectory selection does not always guide us to the trajectory with minimum loss, which justifies the difference between the min and probability based losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This work proposed an adapted attention-based method for modeling vehicle interactions during the tasks of vehicle trajectory prediction on highways. We extended our first method to acknowledge the future uncertainties and generate a multimodal solution presenting different possible future trajectories. The proposed method caused each attention head to specialize in extracting interaction features characterizing a distinct class of driver behavior without requiring explicit labels. Experiments showed that our approach MHA-LSTM(+f) significantly outperforms the state-of-the-art on two naturalistic large-scale driving datasets based on the RMSE metric. Furthermore, the presented visualisation of the attention maps enabled us to recognize the importance and the dependencies between vehicles. It confirmed that the attention is directed based on the future maneuver. This justified our choice to use each attention head to generate a possible future trajectory. Our evaluation results confirmed our intuitions: the importance of the relative dynamics and the efficiency of multi-head attention mechanism in modeling interactions between vehicles to predict vehicle trajectories in a highway scenario. Our proposed approach can be extended to consider heteroand mixed traffic scenarios with different road users, such as buses, trucks, cars, scooters, bicycles, or pedestrians. However, further information about the road structure should be integrated in our model for better representation of different driving scenes. Manager with the RITS Team, Inria, Paris-Rocquencourt, France. He played key roles in over 50 European and national French projects, some of which he has coordinated. He is the author of numerous publications and patents in the field of ITS and ADAS systems. In this field, he is known as an international expert. He is a member of the IEEE ITS Society and the Robotics and Automation Society. He is an Associate Editor of several IEEE international conferences, such as ICRA, IROS, IV, ITSC, and ICARCV.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Left lane change at t obs = t lc -2s (b) Left lane change at t obs = t lc -1s (c) Left lane change at t obs = t lc (d) Left lane change at t obs = t lc -2s: blue, red and yellow tracks represent respectively past, future and predicted trajectories. (e) Right lane change at t obs = t lc -2s (f) Right lane change at t obs = t lc -1s (g) Right lane change at t obs = t lc (h) Right lane change at t obs = t lc -2s: blue, red and yellow tracks represent respectively past, future and predicted trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Three heads attention maps for two different lane change maneuvers. For visualisation, The target vehicle is added in green in the center of all the maps. The driving direction is from left to right.</figDesc><graphic coords="9,48.96,503.47,255.68,140.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: RMSE in meters over a 5 second prediction horizon for the generated trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Kaouther</head><label></label><figDesc>Messaoud is currently a Ph.D. student at INRIA in Paris and member of Robotics and Intelligent Transportation Systems (RITS) team. She is also collaborating with LISA lab team at UCSD. She received her engineering degree at Tunisia Polytechnic School. Her research interests are modeling vehicles interactions and predicting drivers behaviors using deep learning based approaches. Itheri Yahiaoui is an Assistant Professor of Computer Science at Universit√© de Reims Champagne-Ardenne and a member of the CReSTIC Lab. She is also an Associate Researcher in the RITS research group of INIRA Paris. She received her PhD degree in Computer Science, in 2003, from the Ecole Nationale Sup√©rieure des T√©l√©communications (TELE-COM Paris). Her research interests include multimedia indexing, pattern recognition, image analyses and time series forcasting. Anne Verroust-Blondet is a senior research scientist in the RITS research group of Inria Paris. She obtained her "Th√®se de 3e cycle" and her "Th√®se d'Etat" in Computer Science (respectively in database theory and in computer graphics) from the University of Paris-Sud. Her current research interests include 2D and 3D object recognition and geometric modeling, environment perception, decision and planning in the context of intelligent transportation systems. Fawzi Nashashibi received the master's degree in automation, industrial engineering, and signal processing from the Laboratory for Analysis and Architecture of Systems/Centre Nationnal de la Recherche Scientifique (LAAS/CNRS), Toulouse, France, the Ph.D. degree in robotics from the LAAS/CNRS Laboratory, Toulouse University, and the HDR Diploma (accreditation to research supervision) from Pierre and Marie Curie University (Paris VI), Paris, France. Since 1994, he has been a Senior Researcher and the Program Manager with the Robotics Center, Mines ParisTech, Paris. Since 2010, he has been a Senior Researcher and a Program</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2020.2991952, IEEE Transactions on Intelligent Vehicles</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>Encoder</cell><cell cols="3">Multi-Head Attention Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Decoder</cell></row><row><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n h</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM LSTM Encoder LSTM Encoder Encoder</cell><cell>œÜ œÜ œÜ l Œ∏ Œ∏ Œ∏ l</cell><cell cols="2">K l . . . Q l</cell><cell>Softmax</cell><cell>. Attention map l head l . . . . . .</cell><cell>Concat.</cell><cell>FC</cell><cell>Concat. Concat. Concat.</cell><cell>LSTM LSTM . . . LSTM</cell><cell>Œò t obs +1 Œò t obs +2 Œò t obs +t f</cell></row><row><cell>LSTM Encoder</cell><cell></cell><cell>V l</cell><cell>. . . . . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LSTM Decoder</cell></row><row><cell></cell><cell>œÅ œÅ œÅ l</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM Encoder</cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell><cell>Leaky Relu</cell><cell>FC</cell><cell>Softmax</cell><cell>Head Proba</cell></row><row><cell>LSTM Encoder</cell><cell>Attention Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 1: Proposed Model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">‚Ä¢ Attention Module, which links the hidden states of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">encoder and decoder. It explicitly extracts the importance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">of the surrounding vehicles based on their spatio-temporal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">encoding in determining the future motion of the target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">vehicle using different operations. Then, it forms a vector</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">representing the context influence.</cell><cell></cell></row></table><note><p><p>‚Ä¢ Decoding layer, which receives the context vector containing the selected information about the neighboring vehicles and the target vehicle motion encoding and generates parameters of the distribution over the target vehicle's predicted future positions.</p>2379-8858 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>RMSE in meters over a 5-second prediction horizon for the models.</figDesc><table><row><cell>Dataset</cell><cell>Prediction Horizon (s)</cell><cell>M-LSTM</cell><cell>S-LSTM</cell><cell>CS-LSTM</cell><cell>CS-LSTM(M)</cell><cell>MATF GAN</cell><cell>NLS-LSTM</cell><cell>MHA-LSTM</cell><cell>MHA-LSTM(+f)</cell></row><row><cell>highD</cell><cell>1</cell><cell>-</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>-</cell><cell>0.20</cell><cell>0.19</cell><cell>0.06</cell></row><row><cell></cell><cell>2</cell><cell>-</cell><cell>0.62</cell><cell>0.61</cell><cell>0.65</cell><cell>-</cell><cell>0.57</cell><cell>0.55</cell><cell>0.09</cell></row><row><cell></cell><cell>3</cell><cell>-</cell><cell>1.27</cell><cell>1.24</cell><cell>1.29</cell><cell>-</cell><cell>1.14</cell><cell>1.10</cell><cell>0.24</cell></row><row><cell></cell><cell>4</cell><cell>-</cell><cell>2.15</cell><cell>2.10</cell><cell>2.18</cell><cell>-</cell><cell>1.90</cell><cell>1.84</cell><cell>0.59</cell></row><row><cell></cell><cell>5</cell><cell>-</cell><cell>3.41</cell><cell>3.27</cell><cell>3.37</cell><cell>-</cell><cell>2.91</cell><cell>2.78</cell><cell>1.18</cell></row><row><cell>NGSIM</cell><cell>1</cell><cell>0.58</cell><cell>0.65</cell><cell>0.61</cell><cell>0.62</cell><cell>0.66</cell><cell>0.56</cell><cell>0.56</cell><cell>0.41</cell></row><row><cell></cell><cell>2</cell><cell>1.26</cell><cell>1.31</cell><cell>1.27</cell><cell>1.29</cell><cell>1.34</cell><cell>1.22</cell><cell>1.22</cell><cell>1.01</cell></row><row><cell></cell><cell>3</cell><cell>2.12</cell><cell>2.16</cell><cell>2.09</cell><cell>2.13</cell><cell>2.08</cell><cell>2.02</cell><cell>2.01</cell><cell>1.74</cell></row><row><cell></cell><cell>4</cell><cell>3.24</cell><cell>3.25</cell><cell>3.10</cell><cell>3.20</cell><cell>2.97</cell><cell>3.03</cell><cell>3.00</cell><cell>2.67</cell></row><row><cell></cell><cell>5</cell><cell>4.66</cell><cell>4.55</cell><cell>4.37</cell><cell>4.52</cell><cell>4.13</cell><cell>4.30</cell><cell>4.25</cell><cell>3.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>RMSE in meters over a 5-second prediction horizon for different numbers of attention heads on the highD dataset.</figDesc><table><row><cell>Time(s)</cell><cell>Heads</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>1</cell><cell></cell><cell>0.21</cell><cell>0.20</cell><cell>0.19</cell><cell>0.20</cell><cell>0.21</cell></row><row><cell>2</cell><cell></cell><cell>0.61</cell><cell>0.61</cell><cell>0.55</cell><cell>0.57</cell><cell>0.59</cell></row><row><cell>3</cell><cell></cell><cell>1.20</cell><cell>1.19</cell><cell>1.10</cell><cell>1.13</cell><cell>1.16</cell></row><row><cell>4</cell><cell></cell><cell>1.96</cell><cell>1.99</cell><cell>1.84</cell><cell>1.87</cell><cell>1.92</cell></row><row><cell>5</cell><cell></cell><cell>2.95</cell><cell>3.01</cell><cell>2.78</cell><cell>2.83</cell><cell>2.93</cell></row><row><cell>attention heads.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>RMSE in meters over a 5-second prediction horizon for different attention operations on the highD dataset.</figDesc><table><row><cell>Time(s)</cell><cell>Methods</cell><cell>Œ±-attention</cell><cell>Dot product</cell><cell>Concatenation</cell></row><row><cell>1</cell><cell></cell><cell>0.06</cell><cell>0.06</cell><cell>0.07</cell></row><row><cell>2</cell><cell></cell><cell>0.10</cell><cell>0.09</cell><cell>0.11</cell></row><row><cell>3</cell><cell></cell><cell>0.26</cell><cell>0.24</cell><cell>0.25</cell></row><row><cell>4</cell><cell></cell><cell>0.62</cell><cell>0.59</cell><cell>0.61</cell></row><row><cell>5</cell><cell></cell><cell>1.25</cell><cell>1.18</cell><cell>1.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Longitudinal and lateral errors in meters over a 5-second prediction horizon for different maneuvers.</figDesc><table><row><cell>Maneuver</cell><cell cols="2">RLC</cell><cell>LLC</cell><cell></cell><cell>LF</cell><cell></cell></row><row><cell>Error</cell><cell>Long</cell><cell>Lat</cell><cell>Long</cell><cell>Lat</cell><cell>Long</cell><cell>Lat</cell></row><row><cell>1</cell><cell>0.07</cell><cell>0.03</cell><cell>0.20</cell><cell>0.03</cell><cell>0.05</cell><cell>0.01</cell></row><row><cell>2</cell><cell>0.12</cell><cell>0.06</cell><cell>0.32</cell><cell>0.07</cell><cell>0.07</cell><cell>0.02</cell></row><row><cell>3</cell><cell>0.34</cell><cell>0.18</cell><cell>0.42</cell><cell>0.19</cell><cell>0.22</cell><cell>0.06</cell></row><row><cell>4</cell><cell>0.79</cell><cell>0.43</cell><cell>0.88</cell><cell>0.45</cell><cell>0.54</cell><cell>0.14</cell></row><row><cell>5</cell><cell>1.43</cell><cell>0.76</cell><cell>1.74</cell><cell>0.78</cell><cell>1.10</cell><cell>0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Neighbor vehicles states.</figDesc><table><row><cell>Example 1</cell><cell>Vehicle</cell><cell>Preceding</cell><cell>Lead</cell><cell></cell><cell></cell></row><row><cell></cell><cell>State</cell><cell>Sl</cell><cell>S +</cell><cell></cell><cell></cell></row><row><cell>Example 2</cell><cell>Vehicle</cell><cell>Preceding</cell><cell>Following</cell><cell>Lead</cell><cell>Rear</cell></row><row><cell></cell><cell>State</cell><cell>S +</cell><cell>F</cell><cell>Sl +</cell><cell>F -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>RMSE in meters over a 5-second prediction horizon for the generated trajectories highD dataset.</figDesc><table><row><cell>Time(s)</cell><cell>Min</cell><cell>Max</cell><cell>H 1</cell><cell>H 2</cell><cell>H 3</cell><cell>Proba</cell></row><row><cell>1</cell><cell>0.07</cell><cell>0.13</cell><cell>0.08</cell><cell>0.11</cell><cell>0.10</cell><cell>0.08</cell></row><row><cell>2</cell><cell>0.12</cell><cell>0.21</cell><cell>0.14</cell><cell>0.20</cell><cell>0.13</cell><cell>0.14</cell></row><row><cell>3</cell><cell>0.19</cell><cell>0.53</cell><cell>0.36</cell><cell>0.45</cell><cell>0.30</cell><cell>0.31</cell></row><row><cell>4</cell><cell>0.36</cell><cell>1.18</cell><cell>0.81</cell><cell>0.91</cell><cell>0.73</cell><cell>0.67</cell></row><row><cell>5</cell><cell>0.69</cell><cell>2.13</cell><cell>1.51</cell><cell>1.58</cell><cell>1.38</cell><cell>1.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Auckland University of Technology. Downloaded on May 28,2020 at 17:06:24 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work presented in this paper has been financially supported by PIA French project CAMPUS (Connected Automated Mobility Platform for Urban Sustainability).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-local social pooling for vehicle trajectory prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messaoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02160409" />
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. Jun. 2019</date>
			<biblScope unit="page" from="975" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human motion trajectory prediction: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<idno>abs/1905.06113</idno>
		<ptr target="http://arxiv.org/abs/1905.06113" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for markovian interactive scene prediction in highway scenarios</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">2017. Jun. 2017</date>
			<biblScope unit="page" from="685" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizable intention prediction of human drivers at intersections</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">2017. Jun. 2017</date>
			<biblScope unit="page" from="1665" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An LSTM network for highway trajectory prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Altch√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Fortelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems, ITSC</title>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="353" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal trajectory prediction of vehicles with maneuver based LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">2018. Jun. 2018</date>
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional social pooling for vehicle trajectory prediction</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018-06">2018. Jun. 2018</date>
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks for vehicle trajectory prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messaoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02195180" />
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference, ITSC 2019</title>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="1813" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deterministic sampling-based switching kalman filtering for vehicle tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schrater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference, ITSC 2006</title>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
			<biblScope unit="page" from="1340" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Where will the oncoming vehicle be the next second?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">2008. Jun. 2008</date>
			<biblScope unit="page" from="1068" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">IMM-based lanechange prediction in highways with low-cost GPS/INS</title>
		<author>
			<persName><forename type="first">R</forename><surname>Toledo-Moreo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zamora-Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mobile agent trajectory prediction using bayesian nonparametric reachability trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aoude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Infotech at Aerospace Conference and Exhibit</title>
		<imprint>
			<date type="published" when="2011-03">2011. Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Firl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. Jun. 2014</date>
			<biblScope unit="page" from="918" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term vehicle motion prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kummert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">2009. Jun. 2009</date>
			<biblScope unit="page" from="652" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A lane change detection approach using feature ranking with maximized predictive power</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlechtriemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hillenbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuhnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. Jun. 2014</date>
			<biblScope unit="page" from="108" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction based on motion model and maneuver recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Houenou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnifait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2013-11">Nov. 2013</date>
			<biblScope unit="page" from="4363" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognition of situation classes at road intersections</title>
		<author>
			<persName><forename type="first">E</forename><surname>K√§fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>W√∂hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kummert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010-05">2010. May 2010</date>
			<biblScope unit="page" from="3960" to="3965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-speed highway scene prediction based on driver models learned from demonstrations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sierra Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dibangoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems, ITSC 2016</title>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="page" from="149" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interaction-aware driver maneuver inference in highways using realistic driver models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sierra Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Romero-Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dibangoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems, ITSC 2017</title>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infogail: Interpretable imitation learning from visual demonstrations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3812</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4565</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep imitative models for flexible inference, planning, and control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1810.06544</idno>
		<ptr target="http://arxiv.org/abs/1810.06544" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting vehicle behaviors over an extended horizon using behavior interaction network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019-05">2019. May 2019</date>
			<biblScope unit="page" from="8634" to="8640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intentionaware long horizon trajectory prediction of surrounding vehicles using dual LSTM networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems, ITSC 2018</title>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="1441" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<idno>abs/1809.10732</idno>
		<ptr target="http://arxiv.org/abs/1809.10732" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scene compliant trajectory forecast with agent-centric spatio-temporal grids</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ridel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.07507" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1909">1909.07507, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06">2019. Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06">2018. Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06">2019. Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long short term memory for driver intent prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nebot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">2017. Jun. 2017</date>
			<biblScope unit="page" from="1484" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence prediction of vehicle trajectory via LSTM encoder-decoder architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">2018. Jun. 2018</date>
			<biblScope unit="page" from="1672" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06">2016. Jun. 2016</date>
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interactive trajectory prediction of surrounding road users for autonomous driving using structural-LSTM network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Us highway 101 dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Colyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halkias</surname></persName>
		</author>
		<idno>FHWA-HRT07-030</idno>
	</analytic>
	<monogr>
		<title level="m">Highway Administration (FHWA)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Us highway i-80 dataset</title>
		<idno>FHWA-HRT-06-137</idno>
	</analytic>
	<monogr>
		<title level="m">Federal Highway Administration (FHWA)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The highD dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kloeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems, ITSC 2018</title>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="2118" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015-05">2015. May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Autodiff Workshop: The Future of Gradientbased Machine Learning Software and Techniques</title>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A critical evaluation of the next generation simulation (NGSIM) vehicle trajectory dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="362" to="377" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
