<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biosignal-Based Spoken Communication: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
							<email>tanja.schultz@uni-bremen.de</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Krusienski</surname></persName>
							<email>deankrusienski@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Com-puter Science and Mathematics</orgName>
								<orgName type="laboratory">Cognitive Systems Lab</orgName>
								<orgName type="institution">University of Bremen</orgName>
								<address>
									<postCode>28359</postCode>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Swiss AI Lab</orgName>
								<address>
									<addrLine>Istituto Dalle Molle di studi sull&apos;intelligenza artificiale</addrLine>
									<postCode>6928</postCode>
									<region>Manno</region>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">GIPSA-Lab</orgName>
								<orgName type="institution">CNRS/Grenoble Alpes University</orgName>
								<address>
									<postCode>38402</postCode>
									<settlement>Greno-ble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Biomedical Engineering Institute</orgName>
								<orgName type="laboratory">ASPEN Lab</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Speech and Applied Neuroscience Lab</orgName>
								<orgName type="department" key="dep2">Speech-Language-Hearing Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Biosignal-Based Spoken Communication: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48EA40965954E155042850DC83B65D02</idno>
					<idno type="DOI">10.1109/TASLP.2017.2752365</idno>
					<note type="submission">received January 29, 2017; revised July 17, 2017; accepted August 11, 2017. Date of current version November 27, 2017. The work of T.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biosignals</term>
					<term>spoken communication</term>
					<term>multimodal technologies</term>
					<term>speech recognition and synthesis</term>
					<term>speech rehabilitation</term>
					<term>electromyography</term>
					<term>ultrasound</term>
					<term>functional near-infrared spectroscopy</term>
					<term>electroencephalography</term>
					<term>electrocorticography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals-stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself-can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research on biosignal-based speech processing is a wide and very active field at the intersection of various disciplines, ranging from engineering, computer science, electronics and machine learning to medicine, neuroscience, physiology, and psychology. Consequently, a variety of methods and approaches have been used to investigate the common goal of creating biosignal-based speech processing devices for communication applications in everyday situations and for speech rehabilitation, as well as gaining a deeper understanding of spoken communication. This paper gives an overview of the various modalities, research approaches, and objectives for biosignal-based spoken communication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>processing, including the central and peripheral nervous systems, muscular action potentials, speech kinematics (tongue, lips, jaw, etc.), and sound pressure. Together, these physiological measurements are known as speech-related "biosignals" and have been used for decades to better understand the underlying mechanisms of human speech production. Modeling the mapping between physiological parameters and acoustic consequences of speech still remains a very active research field. Propelled by technological advances, an increasing number of studies have investigated speech-related biosignals in applied research focused on developing spoken communication (SC) systems. This field is referred to as "Biosignal-based Spoken Communication," and encompasses two primary tracks for converting: <ref type="bibr" target="#b0">(1)</ref> biosignals into text (biosignal-based speech recognition), and (2) biosignals into a synthetic voice (biosignal-based speech synthesis). Examples of these two technical tracks include Brain-Computer Interfaces (BCI) for restoring communication by directly decoding cortical brain activity <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> into speech representations, and Silent-Speech Interfaces (SSI) <ref type="bibr" target="#b3">[4]</ref>, which offer a way to communicate privately without disturbing bystanders and / or provide voice communication for people with severe speech impairments (e.g., laryngectomy patients). Furthermore, several studies have recently investigated biosignals as a means to provide valuable articulatory biofeedback to speakers about their own voice production for increasing articulatory awareness in speech therapy or language learning (e.g., <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>).</p><p>The field of Biosignal-based Spoken Communication has rapidly advanced in recent years and the IEEE Special Issue on this subject is intended as a snapshot and comprehensive review of the current state-of-the-art. This survey paper provides an overview and definition of the methods, sensor technologies, signal processing algorithms, and applications used across the field. We provide specific focus on the processing, analysis, classification, recognition, and interpretation of a large variety of biosignals representing speech and language, including a discussion on advanced machine learning approaches, as well as theory and applications related to spoken language processing. With its broad scope, this survey intends to bridge the gap between the disciplines, provide a linking structure within the special issue, and to generally provide an entry point for readers interested in this very active field of research and development.</p><p>The remainder of this survey paper is organized in five sections. Following this introduction, Section II provides a definition of "biosignals", as well as the different modes of speaking. Section III describes methods used to acquire speechrelated biosignals, ranging from respiratory, laryngeal, and articulatory kinematics, to muscular and neurological activity. Section IV summarizes processing methods needed to analyze each speech-related biosignal and includes descriptions of relevant features, dimensionality reduction and compression methods. This section also discusses the usage of biosignal-based automatic speech recognition and speech synthesis. The paper ends with a discussion of the wide variety of use cases and existing applications in Section V, and a view toward the future of Biosignal-based Spoken Communication in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERAL DEFINITIONS AND USES OF BIOSIGNALS</head><p>In this section we provide a definition for biosignals along with a description of the most important biosignals in speech. We also define a variety of speaking modes referred to throughout the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Biosignals</head><p>We define Biosignals as autonomous signals produced by human activities measured in physical quantities using different sensor technologies. Autonomous signals result from chemical, physical, and biological processes of the human organism and serve the functions of control, regulation, and information transmission throughout the body. Sensor technologies can be used to measure each signal, in terms of kinetic (force, torque, movement), kinematic (position, velocity, acceleration), optical (radiance, luminance), chemical (concentration, pH, olfactory), electrical (potential, current, resistance), acoustic (sound pressure and intensity, impedance), and thermal (temperature) quantities, resulting in the corresponding categories of biosignals.</p><p>Biosignals have been used in medical diagnostics <ref type="bibr" target="#b7">[8]</ref> for decades. More recently, rapid advances in sensor technologies in terms of accuracy, resolution, miniaturization, integration, connectivity, mobility, usability, costs, availability, and many other features, have propelled the application of biosignals to other contexts, including information technologies. In particular, the human-computer interaction (HCI) community has embraced biosignals to extend the number of modalities available for developing robust and intuitive devices. Information obtained from the biosignals is used to interpret not only physical states, but also affective and cognitive states, and activities of a user. Thereby, biosignals provide an inside perspective on human mental processes, intentions, and needs that complement traditional means of observing human interaction from the outside, and thus enable personalized and adaptive services <ref type="bibr" target="#b8">[9]</ref>.</p><p>In speech and language, biosignals are used for basic and translational research and development, including: voice-driven HCI, human-human interaction and communication, speech therapy, and language learning. At a basic level, biosignals can provide a comprehensive description of speech processing by reflecting all speech-related activities of the human body as depicted Fig. <ref type="figure" target="#fig_0">1</ref> (step #1), found in the brain, the peripheral nervous system, the muscles, the speech anatomy of articulation (jaw, lips, tongue, and other orofacial structures), phonation (vocal folds), and respiration. The biosignals of speech then result from being captured through a wide variety of sensors and capturing techniques (step #2 in Fig. <ref type="figure" target="#fig_0">1</ref>, see Section III).</p><p>The remainder of this article focuses on biosignals beyond traditional acoustic waveforms captured by techniques such as electromyography (EMG), electroencephalography (EEG), electrocorticography (ECoG), intracranial microelectrodes, functional near-infrared spectroscopy (fNIRS), ultrasound (US), and permanent magnet articulography (PMA). While acoustic biosignals can only be captured during vocalizations that displace particles of the surrounding medium (usually air) by a vibrating object (usually the vocal folds), kinematic, kinetic, electrical, and optical biosignals do not rely on such air particle displacement and thus extend to many speaking modes beyond the audible one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modal speech</head><p>The vocal folds vibrate for voiced sounds or do not vibrate for unvoiced sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Whispered speech</head><p>Turbulent flow through a constant aperture formed between the vocal folds results in only unvoiced sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II SPEAKING MODE ACCORDING TO LEVEL OF EFFORT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal modal speech</head><p>Modal speech at normal intensity: the "standard" mode of speaking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shouted speech</head><p>Modal speech characterized by higher intensity, higher pitch, and more open articulation than speech at normal intensity. It shares common properties with Lombard speech produced in noisy environments <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Murmured speech</head><p>Characterized by very low-intensity (voiced/unvoiced) sounds that are barely perceptible to bystanders. Residual acoustic activity can, however, be recorded using a specific microphone (see Section III-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speaking Modes</head><p>Speech results from modulation of the expiratory air flow from the lungs through the glottis, which is filtered by the vocal tract <ref type="bibr" target="#b9">[10]</ref>. The acoustic transfer function of the vocal tract depends on the geometry of both oral and nasal cavities, which are configured by positions of the tongue, lips, jaw, and velum. For the purpose of Biosignal-based Spoken Communication, we distinguish different speaking modes based on glottal activity and intensity in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_0">II</ref>.</p><p>Each speaking mode in Tables I and II produces sound pressure waves that can be captured by traditional acousticbased sensors resulting in acoustic biosignals. This survey focuses specifically on biosignal acquisition from speech produced without making any sound. The current literature refers to "speech-without-sound" rather inconsistently, and sometimes equivalently as, imagined, silent, covert, or inner speech, despite differences in their behavioral components. In the context of spoken communication studies, the confusion and inconsistency of terminology might be a result of different instructions given to subjects -or the lack of instructions. In Table <ref type="table" target="#tab_0">III</ref> we propose the classification of speech-without-sounds into three levels: silent, imagined, and inner speech.</p><p>Each speaking mode in Table <ref type="table" target="#tab_0">III</ref> has distinct challenges and opportunities for signal acquisition and application to spoken communication. Some opportunities include: (1) robustness to adverse environments, e.g., measuring articulation is less prone to acoustic noise than airborne signals; (2) less disturbing or more secure, e.g., whispered or silent speech is favored over normal modal speech in quiet environments, and silent or imagined speech allows one to communicate confidentially; and (3) rehabilitation / restoration applications for individuals with voice problems or speech disabilities, e.g., silent speech interfaces as a voice prosthesis for individuals with laryngectomy, and possibly as a neural prosthesis for speech using imagined speech (i.e., speech brain-computer interfaces) for individuals with paralysis and mutism due to neurological disease or trauma (e.g., locked-in syndrome).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III CLASSIFICATION OF SPEAKING MODES WITHOUT ACOUSTIC OUTPUT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Silent speech</head><p>Speakers are instructed to move their articulators as if producing normal modal speech, but to suppress their pulmonary airstream so that no sound is emitted. Silent speech production can be measured by monitoring articulatory movements using motion-capture devices, imaging techniques, or by measuring the activity of muscles (see Section III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imagined speech</head><p>Similar to silent speech, except movements of the articulators are also suppressed. Imagined speech in this context is identical to first-person motor imagery of speaking in which the speakers should feel as though they are producing speech rather than simply talking to themselves. Since imagined speech is produced without any articulatory movements, this speaking mode requires observations at the neural level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner speech</head><p>Though there is a range of descriptions for inner speech (e.g., selftalk, verbal thinking, inner voice, inner dialogue) <ref type="bibr" target="#b11">[12]</ref>, we adopt Vygotsky's model <ref type="bibr" target="#b12">[13]</ref> that defines inner speech as an internalized process in which one thinks in pure meanings. In contrast to imagined and silent speech, no phonological properties and turntaking qualities of an external dialogue are retained. Thus, inner speech is even more difficult to investigate, even at the neural level of observation.</p><p>In addition to challenges of recording and processing biosignals for speech without sound (Section IV-D), a major challenge is precisely due to the lack of auditory feedback and, for imagined and inner speech, a complete lack of behavioral landmarks. Some specific challenges for silent, imagined, and inner speech include: (1) difficulty distinguishing speech from non-speech activity, (2) a lack of temporal information about the speech content, and (3) difficulty for study participants to utter silent speech <ref type="bibr" target="#b13">[14]</ref> due to the absence of auditory feedback. Another confound for silent speech is that articulation may change depending on the communication situation: e.g., a silent speaker communicating in a public place may hypo-articulate to prevent lip-reading and maintain privacy. Careful instruction of study participants is therefore necessary to obtain consistent signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CAPTURING SPEECH-RELATED BIOSIGNALS</head><p>This section describes the production of speech as a result of (1) respiratory, laryngeal, and articulatory activity, (2) intraoral residual acoustic activity, (3) muscle activity, and (4) brain activity, and gives an overview of methods and techniques for their acquisition, (see step #2 in Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Respiratory, Laryngeal, and Articulatory Activity</head><p>Breathing is central to speech production by providing the airflow required to generate sounds. Breathing kinematics can be recorded by means of a face/nose mask or by chest and abdominal plethysmography; their properties during speech production have been extensively studied for more than 40 years (e.g., <ref type="bibr" target="#b15">[15]</ref>). More recently, Rochet-Capellan et al. <ref type="bibr" target="#b16">[16]</ref> revealed that breathing may contribute to timing and coordination between dialogue partners in face-to-face spoken communication.</p><p>Laryngeal activity refers to vocal fold oscillations (in modal and murmured speech), and can be estimated either indirectly from speech acoustics using inverse filtering, or directly by an electroglottography (EGG) <ref type="bibr" target="#b17">[17]</ref>. With EGG, the degree and rate of vocal fold contact is related to changes in electrical resistance between two electrodes placed around the neck. This technique is very sensitive to the exact positioning of the electrodes relative to the location of the vocal folds.</p><p>Articulatory activity refers to the movements of the speech articulators, and can be measured using a number of different techniques. Here, we distinguish techniques based on sensors attached along the vocal tract from imaging techniques.</p><p>Magnetic Articulography (EMA/PMA): Two techniques Electromagnetic Articulography (EMA) <ref type="bibr" target="#b18">[18]</ref> and Permanent-Magnetic Articulography (PMA) <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> are available to measure articulator configurations during speech production using magnetic field sensing. The location where magnetic field generation and sensing take place differentiates each approach.</p><p>To record EMA, participants are seated with their head inside an alternating magnetic field, generated by transmitter coils. This field induces an electrical current in receiver coils glued to the main articulators (tongue, lips, velum). Multiple transmitter and receiver coils are used to recover real-time articulatory movements in a 2D or 3D Cartesian space. EMA records articulatory data with very high spatial and temporal resolution (&lt;1 mm, ∼500 Hz), and is used to model articulatory dynamics during speech production. These data have been explored in different areas of speech technology, such as automatic speech recognition <ref type="bibr" target="#b21">[21]</ref>, low bit-rate speech coding <ref type="bibr" target="#b22">[22]</ref>, and speech synthesis <ref type="bibr" target="#b23">[23]</ref>. EMA is an invasive procedure and requires wires to be run inside the mouth, which can cause discomfort, and is not portable. As a result, EMA is typically used in laboratory settings.</p><p>In PMA, the positions of the sensors are reversed: permanent magnet transmitters are attached to the articulators, and the sum magnetic field is measured by sensors outside the mouth. The resulting field is a superposition of all the transmitter fields, and requires sophisticated analyses to decode the spatial position of articulators. However, since PMA requires only permanent magnets to be fixed inside the mouth without any connecting wires, it is more comfortable than EMA <ref type="bibr" target="#b19">[19]</ref>.</p><p>Palatography: This technique uses sensors embedded inside a pseudo-palate that are placed inside the mouth. In Electropalatography (EPG), contact sensors are used to record the timing and location of palatal contacts during speech. A modification by Birkholz et al. added optical distance sensors to the pseudo-palate (Optopalatography) <ref type="bibr" target="#b24">[24]</ref> to record tongue positions for phonemes that do not involve palatal contacts (e.g., vowels), and lip movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imaging techniques (IMG):</head><p>Video imaging is a straightforward way to capture the movements of the visible speech articulators (i.e., lips and jaw) during speech production. Several sizeable (audio-)visual data corpora are available, such as GRID <ref type="bibr" target="#b25">[25]</ref> and the "Lip Reading in the Wild" corpus <ref type="bibr" target="#b26">[26]</ref>.</p><p>Medical imaging techniques can be used to capture the movements of the intraoral articulators. Magnetic Resonance Imaging (MRI) is widely used in phonetic research <ref type="bibr" target="#b27">[27]</ref>, and obtains high-contrast images of the vocal tract showing all articulators and internal structures. Moreover, recent advances in real-time dynamic MRI (RT-MRI) can be used to record sequences of vocal tract images at 100 fps with acceptable spatial resolution <ref type="bibr" target="#b28">[28]</ref>. However, MRI requires a bulky and expensive equipment which prevents its use as a portable communication device.</p><p>Ultrasound imaging of the vocal tract is a clinically safe technique that records images of tongue movements during speech in the mid-sagittal or coronal planes with good spatial and temporal resolution (∼1 mm, ∼80 Hz), see <ref type="bibr" target="#b29">[29]</ref> for a complete review. Data is recorded by placing an ultrasonic transducer beneath the chin (held manually or using a head strap) to emit ultrasonic waves and detect reflections from the upper surface of the tongue. Ultrasound images have relatively low quality due in part to the presence of speckle noise and to a loss of signal from tongue structures with poor alignment to the ultrasound beam (i.e., non-orthogonal). However, light-weight ultrasound scanners are now available making this technology suitable for practical communication systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intraoral Acoustic Activity</head><p>The acoustic output of very soft vocal productions such as murmured speech is too small to be recorded using a conventional microphone, though it can be captured using a stethoscopic (i.e., tissue-conducted), non-audible murmur (NAM) microphone <ref type="bibr" target="#b30">[30]</ref>. The device is placed just below the ear, and is capable of detecting very low-amplitude sounds generated inside the vocal tract by a soft laryngeal airflow. The main application for NAM microphones is the design of silent speech interfaces. Intraoral acoustic activity can also be exploited for spoken communication (in normal speech) in very noisy / adverse environments (e.g., a helicopter cockpit). Some examples include throat microphones that detect the acoustic variations propagating through the neck tissues <ref type="bibr" target="#b31">[31]</ref>, and bone-conducted microphones that detect intraoral activity via a sensor placed on the skull <ref type="bibr" target="#b32">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Muscle Activity</head><p>Muscular activity can be observed using electromyography (EMG) to capture electrical signals generated during muscle fiber contraction <ref type="bibr" target="#b33">[33]</ref>. EMG can be recorded in two ways: invasively via needle electrodes inserted into muscle tissue or non-invasively using surface electrodes. Surface electrodes are most common in the context of speech processing systems since using needle electrodes requires medical expertise and hygiene precautions, and they are susceptible to dislocation when applied to moving tissue <ref type="bibr" target="#b33">[33]</ref>.</p><p>Surface Electromyography (EMG): Speech-related surface EMG is acquired using electrodes attached to the face positioned either over specific muscles <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> or arranged in a grid <ref type="bibr" target="#b37">[36]</ref>. Signals are acquired as a potential difference between two electrodes, measured either in a monopolar (reference-versus-active) or bipolar (active-versus-active) configuration. The recorded voltage potentials are separated from their generators (i.e., motor units) by layers of tissue with varying conductivity; therefore, they represent a superposition of many activity sources, possibly even several muscles. The EMG signal is further attenuated by skin tissue and the skin-electrode interface, which both act as a low-pass filter <ref type="bibr" target="#b38">[37]</ref>. However, EMG is advantageous for speech synthesis and recognition because the signal appears approximately 60 ms before actual articulatory movements <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Brain Activity</head><p>Brain activity can be measured based on its hemodynamic (fMRI, fNIRS) or electrophysiological (EEG, MEG, ECoG, microelectrodes) dynamics.</p><p>Functional Magnetic Resonance Imaging (fMRI): Neural activity can be acquired using fMRI by observing the changing concentrations of oxygenated and deoxygenated hemoglobin, which are related to the increased demand for oxygen as neurons are active and engaged. Oxygenated and deoxygenated hemoglobin have different magnetic properties that can be detected by the strong magnetic fields produced in the MRI environment. Due to its high spatial resolution over the entire brain, fMRI is the de-facto standard in neuroimaging and has been instrumental in a variety of studies investigating speech and language, for reference, see <ref type="bibr" target="#b41">[40]</ref> for a review. The slow nature of the hemodynamic response, noisy environment, and the large chamber required for fMRI significantly limits the utility for practical communication interfaces.</p><p>Functional Near Infrared Spectroscopy (fNIRS): fNIRS is a brain imaging technique pioneered by Jobsis <ref type="bibr" target="#b42">[41]</ref> that also detects changes in the amount of hemoglobin present in the brain as an indirect marker of neural activity. Light in the near infrared spectrum is absorbed by hemoglobin, but not by biological tissue (e.g., bones, skin, muscle). Therefore, the amount of hemoglobin present can be estimated by placing near infrared light emitters and detectors around the head and calculating the amount of light absorbed. Similar to fMRI paradigms, neural activity increases the demand for energy, which is supplied by fresh oxygenated blood that carries hemoglobin to the site of neural processing. fNIRS is well suited to investigate speech processes in non-clinical populations as it is less affected by motion artifacts that plague EEG <ref type="bibr" target="#b43">[42]</ref> and can quickly be set up in non-laboratory environments. fNIRS emitters can also be easily realized using LEDs <ref type="bibr" target="#b44">[43]</ref>, which enable low-cost fNIRS devices <ref type="bibr" target="#b45">[44]</ref>. Additionally, the light emitters and detectors do not require additional skin preparation steps common to EEG (e.g., skin abrasion and application of conductive gel), which simplifies acquisition.</p><p>Electroencephalography (EEG): EEG is the measurement of the electrical activity of the brain using electrodes placed on the surface of the scalp. EEG signals observed at individual electrode sites are the result of the simultaneous activation of millions of neurons whose summed voltage is conducted through the brain volume, skull, and scalp layers <ref type="bibr" target="#b46">[45]</ref>. The large number of neurons contributing to the EEG signal, combined with the low-pass filter properties of the skull and scalp, result in a spatial resolution on the order of centimeters and spectral bandwidth on the order of 80 Hz. As a non-invasive measure of electrophysiological activity, EEG has desirable temporal properties to adequately characterize the neural processing of speech production. Unfortunately, EEG is highly susceptible to myoelectrical, motion, and environmental artifacts, which interfere with EEG recordings made during overt speech production (e.g., modal, whispered, and silent speech) <ref type="bibr" target="#b47">[46]</ref>. Though some methods have been developed to cancel this interference (e.g., <ref type="bibr" target="#b48">[47]</ref>), validation is still needed to ensure only artifacts are removed from the EEG signal. An alternative is to record EEG during imagined speech (see Table <ref type="table" target="#tab_0">III</ref>), or to restrict analysis to the speech motor planning and preparation phases (e.g., <ref type="bibr" target="#b49">[48]</ref>). See <ref type="bibr" target="#b50">[49]</ref> for a comprehensive review of the EEG components involved in speech and language processing.</p><p>EEG is typically recorded and processed using time-locked averages (i.e., event-related potentials, ERPs) to overcome its comparatively low signal-to-noise ratio <ref type="bibr" target="#b51">[50]</ref>. However, EEG can also be analyzed as single-trial ERPs and for changes in spectral content over time (e.g., event-related (de)synchronization) <ref type="bibr" target="#b52">[51]</ref>. Despite the disadvantages for studying speech, EEG remains the most common technique used in BCIs for communication <ref type="bibr" target="#b0">[1]</ref>.</p><p>Microwire Electrodes &amp; Microarrays: Intracranial wire microelectrodes and microarrays, such as the Utah array <ref type="bibr" target="#b53">[52]</ref>, provide unparalleled spatial and temporal resolution down to single neuron action potentials. The electrodes are typically 1-2 mm long and have recording surfaces that range from 20-80 microns <ref type="bibr" target="#b54">[53]</ref>. They record extracellular potentials of only those neurons nearest to the recording tip, and as an array they can record small brain areas of a few square millimeters simultaneously. Extracellular recordings contain both neural spiking data (action potentials, 300-6000 Hz) and the local field potential (&lt;300 Hz), which represents the neural activity from a larger area around the electrode tip <ref type="bibr" target="#b55">[54]</ref>.</p><p>The invasive procedure to implant microarrays or microwire electrodes into the cortex is only rarely performed with humans and few studies exist investigating speech processes using this technique. In these few examples, implants in cortical areas for speech-motor control have been used to analyze and decode imagined phone production <ref type="bibr" target="#b56">[55]</ref>, <ref type="bibr" target="#b57">[56]</ref>, and to control a vowel speech synthesizer <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b59">[58]</ref>.</p><p>Electrocorticography (ECoG): ECoG is an invasive technique for measuring the electrical activity of the brain from sites directly on the cortical surface. The opportunity to measure ECoG in humans is most common in patients with severe cases of epilepsy, who require temporary implantation of electrode grids for pre-surgical planning, or intra-operative monitoring <ref type="bibr" target="#b60">[59]</ref>. The implanted sub-dural electrode grids can remain on the brain surface for a period of several days to two weeks, during which patients consent to participate in scientific experiments.</p><p>Clinical ECoG recordings typically have an electrode spacing on the order of 10 mm, while micro-ECoG recordings can have spacing on the order of 1 mm. In contrast to scalp EEG, ECoG signals measured on the brain surface do not suffer from spatial blurring from dura matter, skull, and scalp <ref type="bibr" target="#b61">[60]</ref>, record electrical activity from neural tissue directly underneath each electrode, and are less susceptible to muscle and environmental artifacts. ECoG recordings have a spectral bandwidth in excess of 200 Hz, and special emphasis has been placed on the high-gamma band (&gt;70 Hz), which is not readily observable in scalp EEG <ref type="bibr" target="#b62">[61]</ref>. The high-gamma range is very spatially localized and highly correlated with cognitive functions and behavioral output, including speech processes <ref type="bibr" target="#b63">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROCESSING BIOSIGNALS FOR SPOKEN COMMUNICATION</head><p>While the analysis of the described speech-related biosignals can be used to gain a better understanding of speech processes in general, the development of biosignal-based applications for spoken communication requires further processing (see step #3 of Fig. <ref type="figure" target="#fig_0">1</ref>). Biosignals are first processed to extract suitable features and to handle artifacts, followed by classification or regression methods to generate the output for the targeted application. The classification approach typically consists of using automatic speech recognition for the transformation of spoken commands or continuous speech into text (e.g., phones, words, phrases or complete sentences), which then can be displayed on a screen or synthesized using text-to-speech synthesis. The regression method typically involves using speech synthesis for the direct mapping of biosignal-captured spoken input to audible speech output. While the boundaries between these three steps are sometimes blurred in practice, for simplicity, we describe them separately. Thus, this section starts with a summary of feature extraction methods for each biosignal, followed by short overviews of speech recognition and synthesis approaches applied to biosignal-based spoken communication with emphasis on the peculiarities of non-acoustic speech-related biosignals. Finally, we summarize the current status of these systems and discuss open challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extracting Relevant Features From Biosignals</head><p>Following the acquisition of speech-related biosignals (Section III), relevant features are extracted according to modespecific standards in physiological signal processing.</p><p>Acoustic signals, limited here to body conduction microphones (including NAM), are typically processed similarly to standard speech signals from normal (modal) speech. For example, Mel-Frequency Cepstral Coefficients (MFCC) plus context features can be estimated from NAM recordings <ref type="bibr" target="#b30">[30]</ref>.</p><p>Visual articulatory data (e.g., video images of the lips, ultrasound images of the tongue, etc.) are usually acquired as high-resolution 2D or 3D data. We briefly review three main approaches that have been investigated in the context of audiovisual and visual-only speech recognition, silent speech interfaces, and articulatory biofeedback. See <ref type="bibr" target="#b64">[63]</ref> for a more detailed review.</p><p>In one approach, automatic segmentation of the articulators in each video image (i.e., the extraction of their contours) has been used to track lip movements using the active shape model (ASM) <ref type="bibr" target="#b65">[64]</ref>, active appearance model (AAM) <ref type="bibr" target="#b66">[65]</ref>, and more recently constrained local neural fields (CLNF) <ref type="bibr" target="#b67">[66]</ref>. For ultrasound images, the robust and fully automatic tracking of the tongue is still an unsolved issue and has been investigated using ASM <ref type="bibr" target="#b68">[67]</ref>, AAM <ref type="bibr" target="#b69">[68]</ref>, and neural networks (shallow <ref type="bibr" target="#b70">[69]</ref>, deep <ref type="bibr" target="#b71">[70]</ref>). A second approach uses dimensionality reduction techniques in which an entire region-of-interest is processed without focusing on a particular object (e.g., lip or tongue contours). Some examples of this approach include the discrete cosine transform to process lip images <ref type="bibr" target="#b72">[71]</ref> and principal components analysis for lip <ref type="bibr" target="#b73">[72]</ref>, and tongue images <ref type="bibr" target="#b74">[73]</ref>. A third approach has recently emerged using the deep learning paradigm in which both discriminative feature extraction and classification can be jointly achieved. One powerful deep architecture is the so-called Convolutional Neural Network (CNN) <ref type="bibr" target="#b75">[74]</ref>, which has been used in a few recent studies for encoding lips <ref type="bibr" target="#b76">[75]</ref>, and for extracting high-level articulatory abstractions from the joint observation of lips and tongue images <ref type="bibr" target="#b77">[76]</ref>.</p><p>Magnetic-articulography techniques (i.e., EMA or PMA) commonly provide a low-dimensional data vector representing the positions of the speech articulators, and requires only minimal pre-processing. EMA systems directly measure the 2D or 3D coordinates of the receiver coils attached to the articulators, and are usable in raw form by a classifier or a regression model. PMA data are less explicit and may require more preprocessing such as low-pass filtering, background cancellation, and normalization for proper identification of articulatory positions and movements <ref type="bibr" target="#b19">[19]</ref>.</p><p>Palatography (i.e., EPG and OPG) EPG provides an exact 2D plus time representation of tongue-palate contact patterns and does not require any post-processing. Additional procedures are required for OPG in order to calibrate the distance sensors (the user must touch each sensor once with the tongue while the pseudopalate is in the mouth) and to compensate for measurement errors made when the tongue is not oriented perpendicular to the axes of the optical sensors <ref type="bibr" target="#b78">[77]</ref>. Once completed, no further data post-processing is required.</p><p>Electromyography provides a timecourse of muscular activation for each recording electrode. Initial approaches used simple thresholding techniques <ref type="bibr" target="#b79">[78]</ref> and comparisons of wholeword EMG averages between channels <ref type="bibr" target="#b80">[79]</ref> to identify muscles active during speech. Modern approaches now use time-domain features <ref type="bibr" target="#b40">[39]</ref> similar to the Hudgins feature set <ref type="bibr" target="#b81">[80]</ref> and spectral features <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b82">[81]</ref>- <ref type="bibr" target="#b84">[83]</ref>.</p><p>Hemodynamic responses measured by fMRI and fNIRS depend on metabolic processes and are relatively slow as a result. Simple features such as the linear data trend well describe neural activity in fNIRS <ref type="bibr" target="#b85">[84]</ref>, and newer approaches subsample the hemodynamic response and employ classification methods to determine information bearing spatio-temporal filters <ref type="bibr" target="#b86">[85]</ref>.</p><p>Electrical brain signals measured by EEG and ECoG use similar techniques for analyzing their respective signals to describe the neural processes underlying speech production, and focus on spatial, temporal, and spectral properties. With EEG, it is common to apply a bandpass filter from 1-30 Hz since signals &gt;30 Hz are often unreliable due to low SNR. After filtering, ERPs can be aligned to the onset of speech production and averaged to focus on either the time periods preceding or following production onset. The times preceding speech have been well studied and have revealed two major slow-wave potentials that systematically vary with speech production: (1) the bereitschaftspotential (BP), a negativity that occurs in the 1-2 s prior to self-paced speech production <ref type="bibr" target="#b49">[48]</ref>, and (2) the contingent negative variation, a negativity that occurs prior to cued speech production <ref type="bibr" target="#b87">[86]</ref>. Analysis of the intervals during speech production is difficult due to EMG contamination (cf. <ref type="bibr" target="#b47">[46]</ref>); Alternatively, EEG can be used to interpret neural processes involved in cued imagined speech using both the broadband (1-30 Hz) ERP <ref type="bibr" target="#b88">[87]</ref> and narrowband (alpha, beta, and theta) power modulations <ref type="bibr" target="#b89">[88]</ref>. The amplitude envelope of the high-gamma band (&gt;70 Hz) in ECoG closely tracks aspects of the acoustic speech signal and can provide an even more detailed view of the spatio-temporal progression of brain activity during speech processes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b90">[89]</ref>. Similar analyses can be applied to microarray recordings using features such as rate codes and tuning curves <ref type="bibr" target="#b56">[55]</ref>, <ref type="bibr" target="#b57">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Biosignal-Based Speech Recognition</head><p>Automatic speech recognition (ASR) systems convert speech (typically audio) into text, i.e., a sequence of written words. The ASR task is characterized by its multi-level sequential nature: small units, usually (context-dependent) phones, are concatenated into words, which in turn are concatenated into continuous sentences. In addition, prior probabilities are assigned to word sequences by means of language models. For more than 30 years ASR has been dominated by multi-level statistical modeling schemes, in particular hidden Markov models (HMMs) <ref type="bibr" target="#b91">[90]</ref> and n-gram language models <ref type="bibr" target="#b92">[91]</ref>. Recent applications of artificial neural networks have revolutionized ASR with the development of hybrid Deep Neural Network Hidden Markov Model systems <ref type="bibr" target="#b93">[92]</ref>, and end-to-end systems that directly map speech features into text <ref type="bibr" target="#b94">[93]</ref>.</p><p>Fundamentally, biosignal-based speech recognition can be approached by replacing the acoustic signal processing frontend with methods tailored to each biosignal while leaving the statistical modeling back-end unchanged. Examples of this approach include isolated word recognition using image-specific features for lipreading <ref type="bibr" target="#b95">[94]</ref>, and continuous phone-based HMM recognition using sEMG signals <ref type="bibr" target="#b40">[39]</ref>. However, there are interdependencies at each processing level in the speech recognition pipeline that allow for adaptation / improvement to back-end systems for each biosignal.</p><p>One important design aspect of biosignal-based speech recognition is the way in which smaller units are concatenated into words and sentences. Large units may be easier to recognize, but harder to share between different words, leading to difficulty recognizing unseen vocabulary and additional training data requirements. Short units may be unstable due to coarticulatory effects, or they might not contain enough information to reliably identify a pattern of articulation. In visual speech recognition, viseme units have been defined by visually grouping phones of similar appearance, or by considering articulatory gestures <ref type="bibr" target="#b96">[95]</ref>. However, speech recognition with visemes causes ambiguities that must be resolved, e.g., by language models. Bundled Phonetic Features <ref type="bibr" target="#b97">[96]</ref> are a data-driven approach that has been successful for EMG-based speech recognition. Finally, biosignal-based speech recognition has been explored using syllables <ref type="bibr" target="#b98">[97]</ref> and context-independent or context-dependent phones <ref type="bibr" target="#b99">[98]</ref>- <ref type="bibr" target="#b101">[100]</ref>.</p><p>In multi-modal speech recognition, combining sources of information is of particular interest, both for recognition and for possible audio-based bootstrapping. The reliability of each biosignal modality is highly variable, depending on phonetic properties <ref type="bibr" target="#b102">[101]</ref> and on the environmental conditions (e.g., noise). Frameworks for dynamic estimation of stream weights have been developed for audio-visual and audio-plusmyoelectric speech recognition <ref type="bibr" target="#b103">[102]</ref>, <ref type="bibr" target="#b104">[103]</ref>. Furthermore, manifestations of the articulation (e.g., brain signal, EMG onset, visible muscle contraction, and sound) are not synchronous <ref type="bibr" target="#b40">[39]</ref>, <ref type="bibr" target="#b73">[72]</ref> due to the multi-step nature of speech motor control and the complex relation between articulatory gestures and speech sounds <ref type="bibr" target="#b105">[104]</ref>, <ref type="bibr" target="#b106">[105]</ref>. Articulatory information (place, manner voicing) can also be used to augment conventional (i.e., audio-based) ASR. Incorporating explicit speech production knowledge in ASR can improve the recognition of spontaneous speech and increase robustness to noise, by modeling more efficiently some co-articulation effects, see <ref type="bibr" target="#b107">[106]</ref> for a complete review on this line of research.</p><p>Visual articulatory data: Audiovisual speech recognition (AVSR) combines video of a speaker's lips / face with traditional speech audio signals to improve ASR performance in adverse conditions (i.e., background noise) <ref type="bibr" target="#b108">[107]</ref>. AVSR continues to be widely investigated (see <ref type="bibr" target="#b109">[108]</ref> for a complete review) and has been extended to purely visual speech recognition (VSR). In that case, no audio signal is used and speech recognition is performed only from visual information. Lip movements observable by video provide only partial information on speech articulation; therefore, recent efforts have also explored the combination of video and ultrasound imaging to capture both lip and tongue movements <ref type="bibr" target="#b110">[109]</ref>.</p><p>Similar to audio-based speech recognition, classification in AVSR or VSR systems is often accomplished using models that explicitly account for speech dynamics including: HMM <ref type="bibr" target="#b95">[94]</ref>, <ref type="bibr" target="#b100">[99]</ref>, <ref type="bibr" target="#b111">[110]</ref>, deep neural networks <ref type="bibr" target="#b112">[111]</ref> and long short-term memory neural networks <ref type="bibr" target="#b113">[112]</ref>. Though the addition of visual to audio modalities can augment speech representations, they may be acquired with different temporal structures that must be reconciled, e.g., coupled-HMM <ref type="bibr" target="#b114">[113]</ref> and dynamic Bayesian network <ref type="bibr" target="#b115">[114]</ref>.</p><p>Magnetic articulography: Automatic continuous speech recognition from EMA data have been investigated for English <ref type="bibr" target="#b116">[115]</ref> and French <ref type="bibr" target="#b117">[116]</ref> (in conjunction with the audio signal), and small vocabulary recognition using PMA <ref type="bibr" target="#b118">[117]</ref> using standard ASR techniques.</p><p>Electromyographic signals: Early EMG-based speech recognition used just three surface EMG electrodes to discriminate Japanese vowels <ref type="bibr" target="#b79">[78]</ref> and has since been combined with auditory signals for better performance in noisy environments <ref type="bibr" target="#b119">[118]</ref>. More recently, EMG-based recognition has been applied to silent speech applications <ref type="bibr" target="#b82">[81]</ref>, including whole phrases spoken in silent and normal modal speech <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b40">[39]</ref>, syllables <ref type="bibr" target="#b98">[97]</ref>, and phones <ref type="bibr" target="#b101">[100]</ref>. Further developments include modeling context-dependent Bundled Phonetic Features to address data sparsity <ref type="bibr" target="#b97">[96]</ref>, adaptation to cope with recording session discrepancies <ref type="bibr" target="#b120">[119]</ref>, and development of a hybrid neural network -HMM system for EMG-based ASR <ref type="bibr" target="#b121">[120]</ref>.</p><p>Hemodynamic responses: Both fMRI and fNIRS have mostly been used to study speech neuroscience examining the averaged hemodynamic responses over many repetitions of speech tasks. However, a successful silent speech interface must be able to detect speech events in a single trial. A few studies have investigated this decoding approach using fMRI for decoding three Dutch vowels <ref type="bibr" target="#b122">[121]</ref>, nouns <ref type="bibr" target="#b123">[122]</ref>, and functional representations of natural speech <ref type="bibr" target="#b124">[123]</ref>. Initial applications of fNIRS to speech recognition have focused on discriminating between the speech modes: modal, silent, and imagined <ref type="bibr" target="#b125">[124]</ref>, <ref type="bibr" target="#b127">[125]</ref>. Using fNIRS, these modes can be discriminated from each other and from intervals without speech activity in single trial. However, the slow nature of the hemodynamic response prohibits investigation on a more fine-grained time scale than whole sentences, and thus does not scale up to spoken communication in any speaking style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrical brain signals:</head><p>The earliest attempts for speech recognition in EEG were used to predict the word a participant was attending to in a passive listening paradigm, without any speaking involvement (silent, imagined, or other) <ref type="bibr" target="#b128">[126]</ref>. This approach has been improved using ECoG to reconstruct a stimulus from auditory cortical activity during passive listening <ref type="bibr" target="#b129">[127]</ref>. Additional attempts have focused on speech production (actual or imagined) paradigms for decoding acoustic features and phonemes (EEG: <ref type="bibr" target="#b88">[87]</ref>, <ref type="bibr" target="#b130">[128]</ref>, <ref type="bibr" target="#b131">[129]</ref>; ECoG: <ref type="bibr" target="#b132">[130]</ref>, <ref type="bibr" target="#b133">[131]</ref>; microelectrodes: <ref type="bibr" target="#b56">[55]</ref>, <ref type="bibr" target="#b57">[56]</ref>), syllables (EEG: <ref type="bibr" target="#b89">[88]</ref>; ECoG: <ref type="bibr" target="#b134">[132]</ref>), and words (ECoG: <ref type="bibr" target="#b135">[133]</ref>). ECoG has also been used to decode speech articulatory features <ref type="bibr" target="#b136">[134]</ref>, and recently HMMbased ASR was applied to ECoG signals to decode continuously spoken speech <ref type="bibr" target="#b99">[98]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Biosignal-Based Speech Synthesis</head><p>In contrast to speech recognition approaches, speech synthesis is a means to artificially produce human speech from a given input signal. Current biosignal-based approaches usually consist of three processing steps: (1) features extraction from biosignals and (imagined) speech (e.g., Mel-cepstral features), (2) biosignal features are mapped to speech features, and (3) speech is synthesized from the predicted speech features, for example by a vocoder (a digital filter that models the spectral envelope and is excited with a proper signal). In this section we focus on the second step.</p><p>The mapping between biosignal and speech features is usually described as a regression problem between multidimensional continuous variables. Gaussian Mixture Regression is a classical approach inspired by statistical voice conversion <ref type="bibr" target="#b137">[135]</ref>, <ref type="bibr" target="#b138">[136]</ref> and has been used for EMG <ref type="bibr" target="#b139">[137]</ref>, PMA <ref type="bibr" target="#b20">[20]</ref> and US <ref type="bibr" target="#b140">[138]</ref> applications. Specifically, the joint probability density function of biosignal inputs and speech outputs is modeled by a Gaussian Mixture Model (GMM). The mapping from biosignal to speech is accomplished either frame-by-frame using the conventional mean square error estimator <ref type="bibr" target="#b137">[135]</ref>, or sequenceby-sequence using a maximum-likelihood estimator <ref type="bibr" target="#b138">[136]</ref>. Artificial Neural Networks are also powerful regressors that can be used for direct biosignal-to-speech mapping, and have been used for EMA-to-speech <ref type="bibr" target="#b141">[139]</ref>, <ref type="bibr" target="#b142">[140]</ref>, EMG-to-speech <ref type="bibr" target="#b143">[141]</ref>, and ultrasound/video-to-speech <ref type="bibr" target="#b74">[73]</ref>. An HMM-based regression technique based on full-covariance GMM has been proposed to explicitly model phoneme-specific dynamics of articulation, and to use linguistic priors for regularizing biosignal-to-speech conversion <ref type="bibr" target="#b140">[138]</ref>. A performance comparison of different mapping approaches in terms of real-time capabilities and conversion quality has been carried out for EMG-to-speech in <ref type="bibr" target="#b144">[142]</ref>.</p><p>Brain-based biosignals have primarily been used for speech recognition applications (Section IV-B), and do not directly incorporate speech synthesis into their decoding models. Only very few attempts have been made for direct speech synthesis using electrophysiological signals from the human brain. In one example, a microelectrode device implanted into the speech motor cortex was used to control a formant frequency speech synthesizer <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b59">[58]</ref>. This BCI-speech synthesizer converted changes in neural activity into the first two formant frequencies using an adaptive filter neural decoder, followed by synthesis for immediate audio output. A recent study has extended the BCIbased formant synthesizer technique for use with EEG instead of implanted microelectrodes <ref type="bibr" target="#b145">[143]</ref>. ECoG has also been used for direct synthesis BCIs by applying regression methods to reconstruct the speech spectrogram which can then be converted to an audio waveform <ref type="bibr" target="#b146">[144]</ref>.</p><p>Finally, we note that biosignal-based speech synthesis can be achieved by performing speech recognition followed by applying conventional text-to-speech systems. This method produces high-quality output, but is constrained by the limitations of the underlying recognizer. In particular, speech recognition operates on limited vocabulary, produces recognition errors, and causes an unavoidable delay between articulation and synthesis since words must be completely articulated and recognized before synthesis is possible. This delay is often undesired, particularly in biofeedback applications (see Section V-B), which encourages further work on direct speech synthesis from biosignals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Current Research and Open Challenges</head><p>All systems described above have made substantial progress in recent years, particularly in algorithm improvements, system miniaturization, and field studies. This section summarizes the status of different biosignal processing systems, their current applicability, and open challenges identified from both the literature and our own work.</p><p>The reviewed technologies can be grouped into two categories based on their practical applicability and maturity: (i) a stable baseline system is being tested in field studies with a substantial number of potential users (including patients where applicable); (ii) studies are performed on a small number of subjects under laboratory conditions. Technology in category (i) must necessarily provide an easy-to-use recording system and a reasonable speech recognition / synthesis quality.</p><p>Visual articulatory data are typically in category (i); large visual speech corpora exist and have been used in various AVSR and VSR studies <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b112">[111]</ref>, <ref type="bibr" target="#b113">[112]</ref>, benefiting from the fact that data can be recorded without special equipment, or is already available. One study achieved 65.4% word accuracy for the recognition of 333 word classes without use of a language model applying television broadcasts data <ref type="bibr" target="#b26">[26]</ref>. This result is considered state-of-the-art given the large variation in the data and the size of the vocabulary. Yet, improvements can be made considering that short words were excluded due to the presence of visual ambiguities.</p><p>EMG-based speech recognition is also considered in category (i); it has been used with large amounts of data from many subjects <ref type="bibr" target="#b147">[145]</ref> and has even been extended to speech restoration applications for individuals with laryngectomies <ref type="bibr" target="#b84">[83]</ref>. Also, PMA-based speech recognition <ref type="bibr" target="#b118">[117]</ref> and EMA-based speech synthesis <ref type="bibr" target="#b142">[140]</ref> have been successfully applied to speech restoration, and EPG-based biofeedback to speech therapy <ref type="bibr" target="#b148">[146]</ref>. Recognition accuracy has sufficiently improved for feasibility in basic communication scenarios using EMG, e.g., Word Error Rate of 10.3% on a 2000-word vocabulary <ref type="bibr" target="#b84">[83]</ref>. In addition, EMG recording systems are mobile and non-intrusive, though it requires time and experience to properly attach the recording electrodes, and best results are obtained when training and test data are recorded in one session, without intermediate removal of the electrodes. Unsupervised adaptation schemes show potential to compensate for these session dependencies <ref type="bibr" target="#b120">[119]</ref>.</p><p>Speech recognition from electrical brain signals has so far been limited to laboratory environments (category (ii)), due to methodological complexity and in some cases surgical intervention is required to be performed in specialized hospital environments (e.g., ECoG). Progress using hemodynamic approaches is limited by portability constraints and limited temporal resolution of metabolic processes (e.g., fMIR, fNIRS).</p><p>Research foci in biosignal-based speech processing naturally follow from the observed limitations of the existing systems, and include the following:</p><p>1) Robust and Portable Recording systems: Portability has been achieved for PMA <ref type="bibr" target="#b20">[20]</ref>, EMG <ref type="bibr" target="#b34">[34]</ref>, and to some extent video/ultrasound <ref type="bibr" target="#b140">[138]</ref>. In the case of lipreading, a system could also be based on fixed cameras (e.g., for forensic purposes) instead of a personal device, but this only works if there is an unobstructed view of the subject's face. EEG data can in theory be obtained with a portable device, however in practice high-quality signals are only obtained under laboratory conditions. ECoG, as described above, requires a specialized hospital environment. 2) Feedback: In Section II-B we note that silent, imagined, and inner speech may be difficult to utter reliably and consistently. Real-time feedback <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b142">[140]</ref>, is considered a promising approach to resolve this issue, though it is both a technical challenge (since data must be processed very quickly) and a modeling challenge (due to the asynchronicity between different manifestations of the speech process, see Section IV-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) System Adaptation for Silent or Inner/Imagined Speech:</head><p>Instead of relying on speakers to properly use real-time feedback, an alternative approach proposes algorithmic adaptation to account for variability in speaking modes, as has been done for an EMG-based speech recognizer <ref type="bibr" target="#b149">[147]</ref>, <ref type="bibr" target="#b150">[148]</ref>. While discrimination of speaking modes from brain activity has been shown to be possible <ref type="bibr" target="#b125">[124]</ref>, understanding the difference between modal and imagined speech processes and creating large-scale recognizers for imagined speech are still significant open issues. 4) Multi-Session and Multi-Speaker systems: Most existing systems are speaker-specific, with the exception of some lipreading systems <ref type="bibr" target="#b26">[26]</ref>. Even when data is only taken from one speaker, there may be inter-session differences due to a variety of factors (e.g., environmental artifacts, sensor positioning, etc.), which can be remedied by stan-dard methods (adaptation <ref type="bibr" target="#b120">[119]</ref>, recalibration <ref type="bibr" target="#b142">[140]</ref>, integration of session independence as a neural network training objective <ref type="bibr" target="#b151">[149]</ref>). 5) Sufficiency of speech representations: Visual speech recognition using only lip images (i.e., lipreading) is insufficient, and suffers from ambiguities, which can be resolved by including ultrasound images of the vocal tract as an additional input. EMA/PMA and EMG are more sufficient, though EMA/PMA do not represent facial gestures, and without needle electrodes, EMG can not represent specific tongue muscles. That said, these methods provide a fairly complete representation of the speech process with ambiguities in voicing only (cf. <ref type="bibr" target="#b102">[101]</ref>). Acquiring appropriate and sufficient signals directly from the speech-and language-related areas of the brain should also provide a complete representation of the processes needed to understand and generate speech, though there is a practical limit on signal acquisition and interpretation. For hemodynamic and electrophysiological techniques, sampling the speech and language-related areas of the brain remains an intriguing challenge. A comparison between biosignal-based speech processing systems is difficult at this time since available data corpora differ in size, vocabulary, recording setup, etc., and benchmark data have not been established yet. For speech recognition with medium-sized vocabularies, the three major articulation-based systems (PMA, video+ultrasound, EMG) all perform reasonably and similarly, and further improvements are likely in the near future. The availability of large data corpora will be crucial in extending these systems to truly large-scale speech recognition (with tens of thousands of vocabulary words), and equally to high-quality speech synthesis. Ultimately, the "best" system will be the one which most convincingly resolves the issues summarized above, and will depend upon the constraints of the intended application, including factors such as user preference, performance, reliability, environment, comfort, aesthetics, etc., see Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. USE CASES OF BIOSIGNAL-BASED SPOKEN COMMUNICATION</head><p>Capturing, processing, and interpretation of biosignals related to speech in the absence of an intelligible airborne acoustic signal opens up novel use cases in spoken communication (see step # 4 in Fig. <ref type="figure" target="#fig_0">1</ref>). A survey on Silent Speech Interfaces (SSI) <ref type="bibr" target="#b3">[4]</ref> introduced relevant human-computer interfaces developed before 2010. Sensor technologies and machine learning advanced this field in the past few years. Published use cases and applications of "Biosignal-based Spoken Communication" fall into four main categories, (1) voice prostheses and devices to restore spoken communication, for individuals unable to speak due to impairment, disease, or accident; (2) methods to deliver articulatory biofeedback of voice production to increase articulatory awareness for therapy and training for spoken communication, such as speech therapy and language learning; (3) approaches to enhance speech recognition and synthesis performance for robust spoken communication in noisy environments, like the fusion of complementary speech-related biosignals to compensate for signal corruptions under adverse noise conditions; and (4) strategies for mute spoken communication in situations, when audible communication is prohibited or unwanted, e.g., avoiding disruptions in quiet environments or securing against eavesdropping. The concrete systems which we describe in this section frequently address several of these challenges, but often target just a single application. This strategic approach affects the direction of research, requires diverse ethical considerations (e.g., for working with patients), and also influences the design of the communication system: for example, individuals with speech impairments may be willing to invest a significant amount of time into the optimization of their personal communication system, whereas healthy users typically expect little or no enrollment time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Restoring Spoken Communication</head><p>An important goal for biosignal-based speech synthesis techniques is to restore spoken communication for individuals with disordered or absent vocalization. Each of the modalities described has specific applications and is most appropriate for specific clinical populations (e.g., individuals with dysarthria, laryngectomy, or paralysis). In laryngectomy, an individual's larynx is surgically removed, and traditional options to restore voice include: using an electrolarynx device resulting in a very robotic voice, using oesophageal speech, or using a tracheosophageal prosthesis, which has to be replaced every few months. Biosignal-based alternatives for this population include PMA-synthesis <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b118">[117]</ref> and EMG-based speech recognition <ref type="bibr" target="#b84">[83]</ref>.</p><p>For individuals with the most severe speech and motor impairments, the objective is to supplement or bypass the speech-motor pathways using available biosignals for improved speech output. In this use case, current research focuses on synthesizing speech during imagined speech, or speech attempts by individuals with total paralysis, directly from brain signal recordings. The superior spatial resolution and signal fidelity of invasive techniques such as microelectrodes and ECoG make them promising approaches for the design of practical speech-based BCIs and neuroprosthetics <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b59">[58]</ref>, <ref type="bibr" target="#b99">[98]</ref>. Such systems may perform a continuous reconstruction of speech or a discrete classification and output of sounds, words, etc. (see Section IV-B), depending on the objective and constraints of the system. While it may be possible to decode individual words or phrases discretely, scaling this approach to a larger vocabulary can become intractable. Alternatively, the ability to decode basic units of speech, such as formants or phones <ref type="bibr" target="#b99">[98]</ref>, will enable the creation of generative models that are not limited to a fixed vocabulary. In any case, effectively developing and transferring models trained on normal modal speech to imagined speech remains an active research challenge since the neural representations of normal modal and imagined speech are not identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Therapy and Training for Spoken Communication</head><p>The methods developed for biosignal-based speech processing can also be used for multimodal biofeedback in order to study speech production, facilitate second language (L2) learning, and rehabilitate speech impairments. Visual feedback of the articulators (e.g., lip reading) can have a dramatic effect on perception <ref type="bibr" target="#b152">[150]</ref>, and can even improve speech perception and comprehension by individuals with hearing impairments <ref type="bibr" target="#b4">[5]</ref>. Articulatory kinematics captured using EMA have been used for speech training with an emphasis on improving second language learning <ref type="bibr" target="#b5">[6]</ref>, and investigating articulatory deficits in dysarthria <ref type="bibr" target="#b6">[7]</ref>. EPG has also been successfully used as a biofeedback tool for speech therapy <ref type="bibr" target="#b148">[146]</ref> and L2 pronunciation training <ref type="bibr" target="#b153">[151]</ref>. Promising results using ultrasound imaging have also been obtained for rehabilitation of the English /r/ <ref type="bibr" target="#b154">[152]</ref> and persisting speech sound disorders <ref type="bibr" target="#b155">[153]</ref>.</p><p>Notably, biosignal-based speech recognition and synthesis performance declines for silent compared to normal speaking, even when the ASR system is trained and tested exclusively on the respective speaking modes <ref type="bibr" target="#b150">[148]</ref>. Speakers report difficulties to steadily producing silent speech <ref type="bibr" target="#b13">[14]</ref>, in part due to the absence of auditory feedback that is critical for normal speech production <ref type="bibr" target="#b156">[154]</ref>, <ref type="bibr" target="#b157">[155]</ref>. Biofeedback created by real-time speech output could provide an optimal solution to alleviate the challenges in BCI and SSI (see Section V-D below). In a closed-loop paradigm, speakers can rely on synthetic speech for auditory feedback and exploit it to regulate their own production, as in <ref type="bibr" target="#b142">[140]</ref> for SSI and <ref type="bibr" target="#b58">[57]</ref> for BCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robust Spoken Communication in Noisy Environments</head><p>Improving spoken communication under adverse noise conditions has long been a challenge for speech research and development. Large-scale DARPA programs (e.g., ASE, SPINE, RATS) targeted improvements to speech processing in military and civilian contexts, such as in combat situations, air traffic control, search-and-rescue operations, and security scenarios. Beside the development of noise-robust algorithms, this led to the creation of new sensors like throat and bone-conduction microphones, which can be combined with traditional microphones for improved, fused biosignal ASR <ref type="bibr" target="#b158">[156]</ref>. EMG is a natural extension of these techniques and has been used for small vocabulary recognition in acoustically harsh environments <ref type="bibr" target="#b159">[157]</ref>, and spoken communication for firefighters, pilots, and astronauts through electrodes integrated into self-contained breathing apparatuses <ref type="bibr" target="#b82">[81]</ref>, <ref type="bibr" target="#b160">[158]</ref>. Like subaudible microphones, the EMG combined with conventional acoustic signals can further improve ASR performance in noisy environments <ref type="bibr" target="#b119">[118]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mute Spoken Communication</head><p>In many situations, spoken communication is desired but making any sounds is prohibited or socially inappropriate. For example, carrying out phone conversations may disturb bystanders in quiet environments like libraries or is inappropriate during group meetings. Eavesdropping is a risk when communicating private information in public places. Furthermore, safety and security settings may require a silent communication. Several different biosignal-based systems address the challenges of mute spoken communication. For instance, silent speech interfaces have been developed using ultrasound imaging, combined with a conventional video camera to capture tongue and lip movements simultaneously, without any audio signals <ref type="bibr" target="#b110">[109]</ref>, <ref type="bibr" target="#b140">[138]</ref>. Importantly, several studies show that performance drops when speaking modes are mixed in training and testing <ref type="bibr" target="#b140">[138]</ref>. In the case of surface electromyography, signal-based adaptation methods are proposed to reduce the differences between speaking modes <ref type="bibr" target="#b149">[147]</ref> and EMG-based speech recognizers are designed which are trained and tested on silent speech <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b150">[148]</ref>. Another way to alleviate the impact of articulatory differences between modal and silent speech is to provide a silent speaker with a synthetic auditory feedback, in real-time <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b142">[140]</ref>. One example involves an articulatory synthesizer that converts EMA data into spectral features using a deep neural network, and can be controlled in real-time by naive subjects articulating silently <ref type="bibr" target="#b142">[140]</ref>.</p><p>PMA-based speech recognition and synthesis has now been achieved in a highly portable manner <ref type="bibr" target="#b20">[20]</ref>. While most published research focuses on the aim of restoring speech communication to speech-impaired persons (see Section V-A above), PMA was originally proposed for mute communication of individuals without impairment <ref type="bibr" target="#b118">[117]</ref>. However, a full study on using silent speech to drive a PMA-based synthesizer has not yet been published.</p><p>Studies toward EEG-based speech recognition on silent or imagined speech include classification of single phonemes <ref type="bibr" target="#b88">[87]</ref>, <ref type="bibr" target="#b89">[88]</ref>, <ref type="bibr" target="#b131">[129]</ref>. Alternative approaches use limb motor imagery to control a formant frequency speech synthesizer without the presence of an acoustic speech signal <ref type="bibr" target="#b145">[143]</ref>. Imagined speech decoding has been accomplished with a greater range of speech output using intracortical recording methods including formant frequency prediction using microelectrodes <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b59">[58]</ref>, phoneme classification with microelectrodes <ref type="bibr" target="#b56">[55]</ref>, spectrotemporal features using ECoG <ref type="bibr" target="#b161">[159]</ref>, and word pairs using ECoG <ref type="bibr" target="#b162">[160]</ref>.</p><p>Beyond mute communication, the technologies described in this survey may be combined with speech translation to bridge the language barrier <ref type="bibr" target="#b40">[39]</ref>. Using current procedures, simultaneous translation of a spoken conversation results in the overlap of two voices (one voice from the speaker in the source language and one voice in the target language, coming either from a human interpreter or from the synthesized output of a speech translation system). To avoid such inconvenient scenarios, speakers could instead silently speak (or imagine) in their native tongue, while listeners hear only the translated output. Thus, the combination of mute communication plus translation creates the illusion of speaking in a foreign tongue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND PERSPECTIVES</head><p>Biosignal-based Spoken Communication is a rapidly evolving cross-disciplinary field. Research and development takes place at the intersection of engineering, computer science, medicine, psychology, and neurosciences. It requires the mastering of sensor technologies, signal, speech and language processing, as well as human-machine interfaces.</p><p>This survey paper is intended to provide an entry point for readers interested in this very active field, to define and describe terminology, to recite relevant publications, and thereby to bridge the gap between disciplines. It presents a broad overview over the state-of-the-art technologies, methods, and applications. Table <ref type="table" target="#tab_2">IV</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Biosignal-based spoken communication resulting from (1) speech-related activities of the human body, (2) signal acquisition using various activitydependent sensor technologies, (3) biosignal processing including feature extraction followed by output generation for (4) various target applications.</figDesc><graphic coords="2,59.16,66.68,480.00,210.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SPEAKING</head><label>I</label><figDesc>MODE DEPENDENT ON GLOTTAL ACTIVITY</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>summarizes the applicability of biosignals for</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV APPLICABLE</head><label>IV</label><figDesc>TECHNOLOGIES FOR USE CASES AND SPEAKING MODES (GRAYED OUT CELLS = NO TARGET SPEAKING MODE FOR USE CASE, Italic Font = APPLICABLE BUT NO PUBLICATIONS YET, "-" = NOT APPLICABLE)</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Schultz, D. J. Krusienski, and C. Herff was supported by the Federal Ministry of Education and Research (BMBF) in Germany and the National Science Foundation (NSF) in the USA for the project "RESPONSE -REvealing SPONtaneous Speech processes in Electrocorticography" under references 01GQ1602 (BMBF) and 1608140 (NSF). The work of M. Wand was supported by the EU H2020 programme (#687795). The work of J. S. Brumberg was supported by the National Institutes of Health (R03-DC011304). The guest editor coordinating the review of this manuscript and approving it for publication was Dr. Junichi Yamagishi.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use Cases</head><p>Speaking Modes (Section II, Tables <ref type="table">1</ref><ref type="table">2</ref><ref type="table">3</ref>) Driven by recent advances in sensor technologies (resolution, accuracy, miniaturization, energy consumption, connectivity, mobility, and costs, to name only a few), the large attention and developments in neurosciences, and the impact of deep learning approaches to automatic speech processing, we expect major breakthroughs in the years to come.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors gratefully acknowledge the contributions of many of their past and present collaborators, including X. Alameda-Pineda, P. Badin, G. Bailly, AW Black, F. Bocquelet, G. Chollet, B. Denby, F. Elisei, D. Fabre, L. Girin, C. Guan, M. Janke, S-C. Jou, K. Nakamura, P. Perrier, K. Prahallad, M. Pouget, P. Roussel, G. Schalk, J.L. Schwartz, E. Tatulli, A. Toth, and B. Yvert. Their insights and hard work have made the subject of biosignal-based spoken communication a very fruitful area in recent years. Special thanks to L. Diener, who helped with the write up of this material and to the anonymous reviewers for their very valuable comments. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brain-computer interfaces for communication and control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wolpaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Birbaumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="767" to="791" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic speech recognition from neural signals: A focused review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progress in speech decoding from the electrocorticogram</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Krusienski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Silent speech interfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="287" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auditory-visual perception of speech</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Erber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Disorders</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="492" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual feedback of tongue movement for novel speech sound learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">612</biblScope>
			<date type="published" when="2015">2015</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech sensorimotor learning through a virtual vocal tract</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3342" to="3342" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kaniusas</surname></persName>
		</author>
		<title level="m">Biomedical Signals and Sensors I</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Biosignalebasierte Mensch-Maschine-Schnittstellen</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Putze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatisierungstechnik</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="760" to="769" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Deller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Proakis</surname></persName>
		</author>
		<title level="m">Discrete Time Processing of Speech Signals</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Influence of sound immersion and communicative interaction on the Lombard effect</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech, Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="588" to="608" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inner speech: Development, cognitive functions, phenomenology, and neurobiology</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alderson-Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernyhough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">141</biblScope>
			<biblScope unit="page" from="931" to="965" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Problems of General Psychology, Including the Volume Thinking and Speech (Cognition and Language: A Series in Psycholinguistics)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vygotsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Vygotsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Plenum</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>The Collected Works of</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Impact of different feedback mechanisms in EMG-based speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Conf. Int. Speech Commun. Assoc</title>
		<imprint>
			<biblScope unit="page" from="2213" to="2216" />
			<date type="published" when="2011">2011</date>
			<pubPlace>Florence, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kinematics of the chest wall during speech production: Volume displacements of the rib cage, abdomen, and lung</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech, Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="115" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Take a breath and take the turn: How breathing meets turns in spontaneous dialogue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rochet-Capellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosoph. Trans. Roy. Soc. B</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">1658</biblScope>
			<date type="published" when="2014">2014. 20130399</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multichannel electroglottograph</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rothenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Voice</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Electromagnetic articulography: Use of alternating magnetic fields for tracking movements of multiple points inside and outside the vocal tract</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Schönle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gräbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Lang</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="26" to="35" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Isolated word recognition of silent speech using magnetic implants and sensors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Eng. Phys</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1189" to="1197" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A silent speech system based on permanent magnet articulography and direct synthesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="67" to="87" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature-based pronunciation modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng. Comput. Sci., MIT</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Articulatory based low bit-rate speech coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chennoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3163" to="3163" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating articulatory features into HMM-based parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1171" to="1185" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combined optical distance sensing and electropalatography to measure articulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Birkholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neuschaefer-Rube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Annu</title>
		<meeting>12th Annu</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="285" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Asian Conf. Comput. Vis</title>
		<meeting>13th Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech MRI: Morphology and function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wylezinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Miquel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica Medica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="604" to="618" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-speed real-time magnetic resonance imaging of fast tongue movements in elite horn players</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Iltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Voit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schoonderwaldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Altenmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imag. Med. Surg</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="381" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A guide to analysing tongue motion from ultrasound images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Linguistics Phonetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="455" to="501" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2003 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2003 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The physiological microphone (PMIC): A competitive alternative for speaker assessment in stress detection and speaker verification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speech input hardware investigation for future dismounted soldier computer systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tack</surname></persName>
		</author>
		<idno>CR2005-064</idno>
	</analytic>
	<monogr>
		<title level="j">Def. Res. Develop. Canada</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Toronto, ON, Canada, DRDC Toronto Rep</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Guidelines for human electromyographic research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="567" to="589" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Session independent non-audible speech recognition using surface electromyography</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Automat. Speech Recognit. Understanding</title>
		<meeting>IEEE Workshop Automat. Speech Recognit. Understanding<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards a practical silent speech recognition system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Meltzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu. Conf. Int. Speech Commun</title>
		<meeting>15th Annu. Conf. Int. Speech Commun</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Assoc</surname></persName>
		</author>
		<author>
			<persName><surname>Singapore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1164" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Array-based electromyographic silent speech interface</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Biosignals</title>
		<meeting>Biosignals</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Merletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parker</surname></persName>
		</author>
		<title level="m">Electromyography: Physiology, Engineering, and Non-Invasive Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Chapter 4.2</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural and mechanical response time for speech production</title>
		<author>
			<persName><forename type="first">R</forename><surname>Netsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="608" to="618" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards continuous speech recognition using surface electromyography</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walliczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Spoken Lang. Process</title>
		<meeting>9th Int. Conf. Spoken Lang. ess<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="816" to="847" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Noninvasive, infrared monitoring of cerebral and myocardial oxygen sufficiency and circulatory parameters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jobsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">4323</biblScope>
			<biblScope unit="page" from="1264" to="1267" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-invasive neuroimaging using near-infrared light</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strangman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="679" to="693" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Continuous monitoring of brain dynamics with functional near infrared spectroscopy as a tool for neuroergonomic research: Empirical examples and a technological development</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Onaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Izzetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shewokis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">871</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards a wireless open source instrument: Functional near-infrared spectroscopy in mobile neuroergonomics and BCI applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Von Lühmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">617</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Electric Fields of the Brain: The Neurophysics of EEG</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EMG contamination of EEG: Spectral and topographical characteristics</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Goncharova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wolpaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1580" to="1593" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Removal of muscle artifacts from EEG recordings of spoken language production</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bereitschaftspotential preceding speech after holding breath</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Kornhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exp. Brain Res</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The spatial and temporal signatures of word production components</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indefrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J M</forename><surname>Levelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="101" to="144" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Guidelines for using human event-related potentials to study cognition: Recording standards and publication criteria</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Picton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Event-related EEG/MEG synchronization and desynchronization: Basic principles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopes Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1842" to="1857" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Utah Intracortical Electrode Array: A recording structure for potential braincomputer interfaces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Nordhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Normann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalogr. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="228" to="239" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cortical neural prosthetics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="487" to="507" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting movement from multiunit activity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abeles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="8387" to="8394" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech motor cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Andreasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Structured neuronal encoding and decoding of human speech features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tankus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1015</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A wireless brain-machine interface for real-time speech synthesis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Art. no. e8218</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Brain-computer interfaces for speech communication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nieto-Castanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Speech Commun.</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. I. Alpha and beta eventrelated desynchronization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Crone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2271" to="2299" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Subdural grid recordings of distributed neocortical networks involved with somatosensory discrimination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gevins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="282" to="290" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Electroencephalogr</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Frequency dependence of the transmission of the EEG from cortex to scalp</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalogr. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High-frequency gamma oscillations and human brain mapping with electrocorticography</title>
		<author>
			<persName><forename type="first">N</forename><surname>Crone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korzeniewska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress Brain Res</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="275" to="295" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visual speech recognition using active shape models and hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Thacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Beet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1996 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>1996 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="817" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multiple cameras audio visual speech recognition using active appearance model visual features in car environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="171" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Automatic dynamic template tracking of inner lips based on CLNF</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beautemps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2017 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5130" to="5134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic contour tracking in ultrasound images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Linguistics Phonetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="545" to="554" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Tongue tracking in ultrasound images with active appearance models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1733" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tongue tracking in ultrasound images using Eigentongue decomposition and artificial neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bocquelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Badin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Annu</title>
		<meeting>16th Annu<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2410" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep belief networks for real-time extraction of tongue contours from ultrasound during speech</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Conf. Pattern Recognit</title>
		<meeting>IEEE 20th Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1493" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">DCTbased video features for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Savariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthommier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Spoken Lang. Process</title>
		<meeting>7th Int. Conf. Spoken Lang. ess<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1925" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Eigenlips for robust speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Konig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1994 IEEE Int. Conf. Acoust., Speech, Signal Process., Adelaide, SA, Australia</title>
		<meeting>1994 IEEE Int. Conf. Acoust., Speech, Signal ess., Adelaide, SA, Australia</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Eigentongue feature extraction for an ultrasound-based silent speech interface</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2007 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2007 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1245" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">The Handbook of Brain Theory and Neural Networks (Convolutional Networks for Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>M. A. Arbib</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="255" to="258" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu</title>
		<meeting>15th Annu<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1149" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Feature extraction using multimodal convolutional neural networks for visual speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tatulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2017 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2971" to="2975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Angle correction in optopalatographic tongue distance measurements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Birkholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A speech prosthesis employing a speech synthesizer-vowel discrimination from perioral muscle activities and vowel production</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sugie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsunoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="485" to="490" />
			<date type="published" when="1985-07">Jul. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Use of myoelectric signals to recognize speech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Trull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf</title>
		<meeting>11th Annu. Conf</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1793" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A new strategy for multifunction myoelectric control</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hudgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="1993-01">Jan. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Sub auditory speech recognition based on EMG/EPG signals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agabon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="3128" to="3133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Disordered speech recognition using acoustic and sEMG signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu</title>
		<meeting>10th Annu<address><addrLine>Brighton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="644" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Silent speech recognition as an alternative communication device for persons with laryngectomy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Meltzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="xxxx" to="xxxx" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Mental workload during n-back task-quantified in the prefrontal cortex using fNIRS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fortmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Putze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">935</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Combining feature extraction and classification for fNIRS BCIs by regularized least squares optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Annu. Int. Conf</title>
		<meeting>36th Annu. Int. Conf<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2012" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Early and late components of the contingent negative variation prior to manual and speech responses in stutterers and non-stutterers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Psychophysiol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Single-trial classification of vowel speech imagery using common spatial patterns</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dasalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kambara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1334" to="1339" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Toward EEG sensing of imagined speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>D'zmura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<editor>Human Computer Interaction, J. A. Jacko</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="40" to="48" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Speech Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-F</forename><surname>Lee</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Annu</title>
		<meeting>12th Annu<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2016 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2016 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Lipreading from color video</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1192" to="1195" />
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Viseme definitions comparison for visualonly speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cappelletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Signal Process. Conf</title>
		<meeting>Eur. Signal ess. Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2109" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Modeling coarticulation in large vocabulary EMG-based speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="353" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Syllablebased speech recognition using EMG</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lopez-Larraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Mozos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Antelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Minguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annu. Int. Conf</title>
		<meeting>32nd Annu. Int. Conf<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4699" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Brain-to-text: Decoding spoken phrases from phone representations in the brain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Visuo-phonetic decoding using multi-stream and contextdependent models for an ultrasound-based silent speech interface</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu</title>
		<meeting>10th Annu<address><addrLine>Brighton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="640" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Sub-word unit based non-audible speech recognition using surface electromyography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Walliczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Spoken Lang. Process</title>
		<meeting>9th Int. Conf. Spoken Lang. ess<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1487" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Analysis of phone confusion in EMG-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2011 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="757" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Learning dynamic stream weights for coupled-HMM-based audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="863" to="876" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">SNR-adaptive stream weighting for audio-MES ASR</title>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2001">2001-2010, Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Neural modeling and imaging of the cortical interactions underlying syllable production</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tourville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Lang</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="280" to="301" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Savariaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Art. no. e1003743</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Speech production knowledge in automatic speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="723" to="742" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Automatic lipreading to enhance speech recognition (speech reading)</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Champaign, IL, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Illinois Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Audio-visual automatic speech recognition: An overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Issues in Visual and Audio-Visual Speech Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Bailly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Vatikiotis-Bateson</surname></persName>
		</editor>
		<editor>
			<persName><surname>Perrier Eds</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2015 IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Lipreading with long shortterm memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2016 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2016 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Asynchrony modeling for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Hum. Lang</title>
		<meeting>Int. Conf. Hum. Lang</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">DBN based multi-stream models for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gowdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2004 IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>2004 IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="993" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Continuous speech recognition using articulatory data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wrench</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Spoken Lang. Process</title>
		<meeting>6th Int. Conf. Spoken Lang. ess</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A pilot study on augmented speech communication based on electro-magnetic articulography</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heracleous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Badin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Development of a (silent) speech recognition system for patients following laryngectomy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sarrazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Eng. Phys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="425" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Myoelectric signals to augment speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Englehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hudgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Lovely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="500" to="506" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Towards real-life application of EMGbased speech recognition by using unsupervised adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu</title>
		<meeting>15th Annu<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1189" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Deep neural network frontend for continuous EMG-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Annu</title>
		<meeting>17th Annu<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3032" to="3036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Who&quot; is saying &quot;what&quot;? Brain-based decoding of human voice and speech</title>
		<author>
			<persName><forename type="first">E</forename><surname>Formisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="issue">5903</biblScope>
			<biblScope unit="page" from="970" to="973" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Predicting human brain activity associated with the meanings of nouns</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">5880</biblScope>
			<biblScope unit="page" from="1191" to="1195" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Natural speech reveals the semantic maps that tile human cerebral cortex</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Theunissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">532</biblScope>
			<biblScope unit="issue">7600</biblScope>
			<biblScope unit="page" from="453" to="458" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Speaking mode recognition from functional near infrared spectroscopy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Putze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th</title>
		<meeting>34th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Int. Conf. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="1715" to="1718" />
			<date type="published" when="2012">2012</date>
			<pubPlace>San Diego, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Cross-subject classification of speaking modes using fNIRS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Putze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><surname>Leung</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7664</biblScope>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Brain wave recognition of words</title>
		<author>
			<persName><forename type="first">P</forename><surname>Suppes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="14965" to="14969" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Reconstructing speech from human auditory cortex</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Pasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1001251</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Decoding of covert vowel articulation using electroencephalography cortical currents</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">EEG-based speech recognition-Impact of temporal effects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Bio-Inspired Syst. Signal Process</title>
		<meeting>2nd Int. Conf. Bio-Inspired Syst. Signal ess<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="376" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Direct classification of all American English phonemes using signals from functional speech motor cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Mugler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35015</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blakely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ojemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30nd Annu. Int. Conf</title>
		<meeting>30nd Annu. Int. Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="4964" to="4967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Annu. Int. Conf</title>
		<meeting>36th Annu. Int. Conf<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6782" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Decoding spoken words using local field potentials recorded from the cortical surface</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Greger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56007</biblScope>
			<date type="published" when="2010">2010</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Electrocorticographic representations of segmental features in continuous speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">97</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Continuous probabilistic transform for voice conversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1998-03">Mar. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2222" to="2235" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Synthesizing speech from electromyography using voice transformation techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu</title>
		<meeting>10th Annu<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Statistical conversion of silent articulation into audible speech using full-covariance HMM</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bailly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="274" to="293" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A neural network model of the articulatoryacoustic forward mapping trained on recordings of articulatory parameters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2354" to="2364" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Real-time control of an articulatory-based speech synthesizer for brain computer interfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bocquelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Savariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Art. no. e1005119</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Direct conversion from facial myoelectric signals to speech using deep neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Diener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Killarney, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">EMG-to-speech: Direct generation of speech from facial electromyographic signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Diener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="xxxx" to="xxxx" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Using motor imagery to control brain-computer interfaces for communication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Burnison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience</title>
		<meeting><address><addrLine>Basel, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Spring Int. Publ</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Towards direct speech synthesis from ECoG: A pilot study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Diener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krusienski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Annu. Int. Conf</title>
		<meeting>38th Annu. Int. Conf<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1540" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">The EMG-UKA corpus for electromyographic speech processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu</title>
		<meeting>15th Annu<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1593" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Electropalatography for older children and adults with residual speech errors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminars in Speech and Language</title>
		<meeting><address><addrLine>Stuttgart, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Thieme Med. Publ</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="271" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A spectral mapping method for EMG-based recognition of silent speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. B-INTERFACE</title>
		<meeting>B-INTERFACE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Tackling speaking mode varieties in EMG-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2515" to="2526" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Improving speaker-independent lipreading with domain-adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Annu. Conf. Int. Speech Commun. Assoc</title>
		<meeting>18th Annu. Conf. Int. Speech Commun. Assoc</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3662" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="691" to="811" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Bibliography of electropalatographic (EPG) studies in English</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gibbon</surname></persName>
		</author>
		<ptr target="http://www.articulateinstruments.com/EPGrefs.pdf" />
	</analytic>
	<monogr>
		<title level="j">Dept. Speech Hear. Sci., Univ</title>
		<imprint>
			<date type="published" when="1957">1957-2013. 2013-05-21, 2011</date>
			<pubPlace>College Cork, Ireland, Rep. Staeno</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">The use of ultrasound biofeedback for improving English /r</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Working Papers Linguistics Circle</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Using ultrasound visual biofeedback to treat persistent primary speech sound disorders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Scobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Wrench</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Linguistics Phonetics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8-10</biblScope>
			<biblScope unit="page" from="575" to="597" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Speech motor control: Acoustic goals, saturation effects, auditory feedback and internal models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perkell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="227" to="250" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Neural mechanisms underlying auditory feedback control of speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tourville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1429" to="1443" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Combining standard and throat microphones for robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="72" to="74" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Small-vocabulary speech recognition using surface electromyography</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Betts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jorgensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interact. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1242" to="1259" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Speech interfaces based upon surface electromyography</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dusan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="366" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Decoding spectrotemporal features of overt and covert speech from the human cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neuroeng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Word pair classification during imagined speech using direct brain recordings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">25803</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
