<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIZE-INVARIANT GRAPH REPRESENTATIONS FOR GRAPH CLASSIFICATION EXTRAPOLATIONS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-10">March 10, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
							<email>ribeiro@cs.purdue.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIZE-INVARIANT GRAPH REPRESENTATIONS FOR GRAPH CLASSIFICATION EXTRAPOLATIONS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-10">March 10, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.05045v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Representation Learning</term>
					<term>Graph Neural Networks</term>
					<term>Graph Classification</term>
					<term>Causal Extrapolation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In general, graph representation learning methods assume that the test and train data come from the same distribution. In this work we consider an underexplored area of an otherwise rapidly developing field of graph representation learning: The task of out-of-distribution (OOD) graph classification, where train and test data have different distributions, with test data unavailable during training. Our work shows it is possible to use a causal model to learn approximately invariant representations that better extrapolate between train and test data. Finally, we conclude with synthetic and realworld dataset experiments showcasing the benefits of representations that are invariant to train/test distribution shifts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In general, graph representation learning methods assume that the test and train data come from the same distribution. Unfortunately, this assumption is not always valid in real-world deployments <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b30">31]</ref>. When the test distribution is different from training, the test data is described as out of distribution (OOD). Differences in train/test distribution may be due to environmental factors such as those related to the way the data is collected or processed.</p><p>Particularly, in graph classification tasks, where G is the graph and Y its label, we often see different graph sizes and/or distinct arrangements of vertex attributes associated with the same target label. How should we learn a graph representation for out-of-distribution inductive tasks (extrapolations), where the graphs in training and test (deployment) have distinct characteristics (i.e., P tr (G) = P te (G))? Are inductive graph neural networks (GNNs) robust to distribution shifts between P tr (G) and P te (G)? If not, is it possible to design a graph classifier that is robust to such OOD shifts without access to samples from P te (G)?</p><p>In this work we consider an OOD graph classification task with different train and test distributions based on graph sizes and vertex attributes. Our work focuses on simple (no self-loops) undirected graphs with discrete vertex attributes. We make the common assumption of independence between cause and mechanisms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">103,</ref><ref type="bibr" target="#b110">112,</ref><ref type="bibr" target="#b6">7]</ref>, which states that P(Y |G) remains the same between train and test. We also assume we do not have access to samples from P te (G), hence covariate shift adaptation methods (such as Yehudai et al. <ref type="bibr" target="#b144">[146]</ref>) are unfit for our scenario. In our setting we need to learn to extrapolate from a causal model. Figure <ref type="figure">1</ref>: The twin network DAG <ref type="bibr" target="#b10">[11]</ref> of our structural causal model (SCM). Gray (resp. white) vertices represent observed (resp. hidden) random variables.</p><p>1. We provide a causal model that formally describes a class of graph classification tasks where the training (P tr (G)) and test (P te (G)) graphs have different size and vertex attribute distributions.</p><p>2. Assuming independence between cause and mechanism (ICM) <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b112">114]</ref>, we introduce a graph representation method based on the work of Lovász and Szegedy <ref type="bibr" target="#b78">[79]</ref> and Graph Neural Networks (GNNs) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b145">147]</ref> that is invariant to the train/test distribution shifts of our causal model. Unlike existing invariant representations, this representation can perform extrapolations from single training environment (e.g., all training graphs have the same size).</p><p>3. Our empirical results show that, in most experiments, neither Invariant Risk Minimization (IRM) <ref type="bibr" target="#b6">[7]</ref> nor the GNN extrapolation modifications proposed by Xu et al. <ref type="bibr" target="#b141">[143]</ref> are able to perform well in graph classification tasks over the OOD test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Classification: A Causal Model Based on Random Graphs</head><p>Out-of-distribution (OOD) shift. For any joint distribution P(Y, G) of graphs G and labels Y , there are infinitely many causal models that give the same joint distribution <ref type="bibr">[97]</ref>. This phenomenon is known as model underspecification.</p><p>Hence, if the training data distribution P tr (Y, G) does not have the same support as the test distribution P te (Y, G), a model trained with samples drawn from P tr (Y, G) needs to be able to extrapolate in order to correctly predict P te (Y |G).</p><p>In this work, we assume Independence between Cause and Mechanism (ICM): P tr (Y |G) = P te (Y |G), which is a common assumption in the causal deep learning literature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">103,</ref><ref type="bibr" target="#b110">112,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>In inductive graph classification tasks, ICM implies that the shift between train and test distributions P tr (Y, G) = P te (Y, G) comes from P tr (G) = P te (G), since P tr (Y |G) = P te (Y |G). And because our task is inductive, i.e., no data from P te (G) or a proxy variable, we must make assumptions about the causal mechanisms in order to extrapolate.</p><p>Causal model. A graph representation that is robust (invariant) to shifts in P te (G) must know how the distribution shifts. Either it is given some examples from P te (G) (a.k.a. covariate shift adaptation <ref type="bibr" target="#b122">[124]</ref>, which is not out scenario) or be given a causal structure that describes how the test distribution can shift. Our paper focuses on the latter by giving a Structural Causal Model (SCM) for the data generation process. Figure <ref type="figure">1</ref> depicts the Directed Acyclic Graph (DAG) of our causal model. It uses the twin network DAGs structure first proposed by Balke and Pearl <ref type="bibr" target="#b10">[11]</ref> (see Pearl [97, <ref type="bibr">Chapter 7.1.4]</ref>) in order to define counterfactual queries.</p><p>In what follows we detail the SCM in Definitions 1 and 2. Our causal model is inspired by Stochastic Block Models (SBMs) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b117">119]</ref> and their connection to graphon random graph models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b78">79]</ref>:</p><formula xml:id="formula_0">Definition 1 (Training Graph G tr N tr ).</formula><p>The training graph SCM is depicted at the left side of the twin network DAG in Figure <ref type="figure">1</ref>.</p><p>• The training graph is characterized by a sampled graphon W ∼ P(W ), where W : [0, 1] 2 → [0, 1] is a random symmetric measurable function <ref type="bibr" target="#b78">[79]</ref> sampled (according to some distribution) from D W , the set of all symmetric measurable functions on [0, 1] 2 → [0, 1]. W defines both the graph's target label and some of its structural and attribute characteristics.</p><p>• The training environment E tr ∼ P tr (E) is a hidden environment variable that represents specific graph properties that change between the training and test. E tr ∈ E for some properly defined environment space E.</p><p>• The graph's size is determined by its environment N tr := η(E tr ), where η is an unknown deterministic function.</p><p>• The graph's target label is given by Y := h(W, Z Y ), Y ∈ Y, with Y some properly defined discrete target space. Z Y is an independent random noise variable and h is a deterministic function on the input space D W × R.</p><p>• The vertices are numbered V tr = {1, . . . , N tr }. Each vertex v ∈ V tr has an associated hidden variable U v ∼ Uniform(0, 1) sampled i.i.d.. The graph is undirected and its adjacency matrix A tr ∈ {0, 1} N tr ×N tr is defined by</p><formula xml:id="formula_1">A tr u,v := 1(Z u,v &gt; W (U u , U v )), ∀u, v ∈ V tr , u = v.<label>(1)</label></formula><p>The diagonals are set to 0 because there is no self-loop. Here 1 is an indicator function, and {Z u,v = Z v,u } u,v∈V tr are independent uniform noises on [0, 1].</p><p>• The graph may contain discrete vertex attributes X tr ∈ X N tr defined as</p><formula xml:id="formula_2">X tr v := g X (E tr , W (U v , U v )), ∀v ∈ V tr ,</formula><p>where X tr v ∈ X, and X is some properly defined attribute space. g X is a deterministic function that determines a vertex attribute using W (U v , U v ) ∈ [0, 1] via, say, inverse sampling <ref type="bibr" target="#b128">[130]</ref> the vertex attribute distribution.</p><p>• Then the training graph is described as G tr N tr := (A tr , X tr ).</p><p>The test data comes from the following (coupled) distribution, that is, the model uses some of the same random variables of the training graph model, effectively only replacing E tr by E te , as shown in the DAG of Figure <ref type="figure">1</ref>.</p><p>Definition 2 (Test Graph G te N te ). The SCM of the test graph is given by the right side of the twin network DAG in Figure <ref type="figure">1</ref>, changing the following variables from Definition 1:</p><p>• The test environment E te ∼ P te (E), and E te ∈ E belongs to the same space as E tr . It represents specific properties of the graphs that change between the test and training data. Denote supp(•) := {x|P(x) &gt; 0} as the support of a random variable. The supports of E te and E tr may not overlap (i.e., supp(E te ) ∩ supp(E tr ) = ∅).</p><p>• The change in environment from E tr to E te may change the graph's size as N te := η(E te ), where η is the same unknown deterministic function as in Definition 1.</p><p>• The vertices are numbered V te = {1, . . . , N te }. The adjacency matrix A te ∈ {0, 1} N te ×N te is defined as in Equation (1).</p><p>• The graph may contain discrete vertex attributes X te ∈ X N te defined as</p><formula xml:id="formula_3">X te v := g X (E te , W (U v , U v )), ∀v ∈ V te ,</formula><p>with g X as given in Definition 1.</p><p>• Then the test graph is described as G te N te := (A te , X te ).</p><p>Our SCM has a direct connection with graphon random graph model <ref type="bibr" target="#b78">[79]</ref>, and extend it by considering vertex attributes. Now we introduce examples of our graph classification tasks based on Definitions 1 and 2 using two classic random graph models.</p><p>Notation:</p><formula xml:id="formula_4">(G * N * , E * , A * , V * , X * )</formula><p>In what follows we use the superscript * as a wildcard to describe both train and test random variables. For instance, G * N * is a variable that is a wildcard for referring to either G tr N tr or G te N te . Also, from now on we define P te (G) = P(G te N te ) and P tr (G) = P(G tr N tr ).</p><p>Erdős-Rényi example. Consider a random training environment E tr such that N tr = η(E tr ) is the number of vertices for graphs in our training data. Let p be the probability that any two distinct vertices of the graph have an edge. Define W as a constant function that always outputs p. Sample independent uniform noises Z u,v ∼ Uniform(0, 1) (for each possible edges, Z u,v = Z v,u ). An Erdős-Rényi graph can be defined as a graph whose adjacency matrix A tr is A tr</p><formula xml:id="formula_5">u,v = 1(Z u,v &gt; W (U u , U v )) = 1(Z u,v &gt; p), ∀u, v ∈ V tr , u = v.</formula><p>Here vertex attributes are not considered and can define X tr v = , ∀v ∈ V tr as the null attribute. In the test data, we have a different environment E te and graph sizes N te = η(E te ), with supp(N te ) ∩ supp(N tr ) = ∅. The variable {Z u,v } u,v∈{1,...,max(supp(N tr )∪supp(N te ))} can be thought as the seed of a random number generator to determine if two distinct vertices u and v are connected by an edge. The above defines our training and test data as a set of Erdős-Rényi random graphs of sizes N tr and N te with probability p. The targets of the Erdős-Rényi graphs can be, for instance, the value Y = p in Definition 1, which is determined by W and invariant of graph sizes.</p><p>Stochastic Block Model (SBM) <ref type="bibr" target="#b117">[119]</ref>. A SBM can be seen as a generalization of Erdos-Renyi graphs. SBMs partition the vertex set into disjoint subsets S 1 , S 2 , ..., S r (known as blocks or communities) with an associated r × r symmetric matrix P , where the probability of an edge (u, v), u ∈ S i and v ∈ S j is P ij , for i, j ∈ {1, . . . , r}. In the training and test data, we still have i.i.</p><formula xml:id="formula_6">d sampled Z u,v = Z v,u and different environments E tr , E te . Divide the interval [0, 1] into disjoint convex sets [t 0 , t 1 ), [t 1 , t 2 ), . . . , [t r−1 , t r ], where t 0 = 0 and t r = 1, such that if U v ∼ Uniform(0, 1) satisfies U v ∈ [t i−1 , t i ), then vertex v belongs to block S i . Thus W (U u , U v ) = i,j∈{1,...,r} P ij 1(U u ∈ [t i−1 , t i ))1(U v ∈ [t j−1 , t j )).</formula><p>A SBM graph in train or test can be defined as a graph whose adjacency matrix</p><formula xml:id="formula_7">A * is A * u,v = 1(Z u,v &gt; W (U u , U v )), ∀u, v ∈ V * , u = v. Now</formula><p>we have a set of SBM random graphs of sizes N tr and N te with P . Consider if there are only two blocks, the target Y can be P 1,2 which is the probability of an edge connecting between blocks. It is determined by W and invariant of graph sizes.</p><p>SBM with vertex attributes. For the SBM, assume the vertex attributes are tied to blocks, and are distinct for each block. The environment variable operates on changing the distributions of attributes assigned in each block. Consider the following SBM example with two blocks: Define</p><formula xml:id="formula_8">W (U v , U v ) = Uv 2t1 1(U v ∈ [0, t 1 )) + ( 1 2 + Uv−t1 2(1−t1) )1(U v ∈ [t 1 , 1]). So W (U v , U v ) &lt; 1</formula><p>2 if and only if v belongs to the first block. We only change the values of W for points on a zero-measure space. Let g X be such that it defines constants 0 &lt; α E * ,1 &lt; 1 2 &lt; α E * ,2 &lt; 1, and vertex attributes as</p><formula xml:id="formula_9">X * v = g X (E * , W (U v , U v )) =    1(W (U v , U v ) ∈ [0, α E * ,1 )) 1(W (U v , U v ) ∈ [α E * ,1 , .5)) 1(W (U v , U v ) ∈ [.5, α E * ,2 )) 1(W (U v , U v ) ∈ [α E * ,2 , 1])   ,</formula><p>where the attribute of vertex v, X * v , is one-hot encoded to represent 4 colors: red and blue (if v is in block 1) and green and yellow (if v is in block 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">E-Invariant Graph Representations</head><p>In this section we discuss shortcomings of traditional graph representation methods on out-of-distribution (OOD) graph classification tasks based on our Structural Causal Model (SCM) (described in Definitions 1 and 2 and Figure <ref type="figure">1</ref>), the possible solution to solve this extrapolation problem and an environment-invariant graph representation that is able to extrapolate to OOD test data.</p><p>The shortcomings of standard graph representation methods. Figure <ref type="figure">1</ref> shows that our target variable Y is a function only of the graphon variable W , rather than the training or test environments, E tr and E te , respectively. However, due to the reverse (backdoor) path between Y and E tr through N tr and X tr in the DAG, Y is not independent of E tr given G tr N tr . Hence, traditional graph representation learning methods can pick-up this correlation in the training data, which would prevent the model learning the correct OOD test predictor P(Y |G te N te ) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b110">112,</ref><ref type="bibr" target="#b32">33]</ref>. To address the challenge of correctly predicting Y in our OOD test data, regardless of backdoor paths, we need an estimator that can account for it. In our SCM, we can apply Pearl's backdoor adjustment [97, Theorem 3.3.2]:</p><formula xml:id="formula_10">P(Y |G te N te ) = e † ∈supp(E te ) P(Y |G tr N tr , E tr = e † )P(E te = e † ).<label>(2)</label></formula><p>There are, however, two big challenges to applying Equation ( <ref type="formula" target="#formula_10">2</ref>): (a) We do not know P(E te = e) and, most importantly, (b) the test support may not be a subset of the train support, i.e., supp(E te ) ⊆ supp(E tr ).</p><p>Hence, in what follows we focus on environment-invariant (E-invariant) graph representations Γ(•), such that ∀e ∈ supp(E tr ), ∀e † ∈ supp(E te ), P(Y |Γ(G tr N tr ), E tr = e) = P(Y |Γ(G te N te ), E te = e † ), which eliminates the need for conditioning on the environments in Equation <ref type="bibr" target="#b1">(2)</ref>. For this, we first introduce the effect of E-invariant representations on downstream classification tasks. Proposition 1. [E-invariant Representation's Effect on Classification] Consider a permutation-invariant graph representation Γ :</p><formula xml:id="formula_11">∪ ∞ n=1 {0, 1} n×n × X n → R d , d ≥ 1</formula><p>, and a downstream function ρ : Y × R d → [0, 1] (e.g., a feedforward neural network (MLP) with softmax outputs) such that, for some , δ &gt; 0, the generalization error over the training distribution is: For ∀y ∈ Y,</p><formula xml:id="formula_12">P( |P(Y = y|G tr N tr ) − ρ(y, Γ(G tr N tr ))| ≤ ) ≥ 1 − δ, Γ is said to be environment-invariant (E-invariant) if ∀e ∈ supp(E tr ), ∀e † ∈ supp(E te ), Γ(G tr N tr |E tr = e) = Γ(G te N te |E te = e † ).</formula><p>Then, the OOD test error is the same as the generalization error over the training distribution, i.e., for ∀y ∈ Y, In what follows we leverage the stability of subgraph densities (more precisely, induced homomorphism densities) in graphon random graph models <ref type="bibr" target="#b78">[79]</ref> to learn E-invariant representations for the SCM defined in Definitions 1 and 2, whose DAG is illustrated in Figure <ref type="figure">1</ref>.</p><formula xml:id="formula_13">P(|P(Y = y|G te N te ) − ρ(y, Γ(G te N te ))| ≤ ) ≥ 1 − δ.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Approximately E-Invariant Graph Representations for Our Model</head><p>Let G * N * denote either an N tr -sized train or N te -sized test graph from the SCM in Definitions 1 and 2. For a given k-vertex graph F k (k &lt; N * ), let ind(F k , G * N * ) be the number of induced homomorphisms of F k into G * N * , informally, the number of mappings from V (F k ) to V (G * N * ) such that the corresponding subgraph induced in G * N * is isomorphic to F k . The induced homomorphism density is defined as</p><formula xml:id="formula_14">t ind (F k , G * N * ) = ind(F k , G * N * ) N * !/(N * − k)! ,<label>(4)</label></formula><p>where the denominator is the number of possible mappings. Let F ≤k be the set of all connected vertex-attributed graphs of size k ≤ k. Using the subgraph densities (induced homomorphism densities) {t ind (F k , G * N * )} F k ∈F ≤k we will construct a (feature vector) representation for G * N * , similar to Hancock and Khoshgoftaar <ref type="bibr" target="#b51">[52]</ref>, Pinar et al. <ref type="bibr" target="#b98">[100]</ref>,</p><formula xml:id="formula_15">Γ 1-hot (G * N * ) = F k ∈F ≤k t ind (F k , G * N * )1 one-hot {F k , F ≤k },<label>(5)</label></formula><p>where 1 one-hot {F k , F ≤k } assigns a unique one-hot vector to each distinct graph F k in F ≤k . For instance, for k = 4, the one-hot vectors could be (1,0,. . . ,0)= , (0,1,. . . ,0)= , (0,0,. . . ,1,. . . ,0)= , (0,0,. . . ,1)= , etc.. In Section 3.2 we show that the (feature vector) representation in Equation ( <ref type="formula" target="#formula_15">5</ref>) is approximately environment-invariant in our SCM model.</p><p>An alternative approach is to replace the one-hot vector representation with learnable graph representation models. We first use Graph Neural Networks (GNNs) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b145">147]</ref> to learn representations that can capture information from vertex attributes. Simply speaking, GNNs proceed by vertices passing messages, amongst each other, through a learnable function such as an MLP, and repeating L ∈ Z ≥1 layers.</p><p>Consider the following simple GNN example. Let V * be the set of vertices. At each iteration l ∈ {1, 2, . . . , L}, all vertices v ∈ V * are associated with a learned vector h (l)</p><p>v . Specifically, we begin by initializing a vector as h (0) v = X v for every vertex v ∈ V * . Then, we recursively compute an update such as the following ∀v ∈ V * ,</p><formula xml:id="formula_16">h (l) v = MLP (l) h (l−1) v , READOUT Neigh ((h (l−1) u ) u∈N(v) ) ,<label>(6)</label></formula><p>where N (v) ⊆ V * denotes the neighborhood set of v in the graph, READOUT Neigh is a permutation-invariant function (e.g. sum) of the neighborhood learned vectors, MLP (l) denotes a multi-layer perceptron and whose superscript l indicates that the MLP at each recursion layer may have different learnable parameters. There are other alternatives to Equation ( <ref type="formula" target="#formula_16">6</ref>) that we will also test in our experiments.</p><p>Then, we arrive to the following representation of G * N * :</p><formula xml:id="formula_17">Γ GNN (G * N * ) = F k ∈F ≤k t ind (F k , G * N * )READOUT Γ (GNN(F k )),<label>(7)</label></formula><p>where READOUT Γ is a permutation-invariant function that maps the vertex-level outputs of a GNN to a graph-level representation (e.g. by summing all vertex embeddings). Unfortunately, GNNs are not most-expressive representations of graphs <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b140">142]</ref> and thus Γ GNN (•) is less expressive than Γ 1-hot (•) in generalization over the training distribution.</p><p>A representation with greater expressive power is</p><formula xml:id="formula_18">Γ GNN + (G * N * ) = F k ∈F ≤k t ind (F k , G * N * )READOUT Γ (GNN + (F k )),<label>(8)</label></formula><p>where GNN + is a most-expressive k -vertex graph representation, which can be achieved by any of the methods of Vignac et al. <ref type="bibr" target="#b132">[134]</ref>, Maron et al. <ref type="bibr" target="#b80">[81]</ref>, Murphy et al. <ref type="bibr" target="#b88">[89]</ref>. Since GNN + is most expressive, GNN + can ignore attributes and map each F k to a one-hot vector 1 one-hot {F k , F ≤k }; therefore, Γ GNN + (•) generalizes Γ 1-hot (•) of Equation <ref type="bibr" target="#b4">(5)</ref>. But note that greater expressiveness does not imply better extrapolation.</p><p>More importantly, GNN and GNN + representations allow us to increase their E-invariance by adding a penalty for having different representations of two graphs F k and H k with the same topology but different vertex attributes (say, F k = and H k = ), as long as these differences do not significantly impact downstream model accuracy in the training data. We will discuss more about the theoretical underpinnings in the next section. Hence, for each k -sized vertex-attributed graph F k , we consider the set H(F k ) of all k -sized vertex-attributed graphs having the same underlying topology as F k but with all possible different vertex attributes. We then define the regularization penalty</p><formula xml:id="formula_19">1 |F ≤k | F k ∈F ≤k E H k ∈H(F k ) READOUT Γ (GNN * (F k )) − READOUT Γ (GNN * (H k )) 2 ,<label>(9)</label></formula><p>where GNN * = GNN if we choose the representation Γ GNN , or GNN * = GNN + if we choose the representation Γ GNN + . In practice, we assume H k is uniformly sampled from H(F k ) and we sample one H k for each F k in order to obtain an unbiased estimator of Equation ( <ref type="formula" target="#formula_19">9</ref>).</p><p>Practical considerations. Efficient algorithms exist to obtain induced homomorphism densities over all possible connected k-vertex subgraphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b104">106,</ref><ref type="bibr" target="#b135">137]</ref>. For unattributed graphs and k ≤ 5, we use ESCAPE <ref type="bibr" target="#b98">[100]</ref> to obtain exact densities. For attributed graphs or unattributed graphs with k &gt; 5, exact counting becomes intractable, so we use R-GPM <ref type="bibr" target="#b124">[126]</ref> to obtain unbiased estimates of densities. Finally, Proposition 2 in Appendix C shows that certain biased estimators can be used if READOUT Γ is the sum of vertex embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Theoretical Description of our E-Invariant Graph Representations</head><p>In this section, we show that the graph representations seen in the previous section are approximately environmentinvariant in our SCM model under slight assumptions. Theorem 1 (Approximate E-invariant Graph Representation). Let G tr N tr and G te N te be two samples of graphs of sizes N tr and N te from the training and test distributions, respectively, both defined over the same graphon variable W and satisfying Definitions 1 and 2. Assume the vertex attribute function g X (•) of Definitions 1 and 2 is invariant to E tr and E te (the reason for this assumption will be clear later). Let || • || ∞ denotes the L-infinity norm. For any integer k ≤ min(N tr , N te ), and any constant 0 &lt; &lt; 1,</p><formula xml:id="formula_20">P( Γ 1-hot (G tr N tr ) − Γ 1-hot (G te N te ) ∞ &gt; ) ≤ 2|F ≤k |(exp(− 2 N tr 8k 2 ) + exp(− 2 N te 8k 2 )).<label>(10)</label></formula><p>Theorem 1 shows how the graph representations given in Equation ( <ref type="formula" target="#formula_15">5</ref>) are approximately E-invariant. Note that for unattributed graphs, we can define g X (•, •) = as the null attribute, which is invariant to any environment by construction. For graphs with attributed vertices, g X (•, •) being invariant to E tr and E te means for any two environment e ∈ supp(E tr ), e † ∈ supp(E te ), g X (e, •) = g X (e † , •).</p><p>Theorem 1 shows that for k min(N tr , N te ), the representations Γ 1-hot (•) of two possibly different-sized graphs with the same W are nearly identical, indicating Γ 1-hot (G * N * ) is an approximately E-invariant representation. Theorem 1 also exposes a trade-off, however. If the observed graphs tend to be relatively small, the required k for approximately E-invariant representations can be small, and then the expressiveness of Γ 1-hot (•) gets compromised. That is, the ability of Γ 1-hot (G * N * ) to extract information about W from G * N * reduces as k decreases. Finally, this guarantees that for appropriate k, passing the representation Γ 1-hot (G * N * ) to a downstream classifier provably approximates the classifier in Equation ( <ref type="formula" target="#formula_13">3</ref>) of Proposition 1.</p><p>Note that when the vertex attributes are not invariant to the environment variable, Γ 1-hot (•) is not E-invariant and we can not extrapolate using Γ 1-hot (•). Thankfully, for the GNN-based graph representations Γ GNN (G * N * ) and Γ GNN + (G * N * ) in Equations ( <ref type="formula" target="#formula_17">7</ref>) and ( <ref type="formula" target="#formula_18">8</ref>), respectively, the regularization penalty in Equation ( <ref type="formula" target="#formula_19">9</ref>) pushes the representation of vertex attributes to be more E-invariant, making it more likely to satisfy the conditions of E-invariance in Theorem 1. In particular, consider the attributed SBM example in Section 2, the environment operates on changing the distributions of attributes assigned in each block. If we are going to predict cross-block edge probabilities (see Section 5.2), we need the representations to merge attributes that are assigned to the same block to achieve E-invariance. By regularizing the GNN-based graph representation towards merging the representations of different vertex attributes, we can get an approximately E-invariant representation in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>This section presents an overview of the related work. Due to space constraints, a more in-depth discussion with further references is given in Appendix E. In particular, Appendix E gives a detailed description of environment-invariant methods that require multiple environments in training, including Independence of Causal Mechanism (ICM), Causal Discovery from Change (CDC), and representation disentanglement methods. None of these works focus on graphs.</p><p>OOD extrapolation in graph classification and size extrapolation in GNNs. Our work ascertain a causal relationship between graphs and their target labels. We are unaware of existing work on this topic. Xu et al. <ref type="bibr" target="#b141">[143]</ref> is interested on a geometric (non-causal) definition of extrapolation for a class of graph algorithms. Hu et al. <ref type="bibr" target="#b56">[57]</ref> introduces a large graph dataset presenting significant challenges of OOD extrapolation, however, their shift is on the two-dimensional structural framework distribution of the molecules, and no causal model is provided. The parallel work of Yehudai et al. <ref type="bibr" target="#b144">[146]</ref> improves size extrapolation in GNNs using self-supervised and semi-supervised learning on both the training and test domain, which is orthogonal to our problem. Previous works also examine empirically the ability of graph neural networks to extrapolate in various applications, such as physics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b106">108]</ref>, mathematical and abstract reasoning <ref type="bibr" target="#b107">[109,</ref><ref type="bibr" target="#b109">111]</ref>, and graph algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b131">133]</ref>. These works do not provide guarantees of test extrapolation performance, a causal model, or a proof that the tasks require extrapolation over different environments.</p><p>Graph classification using induced homomorphisms. A related set of works look at induced homomorphism densities as graph features for a kernel <ref type="bibr" target="#b114">[116,</ref><ref type="bibr" target="#b142">144,</ref><ref type="bibr" target="#b133">135]</ref>. These methods can perform poorly in some tasks <ref type="bibr" target="#b68">[69]</ref>. Recent work has also shown an interest in induced subgraphs, which are used to improve predictions of GNNs <ref type="bibr" target="#b21">[22]</ref> or as input for newly-proposed architectures <ref type="bibr" target="#b127">[129]</ref>. None of these methods focus on invariant representations or extrapolations.</p><p>Expressiveness of graph representations. The expressiveness of a graph representation method is a measure of model family bias <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b140">142,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b88">89]</ref>. That is, given enough training data, a neural network from a more expressive family can achieve smaller generalization error over the training distribution than a neural network from a less expressive family, assuming appropriate optimization. However, this power is a measure of generalization capability over the training distribution, not OOD extrapolation. Hence, the question of representation expressiveness is orthogonal to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Results</head><p>This section is dedicated to the empirical evaluation of our theoretical claims, including the ability of the representations in Equations ( <ref type="formula" target="#formula_15">5</ref>), ( <ref type="formula" target="#formula_17">7</ref>) and ( <ref type="formula" target="#formula_18">8</ref>) to extrapolate as predicted by Proposition 1 for tasks that abide by Definitions 1 and 2. Due    to space constraints, our results are summarised here, while further details are relegated to Appendix F. Our code is available <ref type="foot" target="#foot_0">1</ref> .</p><formula xml:id="formula_21">TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] PNA 0.</formula><formula xml:id="formula_22">TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] TRAIN [P (Y, G TR N TR )] VAL. [P (Y, G TR N TR )] TEST (↑) [P (Y, G TE N TE )] PNA 1.</formula><p>We explore the extrapolation power of Γ 1-hot , Γ GIN and Γ RPGIN of Equations ( <ref type="formula" target="#formula_15">5</ref>), ( <ref type="formula" target="#formula_17">7</ref>) and (8) using the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b140">[142]</ref> as our base GNN model, and Relational Pooling GIN (RPGIN) <ref type="bibr" target="#b88">[89]</ref> as a more expressive GNN. The graph representations are then passed to a L-hidden layer feedforward neural network (MLP) with softmax outputs that give the predicted classes, L ∈ {0, 1}. As described in Section 3.1, we obtain induced homomorphism densities of connected graphs. For practical reasons, we focus only on densities of graphs of size exactly k, which is treated as a hyperparameter.</p><p>Baselines. Our baselines include the Graphlet Counting kernel (GC Kernel) <ref type="bibr" target="#b114">[116]</ref>, which uses the Γ 1-hot representation as input to a downstream classifier. We report Γ 1-hot separately from GC Kernel since Γ 1-hot differs from GC Kernel in that we add the same feedforward neural network (MLP) classifier used in the Γ GNN model. We also include GIN <ref type="bibr" target="#b140">[142]</ref>, GCN <ref type="bibr" target="#b64">[65]</ref> and PNA <ref type="bibr" target="#b29">[30]</ref>, considering the sum, mean, and max READOUTs as proposed by Xu et al. <ref type="bibr" target="#b141">[143]</ref> for extrapolations (which we denote as XU-READOUT to not confuse with our READOUT Γ ). We also examine a more-expressive GNN, RPGIN <ref type="bibr" target="#b88">[89]</ref>, and the WL Kernel <ref type="bibr" target="#b115">[117]</ref>. We do not use the method of Yehudai et al. <ref type="bibr" target="#b144">[146]</ref> as a baseline since it is a covariate shift adaptation approach that requires samples from P(G te N te ), which are not available in our setting. In our experiments we perform early stopping as per Hu et al. <ref type="bibr" target="#b56">[57]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Size extrapolation tasks for unattributed graphs</head><p>Schizophrenia task. We use the fMRI brain graph data on 71 schizophrenic patients and 74 controls for classifying individuals with schizophrenia <ref type="bibr" target="#b31">[32]</ref>. Vertices represent brain regions (voxels) with edges as functional connectivity. We process the graph differently between training and test data, where training graphs have exactly 264 vertices (a single environment) and control-group graphs in test have around 40% fewer vertices. We employ a 5-fold cross-validation for hyperparameter tuning.</p><p>Erdős-Rényi task. We simulate Erdős-Rényi graphs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38]</ref> as a simple graphon random graph model. The task is to classify the edge probability p ∈ {0.2, 0.5, 0.8} of the generated graph. First we consider a single-environment version of the task, where we train and validate on graphs of size 80 and extrapolate to graphs with size 140 in test. We also consider another experiment with training/validation graph sizes uniformly selected from {70, 80} (so we can use IRM), with the test data same as before (graphs of size 140 in test).</p><p>Results. Table <ref type="table" target="#tab_2">1</ref> shows that all methods perform well in validation (generalization over the training distribution). However, only Γ 1-hot (GC Kernel and our simple classifier), Γ GIN , Γ RPGIN are able to extrapolate, while displaying very similar -often identical-accuracies in validation (sampled from P(G tr N tr )) and test (sampled from P(G te N te )) in all experiments, as predicted by combining the theoretical results in Proposition 1 and Theorem 1. Using IRM in the Erdős-Rényi task shows no improvement over not using IRM in the multi-environment setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Size/attribute extrapolation for attributed graphs</head><p>We now define a Stochastic Block Model (SBM) task with vertex attributes. The SBM has two blocks. Our goal is to classify the cross-block edge probability P 1,2 = P 2,1 ∈ {0.1, 0.3} of a sampled graph. Vertex attribute distributions depend on the blocks. In block 1 vertices are randomly assigned red and blue attributes, while in block 2 vertices are randomly assigned green and yellow attributes (see SBM with vertex attributes in Section 2).</p><p>The change in environments between training and test introduces a joint attribute-and-size distribution shift: In training, the vertices are 90% red (resp. green) and 10% blue (resp. yellow) in block 1 (resp. block 2). While in test, the distribution is flipped and vertices are 10% red (resp. green) and 90% blue (resp. yellow) in block 1 (resp. block 2). We consider three scenarios, with the same test data made of graphs of size 40: (a) A single-environment case, where all training graphs have size 20; (b) A multi-environment case, where training graphs have sizes 14 and 20; (c) A multi-environment case, where training graphs have sizes 20 and 30. These differences in training data will check whether having graphs of sizes closer to the test graph sizes improves the performance of traditional graph representation methods.</p><p>Results. Table <ref type="table" target="#tab_4">2</ref> shows how traditional graph representations and Γ 1-hot (both GC Kernel and our neural classifier) taps into the easy correlation between Y and the density of red and green vertex attributes in the training graphs, while Γ GIN and Γ RPGIN , with their attribute regularization (Equation ( <ref type="formula" target="#formula_19">9</ref>)), are approximately E-invariant, resulting in higher test accuracy that more closely matches their validation accuracy. Moreover, applying IRM has no beneficial impact, while adding larger graphs in training (closer to test graph sizes) increases the extrapolation accuracy of most methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments with real-world datasets that violate our causal model</head><p>Finally, we test our E-invariant representations on datasets that violate Definitions 1 and 2 and the conditions of Theorem 1. We consider four vertex-attributed datasets (NCI1, NCI109, DD, PROTEINS) from Morris et al. <ref type="bibr" target="#b86">[87]</ref>, and split the data as proposed by Yehudai et al. <ref type="bibr" target="#b144">[146]</ref>. As mentioned earlier, Yehudai et al. <ref type="bibr" target="#b144">[146]</ref> is not part of our baselines since it requires samples from the test distribution P(G te N te ). Training and test data are created as follows: Graphs with sizes smaller than the 50-th percentile are assigned to training, while graphs with sizes larger than the 90-th percentile are assigned to test. A validation set for hyperparameter tuning consists of 10% held out examples from training.</p><p>Results. Table <ref type="table" target="#tab_6">3</ref> shows the test results using the Matthews correlation coefficient (MCC) -MCC was chosen due to significant class imbalances in the OOD shift of our test data, see Appendix F for more details. We observe that always one of our E-invariant representations Γ GIN and Γ RPGIN is amongst the top 4 best methods in all datasets except NCI109. We also note that the WL KERNEL performs really well at NCI1 and very poorly (random) on PROTEINS and DD, showcasing the importance of consistency across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work we looked at the task of out-of-distribution (OOD) graph classification, where train and test data have different distributions. By introducing a structural causal model inspired by graphon models <ref type="bibr" target="#b78">[79]</ref>, we defined a representation that is approximately invariant to the train/test distribution changes of our causal model, empirically showing its benefits on both synthetic and real-world datasets against standard graph classification baselines. Finally, our work contributed a blueprint for defining graph extrapolation tasks through causal models.</p><p>Here W is the graphon function as illustrated in Definitions 1 and 2. As defined in Lovász and Szegedy <ref type="bibr" target="#b78">[79]</ref>,</p><formula xml:id="formula_23">t(F k , W ) = [0,1] k ij∈E(F k ) W (x i , x j )dx 1 • • • dx k ,</formula><p>where E(F k ) denotes the edge set of F k . This bound shows that t inj (F k , G * N * ) converges to t(F k , W ) as N * → ∞. Actually, we can get similar bounds for t ind using the similar proof technique. Although the value it converges to is different, the difference between values is preserved as will be proved in the following text. More importantly, it can be extended to vertex-attributed graphs under our SCM assumptions depicted in Definitions 1 and 2.</p><p>We can do this because for vertex-attributed graphs, in Definitions 1 and 2, g X operates on attributed graphs similarly as the graphon on unattributed graphs. We can consider the graph generation procedure as first generate the underlying structure, and then add vertex attribute accordingly to its corresponding random graphon value ∈ Uniform(0, 1) and graphon W . g X (•, •) being invariant to E tr and E te means for any two environment e ∈ supp(E tr ), e † ∈ supp(E te ), g X (e, •) = g X (e † , •).</p><p>Then for a given k-vertices vertex-attributed graph F k and a given N * , we can define φ as an induced map φ :</p><formula xml:id="formula_24">[k] → [N * ],</formula><p>which can be thought about as how the k vertices in F k are mapped to the vertices in G * N * . A φ denotes the event φ is a homomorphism from F k to the W -random graph G * N * . We define G * m as the subgraph of G * N * induced by vertices {1, ..., m}. Note here m has two meanings. First, it represents the m-th vertex. Second, it also indicates the size of the subgraph. We define</p><formula xml:id="formula_25">B m = 1 ( N * k ) φ P(A φ |G * m )</formula><p>, 0 ≤ m ≤ N * as the expected induced homomorphism densities once we observe the subgraph G * m . Here B 0 = 1 ( N * k ) φ P(A φ ) denotes the expectation before we observe any vertices. B m is a martingale for unattributed graphs (Theorem 2.5 of <ref type="bibr" target="#b78">[79]</ref>). And since in Definitions 1 and 2 we also use the graphon W and the g X operates on attributed graphs using the graphon W and U v , it is also a martingale here. We do not need to care about the environment variable E * here because the function g X is invariant of E * and, therefore, it can be treated as a constant. Then,</p><formula xml:id="formula_26">|B m − B m−1 | = 1 N * k | φ P(A φ |G * m ) − P(A φ |G * m−1 )| ≤ 1 N * k φ |P(A φ |G * m ) − P(A φ |G * m−1 )|.</formula><p>Here, for all φ : [k] → [N * ] that does not contain the value m in its image (which means no vertex in F k is mapped to the m-th vertex in G * N * ) , the difference is 0. For all other terms, the terms are at most 1. Thus,</p><formula xml:id="formula_27">|B m − B m−1 | ≤ N * −1 k−1 N * k = k n . By definition, B 0 = 1 ( N * k ) φ P(A φ ) = t * (F k , W ),<label>and</label></formula><formula xml:id="formula_28">B N * = 1 ( N * k ) ind(F k , G * N * ) = t ind (F k , G * N * )</formula><p>, where t * (F k , W ) is defined as B 0 -the expected induced homomorphism densities if we only know the graphon W and did not observe any vertex in the graph.</p><p>Then, we can use Azuma's inequality for Martingales,</p><formula xml:id="formula_29">P(B N * − B 0 &gt; ) ≤ exp(− 2 2N * (k/N * ) 2 ) = exp(− 2 2k 2 N * ). Since B N * = t ind (F k , G * N * )</formula><p>, and B 0 = t * (F k , W ), we get the similar bound as in Equation <ref type="bibr" target="#b11">(12)</ref>,</p><formula xml:id="formula_30">P(|t ind (F k , G * N * ) − t * (F k , W )| &gt; ) ≤ 2 exp(− 2 2k 2 N * ). Since |t ind (F k , G tr N tr ) − t * (F k , W )| ≤ 2 , |t ind (F k , G te N te ) − t * (F k , W )| ≤ 2 implies |t ind (F k , G tr N tr ) − t ind (F k , G te N te )| ≤ , we have, P(|t ind (F k , G tr N tr ) − t ind (F k , G te N te )| &gt; ) = 1 − P(|t ind (F k , G tr N tr ) − t ind (F k , G te N te )| ≤ ) ≤ 1 − P(|t ind (F k , G tr N tr ) − t * (F k , W )| ≤ 2 ) • P(|t ind (F k , G te N te ) − t * (F k , W )| ≤ 2 ) ≤ 1 − (1 − 2 exp(− 2 8k 2 N tr ))(1 − 2 exp(− 2 8k 2 N te )) = 2(exp(− 2 8k 2 N tr ) + exp(− 2 8k 2 N te )) − 4 exp(− 2 8k 2 (N tr + N te )) ≤ 2(exp(− 2 8k 2 N tr ) + exp(− 2 8k 2 N te )).<label>(13)</label></formula><p>Then we know,</p><formula xml:id="formula_31">P(||Γ 1-hot (G tr N tr ) − Γ 1-hot (G te N te )|| ∞ ≤ ) = P(|t ind (F k , G tr N tr ) − t ind (F k , G te N te )| ≤ , ∀F k ∈ F ≤k ) ≥ 1 − F k ∈F ≤k P(|t ind (F k , G tr N tr ) − t ind (F k , G te N te )| &gt; ) ≥ 1 − 2|F ≤k |(exp(− 2 N tr 8k 2 ) + exp(− 2 N te 8k 2 )).<label>(14)</label></formula><p>It follows from the Bonferroni inequality that</p><formula xml:id="formula_32">P(∩ N i=1 A i ) ≥ 1 − N i=1 P( Ãi )</formula><p>, where A i and its complement Ãi are any events. Therefore,</p><formula xml:id="formula_33">P(||Γ 1-hot (G tr N tr ) − Γ 1-hot (G te N te )|| ∞ &gt; ) ≤ 2|F ≤k |(exp(− 2 N tr 8k 2 ) + exp(− 2 N te 8k<label>2</label></formula><p>)), concluding the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Biases in estimating induced homomorphism densities</head><p>Induced (connected) homomorphism densities of a given graph F k over all possible k -vertex (k ≤ k) connected graph for an N * -vertex graph G * N * are defined as</p><formula xml:id="formula_34">ω(F k , G * N * ) = ind(F k , G * N * ) F k ∈F ≤k ind(F k , G * N * )</formula><p>. This is a slightly different definition from the induced homomorphism densities in Equation ( <ref type="formula" target="#formula_14">4</ref>). In the main text, the denominator is the total number of possible mappings (which can include mappings that are disconnected). Here we mainly consider the total numbers of induced mappings that are connected.</p><p>Achieving unbiased estimates for induced (connected) homomorphism densities usually requires sophisticated methods and enormous amount of time. We show that a biased estimator can also work for the GNN + in Equation ( <ref type="formula" target="#formula_18">8</ref>) if the bias is multiplicative and the READOUT Γ is simply the sum of the vertex embeddings. We formalize it as followed.</p><formula xml:id="formula_35">Proposition 2. Assume ω(F k , G * N * ) is a biased estimator for ω(F k , G * N * ) for any k and k -sized connected graphs F k in a N * -vertex G * N * , such that E(ω(F k , G * N * )) = β(F k )ω(F k , G * N * ), where β(F k ) (β(•) &gt; 0)</formula><p>is the bias related to the graph F k , and the expectation is over the sampling procedure. The expected learned representation</p><formula xml:id="formula_36">E( F k ∈F ≤k ω(F k , G * N * )1 T (GNN + (F k ))</formula><p>) can be the same as using the true induced (connected</p><formula xml:id="formula_37">) homomorphism densities ω(F k , G * N * ), ∀F k ∈ F ≤k .</formula><p>Proof. W.L.O.G, assume GNN + 0 (F k ) is the representation we can learn from the true induced (connected) homomorphism densities ω(F k , G * N * ), ∀F k ∈ F ≤k . When only using the biased estimators, if we are able to learn the representation GNN + (F k ) = GNN + 0 (F k )/β(F k ) for all F k ∈ F ≤k , then we can still get the graph representation in Equation ( <ref type="formula" target="#formula_18">8</ref>) the same as using the true induced (connected) homomorphism densities. This is possible because GNN + is proven to be a most expressive k -vertex graph representation, thus it is able to learn any function on the graph F k . Then,</p><formula xml:id="formula_38">E   F k ∈F ≤k ω(F k , G * N * )1 T (GNN + (F k ))   = F k ∈F ≤k ω(F k , G * N * )1 T (GNN + 0 (F k )),<label>(15)</label></formula><p>where 1 T (GNN + (F k )) is the sum of the vertex embeddings given by the GNN + if it is an equivariant representation of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Review of Graph Neural Networks</head><p>Graph Neural Networks (GNNs) constitute a popular class of methods for learning representations of vertices in a graph or graph-wide representations <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b130">132,</ref><ref type="bibr" target="#b140">142,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b145">147,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b23">24]</ref>. We will explain the general case of learning graph-wide representations but the idea applies straightforwardly to applying GNNs to connected induced subgraphs in a larger graph to learn their latent representations. That is, in our work, we have applied GNNs to connected induced subgraphs in a graph, and then aggregated them to obtain the representation of the graph. We briefly summarize the idea, but more details can be found in texts such as by Hamilton <ref type="bibr" target="#b50">[51]</ref> and reviews by Wu et al. <ref type="bibr" target="#b138">[140]</ref> and Zhang et al. <ref type="bibr" target="#b148">[150]</ref> and the references therein.</p><p>Suppose we have a graph G with vertex set V = {1, 2, . . . , N }, and each vertex in our data may carry some vertex feature (also called an attribute). For instance, in a molecule, vertices may represent atoms, edges may represent bonds, and features may indicate the atomic number <ref type="bibr" target="#b35">[36]</ref>. These vertex features can be stored in an N × d matrix X, where d is the dimension of the vertex feature vector. In particular, row v ∈ V of X v holds the attribute associated with vertex v.</p><p>Simply speaking, GNNs proceed by vertices passing messages, amongst each other, passing these through a learnable function such as an MLP, and repeating T ∈ Z ≥1 times. At each iteration t = {1, 2, . . . , T }, all vertices v ∈ V are associated with a learned vector h (t) . Specifically, we begin by initializing a vector as h (0) v = X v for every vertex v ∈ V . Then, we recursively compute an update such as the following</p><formula xml:id="formula_39">h (t) v = MLP (t) h (t−1) v , u∈N (v) h (t−1) u , ∀v ∈ V,<label>(16)</label></formula><p>where N (v) ⊆ V denotes the neighborhood set of v in the graph, MLP (t) denotes a multi-layer perceptron, and whose superscript t indicates that the MLP at each recursion layer may have different learnable parameters. We can replace the summation with any permutation-invariant function of the neighborhood. We see that GNNs recursively update vertex states with states from their neighbors and their state from the previous recursion layer. Additionally, we can sample from the neighborhood set rather than aggregating over every neighbor. Generally speaking there is much research into the variations of this recursion step and we refer the reader to aforementioned references for details.</p><p>To learn a graph representation, we can aggregate the vertex representations using a so-called READOUT function defined to be permutation-invariant over the labels. A graph representation h G by a GNN is thus</p><formula xml:id="formula_40">h G = READOUT h (t) v v,t∈V ×{1...,T }</formula><p>where the vertex features h</p><formula xml:id="formula_41">(t)</formula><p>v are as in Equation ( <ref type="formula" target="#formula_39">16</ref>). READOUT may or may not contain learnable weights. We denote it as XU-READOUT to not confuse with our notation READOUT Γ .</p><p>The entire function is differentiable and can be learned end-to-end. These models are thus typically trained with variants of Stochastic Gradient Descent. In our work, we apply this scheme over connected induced subgraphs in the graph, making them a differentiable module in our end-to-end representation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Further Related Work</head><p>This section provides a more in-depth discussion placing our work in the context of existing literature. We explain why existing state-of-the-art graph learning methods will struggle to extrapolate, subgraph methods, and explore perspectives of causality and extrapolation at large as well as in the context of graph classification.</p><p>Causal reasoning and invariances. Recent efforts have brought counterfactual inference to machine learning models, including Independence of causal mechanism (ICM) methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">103,</ref><ref type="bibr" target="#b110">112]</ref>, Causal Discovery from Change (CDC) methods <ref type="bibr" target="#b126">[128]</ref>, and representation disentanglement methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b75">76]</ref>. Invariant risk minimization (IRM) <ref type="bibr" target="#b6">[7]</ref> is a type of ICM <ref type="bibr" target="#b110">[112]</ref>. Broadly, these efforts look for representations (or mechanism descriptions) that are invariant across multiple environments observed in the training data. In our work, we are interested in techniques that can work with a single training environment and when the test support is not a subset of the train support-a common case in graph data. Moreover, these works are not specifically designed for graphs, and it is unclear how they can be efficiently adapted for graph tasks. To the best of our knowledge there is no clear effort for counterfactual graph extrapolations from a single environment.</p><p>Extrapolation. Geometrically, extrapolation can be thought as reasoning beyond a convex hull of a set of training points <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b141">143]</ref>. However, for neural networks-and their arbitrary representation mappings-this geometric interpretation is insufficient to describe a truly broad range of tasks. Rather, extrapolations are better described through counterfactual reasoning <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b105">107,</ref><ref type="bibr">97,</ref><ref type="bibr" target="#b110">112]</ref>.</p><p>There are other approaches for conferring models with extrapolation abilities. These ideas have started to permeate graph literature, which we touch on here, but remain outside the scope of our systematic counterfactual modeling framework.</p><p>Incorporating domain knowledge is an intuitive approach to learn a function that predicts adequately outside of the training distribution, data collection environment, and heuristic curation. This has been used, for example, in time series forecasting <ref type="bibr" target="#b111">[113,</ref><ref type="bibr" target="#b7">8]</ref>. This can come in the form of re-expressing phenomena in a way that can be adequately and accurately represented by machine learning methods <ref type="bibr" target="#b71">[72]</ref> or specifically augmenting existing general-purpose methods to task <ref type="bibr" target="#b66">[67]</ref>. In the context of graphs, it has been used to pre-process the graph input to make a learned graph neural network model a less complex function and thus extend beyond training data <ref type="bibr" target="#b141">[143]</ref>, although this does not necessarily fall into the framework we consider here.</p><p>Another way of moving beyond the training data is robustness. Relevant for deep learning systems are adversarial attacks <ref type="bibr" target="#b94">[95]</ref>. Neural networks can be highly successful classifiers on the training data but become wildly inaccurate with small perturbations of those training examples <ref type="bibr" target="#b43">[44]</ref>. This is important, say, in self-driving cars <ref type="bibr" target="#b116">[118]</ref>, which can become confused by graffiti. This becomes particularly problematic when we deploy systems to real-world environments outside the training data. Learning to defend against adversarial attacks is in a way related to performing well outside the environment and curation heuristics encountered in training. An interesting possibility for future work is to explore the relationships between the two approaches.</p><p>Overfitting will compromise even generalization (interpolation). Regularization schemes such as explicit penalization are a well known and broadly applicable strategy <ref type="bibr" target="#b52">[53]</ref>. Another implicit approach is data augmentation <ref type="bibr" target="#b54">[55]</ref>, and the recent GraphCrop method proposes a scheme for graphs that randomly extracts subgraphs from certain graphs in a minibatch during training <ref type="bibr" target="#b136">[138]</ref>. These directions differ from our own in that we seek a formulation for extrapolation even when overfitting is not necessarily a problem but the two approaches are both useful in the toolbox of an analyst.</p><p>We would like to point out that representation learning on dynamic graphs <ref type="bibr" target="#b61">[62]</ref>, including tasks like link prediction on growing graphs <ref type="bibr" target="#b5">[6]</ref>, is a separate vein of work from what we consider here. In these scenarios, there is a direct expectation that the process we model will change and evolve. For instance, knowledge bases -a form of graph encoding facts and relationships -are inevitably incomplete <ref type="bibr" target="#b123">[125]</ref>. Simply put, developments in information and society move faster than they can be curated. Another important example is recommendation systems <ref type="bibr" target="#b70">[71]</ref> based on evolving user-item networks. These concepts are related to the counterfactuals on graphs <ref type="bibr" target="#b36">[37]</ref> that we discuss. This is fundamentally different from our work where we do graph-wide learning and representation of a dataset of many graphs rather than one constantly evolving graph.</p><p>Subraph methods and Graphlet Counting Kernels. A foundational principle here is that exploiting subgraphs confers graph classifications models with both the ability to fit the training data and extrapolate to graphs generated from a different environment. As detailed in Section 3.2, this insight follows from the Aldous-Hoover representation exchangeable distributions over graphs <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b92">93]</ref> and work on graph limits <ref type="bibr" target="#b77">[78]</ref>. We discuss the large literature using subgraphs in machine learning.</p><p>Counting kernels <ref type="bibr" target="#b114">[116]</ref> measure the similarity between two graphs by the dot product of their normalized counts of connected induced subgraphs (graphlet). This can be used for classification via kernelized methods like Support Vector Machines (SVM). Yanardag and Vishwanathan <ref type="bibr" target="#b142">[144]</ref> argue that the dot product does not capture dependence between subgraphs and extend to a general bilinear form over a learned similarity matrix. These approaches are related to the Reconstruction Conjecture, which posits graphs can be determined through knowledge of their subgraphs <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b129">131,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b82">83]</ref>. It is known that computing a maximally expressive graph kernel, or one that is injective over the class of graphs, is as hard as the Graph Isomorphism problem, and thus intractable in general <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b69">70]</ref>. Kriege et al. <ref type="bibr" target="#b68">[69]</ref> demonstrate graph properties that subgraph counting kernels fail to capture and propose a method to make them more expressive, but only for graphs without vertex attributes. Most applications of graphlet counting do not exploit vertex attributes, and even those that do (e.g. <ref type="bibr" target="#b133">[135]</ref>) are likely to fail under a distribution shift over attributes; recording a count for each type of attributed subgraph (e.g. red clique, blue clique) is sensitive to distribution shift. In comparison, our use of Relational Pooling Graph Neural Networks confers our framework with the ability learn a compressed representation of different attributed subgraphs, tailored for the task, and extrapolate even under attribute shift. We demonstrate this in the experiments below. Last, a recent work of Ye et al. <ref type="bibr" target="#b143">[145]</ref> propose to pass the attributed subgraph counts to a downstream neural model to better compress and represent the high dimensional feature space. However, with extreme attribute shift, it may be that the downstream layers did not see certain attributed subgraph types in training enough to learn how to correctly represent them. We feel that it is better to compress the attributed signal in the process of representing the graph to handle these vertex features, the approach we take here.</p><p>There are many graph kernel methods that do not leverage subgraph counts but other features to measure graph similarity, such as the count of matching walks, e.g. Kashima et al. <ref type="bibr" target="#b60">[61]</ref>, Borgwardt et al. <ref type="bibr" target="#b20">[21]</ref>, Borgwardt and Kriegel <ref type="bibr" target="#b19">[20]</ref>. The WL Kernel uses the WL algorithm to compare graphs <ref type="bibr" target="#b115">[117]</ref> and will inherit the limitations of WL GNNs like inability to represent cycles. Rieck et al. <ref type="bibr" target="#b102">[104]</ref> propose a persistent WL kernel that uses ideas from Topological Data Analysis <ref type="bibr" target="#b87">[88]</ref> to better capture such structures when comparing graphs. Methods that do not count subgraphs will not inherit properties regarding a graph-size environment change -from our analysis of asymptotic graph theory -but all extrapolation tasks require an assumption and our framework can be applied to studying the ability of various kernel methods to extrapolate under different scenarios. Those relying on attributes to build similarities are also likely to suffer from attribute shift.</p><p>Subgraphs are studied to understand underlying mechanisms of graphs like gene regulatory networks, food webs, and the vulnerability of networks to attack, and sometimes used prognostically. A popular example investigates motifs, subgraphs that appear more frequently than under chance <ref type="bibr" target="#b119">[121,</ref><ref type="bibr" target="#b113">115,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b118">120,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b120">122,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b134">136]</ref>. Although the study of motifs is along a different direction and often focus on one-graph datasets, our framework learns rich latent representations of subgraphs. Another line of work uses subgraph counts as graph similarity measures, an example being matching real-world graphs to their most similar random graph generation models <ref type="bibr" target="#b99">[101]</ref>.</p><p>Other machine learning methods based on subgraphs have also been proposed. Methods like mGCMN <ref type="bibr" target="#b73">[74]</ref>, HONE <ref type="bibr" target="#b103">[105]</ref>, and MCN <ref type="bibr" target="#b72">[73]</ref> learn representations for vertices by extending classical methods over edges to a new neighborhood structure based on subgraphs; for instance, mGCMN runs a GNN on the new graph. These methods do not exploit all subgraphs of size k and will not learn subgraph representations in a manner consistent with our extrapolation framework. Teru et al. <ref type="bibr" target="#b125">[127]</ref> use subgraphs around vertices to predict missing facts in a knowledge base. Further examples include the Subgraph Prediction Neural network <ref type="bibr" target="#b83">[84]</ref> that predicts subgraph classes in one dynamic heterogeneous graph; counting the appearance of edges in each type of subgraph for link prediction tasks <ref type="bibr" target="#b0">[1]</ref>; and SEAL <ref type="bibr" target="#b147">[149]</ref> runs a GNN over subgraphs extracted around candidate edges to predict whether an edge exists. While these methods exploit small subgraphs for their effective balance between rich graph information and computational tractability, they are along an orthogonal thread of work.</p><p>Graph Neural Networks. Among the many approaches for graph representation learning and classification, which include methods for vertex embeddings that are subsequently read-out into graph representations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b97">99,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b146">148,</ref><ref type="bibr" target="#b100">102,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b138">140,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b24">25]</ref>, we focus our discussion and modeling on Graph Neural Network (GNN) methods <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b130">132,</ref><ref type="bibr" target="#b140">142,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b145">147,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b23">24]</ref>. GNNs are trained end-to-end, can straightforwardly provide latent graph representations for graphs of any size, easily handle vertex/edge attributes, are computationally efficient, and constitute a state-of-the-art method. However, GNNs lack extrapolation capabilities due also to their inability to learn latent representations that capture the topological structure of the graph <ref type="bibr" target="#b140">[142,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b108">110]</ref>. Relevantly, many cannot count the number of subgraphs such as triangles (3-cliques) in a graph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>. In general, our theory of extrapolating in graph tasks requires properly capturing graph structure. In our work we consider GIN <ref type="bibr" target="#b140">[142]</ref>, GCN <ref type="bibr" target="#b64">[65]</ref> and PNA <ref type="bibr" target="#b29">[30]</ref> as baseline GNN models. GIN and GCN are some of the most diffused models in literature. PNA generalizes different GNN models by considering multiple neighborhood aggregation schemes. Note that since we compare against PNA we do not need to consider other neighboorhod aggregation schemes in GNNs, as studied in Veličković et al. <ref type="bibr" target="#b131">[133]</ref>. To test whether more expressive models are able to extrapolate, we employ RPGIN <ref type="bibr" target="#b88">[89]</ref>. In our experiments, we show that these state-of-the-art methods are expressive in-distribution but fail to extrapolate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experiments</head><p>In this appendix we present the details of the experimental section, discussing the hyperparameters that have been tuned. Training was performed on NVIDIA GeForce RTX 2080 Ti, GeForce GTX 1080 Ti, TITAN V, and TITAN Xp GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Model implementation</head><p>All neural network approaches, including the models proposed in this paper, are implemented in PyTorch <ref type="bibr" target="#b95">[96]</ref> and Pytorch Geometric <ref type="bibr" target="#b38">[39]</ref>.</p><p>Our GIN <ref type="bibr" target="#b140">[142]</ref>, GCN <ref type="bibr" target="#b64">[65]</ref> and PNA <ref type="bibr" target="#b29">[30]</ref> implementations are based on their Pytorch Geometric implementations. We consider sum, mean, and max READOUTs as proposed by Xu et al. <ref type="bibr" target="#b141">[143]</ref> for extrapolations (denoted by XU-READOUT). For RPGIN <ref type="bibr" target="#b88">[89]</ref>, we implement the permutation and concatenation with one-hot identifiers (of dimension 10) and use GIN as before. Other than a few hyperparameters and architectural choices, we use standard choices (e.g. Hu et al. <ref type="bibr" target="#b56">[57]</ref>) for neural network architectures. If the graphs are unattributed, we follow convention and assign a constant 1 dummy feature to every vertex.</p><p>We use the WL graph kernel implementations provided by the graphkernels package <ref type="bibr" target="#b121">[123]</ref>. All kernel methods use a Support Vector Machine on scikit-learn <ref type="bibr" target="#b96">[98]</ref>.</p><p>The Graphlet Counting kernel (GC kernel), as well as our own procedure, relies on being able to efficiently count attributed or unattributed connected induced homomorphisms within the graph. We use ESCAPE <ref type="bibr" target="#b98">[100]</ref> and R-GPM <ref type="bibr" target="#b124">[126]</ref> as described in the main text. The source code of ESCAPE is available online and the authors of Teixeira et al. <ref type="bibr" target="#b124">[126]</ref> provided us their code. We pre-process each graph beforehand and save the obtained estimated induced homomorphism densities. Note that R-GPM takes around 20 minutes per graph in the worst case considered, but graphs can be pre-processed in parallel. ESCAPE takes up to one minute per graph.</p><p>All the models learn graph representations Γ(G * N * ), which we pass to a L-hidden layer feedforward neural network (MLP) with softmax outputs (L ∈ {0, 1} depending on the task) to obtain the prediction. For Γ GIN , and Γ RPGIN , we use respectively GIN and RPGIN as our base models to obtain latent representations for each k-sized connected induced subgraph. Then, we sum over the latent representations, each weighted by its corresponding induced homomorphism density, to obtain the graph representation. For Γ 1-hot , the representation Γ 1-hot (G * N * ) is a vector containing densities of each (possibly attributed) k-sized connected subgraph. To map this into a graph representation, we apply Γ 1-hot (G * N * ) T W where W is a learnable weight matrix whose rows are subgraph representations. Note that this effectively learns a unique weight vector for each subgraph type.</p><p>We use the Adam optimizer to optimize all the neural network models. When an in-distribution validation set is available (see below), we use the weights that achieve best validation-set performance for prediction. Otherwise, we train for a fixed number of epochs.</p><p>The specifics of hyperparameter grids and downstream architectures are discussed in each section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Schizophrenia Task: Size extrapolation</head><p>The results of these experiments are reported in Table <ref type="table" target="#tab_2">1</ref> (left). The data was graciously provided by the authors of De Domenico et al. <ref type="bibr" target="#b31">[32]</ref>, which they pre-processed from publicly available data from The Center for Biomedical Research Excellence. There are 145 graphs which represent the functional connectivity brain networks of 71 schizophrenic patients and 74 healthy controls. Each graph has 264 vertices representing spherical regions of interest (ROIs). Edges represent functional connectivity. Originally, edges reflected a time-series coherence between regions. If the coherence between signals from two regions was above a certain threshold, the authors created a weighted edge. Otherwise, there is no edge. For simplicity, we converted these to unweighted edges. Extensive pre-processing must be done over fMRI data to create brain graphs. This includes discarding signals from certain ROIs. As described by the authors, these choices make highly significant impacts on the resulting graph. We refer the reader to the paper <ref type="bibr" target="#b31">[32]</ref>. Note that there are numerous methods for constructing a brain graph, and in ways that change the number of vertices. The measurement strategy taken by the lab can result in measuring about 500 ROIs, 1000 ROIs, or 264 as in the case we consider <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b137">139,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>For our purposes, we wish to create an extrapolation task, where a change in environment leads to an extrapolation set that contains smaller graphs. For this, we randomly select 20 of the 145 graphs in the dataset, balanced among the healthy and schizophrenic patients, to be used as test. For each healthy-group graph in these 20 graphs, we sample (with replacement) 0.4 × 264 vertices to be removed. In average, the new size for the healthy-group graphs in these 20 graphs is 178.2.</p><p>We hold out the test graphs that are later used to assess the extrapolation capabilities. Over the remaining data, we use a stratified 5-fold cross-validation to choose the hyperparameters and to report the validation accuracy.</p><p>Once the best hyperparameters are chosen, we re-train the model on the entire training data using 10 different initialization seeds, and predict on the test.</p><p>For Γ GIN and Γ RPGIN , in their GNNs, the aggregation MLP of Equation ( <ref type="formula" target="#formula_39">16</ref>) has hidden neurons chosen among {32, 64, 128, 256} and number of layers (i.e. recursions of message-passing) among {1, 2}. The learning rate is chosen in {0.001, 0.0001}. The value of k is treated as a hyperparameter chosen in {4, 5}.</p><p>For Γ 1-hot , recall that we wish to learn the matrix W whose rows are subgraph representations. We choose the dimension of the representations among {32, 64, 128, 256} and the learning rate in {0.001, 0.0001}. The value of k is treated as a hyperparameter chosen in {4, 5}.</p><p>For the GNNs, we tune the learning rate in {0.01, 0.001}, the number of hidden neurons of the MLP in Equation ( <ref type="formula" target="#formula_39">16</ref>) in {32, 64, 128}, the number of layers among {1, 2, 3}.</p><p>For all these models, we use a batch size of 32 graphs and a single final linear layer with a softmax activation as the downstream classifier. We optimize for 400 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Erdős-Rényi Connection Probability: Size Extrapolation</head><p>We simulated Erdős-Rényi graphs (Gnp model) using NetworkX <ref type="bibr" target="#b47">[48]</ref>. Table <ref type="table" target="#tab_2">1</ref> shows results for a single environment task (middle), where graphs in training have all size 80, and a multiple environment task (right), where training graphs have sizes in {70, 80} chosen uniformly at random. In both cases, the test is composed of graphs of size 140. The training, validation, and test sets are fixed. The number of graphs in training, validation, and test are 80, 40, and 100, respectively. The induced homomorphism densities are obtained for subgraphs of a fixed size k = 5.</p><p>For Γ 1-hot , we hyperparameter tune the dimension of the subgraph representations in {32, 64, 128, 256} and the learning rate in {0.1, 0.01, 0.001}.</p><p>For the GNNs and for Γ GIN , and Γ RPGIN , we hyperparameter tune the number of hidden neurons in the MLP of the GNN (Equation ( <ref type="formula" target="#formula_39">16</ref>)) in {32, 64, 128, 256} (GNN is used to learn the representation for k-sized subgraph for Γ GIN , and Γ RPGIN ). The number of layers is also a hyperparameter in {1, 2, 3} (3 layers only for the GNNs), and the learning rate in {0.1, 0.01, 0.001}. We also hyperparameter tune the presence or absence of the Jumping Knowledge mechanism from Xu et al. <ref type="bibr" target="#b139">[141]</ref>.</p><p>For IRM, we consider the two distinct graph sizes to be the two training environments. We tune the regularizer λ [7, Section 3] in {4, 8, 16, 32}, stopping at 32 because increasing its value decreased performances.</p><p>We train all neural models for 500 epochs with batch size equal to the full training data. The downstream classifier is composed by a single linear layer with softmax activations. We perform early stopping as per Hu et al. <ref type="bibr" target="#b56">[57]</ref>. The hyperparameter search is performed by training all models with 10 different initialization seeds and selecting the configuration that achieved the highest mean accuracy on the validation data. Then, we report the mean (and standard deviation) accuracy over the training, the validation, and the test data in Table <ref type="table" target="#tab_2">1</ref> (right).</p><p>For the graph kernels, following Kriege et al. <ref type="bibr" target="#b69">[70]</ref>, we tune the regularization hyperparameter C in SVM over the set {10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 }. We tune the number of Weisfeiler-Lehman iterations of the WL kernel to be among {1, 2, 3, 4} (see Kriege et al. [70, Section 3.1]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Extrapolation performance over SBM attributed graphs</head><p>We sample Stochastic Block Model graphs (SBM) using NetworkX <ref type="bibr" target="#b47">[48]</ref>. Each graph has two blocks, having a withinblock edge probability of P 1,1 = P 2,2 = 0.2. The cross-block edge probability is P 1,2 = P 2,1 ∈ {0.1, 0.3}. The label of a graph is its cross-block edge probability, i.e., Y = P 1,2 .</p><p>Vertex color distributions change with train and test environments. In training, vertices in the first block are either red or blue, with probabilities {0.9, 0.1}, respectively, while vertices in the second block are either green or yellow, with probabilities {0.9, 0.1}, respectively. In test, the probability distributions are reversed: Vertices in the first block are either red or blue, with probabilities {0.1, 0.9}, respectively, and vertices in the second block are green or yellow with probabilities {0.1, 0.9}, respectively. and 100, respectively. We obtain the induced homomorphism densities for Γ GIN , Γ RPGIN , Γ 1-hot for a fixed subgraph size k = 5.</p><p>For the GNNs and for Γ GIN and Γ RPGIN , we choose the number of hidden neurons in the MLP of the GNN (Equation ( <ref type="formula" target="#formula_39">16</ref>)) in {32, 64, 128, 256}, the number of layers in {1, 2, 3} (3 layers only for the GNNs) and hyperparameter tune the presence or absence of the Jumping Knowledge mechanism from Xu et al. <ref type="bibr" target="#b139">[141]</ref>. We add the regularization penalty in Equation ( <ref type="formula" target="#formula_19">9</ref>) for Γ GIN and Γ RPGIN in this experiments. For Γ GIN and Γ RPGIN , we choose the learning rate in {0.01, 0.001} and the regularization weight in {0.1, 0.15}. For the GNNs we choose the learning rate in {0.1, 0.01, 0.001}.</p><p>For IRM, we consider the two distict graph sizes to be the two training environments. We can not treat vertex attributes as environment here since we only have a single vertex-attribute distribution in training. We tune the regularizer λ [7, Section 3] in {4, 8, 16, 32}, stopping at 32 because increasing its value decreased performances.</p><p>For Γ 1-hot , we hyperparameter tune the dimension of the subgraph representations in {32, 64, 128, 256} and the learning rate in {0.01, 0.001}.</p><p>We optimize all neural models for 500 epochs with batch size equal to the full training data. We use a single layer with softmax outputs as the downstream classifier. We perform early stopping as per Hu et al. <ref type="bibr" target="#b56">[57]</ref>. The hyperparameter search is performed by training all models with 10 different initialization seeds and selecting the configuration that achieved the highest mean accuracy on the validation data. Then, we report the mean (and standard deviation) accuracy over the training, the validation, and the test data in Table <ref type="table" target="#tab_4">2</ref>.</p><p>For the graph kernels, following Kriege et al. F.5 Extrapolation performance in real world tasks that violate our causal model</p><p>Results are reported in Table <ref type="table" target="#tab_6">3</ref>. We use the datasets from Morris et al. <ref type="bibr" target="#b86">[87]</ref>, split into train, validation and test as proposed by Yehudai et al. <ref type="bibr" target="#b144">[146]</ref>. In particular, train is obtained by considering the graphs with sizes smaller than the 50-th percentile, and test those with sizes larger than the 90-th percentile. Additionally, 10% of the training graphs is held out from training and used as validation. For statistics on the datasets and corresponding splits, see Yehudai et al. <ref type="bibr" target="#b144">[146]</ref>.</p><p>We obtain the homomorphism densities for a fixed subgraph size k = 4. We observed that larger subgraph sizes, k ≥ 5, implies a larger number of distinct subgraphs and consequently a smaller proportion of shared subgraphs in different graphs. To further reduce the number of distinct subgraphs seen by the models, we only consider the most common subgraphs in training and validation when necessary. Specifically, for NCI1 and NCI109, we only use the top 100 subgraphs (out of a total of around 300), and for DD only the 30k most common (out of a total of around 200k). For PROTEINS we keep all the distinct subgraphs (which are around 180).</p><p>For the GNNs, we follow the setup proposed in Yehudai et al. <ref type="bibr" target="#b144">[146]</ref>, where all the GNNs have 3 layers and a final classifier composed of a feedforward neural network (MLP) with 1 hidden layer and softmax outputs. We tune the batch size in {64, 128}, the learning rate in {0.01, 0.005, 0.001} and the network width in {32, 64}. For Γ GIN and Γ RPGIN , the setup is the same, except for the number of GNN layers that is set to 2. For DD we use a fixed batch size of 256 to reduce the number of times the subgraphs are passed to the network, in order to speed up training.</p><p>For Γ 1-hot , we choose the batch size in {64, 128}, the learning rate in {0.01, 0.005, 0.001} and the dimension of the subgraph representations in {32, 64}.</p><p>For IRM we tune the regularizer λ [7, Section 3] in {8, 32, 128, 512}. The two environments are considered to be graphs with size smaller than the median size in the training graphs and larger than the median size in the training graphs, respectively.</p><p>To mitigate the imbalance between classes in training, we reweight the classes in the loss with the training proportions for each class. We train all neural models for 1000 epochs using early stopping as per Hu et al. <ref type="bibr" target="#b56">[57]</ref>. We test the models on the epoch achieving the highest mean Matthew Correlation Coefficient on validation because of the significant class imbalance in the test, see Table <ref type="table" target="#tab_8">4</ref>.</p><p>For the graph kernels, following Kriege et al. <ref type="bibr" target="#b69">[70]</ref>, we tune the regularization hyperparameter C in SVM over the set {10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 }. We fix the number of Weisfeiler-Lehman iterations of the WL kernel to 3 (see Kriege et al. [70, Section 3.1]), which is comparable to the 3 GNN layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Experiments with single and multiple graph sizes in training. Our single-environment experiments consist of a single graph size in training, and different sizes in test (different from the training size). Whenever multiple environments are available in training -multiple environments implies different graph sizes-, we employ Invariant Risk Minimization (IRM), considering the penalty proposed by Arjovsky et al. [7] for each environment (defined empirically as a range of training examples with similar graph sizes). For each task, we report (a) training accuracy (b) validation accuracy, which are new examples sampled from P(Y, G tr N tr ); and (c) extrapolation test accuracy, which are new OOD examples sampled from P(Y, G te N te ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>the graph kernels, following Kriege et al. [70], we tune the regularization hyperparameter C in SVM over the set {10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 }. We tune the number of Weisfeiler-Lehman iterations of the WL kernel to be in {1, 2, 3, 4} (see Kriege et al. [70, Section 3.1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b69">[70]</ref>, we tune the regularization hyperparameter C in SVM over the set {10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 }. We tune the number of Weisfeiler-Lehman iterations of the WL kernel to be among {1, 2, 3, 4} (see Kriege et al.<ref type="bibr" target="#b69">[70,</ref> Section 3.1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Our task now becomes finding an E-invariant graph representation Γ that can be used to predict Y .The shortcomings of Invariant Risk Minimization (IRM). Invariant Risk Minimization (IRM)<ref type="bibr" target="#b6">[7]</ref> aims to learn a representation that is invariant across all training environment, ∀e ∈ supp(E tr ), by adding a regularization penalty on the empirical risk. However, IRM will fail if: (i) supp(E te ) ⊆ supp(E tr ), since the penalty provides no guarantee that the representation will still be invariant w.r.t. e † ∈ supp(E te )\supp(E tr ); and (ii) if the training data only contains a single environment, i.e., supp(E tr ) = {e}. For instance, the training data may contain only graphs of a single size. In this case, we are unable to apply IRM for size extrapolations. Our experiments show that the IRM procedure of Arjovsky et al.<ref type="bibr" target="#b6">[7]</ref> does not seem to work for graph representation learning.</figDesc><table><row><cell>Proposition 1 shows that an E-invariant representation will perform no worse on the OOD test data (extrapolation</cell></row><row><cell>samples from (Y, G te N te )) than on a test dataset having the same environment distribution as the training data (samples from (Y, G tr N tr )).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Extrapolation performance over unattributed graphs shows clear advantage of our environment-invariant representations, with or without GNN, over standard methods or IRM in extrapolation test accuracy. Table shows mean (standard deviation) accuracy. Bold emphasises the best test average. NA value indicates IRM is not applicable (when training data has a single graph size).</figDesc><table><row><cell>ACCURACY IN SCHIZOPHRENIA TASK</cell><cell>ACCURACY IN ERD ŐS-RÉNYI TASK</cell><cell></cell></row><row><cell>TRAINING HAS A SINGLE GRAPH SIZE</cell><cell>TRAINING HAS A SINGLE GRAPH SIZE</cell><cell>TRAINING HAS TWO GRAPH SIZES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Extrapolation performance over attributed graphs shows clear advantage of environment-invariant representations with GNNs and the attribute regularization in Equation<ref type="bibr" target="#b8">(9)</ref>. Table shows mean (standard deviation) accuracy. Bold emphasises the best test average. NA value indicates IRM is not applicable (when training data has a single graph size).</figDesc><table><row><cell>TRAINING HAS A SINGLE GRAPH SIZE 20</cell><cell>TRAINING HAS TWO GRAPH SIZES: 14 AND 20</cell><cell>TRAINING HAS TWO GRAPH SIZES: 20 AND 30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Extrapolation performance over real-world graph datasets with OOD tasks violating Definitions 1 and 2 and conditions of Theorem 1. Always one of our E-invariant representations Γ GIN and Γ RPGIN is amongst the top 4 best methods in all datasets except NCI109. Table shows mean (standard deviation) Matthews correlation coefficient (MCC) of the classifiers over the OOD test data. Bold emphasises the top-4 models (in average MCC) for each dataset.</figDesc><table><row><cell>DATASETS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>PROTEINS</cell><cell>DD</cell></row><row><cell>RANDOM</cell><cell>0.00 (0.00)</cell><cell>0.00 (0.00)</cell><cell>0.00 (0.00)</cell><cell>0.00 (0.00)</cell></row><row><cell>PNA</cell><cell>0.21 (0.06)</cell><cell>0.24 (0.06)</cell><cell>0.26 (0.08)</cell><cell>0.24 (0.10)</cell></row><row><cell>PNA MEAN (XU-READOUT)</cell><cell>0.12 (0.05)</cell><cell>0.21 (0.04)</cell><cell>0.25 (0.06)</cell><cell>0.29 (0.08)</cell></row><row><cell>PNA MAX (XU-READOUT)</cell><cell>0.16 (0.05)</cell><cell>0.18 (0.07)</cell><cell>0.20 (0.05)</cell><cell>0.12 (0.14)</cell></row><row><cell>PNA + IRM</cell><cell>0.21 (0.07)</cell><cell>0.27 (0.08)</cell><cell>0.26 (0.10)</cell><cell>0.26 (0.08)</cell></row><row><cell>GCN</cell><cell>0.20 (0.06)</cell><cell>0.15 (0.06)</cell><cell>0.21 (0.09)</cell><cell>0.23 (0.05)</cell></row><row><cell>GCN MEAN (XU-READOUT)</cell><cell>0.20 (0.04)</cell><cell>0.15 (0.09)</cell><cell>0.23 (0.07)</cell><cell>0.19 (0.06)</cell></row><row><cell>GCN MAX (XU-READOUT)</cell><cell>0.20 (0.04)</cell><cell>0.19 (0.07)</cell><cell>0.20 (0.14)</cell><cell>0.09 (0.08)</cell></row><row><cell>GCN + IRM</cell><cell>0.12 (0.05)</cell><cell>0.22 (0.06)</cell><cell>0.20 (0.07)</cell><cell>0.23 (0.07)</cell></row><row><cell>GIN</cell><cell>0.25 (0.06)</cell><cell>0.18 (0.05)</cell><cell>0.23 (0.05)</cell><cell>0.25 (0.09)</cell></row><row><cell>GIN MEAN (XU-READOUT)</cell><cell>0.16 (0.05)</cell><cell>0.14 (0.05)</cell><cell>0.24 (0.05)</cell><cell>0.27 (0.12)</cell></row><row><cell>GIN MAX (XU-READOUT)</cell><cell>0.15 (0.08)</cell><cell>0.18 (0.08)</cell><cell>0.28 (0.11)</cell><cell>0.19 (0.07)</cell></row><row><cell>GIN + IRM</cell><cell>0.18 (0.08)</cell><cell>0.16 (0.04)</cell><cell>0.26 (0.06)</cell><cell>0.21 (0.09)</cell></row><row><cell>RPGIN</cell><cell>0.15 (0.04)</cell><cell>0.19 (0.05)</cell><cell>0.24 (0.09)</cell><cell>0.22 (0.09)</cell></row><row><cell>WL KERNEL</cell><cell>0.39 (0.00)</cell><cell>0.21 (0.00)</cell><cell>0.00 (0.00)</cell><cell>0.00 (0.00)</cell></row><row><cell>GC KERNEL</cell><cell>0.02 (0.00)</cell><cell>0.01 (0.00)</cell><cell>0.29 (0.00)</cell><cell>0.00 (0.00)</cell></row><row><cell>Γ 1-HOT</cell><cell>0.17 (0.08)</cell><cell>0.25 (0.06)</cell><cell>0.12 (0.09)</cell><cell>0.23 (0.08)</cell></row><row><cell>Γ GIN</cell><cell>0.24 (0.04)</cell><cell>0.18 (0.04)</cell><cell>0.29 (0.11)</cell><cell>0.28 (0.06)</cell></row><row><cell>Γ RPGIN</cell><cell>0.26 (0.05)</cell><cell>0.20 (0.04)</cell><cell>0.25 (0.12)</cell><cell>0.20 (0.05)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2</head><label>2</label><figDesc>shows results for the three scenarios we considered: 1. A single environment, where training graphs are of size 20 (left), 2. A multiple environment, where training graphs have size 14 or 20, chosen uniformly at random (middle), 3. A multiple environment, where training graphs are of size 20 or 30, chosen uniformly at random (right). The test is the same in all cases, and contains graphs of size 40. The number of graphs in training, validation, and test are 80, 20,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics, Table from Yehudai et al. [146].</figDesc><table><row><cell></cell><cell></cell><cell>NCI1</cell><cell></cell><cell></cell><cell>NCI109</cell><cell></cell></row><row><cell></cell><cell>ALL</cell><cell>SMALLEST 50%</cell><cell>LARGEST 10%</cell><cell>ALL</cell><cell>SMALLEST 50%</cell><cell>LARGEST 10%</cell></row><row><cell>CLASS A</cell><cell>49.95%</cell><cell>62.30%</cell><cell>19.17%</cell><cell>49.62%</cell><cell>62.04%</cell><cell>21.37%</cell></row><row><cell>CLASS B</cell><cell>50.04%</cell><cell>37.69%</cell><cell>80.82%</cell><cell>50.37%</cell><cell>37.95%</cell><cell>78.62%</cell></row><row><cell>NUM OF GRAPHS</cell><cell>4110</cell><cell>2157</cell><cell>412</cell><cell>4127</cell><cell>2079</cell><cell>421</cell></row><row><cell>AVG GRAPH SIZE</cell><cell>29</cell><cell>20</cell><cell>61</cell><cell>29</cell><cell>20</cell><cell>61</cell></row><row><cell></cell><cell></cell><cell>PROTEINS</cell><cell></cell><cell></cell><cell>DD</cell><cell></cell></row><row><cell></cell><cell>ALL</cell><cell>SMALLEST 50%</cell><cell>LARGEST 10%</cell><cell>ALL</cell><cell>SMALLEST 50%</cell><cell>LARGEST 10%</cell></row><row><cell>CLASS A</cell><cell>59.56%</cell><cell>41.97%</cell><cell>90.17%</cell><cell>58.65%</cell><cell>35.47%</cell><cell>79.66%</cell></row><row><cell>CLASS B</cell><cell>40.43%</cell><cell>58.02%</cell><cell>9.82%</cell><cell>41.34%</cell><cell>64.52%</cell><cell>20.33%</cell></row><row><cell>NUM OF GRAPHS</cell><cell>1113</cell><cell>567</cell><cell>112</cell><cell>1178</cell><cell>592</cell><cell>118</cell></row><row><cell>AVG GRAPH SIZE</cell><cell>39</cell><cell>15</cell><cell>138</cell><cell>284</cell><cell>144</cell><cell>746</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/PurdueMINDS/size-invariant-GNNs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was funded in part by the National Science Foundation (NSF) Awards CAREER IIS-1943364 and CCF-1918483, the Purdue Integrative Data Science Initiative, and the Wabash Heartland Innovation Network. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. Further, we would like to thank Ryan Murphy for many insightful discussions, and Mayank Kakodkar and Carlos H. C. Teixeira for their invaluable help with the subgraph function estimation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Proof of Proposition 1 Proposition 1. [E-invariant Representation's Effect on Classification] Consider a permutation-invariant graph representation Γ : ∪ ∞ n=1 {0, 1} n×n × X n → R d , d ≥ 1, and a downstream function ρ : Y × R d → [0, 1] (e.g., a feedforward neural network (MLP) with softmax outputs) such that, for some , δ &gt; 0, the generalization error over the training distribution is: For ∀y ∈ Y,</p><p>Then, the OOD test error is the same as the generalization error over the training distribution, i.e., for ∀y ∈ Y,</p><p>Proof. First note that Y is only a function of W and an independent random noise (following Definitions 1 and 2, depicted in Figure <ref type="figure">1</ref>). Therefore, Y is E-invariant, and thus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1</head><p>Theorem 1 (Approximate E-invariant Graph Representation). Let G tr N tr and G te N te be two samples of graphs of sizes N tr and N te from the training and test distributions, respectively, both defined over the same graphon variable W and satisfying Definitions 1 and 2. Assume the vertex attribute function g X (•) of Definitions 1 and 2 is invariant to E tr and E te (the reason for this assumption will be clear later). Let || • || ∞ denotes the L-infinity norm. For any integer k ≤ min(N tr , N te ), and any constant 0 &lt; &lt; 1,</p><p>Proof. We first replace t ind by t inj , which is defined by</p><p>where inj(F k , G * N * ) is the number of injective homomorphisms of F k into G * N * . We will justify this replacement later. Then, we know from Lovász and Szegedy [79, Theorem 2.5] that for unattributed graphs G * N * ,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Link prediction via higher-order motif features</title>
		<author>
			<persName><forename type="first">Ghadeer</forename><surname>Abuoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianmarco</forename><surname>De Francisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><surname>Aboulnaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="412" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimation of local subgraph counts</title>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Nesreen K Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">Thiago</forename><forename type="middle">B</forename><surname>Edo M Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="692" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representations for partially exchangeable arrays of random variables</title>
		<author>
			<persName><forename type="first">Aldous</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="581" to="598" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental learning on growing graphs</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decomposition by causal forces: a procedure for forecasting complex time series</title>
		<author>
			<persName><forename type="first">Armstrong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Collopy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yokum</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: subgraph counts and related graph properties</title>
		<author>
			<persName><forename type="first">Vikraman</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Fuhlbrück</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1993">1993-2001, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic evaluation of counterfactual queries</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple trophic modules for complex food webs</title>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Bascompte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">J</forename><surname>Melián</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2868" to="2873" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural combinatorial optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6295</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Group invariance principles for causal generative models</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naji</forename><surname>Shajarisales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE international conference on data mining (ICDM&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counting graphlets: Space vs time</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Leucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Panconesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM&apos;17)</title>
				<meeting>the Tenth ACM International Conference on Web Search and Data Mining (WSDM&apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4868" to="4879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identification of breast cancer patients based on human signaling network motifs</title>
		<author>
			<persName><forename type="first">Lina</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mushui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhua</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3368</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining graphlet counts in online social networks</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John Cs</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A general framework for estimating graphlet statistics via random walk</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>VLDB Endowment</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Alexander D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew D Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03395</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mapping multiplex hubs in human functional brain networks</title>
		<author>
			<persName><forename type="first">Manlio</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shuntaro</forename><surname>Sasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Arenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">326</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal confusion in imitation learning</title>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11698" to="11709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">What network motifs tell us about resilience and reliability of complex networks</title>
		<author>
			<persName><forename type="first">Yulia</forename><forename type="middle">R</forename><surname>Asim K Dey</surname></persName>
		</author>
		<author>
			<persName><surname>Gel</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Poor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="19368" to="19373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the statistics of vision: the julesz conjecture</title>
		<author>
			<persName><forename type="first">Persi</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="138" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Design and analysis of experiments in networks: Reducing bias from interference</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On random graphs i</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. math. debrecen</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
				<meeting>the 37th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Random graphs. The Annals of Mathematical Statistics</title>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Edgar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1141" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Caillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08936</idno>
		<title level="m">Causal generative neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
				<meeting>of KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Escaping the convex hull with extrapolated vector machines</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName><surname>Swart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Python in Science Conference</title>
				<editor>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Travis</forename><surname>Vaught</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jarrod</forename><surname>Millman</surname></persName>
		</editor>
		<meeting>the 7th Python in Science Conference<address><addrLine>Pasadena, CA USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mapping human whole-brain structural networks with diffusion mri</title>
		<author>
			<persName><forename type="first">Patric</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kurant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gigandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reto</forename><surname>Van J Wedeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Meuli</surname></persName>
		</author>
		<author>
			<persName><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e597</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Survey on categorical data for neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer series in statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On reconstructing a graph</title>
		<author>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Hemminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="187" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Data augmentation instead of explicit regularization</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>König</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03852</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Relations on probability spaces and arrays of random variables</title>
		<author>
			<persName><forename type="first">N</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><surname>Hoover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Princeton, NJ, 2</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Advanced Study</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Learning tsp requires rethinking generalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Martin</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07054</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Probabilistic symmetries and invariance principles</title>
		<author>
			<persName><forename type="first">Olav</forename><surname>Kallenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (ICML-03)</title>
				<meeting>the 20th international conference on machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Representation learning for dynamic graphs: A survey</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A congruence theorem for trees</title>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="961" to="968" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The dangers of extreme counterfactuals</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langche</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="159" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07421</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A property testing framework for the theoretical expressivity of graph kernels</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2348" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName><forename type="first">Fredrik</forename><forename type="middle">D</forename><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Predicting dynamic embedding trajectory in temporal interaction networks</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;19</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
	<note>ISBN 9781450362016</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Charton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Higher-order graph convolutional networks</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07697</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Representation learning of graphs using graph convolutional multilayer networks based on motifs</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15838</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8230" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6446" to="6456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Large networks and graph limits</title>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Structure and function of the feed-forward loop network motif</title>
		<author>
			<persName><forename type="first">Shmoolik</forename><surname>Mangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="11980" to="11985" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Small graphs are reconstructible</title>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian Journal of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="126" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Subgraph pattern neural networks for high-order graph evolution prediction</title>
		<author>
			<persName><forename type="first">Changping</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Mouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3778" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A user&apos;s guide to topological data analysis</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Munch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Learning Analytics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Sur les applications de la theorie des probabilites aux experiences agricoles: essai des principes (masters thesis); justification of applications of the calculus of probabilities to the solutions of certain questions in agricultural experimentation. excerpts english translation (reprinted)</title>
		<author>
			<persName><surname>Neyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="463" to="472" />
			<date type="published" when="1923">1923</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afonso</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 34th International Conference on Machine Learning (ICML)</title>
				<meeting>eeding of the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Bayesian models of graphs, arrays and other exchangeable random structures</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="437" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
				<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Escape: Efficiently counting all 5-vertex subgraphs</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Pinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaidyanathan</forename><surname>Seshadhri</surname></persName>
		</author>
		<author>
			<persName><surname>Vishal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">Nataša</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="e177" to="e183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Causal feature selection via orthogonal search</title>
		<author>
			<persName><forename type="first">Anant</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02938</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Higher-order network representation learning</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Arbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10026</idno>
		<title level="m">Heterogeneous network motifs</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10500</idno>
		<title level="m">Causality for machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Causal forces: Structuring knowledge for time-series extrapolation</title>
		<author>
			<persName><forename type="first">Armstrong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Collopy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Telling cause from effect in deterministic linear dynamical systems</title>
		<author>
			<persName><forename type="first">Naji</forename><surname>Shajarisales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Network motifs in the transcriptional regulation network of escherichia coli</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Shai S Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmoolik</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Mangan</surname></persName>
		</author>
		<author>
			<persName><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="68" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Rogue signs: Deceiving traffic sign recognition with malicious ads and logos</title>
		<author>
			<persName><forename type="first">Chawin</forename><surname>Sitawarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsalan</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Mosenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mung</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
		<idno>CoRR, abs/1801.02780</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="100" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Motifs in brain networks</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Kötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e369</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Competitive exclusion, or species aggregation?</title>
		<author>
			<persName><forename type="first">Lewi</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oecologia</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="424" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Network motifs and their origins</title>
		<author>
			<persName><forename type="first">Lewi</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simberloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Artzy-Randrup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e1006749</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">graphkernels: R and python packages for graph comparison</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Elisabetta</forename><surname>Mahito Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="532" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Graph pattern mining and learning through user-defined relations</title>
		<author>
			<persName><forename type="first">Leornado</forename><surname>Carlos Hc Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wagner</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Meira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1266" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Komal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
				<meeting>the 37th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Causal discovery from changes</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><surname>Toenshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrikus</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08786</idno>
		<title level="m">Graph learning with 1d convolutions on random walks</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Inverse statistical variates</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Ck Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">3937</biblScope>
			<biblScope unit="page" from="453" to="453" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">A collection of mathematical problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName><surname>Ulam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">29</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural message-passing</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Identifying functions and prognostic biomarkers of network motifs marked by diverse chromatin states in human cell lines</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Yifei Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncogene</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Efficiently estimating motif statistics of large networks</title>
		<author>
			<persName><forename type="first">Pinghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Cs</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Graphcrop: Subgraph cropping for graph classification</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10564</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Mapping complex tissue architecture with diffusion spectrum magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Patric</forename><surname>Van J Wedeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Yih Isaac</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName><surname>Weisskoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1377" to="1386" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Shaolei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Askarisichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02131</idno>
		<title level="m">Deepmap: Learning deep representations for graph classification</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">On size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08853</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Learning deep network representations with adversarially regularized autoencoders</title>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
