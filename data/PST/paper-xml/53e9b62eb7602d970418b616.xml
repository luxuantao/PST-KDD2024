<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Celebrities in Billions of Web Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-07-13">July 13, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@microsoft.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xin-Jing</forename><surname>Wang</surname></persName>
							<email>xjwang@microsoft.com</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
							<email>hshum@microsoft.com</email>
						</author>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Daniel</forename><forename type="middle">X</forename><surname>Gatica-Perez</surname></persName>
						</author>
						<author>
							<persName><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Advanced Study</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<postCode>98054</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Celebrities in Billions of Web Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-07-13">July 13, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">CB72833A2BC9AC4BBD3FB31FEC529A97</idno>
					<idno type="DOI">10.1109/TMM.2012.2186121</idno>
					<note type="submission">Manuscript received June 24, 2011; revised October 25, 2011; accepted January 10, 2012. Date of publication January 26, 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>image annotation</term>
					<term>image database</term>
					<term>image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a face annotation system to automatically collect and label celebrity faces from the web. With the proposed system, we have constructed a large-scale dataset called "Celebrities on the Web," which contains 2.45 million distinct images of 421 436 celebrities and is orders of magnitude larger than previous datasets.</p><p>Collecting and labeling such a large-scale dataset pose great challenges on current multimedia mining methods. In this work, a two-step face annotation approach is proposed to accomplish this task. In the first step, an image annotation system is proposed to label an input image with a list of celebrities. To utilize the noisy textual data, we construct a large-scale celebrity name vocabulary to identify candidate names from the surrounding text. Moreover, we expand the scope of analysis to the surrounding text of webpages hosting near-duplicates of the input image. In the second step, the celebrity names are assigned to the faces by label propagation on a facial similarity graph. To cope with the large variance in the facial appearances, a context likelihood is proposed to constrain the name assignment process. In an evaluation on 21 735 faces, both the image annotation system and name assignment algorithm significantly outperform previous techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collecting and labeling celebrity faces from general web images is a challenging task. The major difficulties are caused by the noise in web data. Firstly, the surrounding text of a web image often comprises words and phrases lacking a standard grammar structure. Therefore it is difficult to apply natural language processing techniques to extract celebrity names and estimate the likelihood of a celebrity appearing in the image. Secondly, celebrity faces on the web exhibit large visual variation due to pose, makeup, expression and occlusion caused by sunglasses or fancy hairstyles, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This layer of "visual noise" imposes great difficulty for associating names with faces by visual analysis. This is much more difficult than the tasks in previous work, which mostly focus on labeling faces in special types of web images. For example, work in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> were conducted on news images, where descriptive captions are usually provided. However, there is large uncertainty on the quality of the captions for general web images. On the other hand, Stone et al. <ref type="bibr" target="#b4">[5]</ref> studied the problem of recognizing faces in social network, where the "friendship" information can be leveraged to improve the accuracy of face labeling. Nonetheless, such information is rarely available for general web images.</p><p>To address the aforementioned challenges, a two-step approach is proposed. In the first step, a novel image annotation system is proposed to label an input image with a list of celebrities by mining the surrounding text. To handle the noisy textual information, a large-scale celebrity name vocabulary is automatically constructed from the web to filter the surrounding text and identify candidate names. Moreover, we expand the analysis to the surrounding text of the near-duplicates of the input image. The intuition is that, the names of the celebrities who truly appear in the input image tend to occur frequently in the hosting webpages of its near-duplicates. In the second step, we assign the celebrity names in the image annotation results to the faces. To handle the tremendous volume of web images, the assignment is performed in an unsupervised manner. This is achieved by first deriving (weak) face labels based on the image annotation results, and then refine the face labels by propagating information from highly confidence labels to less confident ones on a facial similarity graph. To handle the large variation in the facial appearances, we propose a context likelihood to constrain the label propagation process. In the experiment section, we show that both the image annotation system and the name assignment algorithm achieve significant accuracy improvements over baseline algorithms.</p><p>Next, we will first introduce the collected large-scale face dataset in Section II, and then discuss related work in Section III. An overview of the proposed web face annotation approach is given in Section IV, followed by discussions on the proposed image annotation system and the name assignment algorithm in Section V and Section VI, respectively. Finally, we discuss the evaluation results in Section VII and conclude the paper in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CFW DATASET CONSTRUCTION</head><p>A large-scale celebrity dataset named "Celebrities on the Web" (CFW) was constructed using the proposed approach. Starting from two billion web images collected from the Internet, 300 million face images were detected using a robust face detector <ref type="bibr" target="#b5">[6]</ref>, among which 70.8 million images were processed by the proposed face annotation system. To ensure the diversity of the dataset, we discarded near-duplicate images during the data collection process. Following this procedure, a database of 2.45 million celebrity images corresponding to 421 436 celebrities was collected.</p><p>The CFW dataset is advantageous in several aspects. First, it is orders of magnitude larger than previous datasets, most of which contain only tens of thousands of faces <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Second, since all faces in CFW are collected from the web, they exhibit large variation in pose, expression, hairstyle and makeup. Examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Finally, the labels of CFW dataset are much more accurate than previous automatically generated datasets. For example, the error rate of the labels of Faces in the News dataset <ref type="bibr" target="#b0">[1]</ref> is 23%, while the overall error rate of the labels of CFW dataset is 13.93% and a significant portion of CFW dataset (constituting over half of the CFW dataset) achieves an error rate as low as 4.07%. Detailed comparisons between CFW and existing popular face datasets are shown in Section VII-B. Because of its scale, diversity and accuracy, the collected CFW dataset is of great value to both the multimedia research community <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b0">[1]</ref> and many industrial applications <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>Due to the importance of training data for face recognition, many works have been done to create large-scale face databases with name labels. The majority of the existing approaches require human intervention to produce reliable labels, such as <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b17">[18]</ref>, which is difficult to scale up to labeling millions of faces.</p><p>In order to create larger scale databases, a number of works attempted to associate names to faces for special types of web images. For example, work in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, and <ref type="bibr" target="#b18">[19]</ref> focused on associating faces in news photos with names. Berg et al. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b0">[1]</ref>  applied a named entity detector on the image captions, and a face detector on the images, then tried to find associations between detected names and faces. In their work, faces were represented based on kernel PCA and clustered using a Gaussian mixture model, where each component was assumed to correspond to a specific person. The parameters of the Gaussian mixture model were learnt together with the most probable associations between names and faces. Guillaumin et al. <ref type="bibr" target="#b2">[3]</ref> developed a graph based method to resolve name face association. They modeled the name-face association as a bipartite graph matching problem with names as one set of nodes and faces as another set, and used a min-cost max-flow algorithm to speed up the matching process. To further extend the previous framework, Jie et al., <ref type="bibr" target="#b18">[19]</ref> proposed a generative model to incorporate verbs in the caption for simultaneous face and pose annotation and solving the image-caption correspondence problem. Besides, they proposed a exemplar based likelihood function for appearance modeling which is similar to our approach. Cour et al. <ref type="bibr" target="#b20">[21]</ref> approached the name-face correspondence problem in an ambiguous label setting and proposed a general convex learning formulation to learn from ambiguously labeled data. An advantage of their approach is that it does not rely on heuristics and does not suffer from local optima of nonconvex methods.</p><p>Inspired by machine translation methods, Pham et al. <ref type="bibr" target="#b3">[4]</ref> proposed to model the name face association problem as crossmedia alignment by exploring the symmetry between the visual and textual modalities. An interesting idea in their work is to model the probability of a face being described in the corresponding image caption based purely on visual information, e.g., the size and position of the face.</p><p>The reason why news photos are favored in the above works is because their associated captions are usually well-written so that names (and verbs) can be easily extracted by NLP-based techniques. However, this is not true for general web images, the surrounding text of which is often noisy and lacks a rigorous grammar structure. Besides, a common assumption made in the above works is that the same person must not appear multiple times in an image. This assumption might be acceptable for news pictures but it is inappropriate for general web images which contains an large amount of user edited pictures, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Compared with <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and <ref type="bibr" target="#b20">[21]</ref>, our approach has several advantages. First, the proposed image annotation system is capable of labeling names to general web images, while name label extraction algorithms in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b3">[4]</ref> are not suitable for this task as previously discussed. Second, our name assignment algorithm does not impose any assumption on the facial feature distribution, while a Gaussian distribution is assumed for the faces of a person in <ref type="bibr" target="#b0">[1]</ref>, which may be unrealistic for real-world face database. Third, only visual cues are used in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>, which are not sufficient for annotating web celebrity faces not only because it is difficult to infer facial identity based only on visual similarity, but also because of a large variation in facial appearances. In contrast, our method combines a context likelihood and visual similarity to improve the assignment accuracy.</p><p>Besides generating a face dataset from news image, search engines are used in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b14">[15]</ref> to obtain celebrity images with name labels. However, these name labels are unreliable because the error rate of image search engine increases dramatically for images with lower ranks. And human verification is required to improve the accuracy as in <ref type="bibr" target="#b21">[22]</ref>. Furthermore, search engine-based methods can only associate the query names to the returned images, which is insufficient for handling images containing multiple celebrities.</p><p>Another related area is image annotation. Recent advances in image annotation have led to large-scale systems capable of annotating very specific information to images, e.g., Kennedy et al. <ref type="bibr" target="#b22">[23]</ref> and Arista <ref type="bibr" target="#b23">[24]</ref>. For example, for a picture of Michael Jackson, while early image annotation prototypes may only label this picture as "human," Arista is able to generate more specific and informative tags like "Michael Jackson," "rock star," "pop king." Our work is related to Arista <ref type="bibr" target="#b23">[24]</ref> in that both work analyze the surrounding text of the near-duplicates of input images in order to generate accurate image annotation results. The main difference is that the image annotation system in this paper focuses on labeling a special type of tags, i.e., celebrity names. This leads to a substantial difference in the algorithm design. By leveraging a large celebrity name vocabulary to filter surrounding text and exploring novel features to represent annotation keywords, the proposed algorithm achieves higher name labeling accuracy.</p><p>Also related is the large-scale face retrieval work proposed by Zhong et al. <ref type="bibr" target="#b24">[25]</ref>, which enables efficient face retrieval in a repository of millions of faces. Combining the CFW database with such a large-scale face retrieval system will lead to a largescale real world face recognition engine. We evaluate such a prototype system in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERVIEW OF OUR APPROACH</head><p>In this section, we provide an overview to the proposed approach for celebrity face annotation, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The proposed web face annotation system consists of two components. First of all, we propose an image annotation system to determine who might appear in an input image by identifying celebrity names from surrounding text, as shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref> and<ref type="figure">(c</ref>). Then, given a set of names from image annotation results, we propose a name assignment algorithm to assign the names to the faces in the input image, corresponding to Fig. <ref type="figure" target="#fig_2">3(d)</ref> and<ref type="figure">(e)</ref>. Next, we will give a brief overview to the two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Annotation System</head><p>The proposed image annotation system consists of three steps:</p><p>1) construct a large-scale celebrity name vocabulary;</p><p>2) given an input image, discover all webpages hosting its near-duplicates; 3) use the name vocabulary to filter the surrounding text of discovered webpages and identify candidate names. Compute confidence scores of candidate names by analyzing their distribution in the surrounding text. The motivation is twofold. First, due to the possible lack of grammar and structure, textual information on the web might be more effectively analyzed by a name vocabulary based approach than methods relying on natural language cues, e.g., named entity recognition. Second, aggregating information from multiple webpages from independent websites helps remove noise and identify names of celebrities who truly appear in the image.</p><p>The annotated images can be divided into three groups:</p><p>• SFSN: images with one face detected and one name label;</p><p>• SFMN: images with one face detected and multiple name labels; • MF: images with multiple faces detected.</p><p>With SFSN images, we can derive a large number of accurately labeled faces. This is because the name labels of SFSN images usually have high confidence, and since there is only one face in an SFSN image, its image-level name can be used directly as its face label. As shown in Section VII-B, the accuracy of face labels for SFSN images is 95.93%. However, for SFMN and MF images, there are no simple ways to derive accurate face labels only from image annotation results. For SFMN images, textual information alone is not sufficient to infer their face labels accurately, as shown in the experiment results in Section VII-C. While for MF images, we can not determine how to assign names to faces without using visual cues. Since SFMN and MF images constitute over 42% of the collected dataset, it is necessary to develop a method to combine textual and visual information to accurately assign their identified names to faces. We will address this problem in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multimodal Name Assignment</head><p>To assign names to faces for SFMN and MF images, we propagate highly confident labels from SFSN images. Given an input face, the propagation is conducted by performing instance search in the SFSN images and then let the nearest neighbors vote for their labeled names.</p><p>Due to the large variation in the visual appearances of celebrity faces, it is very likely that the nearest neighbors include faces from irrelevant individuals. To handle this layer of "visual noise," a context likelihood is developed to constrain the propagation process. The context likelihood incorporates the information from surrounding text by using the confidence scores estimated by the image annotation system. Besides, the context likelihood also incorporates an assumption that it is uncommon for the same person to appear multiple times in one image. A Bayesian framework is adopted to combine the visual propagation results and the context likelihood.</p><p>Next, we will discuss the image annotation system and name assignment algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMAGE ANNOTATION SYSTEM</head><p>The goal of the proposed image annotation system is to label an input image with a list of celebrities who may appear in the image. An illustration of the annotation system and example results are shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref> and<ref type="figure">(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constructing a Large-Scale Celebrity Name Vocabulary</head><p>As aforementioned, we identify names from surrounding text with a large-scale vocabulary. We construct such a vocabulary by utilizing the semi-structured data in Wikipedia. The entire Wikipedia contains five million pages, out of which 1.7 million pages are considered as articles describing some unique concepts <ref type="bibr" target="#b25">[26]</ref>. In this work, we train a classifier to identify people related pages from all Wikipedia pages, and then extract names from their page titles.</p><p>To classify a page, we first extract bag-of-word features from the first paragraph of the page, which gives a brief introduction of the subject of the page. Beside the paragraphs in the article, another part of the page, namely information box, is also informative for classification. Information box is a group of property-value pairs which summarize the basic characteristics of the subject in the page. Since different subjects have different properties, this data is highly informative. For example, some common properties in the information box of a person include "Born," "Occupation," "Spouse," etc. However, the problem with information box is that it is not available for every Wikipedia page. This drawback is alleviated by incorporating category tags, which are usually provided at the bottom of Wikipedia pages for the purpose of grouping pages with similar topics. Some category tags are very discriminative for identifying people pages, such as "1962 births," "20th-century actors," "American computer scientists," etc. From information box and category tags, we also extract simple bag-of-word features.</p><p>In this way, a page is converted to three kinds of feature vectors, including features based on the first paragraph of an article, features based on the information box and features based on the category tags. Support vector machine is used to learn prediction models separately for the three features. After comparing the three features, we found category tags achieved the best performance on our testing data of 3 K Wikipedia pages (another 3 K for training), with classification accuracy over 95%. Therefore we chose to rely on category tags to identify people pages. After parsing the entire Wikipedia corpus, 750 555 people pages were collected and their page titles were used to construct our name vocabulary.</p><p>Besides Wikipedia, we also use Entitycube <ref type="bibr" target="#b26">[27]</ref> to construct the vocabulary, which automatically collects entities from general web pages. The accuracy of the name vocabulary from Entitycube is lower but contains a substantial number of less well-known people (8.78 million).</p><p>After the celebrity name vocabulary is generated, the universe of possible name labels is defined as <ref type="bibr" target="#b0">(1)</ref> where is the total number of celebrities in our name vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discover Related Webpages by Near-Duplicate Image Retrieval</head><p>Annotating an input image with information only from its hosting webpage may suffer from the noise in the surrounding text. Therefore we expand the scope of analysis to the surrounding text of webpages hosting near-duplicates of the input image. The assumption is that, since these webpages come from different websites, aggregating their surrounding text will magnify the signals from correct names and dilute the noises from incorrect ones.</p><p>To perform near-duplicate image retrieval in a web scale database, we collaborated with a product team in Microsoft. Several hundreds of machines were allocated for this task. Although finding near-duplicates for a query image from two billion images seems extremely challenging, the divide and conquer strategy utilizing several hundred machines makes this task possible as each server only needs to deal with a few millions of images. In this way, many near-duplicate image retrieval algorithms <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref> can be employed to perform the task. Similar to the work in <ref type="bibr" target="#b27">[28]</ref>, we divided each image into blocks 1 and extracted average luminance from each block. To further improve the discriminative power of the image feature, we also applied the canny edge detector and extracted an edge directional histogram from each block. Then we performed PCA to reduce the dimension of the feature vectors to 24 dimensions. To improve the efficiency of near-duplicate detection, the feature vectors were thresholded and the resulted hash codes were used for indexing. The 24-dimensional unthresholded features were also stored in the memory for reranking. In this way, to index 10 million images on each machine, it costs only 0.98 GB in memory and the average query speed is 0.026 seconds. On a randomly selected query set of 200 images, the precision of the near-duplicate detection is 91%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Annotating Images by Mining Surrounding Text of Related Webpages</head><p>With the name vocabulary and the expanded set of surrounding text, a group of candidate names can be identified. Next we discuss how to generate the final annotation results. Denote as all the images to be annotated. Denote the image context for as , which includes the surrounding text of all the near duplicates of . Denote names extracted from as . The annotation task is a multi-label problem: to choose a set of names 2 from to annotate . We solve this problem by learning a binary classifier. Given a name , the classifier outputs an image-level name label as follows:</p><p>if a celebrity named appears in if no celebrity in is named .</p><p>(</p><formula xml:id="formula_0">)<label>2</label></formula><p>Denote the features for as . considers the following three aspects.</p><p>1) Type of names: There are three ways that we consider a term in a surrounding text as a candidate name: 1) full name, e.g., "Harry Potter;" 2) partial name, e.g., "Harry" or "Potter;" 3) concatenated name, e.g., "harrypotter." In our experiments, we observed that full name match was a very reliable indicator for the occurrences of celebrity names. Partial match can increase the recall of annotation because people often use partial names for simplicity. And concatenated name match is also important for extracting candidate names, especially in image URLs. 2) Type of surrounding text: Surrounding text of an image is the textual information in the hosting webpage related to the content of the hosted image. In our implementation, seven types of surrounding text are extracted to represent a web image, i.e., caption, alt text, image URL, page title, section title, 40 words surrounding the image link (20 words before and 20 words after the link), and homepage names. We observed that different kinds of surrounding text have various importance. For instance, information in 1 was set to eight in the experiments. 2 A name was included in only if it occured in the surrounding text of at least two near duplicate images.</p><p>image URL or caption is more predictive of the identity of the celebrity than information in page title.</p><p>3) Frequency versus ratio: Here, frequency corresponds to the number of times that occurs in , while ratio measures the percentage of in which occurs. Intuitively, is more likely to correspond to a celebrity appearing if both its frequency and ratio are high. We measure the probability that appears in by <ref type="bibr" target="#b2">(3)</ref> where parameters and are learnt by logistic regression. If , then is labeled as an image-level name for .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MULTIMODAL NAME ASSIGNMENT</head><p>In this section, we discuss the proposed name assignment algorithm which propagates the highly accurate labels of SFSN images to faces in SFMN and MF images. This algorithm is unsupervised and thus avoids manual labeling in order to process large-scale datasets. In the algorithm, we do not propagate labels between faces in SFMN and MF images to avoid propagating noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>Denote the detected faces in image as <ref type="bibr" target="#b3">(4)</ref> where is the total number of faces in , and is a feature vector describing the appearance of the th face in . is set to 500 in the experiments.</p><p>Correspondingly, denote the face labels as null <ref type="bibr" target="#b4">(5)</ref> where is the label of , "null" means no name is assigned to , and is the set of image-level names of as defined in Section V-C.</p><p>For simplicity, we will drop index n in the rest of the section and just use as the image to be labeled, as the image context of as the names extracted from as a name in as the image level name label of as faces detected in and as their name labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of the Assignment Model</head><p>Denote the confidence for label of face as , which is estimated by computing the posterior distribution of conditioned on the facial features , image context as defined in Section V-C and a parameter <ref type="bibr" target="#b5">(6)</ref> where is defined in Section V-C and is a parameter of the proposed algorithm used later in <ref type="bibr" target="#b12">(13)</ref>. From the definition of ( <ref type="formula">6</ref>), we can see that both visual and textual cues are utilized in the name assignment algorithm.</p><p>To compute , we marginalize the joint posterior distribution of all the name assignments in image , i.e., . Assuming and are conditionally independent given , the joint posterior distribution becomes <ref type="bibr" target="#b6">(7)</ref> In <ref type="bibr" target="#b6">(7)</ref>, there are three core components: propagates name labels by visual similarity, is the context likelihood to constrain the propagation process, and is the prior distribution of name assignments. Next we discuss how to estimate the three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Label Propagation from SFSN Images</head><p>In this subsection, we discuss how to propagate labels from SFSN images to SFMN and MF images through a facial similarity graph. Denote the set of SFSN faces as <ref type="bibr" target="#b7">(8)</ref> By using (3), the labels for SFSN faces are initialized directly from their image-level names. Therefore, the confidence of assigning name to a SFSN face is computed as <ref type="bibr" target="#b8">(9)</ref> As introduced in Section VI-B, is the label confidence of SFSN face is the index of the image (i.e., ) where appears, and is a binary variable as defined in (2) to indicate whether the celebrity named appears in . And is the image context of , as defined in Section V-C. Names of SFSN faces are propagated to the detected faces in image independently. Assume that the face labels of is , i.e., to assign name to . Then the propagation formula is null <ref type="bibr" target="#b9">(10)</ref> where is defined as the set of K-nearest-neighbors of among SFSN faces, 3 and is the transition probability from to its neighbor as defined by the following formula:</p><p>(11)</p><p>3 k was set to four in the experiments.</p><p>where distance is used to compute for efficiency, and the parameter controls the width of the neighborhoods. <ref type="foot" target="#foot_0">4</ref>The propagation can be viewed as a weighted voting process. To estimate the confidence of assigning name to , each of its neighbor votes according to the label confidence , and the votes are weighted by the similarity between and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Constrain the Propagation by a Context Likelihood</head><p>In this subsection, we introduce how we model contextual information by in <ref type="bibr" target="#b6">(7)</ref>. Given face labels of , for each image-level name in extracted by <ref type="bibr" target="#b2">(3)</ref>, we know whether it appears in the image (i.e., whether is assigned, indicated by a binary variable as defined in <ref type="bibr" target="#b1">(2)</ref>] and the number of faces it corresponds to [i.e., the number of times is assigned, denoted by ). Here, we want to impose a soft constraint on such that with a higher confidence estimated by (3) has a higher probability to be assigned. Also, should have a small probability to be larger than 1 because it is uncommon for the same person to occur multiple times in an image. Therefore, we model the context likelihood as the joint distribution of and given . Denote the set of name labels assigned to at least one face in as , and the rest of the name labels as , where</p><p>. The context likelihood is modeled as <ref type="bibr" target="#b11">(12)</ref> As defined in (3), measures the confidence of the celebrity named appearing in given the image context . serves as a penalization for not assigning name to any face.</p><p>is defined as when if and otherwise <ref type="bibr" target="#b12">(13)</ref> where is a normalization constant and is the total number of faces in image as defined in Section VI-B. should be set as so that is a monotonically decreasing function of . <ref type="foot" target="#foot_1">5</ref> In this way, serves as a regularization component to softly penalize multiple assignments of the same name.</p><p>Intuitively, the proposed context likelihood assumes that the faces in image arise from the following generative process. 1) For each image-level name , generate a binary variable from as defined in ( <ref type="formula">3</ref>) to indicate whether appears in image .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) If</head><p>, generate faces of name in image from as defined in <ref type="bibr" target="#b12">(13)</ref>. Note that this is different from the hard constraints imposed by previous work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b31">[32]</ref> requiring that the same name must not be assigned to multiple faces in an image, which is not valid for general web images because there exists a significant number of manually edited web images with multiple faces of the same celebrities, as previously shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the power of the context likelihood. On the left of the figure, we show that the image is annotated with two names, which are "Angelina" with confidence 0.65 and "Clint" with confidence 0.60. On the right, it is shown that the algorithm relying solely on visual similarity incorrectly assigns "Angelina" to both faces. This is because there are much more SFSN faces of "Angelina" than "Clint," creating a class imbalance problem for the label propagation. In the middle, we can see that incorporating the context likelihood leads to soft penalization for the incorrect assignment, which are assigning "Angelina" to both faces (i.e., ), and not assigning "Clint" to any face [i.e., ]. With the soft penalization, correct names are assigned to both face A and B. We show in Section VII-D that incorporating the context likelihood leads to a significant accuracy boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Normalization by Name Prior</head><p>In <ref type="bibr" target="#b6">(7)</ref>, represents the prior of names. This prior is estimated using the labels of SFSN images. Assume that the face labels of is , i.e., name is assigned to the th face. And that all of the individual dimensions of are independent. We have <ref type="bibr" target="#b13">(14)</ref> Assume that the distribution of facial descriptors of SFSN faces is uniform, and that the prior distribution of name can be estimated from the label confidences of SFSN faces , as defined in Section VI-C. The formula to compute is then given by <ref type="bibr" target="#b14">(15)</ref> where is the number of faces in image and is the name assigned to the th face in image is the total number of SFSN faces in our dataset.</p><p>Adding a name prior is meaningful not only from a Bayesian perspective but also for performance issue. Without such a normalization, popular names will have better chances to be propagated from SFSN faces to SFMN or MF faces, which is not desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Detail: Face Representation</head><p>In this work, the appearance of each face is described by local binary pattern (LBP) <ref type="bibr" target="#b32">[33]</ref>, which is widely used in face recognition <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>. In our implementation, the face image is divided into small regions from which the LBP features are extracted and concatenated into a single feature histogram. The idea behind the LBP features is that the face images can be seen as composition of micro-patterns which are invariant with respect to monotonic grey scale transformations. This is of important value when handling faces under a large variety of illumination conditions. Although alternative facial descriptors <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> exist, a thorough comparison is beyond the scope of this paper.</p><p>To make facial similarity computation efficient on millions of faces, we apply PCA <ref type="bibr" target="#b40">[41]</ref> to reduce the dimension of face descriptor from over 3000 to 500 dimensions. The number of dimensions is chosen to balance the computation efficiency and the representation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATIONS</head><p>In this section, we evaluate the proposed algorithm, the constructed database, and its application to celebrity recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Groundtruth Dataset Construction</head><p>To evaluate the performance of the proposed algorithm and the accuracy of the automatically annotated CFW database, we manually labeled 21 735 faces randomly sampled from the CFW database. Among these faces, 500 of them were used to train the logistic regression model in <ref type="bibr" target="#b2">(3)</ref>. The rest made up of the groundtruth dataset for all the following evaluations. The detailed information about the groundtruth dataset is summarized in Table <ref type="table">I</ref>.</p><p>To label the groundtruth dataset, we partitioned the faces into seven subsets and invited seven volunteers for the labeling work. Each subset of faces was labeled by two volunteers. If the two volunteers provided different labels, an extra human judge was assigned to determine the appropriate labels. To facilitate the labeling process, candidate names were proposed from our image annotation algorithm. If none of the candidate names were the correct names, "null" were labeled to the corresponding faces. When volunteers encountered an unknown celebrity, they were required to look at top Bing and Google image search results to get familiar with the visual appearance of the celebrity. Moreover, the faces were grouped by their candidate names, which greatly sped up the labeling process. On average, each volunteer spent 10 hours on the labeling work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Properties of CFW Database</head><p>In this section, we evaluate the scale and accuracy of the CFW dataset.</p><p>Scale: The CFW dataset contains 2.45 million distinct images (i.e., near-duplicate faces are removed in the database) of 421 436 individuals. Here we compare the scale of CFW database with popular large-scale face datasets in Table <ref type="table">II</ref>. From the table, it can be observed that the CFW database is much larger than previous face databases in terms of both individuals and images.</p><p>Accuracy: In Table <ref type="table" target="#tab_1">III</ref>, we compare the accuracy of CFW with two other automatically labeled face datasets-Faces in the  News <ref type="bibr" target="#b19">[20]</ref> and Labeled Yahoo News [32] <ref type="foot" target="#foot_2">6</ref> . We can see that the scale and accuracy of our dataset significantly outperform both Faces in the News and Labeled Yahoo News. Besides, we can see that a significant portion of the CFW database is comprised of SFSN images with 95.93% label accuracy, which is far ahead of the results in previous work. Moreover, we can apply a variety of thresholds on the annotation confidences of SFSN faces to get error rates as low as 1.69% while still including a large number of faces compared to other automatically generated dataset <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref>, as shown in Table <ref type="table" target="#tab_2">IV</ref>.</p><p>A useful metric to measure the challenge of the dataset for associating names to faces is the number of faces per image. On average, the CFW dataset has 1.05 faces per image, lower than that of Labeled Yahoo News <ref type="bibr" target="#b31">[32]</ref> dataset which is 1.10. However, our dataset is still quite challenging because it contains much more images in total.</p><p>An interesting observation is that the number of images for a celebrity follows a long tail distribution, as shown in Table <ref type="table" target="#tab_3">V</ref>.</p><p>From the table, it can be seen that the image distribution in the database is highly imbalanced. 64 542 celebrities have more than five images, while only 434 of them have over 500 images. Besides, Table <ref type="table" target="#tab_3">V</ref> shows the error rate of the face labels per individual computed from the evaluation results on the groundtruth dataset. The entire groundtruth dataset contains 1514 individuals in total, a large number of which have only one face in the groundtruth. To obtain reliable statistics, we only include individuals with at least three faces in the groundtruth dataset, leaving us with 438 individuals. We found that the average error rate per individual decreases when individuals are more "popular." This is because more popular individuals tend to have more SFSN images in our dataset, based on which a more reliable visual model can be built. Also, their web images tend to  have more near-duplicates which provide more textual information for annotation. The imbalance nature of the database makes it more realistic and more challenging to existing face recognition algorithms. Finally, we list the top five celebrities with the largest numbers of distinct images in CFW database in Table <ref type="table" target="#tab_4">VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Annotation</head><p>In this section, we evaluate the proposed image annotation algorithm for generating image-level names (IA-ILN). To evaluate the annotation results, we check whether the most confident image-level names appear in an image of faces.</p><p>In the first experiment, we compare our image annotation algorithm with three baseline algorithms based on the distribution of celebrity names in a single webpage <ref type="bibr" target="#b41">[42]</ref>. The result is summarized in Table <ref type="table" target="#tab_5">VII</ref>. It can be seen that, by leveraging information from multiple webpages, our algorithm achieves much better results than the baseline algorithms.</p><p>To further demonstrate the effectiveness of using multiple webpages discovered by near-duplicate image detection, we conduct a second experiment by varying the number of webpages used for image annotation and evaluating the change in the annotation accuracy. The result is summarized in Fig. <ref type="figure" target="#fig_4">5</ref>. It can be seen that the error rate drops down quickly when more webpages are used. This result shows that by mining an expanded set of webpages discovered from near-duplicate image detection, the proposed algorithm can effectively suppresses the noise in the surrounding text. Another interesting observation is that the annotation error rate of SFSN images drops much faster than those of SFMN and MF images when increasing the number of webpages used in the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multimodal Name Assignment</head><p>In this section, the proposed multimodal name assignment algorithm (MMNA) is evaluated.</p><p>Each of SFSN images has a single image-level name which can be used as face label directly. While for SFMN and MF images, MMNA is used to assign their image-level names to the faces in the images. To evaluate MMNA, all SFMN and MF images from the groundtruth dataset are used, which contain 9757 faces in total.</p><p>We compare MMNA with two widely used methods for name assignment. The first one proposes an efficient algorithm based on min-cost maxflow graphs (MaxFlow) <ref type="bibr" target="#b2">[3]</ref> to identify the optimal name-face assignment, which is the state-of-the-art algorithm on associating names with faces on the web. To implement the algorithm in <ref type="bibr" target="#b2">[3]</ref>, two graph construction methods are used. One is KNN linear, where a node in the graph only connects with its K nearest neighbors. The edges between a node and its nearest neighbors are assigned a series of weights:</p><p>. The other graph construction method is -neighborhood method, where nonzero edge weights are assigned only to neighbors whose distances are smaller than . The second baseline algorithm is a generative model proposed in <ref type="bibr" target="#b0">[1]</ref> which leverages both visual and textual cues, and assumes a gaussian mixture model for facial features with each component corresponding to all the faces of a celebrity. Please note that both baseline algorithms used SFSN images for visual modeling. The first baseline <ref type="bibr" target="#b2">[3]</ref> used SFSN images to construct the subgraph for each person and compute scores for assigning names to faces, while the second baseline <ref type="bibr" target="#b0">[1]</ref> used SFSN images to build Gaussian model for each individual.</p><p>Results: The results are summarized in Table <ref type="table" target="#tab_6">VIII</ref>. It can be seen that without the context likelihood (as defined in Section VI-D), the proposed name assignment algorithm achieves moderately better results than baseline algorithms. However, after incorporating context likelihood, the proposed multimodal name assignment significantly outperforms  Discussion: The reasons for the relatively high error rate are twofold. First the number of labeled faces for less popular celebrities is limited, as demonstrated in Table <ref type="table" target="#tab_7">IX</ref>. From the table, it can be seen that the proposed algorithm encounters much more failures when ground truth names have fewer SFSN faces. Second, we are handling uncontrolled real-world faces. Fig. <ref type="figure">6</ref> shows some test faces which are nonfrontal faces or occluded. These faces are much more difficult to recognize than facial images taken in a controlled environment, providing novel challenges and opportunities to face related research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Celebrity Recognition With CFW Dataset</head><p>In this section, we evaluate the power of CFW database on celebrity recognition.</p><p>Recognition Approach: We adopt an instance-search approach for the celebrity recognition task. For a given query face Fig. <ref type="figure">7</ref>. Results for celebrity face recognition (Recall@K). The -axis corresponds to Recall and the -axis corresponds to the number of returned names to be considered, i.e., K. When K is set to 1, Recall@K is equivalent to recognition rate, corresponding to the left-most bars in the figure. "TH=2000" corresponds to evaluation results when the recognition system ignores the votes from CFW faces that look too similar to testing faces, i.e., visual distance smaller than 2000, in order to reduce the number of possible near-duplicates of testing faces.</p><p>, its nearest neighbors in the entire CFW database vote for their labeled names with the following formula: Score</p><p>where is a celebrity name, is the confidence of face being named and is the transition probability from to as defined in <ref type="bibr" target="#b10">(11)</ref>. The name with the highest voting score is returned to the user. Note that the input face is not associated with any image context so no context likelihood can be leveraged.</p><p>Query Images: A query dataset of 4737 facial images for 240 celebrities was downloaded from Internet. The 240 celebrities were randomly sampled from the individuals with at least 50 images in the database. The images were downloaded from top Google image search results. Then faces were extracted from the downloaded images by the same face detector <ref type="bibr" target="#b5">[6]</ref> for generating CFW dataset. Finally, we manually checked the query faces and their labels to ensure the correctness.</p><p>Evaluation Metric: We use Recall@K as our evaluation metric, which measures whether the correct celebrity name can be returned in the top K names ranked by ( <ref type="formula" target="#formula_1">16</ref>). This measurement is equivalent to recognition rate when K is set to 1.</p><p>Results and Observations: The evaluation results are summarized in Fig. <ref type="figure">7</ref>. On average, our system achieves an recognition rate of 85.83%. When considering returned names up to the third highest vote, the probability that a correct name is returned increases to 93.05%. This result is fairly promising, considering that our database contains 421 K individuals and 2.45 million images, and there are large variations in pose, expression and illumination of the testing images.</p><p>To further challenge the recognition system, in the returned similar faces, we ignore top ones whose visual distances to the testing faces [i.e., in <ref type="bibr" target="#b10">(11)</ref>] are smaller than a threshold (TH). This most probably will reduce the number of near-duplicates of testing faces in our database. The evaluation results when TH is set to 2000 are also shown in Fig. <ref type="figure">7</ref>. We can see that even in this challenging situation, a recognition rate of 83.66% is still achieved, which further demonstrates the effectiveness of the developed recognition system as well as the scale and diversity of the CFW dataset.</p><p>Finally, we provide a few examples of testing faces and the recognition results in Fig. <ref type="figure" target="#fig_5">8</ref>. From these examples, one can observe that pose and expression have a large impact on visual similarity. This indicates that, in order to build an accurate instance-search based celebrity recognition engine, it is crucial to collect a database that includes faces of different visual variations for all the celebrities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we have presented an approach to construct a large-scale labeled celebrity face database from web images.</p><p>The approach comprises of two components. The first component is a image annotation system. By analyzing surrounding text of the near-duplicates of an input image, the proposed image annotation system is able to provide a set of names corresponding to the celebrities appearing in the image. With the annotation results, a multimodal name assignment algorithm is proposed to assign names to faces in the image, which is the second component of our approach. The assignment algorithm adopts a probabilistic approach which combines information from both visual features and image context.</p><p>Using the proposed approach, a celebrity database of 2.45 million distinct images and 421 436 celebrities is constructed. A celebrity recognition system has been built and evaluated to demonstrate the potential of the database.</p><p>In the future work, we will further improve the accuracy of face labels by investigating the visual consistency within the group of faces for a celebrity. Besides, we will try to merge the faces of the same person but with different names, i.e., stage name, nickname, birth name, etc. Meanwhile, we will continue enlarging the celebrity database. During this process, the CFW dataset will be delivered to research communities by making it publicly available online. We hope CFW will become a central resource for a broad range of face related research, e.g., face recognition, clustering, gender recognition, naming celebrities in videos, etc. Meanwhile, we will study methods to infer more properties of the celebrity images from the web, such as location, time, event, etc. This will improve the richness of the dataset labels and facilitate users to browse the dataset in a structural manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample faces from automatically collected and labeled "Celebrities on the Web" (CFW) database with diversified pose, expression, and lighting conditions.</figDesc><graphic coords="1,304.98,147.12,246.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example web images with multiple faces belonging to the same persons.</figDesc><graphic coords="2,306.00,64.14,246.00,136.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Celebrity face annotation framework. As shown in (a), the first step of the annotation framework is to automatically construct a large-scale celebrity name vocabulary from semi-structured web data. (b) illustrates the proposed image annotation system to label an input image with celebrity names, with example annotation results in (c). (d) presents weak labels for celebrity faces obtained based on the image annotation result (i.e., the image-level names). (e) shows that the final face labels are estimated by analyzing the weak labels and the visual similarity. Note that, with information from multiple webpages, we are able to remove incorrect names like Connor Cruise in the second surrounding text of (b).</figDesc><graphic coords="3,43.02,64.14,504.00,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example is provided to illustrate the effectiveness of the context likelihood. On the left of the figure is the image annotation results. In the middle of the figure, we show how context likelihood helps correct name assignment error caused by noises in the visual similarity measurement. Here, the noises refer to the large variation in visual appearances of celebrity faces and lead to incorrect nearest neighbors as shown on the right of the figure.</figDesc><graphic coords="7,55.98,67.14,478.98,126.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Annotation error rate on all kinds of web celebrity images drops significantly when more webpages hosting near-duplicates of target images are used.</figDesc><graphic coords="9,310.02,64.14,235.98,183.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example results for celebrity recognition prototype. The prototype adopts an instance-search based approach to recognize input faces. In the figure, the testing faces are shown on the left. Their nearest neighbors retrieved from CFW database are shown in the middle. On the right are the recognized names with confidence scores computed by (16).</figDesc><graphic coords="11,76.98,65.10,435.00,543.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III ACCURACY</head><label>III</label><figDesc>COMPARISON BETWEEN THE CFW DATABASE WITH OTHER AUTOMATICALLY LABELED DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV DATASET</head><label>IV</label><figDesc>SIZES AND ERROR RATES FOR DIFFERENT THRESHOLDS ON ANNOTATION CONFIDENCE. WE SEE THAT WHEN WE ONLY KEEP THE MOST CONFIDENCE SFSN FACES, WE CAN GET VERY SMALL ERROR RATES FOR A LARGE GROUP OF FACES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V NUMBER</head><label>V</label><figDesc>OF INDIVIDUALS AND AVERAGE ERROR RATE PER INDIVIDUAL, WHOSE IMAGES IN THE CFW DATABASE ARE MORE THAN ACERTAIN THRESHOLD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI TOP</head><label>VI</label><figDesc>FIVE CELEBRITIES WITH THE LARGEST NUMBER OF DISTINCT IMAGES IN OUR DATABASE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII ACCURACY</head><label>VII</label><figDesc>COMPARISON BETWEEN THE PROPOSED ILNA ALGORITHM AND SINGLE WEBPAGE BASED METHODS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>ON NAME ASSIGNMENT ACCURACY BETWEEN THE PROPOSED METHOD (MMNA)</figDesc><table /><note><p>AND BASELINE ALGORITHMS Fig. 6. Example testing faces for name assignment evaluations. Faces on top are nonfrontal faces, while faces on the bottom are occluded.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX NAME</head><label>IX</label><figDesc>ASSIGNMENT ERROR RATE DISTRIBUTION AGAINST POPULARITY OF GROUND TRUTH CELEBRITY NAMES. POPULARITY IS MEASURED BY THE NUMBER</figDesc><table /><note><p>OF SFSN FACES BELONGING TO THE CELEBRITIES baseline methods, which demonstrates the power of context likelihood.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>was set to be one tenth of the average distance between two faces.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>was set to be 0.05 in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>The accuracy of CFW database was evaluated on the groundtruth dataset of 21</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="735" xml:id="foot_3"><p>faces as introduced in TableI. The accuracy of Faces in the News and Labeled Yahoo News was evaluated in 1000 faces and 14 827 faces respectively, as reported in<ref type="bibr" target="#b19">[20]</ref> and<ref type="bibr" target="#b31">[32]</ref>. Since the evaluation methods are different, the results are used here only for the purpose of reference.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dr. Shum is a Fellow of the ACM for his contributions on computer vision and computer graphics.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Vancouver, British Columbia</pubPlace>
		</imprint>
	</monogr>
	<note>Who&apos;s in the picture,&quot; presented at the NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A graph based approach for naming faces in news photos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1477" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic face naming with caption-based supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-media alignment of names and faces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="27" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autotagging facebook: Social network context improves photo annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st IEEE Workshop Internet Vis</title>
		<meeting>1st IEEE Workshop Internet Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multiple-instance pruning for learning efficient cascade detectors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>British Columbia</publisher>
			<pubPlace>Vancouver</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pruning training sets for learning of object categories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cas-peal large-scale Chinese face database and baseline evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybernetics-Part A</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cmu pose, illumination, and expression database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Massachusetts</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Naming faces in broadcast news video by image Google</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="717" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding people frequently appearing in news</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIVR</title>
		<meeting>CIVR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale learning and recognition of faces in web videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audiovisual celebrity recognition in unconstrained web videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sargin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1977" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Being John Malkovich</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV (1)</title>
		<meeting>ECCV (1)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging archival video for building face datasets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Who&apos;s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<pubPlace>Vancouver, British Columbia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="848" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from ambiguously labeled images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="919" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reliable tags using image similarity: Mining specificity and expertise from large-scale multimedia databases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia: Workshop Web-Scale Multimedia Corpus</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arista-Image search to annotation on billions of web photos</title>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2987" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable face image retrieval with identity-based quantization and multi-reference re-ranking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3469" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting named entities and relating them over time based on wikipedia</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mladenic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica (Slovenia)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="468" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Web object retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale duplicate detection for web image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="353" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient indexing for large scale visual search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1103" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial-bag-offeatures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename></persName>
		</author>
		<idno>0001</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3352" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face recognition from caption-based supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/inria-00585834/en" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Face recognition with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fusing gabor and lbp feature sets for kernelbased face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMFG</title>
		<meeting>AMFG</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="235" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale local binary pattern histograms for face recognition</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICB</title>
		<meeting>ICB</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="809" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boosting local binary pattern (lbp)-based face recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SINOBIOMET-RICS</title>
		<meeting>SINOBIOMET-RICS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple instance metric learning from automatically labeled bags of faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV (1)</title>
		<meeting>ECCV (1)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="634" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition with learningbased descriptor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2707" to="2714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">News image annotation on a large parallel text-image corpus</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tirilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Claveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC, Malta</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
