<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Backpropagation Algorithm Functions for the Multilayer Perceptron</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marius-Constantin</forename><surname>Popescu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering Aurel</orgName>
								<orgName type="institution">Vlaicu University of Arad</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valentina</forename><surname>Balas</surname></persName>
							<email>balas@inext.ro</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">University of Constantin Brancusi</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onisifor</forename><surname>Olaru</surname></persName>
							<email>onisifor.olaru@yahoo.com</email>
							<affiliation key="aff3">
								<orgName type="institution">ROMANIA Technical University of Sofia</orgName>
								<address>
									<country key="BG">BULGARIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Mastorakis</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electromechanical and Environmental Engineering</orgName>
								<orgName type="institution">University of Craiova</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Backpropagation Algorithm Functions for the Multilayer Perceptron</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1790-2769</idno>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Backpropagation algorithm</term>
					<term>Gradient method</term>
					<term>Multilayer perceptron</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The attempts for solving linear unseparable problems have led to different variations on the number of layers of neurons and activation functions used. The backpropagation algorithm is the most known and used supervised learning algorithm. Also called the generalized delta algorithm because it expands the training way of the adaline network, it is based on minimizing the difference between the desired output and the actual output, through the downward gradient method (the gradient tells us how a function varies in different directions). Training a multilayer perceptron is often quite slow, requiring thousands or tens of thousands of epochs for complex problems. The best known methods to accelerate learning are: the momentum method and applying a variable learning rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The multilayer perceptron is the most known and most frequently used type of neural network. On most occasions, the signals are transmitted within the network in one direction: from input to output. There is no loop, the output of each neuron does not affect the neuron itself. This architecture is called feed-forward (Fig. <ref type="figure">1</ref>). Fig. <ref type="figure">1</ref>: Neural network feed-forward multilayer.</p><p>Layers which are not directly connected to the environment are called hidden. In the reference material, there is a controversy regarding the first layer (the input layer) being considered as a stand-alone (itself a) layer in the network, since its only function is to transmit the input signals to the upper strata, without any processing on the inputs. In what follows, we will count only the layers consisting of stand-alone neurons, but we will mention that the inputs are grouped in the input layer. There are also feed-back networks, which can transmit impulses in both directions, due to reaction connections in the network. These types of networks are very powerful and can be extremely complicated. They are dynamic, changing their condition all the time, until the network reaches an equilibrium state, and the search for a new balance occurs with each input change. Introduction of several layers was determined by the need to increase the complexity of decision regions. As shown in the previous paragraph, a perceptron with a single layer and one input generates decision regions under the form of semiplanes. By adding another layer, each neuron acts as a standard perceptron for the outputs of the neurons in the anterior layer, thus the output of the network can estimate convex decision regions, resulting from the intersection of the semiplanes generated by the neurons. In turn, a three-layer perceptron can generate arbitrary decision areas (Fig. <ref type="figure" target="#fig_0">2</ref>). Regarding the activation function of neurons, it was found that multilayer networks do not provide an increase in computing power compared to networks with a single layer, if the activation functions are linear, because a linear function of linear functions is also a linear function. The power of the multilayer perceptron comes precisely from non-linear activation functions. Almost any non-linear function can be used for this purpose, except for polynomial functions. Currently, the functions most commonly used today are the single-pole (or logistic) sigmoid, shown in Figure <ref type="figure">3</ref>: It may be noted that the sigmoid functions act approximately linear for small absolute values of the argument and are saturated, somewhat taking over the role of threshold for high absolute values of the argument. It has been shown <ref type="bibr" target="#b4">[4]</ref> that a network (possibly infinite) with one hidden layer is able to approximate any continuous function. This justifies the property of the multilayer perceptron to act as a universal approximator. Also, by applying the Stone-Weierstrass theorem in the neural network, it was demonstrated that they can calculate certain polynomial expressions: if there are two networks that calculate exactly two functions f 1 , namely f 2 , then there is a larger network that calculates exactly a polynomial expression of f 1 and f 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The backpropagation algorithm</head><p>The method was first proposed by <ref type="bibr" target="#b1">[2]</ref>, but at that time it was virtually ignored, because it supposed volume calculations too large for that time. It was then rediscovered by <ref type="bibr" target="#b16">[16]</ref>, but only in the mid-'80s was launched by Rumelhart, Hinton and Williams <ref type="bibr" target="#b14">[14]</ref> as a generally accepted tool for training of the multilayer perceptron. The idea is to find the minimum error function e(w) in relation to the connections weights.</p><p>The algorithm for a multilayer perceptron with a hidden layer is the following <ref type="bibr" target="#b8">[8]</ref>:</p><p>Step 1: Initializing. All network weights and thresholds are initialized with random values, distributed evenly in a small range, for example</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? - i i F . , F . 4 2 4 2</formula><p>, where F i is the total number of inputs of the neuron i <ref type="bibr" target="#b6">[6]</ref>. If these values are 0, the gradients which will be calculated during the trial will be also 0 (if there is no direct link between input and output) and the network will not learn. More training attempts are indicated, with different initial weights, to find the best value for the cost function (minimum error). Conversely, if initial values are large, they tend to saturate these units. In this case, derived sigmoid function is very small. It acts as a multiplier factor during the learning process and thus the saturated units will be nearly blocked, which makes learning very slow.</p><p>Step 2: A new era of training. An era means presenting all the examples in the training set. In most cases, training the network involves more training epochs. To maintain mathematical rigor, the weights will be adjusted only after all the test vectors will be applied to the network. Therefore, the gradients of the weights must be memorised and adjusted after each model in the training set, and the end of an epoch of training, the weights will be changed only one time (there is an "on-line" variant, more simple, in which the weights are updated directly, in this case, the order in which the vectors of the network are presented might matter. All the gradients of the weights and the current error are initialized with 0: ?w ij = 0 , E = 0.</p><p>Step 3: The forward propagation of the signal 3.1 An example from the training set is applied to the to the inputs. 3.2 The outputs of the neurons from the hidden layer are calculated:</p><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? - ? = ? = n i j ij i j w ) p ( x f ) p ( y 1 ,<label>(3)</label></formula><p>where n is the number of inputs for the neuron j from the hidden layer, and f is the sigmoid activation function.</p><p>3.3The real outputs of the network are calculated:</p><formula xml:id="formula_2">? ? ? ? ? ? ? ? ? - ? = ? = m i k jk jk k ) p ( w ) p ( x f ) p ( y 1 , (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where m is the number of inputs for the neuron k from the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The error per epoch is updated:</head><p>( )</p><formula xml:id="formula_4">2 e E E 2 k ) p ( + = . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>Step 4: The backward propagation of the errors and the adjustments of the weights 4.1. The gradients of the errors for the neurons in the output layer are calculated:</p><formula xml:id="formula_6">) p ( e ' f ) p ( k k ? = ? , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where f' is the derived function for the activation, and the error .</p><formula xml:id="formula_8">) p ( y ) p ( y ) p ( e k k , d k - =</formula><p>If we use the single-pole sigmoid (equation 1, its derived is:</p><formula xml:id="formula_9">( ) ( ) x ( f ) x ( f e e ) x ( ' f x x - ? = + = - - 1 1 2</formula><p>). (7)   If we use the bipolar sigmoid (equation 2, its derived is:</p><formula xml:id="formula_10">( ) ( )( ) x ( f ) x ( f a e e a ) x ( ' f x a x a + ? - ? = + ? = ? - ? - 1 1 2 1 2 2</formula><p>). <ref type="bibr" target="#b8">(8)</ref> Further, let's suppose that the function utilized is the single-pole sigmoid. Then the equation ( <ref type="formula" target="#formula_6">6</ref>) becomes:</p><p>(</p><p>)</p><formula xml:id="formula_11">) p ( e ) p ( y ) p ( y ) p ( k k k k ? - ? = ? 1 . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>4.2. The gradients for the weights between the hidden layer and the output layer are updated:</p><formula xml:id="formula_13">) p ( ) p ( y ) p ( w ) p ( w k j jk jk ? ? + ? = ? . (10)</formula><p>4.3. The gradients of the errors for the neurons in the hidden layer are calculated:</p><formula xml:id="formula_14">( ) ? = ? ? ? - ? = ? l k jk k j j j ) p ( w ) p ( ) p ( y ) p ( y ) p ( 1 1</formula><p>, <ref type="bibr" target="#b11">(11)</ref> where l is the number of outputs for the network. 4. <ref type="bibr" target="#b4">4</ref> The gradients of the weights between the input layer and the hidden layer are updated:</p><formula xml:id="formula_15">) p ( ) p ( x ) p ( w ) p ( w j i ij ij ? ? + ? = ? . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>Step 5: A new iteration If there are still test vectors in the current training epoch, pass to step 3. If not, the weights af all the connections will be updated, based on the gradients of the weights:</p><formula xml:id="formula_17">ij ij ij w w w ? ? ? + = , (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where ? is the learning rate. After the fourth iteration, the perceptron separates two classes (0 and 1) by a line. After the fourth iteration the perceptron separates by a line two classes (0 and 1). The percepton was tested in the presence of the vector input .</p><formula xml:id="formula_19">? ? ? ? ? ? 1 0</formula><p>The perceptron makes the logic OR function for which the classes are linearly separable; that is one of the conditions of the perceptron. If the previous programs is performed for the exclusive OR function, we will observe that, for any of the two classes, there is no line to allow the separation into two classes (0 and 1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods to accelerate the learning</head><p>The momentum method <ref type="bibr" target="#b14">[14]</ref> proposes adding a term to adjust weights. This term is proportional to the last amendment of the weight, i.e. the values with which the weights are adjusted are stored and they directly influence all further adjustments:</p><formula xml:id="formula_20">) p ( w ) p ( w ) p ( w ij ij ij 1 - ? ? ? + ? = ? . (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>Adding a new term is done after the update of the gradients for the weights from equations 10 and 12.</p><p>The method of variable learning rate <ref type="bibr" target="#b15">[15]</ref> is to use an individual learning rate for each weight and adapt these parameters in each iteration, depending on the successive signs of the gradients <ref type="bibr" target="#b9">[9]</ref>:</p><formula xml:id="formula_22">? ? ? ? ? ? ? ? ? ? - ? - = ? - ? - ? = ? - ? = )) 1 ( sgn( )) ( sgn( ), 1 ( )) 1 ( sgn( )) ( sgn( ), 1 ( ) ( p w p w p d p w p w p u p ij ij ij ij ij ij ij ? ? ? (15)</formula><p>If during the training the error starts to increase, rather than decrease, the learning rates are reset to initial values and then the process continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Practical considerations of working with multilayer perceptrons</head><p>For relatively simple problems, a learning rate of 7 . 0 = ? is acceptable, but in general it is recommended the learning rate to be around 0.2. To accelerate through the momentum method, a satisfactory value for ? is 0.9. If the learning rate is variable, typical values that work well in most situations are u = 1.2 and d = 0.8. Choosing the activation function for the output layer of the network depends on the nature of the problem to be solved. For the hidden layers of neurons, sigmoid functions are preferred, because they have the advantage of both non-linearity and the differentially (prerequisite for applying the backpropagation algorithm). The biggest influence of a sigmoid on the performances of the algorithm seems to be the symmetry of origin <ref type="bibr" target="#b0">[1]</ref>. The bipolar sigmoid is symmetrical to the origin, while the unipolar sigmoid is symmetrical to the point (0, 0.5), which decreases the speed of convergence. For the output neurons, the activation functions adapted to the distribution of the output data are recommended. Therefore, for problems of the binary classification (0/1), the single-pole sigmoid is appropriate. For a classification with n classes, each corresponding to a binary output of the network (for example, an application of optical character recognition), the softmax extension of the single-pole sigmoid may be used. (</p><formula xml:id="formula_23">)<label>16</label></formula><p>For continuous values, we can make a preprocessing and a post processing of data, so that the network will operate with scaled values, for example in the range [-0.9, 0.9] for the hyperbolic tangent. Also, for continuous values, the activation function of the output neurons may be linear, especially if there are no known limits for the range in which these can be found. In a local minimum, the gradients of the error become 0 and the learning no longer continues. A solution is multiple independent trials, with weights initialized differently at the beginning, which raises the probability of finding the global minimum. For large problems, this thing can be hard to achieve and then local minimums may be accepted, with the condition that the errors are small enough. Also, different configurations of the network might be tried, with a larger number of neurons in the hidden layer or with more hidden layers, which in general lead to smaller local minimums. Still, although local minimums are indeed a problem, practically they are not unsolvable. An important issue is the choice of the best configuration for the network in terms of number of neurons in hidden layers. In most situations, a single hidden layer is sufficient.</p><p>There are no precise rules for choosing the number of neurons. In general, the network can be seen as a system in which the number of test vectors multiplied by the number of outputs is the number of equations and the number of weights represents the number of unknown. The equations are generally nonlinear and very complex and so it is very difficult to solve them exactly through conventional means. Training algorithm aims precisely to find approximate solutions to minimize errors. If the network approximates the training set well, this is not a guarantee that it will find the same good solutions for the data in another set, the testing set. Generalization implies the existence of regularities in the data, of a model that can be learned. In analogy with classical linear systems, this would mean some redundant equations. Thus, if the number of weights is less than the number of test vectors, for a correct approximation, the network must be based on intrinsic patterns of data models, models which are to be found in the test data as well. A heuristic rule states that the number of weights should be around or below one tenth of the number of training vectors and the number of exits. In some situations however (eg, if training data are relatively few), the number of weights can be even half of the product. For a multilayer perceptron is considered that the number of neurons in a layer must be sufficiently large so that this layer to provide three or more edges for each convex region identified by the next layer <ref type="bibr" target="#b5">[5]</ref>. So the number of neurons in a layer must be more than three times higher than that of the next layer. As mentioned before, a sufficient number of weights leads to under-fitting, while too many of the weights leads to over-fitting, events presented in Figure <ref type="figure" target="#fig_4">6</ref>.  Example: We associate an input vector X=[1 -0.5] and a target vector T=[0.5 1] of size imposed by two restrictions that can be reduced to two degrees of freedom (the points W and the slopes B) of a single Adaline neuron <ref type="bibr" target="#b9">[9]</ref>. We suggest solving the linear system of 2 equations with 2 unknowns <ref type="bibr" target="#b12">[12]</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Multilayer perceptrons are the most commonly used types of neural networks. Using the backpropagation algorithm for training, they can be used for a wide range of applications, from the functional approximation to prediction in various fields, such as estimating the load of a calculating system or modelling the evolution of chemical reactions of polymerization, described by complex systems of differential equations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b10">[10]</ref>.</p><p>In implementing the algorithm, there are a number of practical problems, mostly related to the choice of the parameters and network configuration.</p><p>First, a small learning rate leads to a slow convergence of the algorithm, while a too high rate may cause failure (algorithm will "jump" over the solution). Another problem characteristic of this method of training is given by local minimums. A neural network must be capable of generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Decision regions of multilayer perceptrons.</figDesc><graphic url="image-5.png" coords="3,58.98,132.60,225.06,329.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig.3: Sigmoid single-pole activation function.And the bipolar sigmoid (the hyperbolic tangent) function, shown in Figure4, for a=2:</figDesc><graphic url="image-6.png" coords="3,62.34,617.16,218.34,109.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: The evolution of the sum of squared errors.</figDesc><graphic url="image-8.png" coords="5,56.70,278.94,224.88,174.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: The capacity for the approximation of a neural network based on the number of weights.</figDesc><graphic url="image-9.png" coords="6,59.70,600.60,223.56,113.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>w+b=0. 5 ,Fig. 7 :</head><label>57</label><figDesc>Fig.7: The points (weight) and slopes (bias) of the neuron identified as algebraic solutions.</figDesc><graphic url="image-10.png" coords="6,309.00,557.40,223.38,197.94" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 11th WSEAS International Conference on Sustainability in Science Engineering ISSN: 1790-2769</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ISBN: 978-960-474-080-2</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multilayer perceptrons, in Handbook of Neural Computation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>IOP Publishing Ltd and Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optimal Control</title>
		<imprint>
			<date type="published" when="1969">1969</date>
			<pubPlace>Blaisdell, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Genetic Algorithms and Neural Networks Used in Optimization of a Radical Polymerization Process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Curteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Ungureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leon</surname></persName>
		</author>
		<imprint>
			<publisher>Buletinul</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">LV, seria tehnic?</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="85" to="93" />
		</imprint>
		<respStmt>
			<orgName>Universit??ii Petrol-Gaze din Ploie?ti</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Control, Signal Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Teorie ?i aplica?ii</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Costin</surname></persName>
		</author>
		<author>
			<persName><surname>Re?ele Neuronale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Teora, Bucure?ti</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural Networks: A Comprehensive Foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Macmillan, IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>G?lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Zaharia</surname></persName>
		</author>
		<title level="m">Load Balancing In Distributed Systems Using Cognitive Behavioral Models</title>
		<meeting><address><addrLine>Tome</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">XLVIII</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
		<respStmt>
			<orgName>Bulletin of Technical University of Ia?i</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Guide to Intelligent Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Negnevitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Addison Wiesley</publisher>
			<pubPlace>England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hybrid neural network for prediction of process parameters in injection moulding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="312" to="319" />
			<pubPlace>Petro?ani</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Annals of University of Petro?ani, Electrical Engineering, Universitas Publishing House</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mastorakis</surname></persName>
		</author>
		<title level="m">Equilibrium Dynamic Systems Integration Proceedings of the 10th WSEAS Int. Conf. on Automation &amp; Information</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">Mar.23-25, 2009</date>
			<biblScope unit="page" from="424" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modelarea ?i simularea proceselor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="261" to="273" />
		</imprint>
		<respStmt>
			<orgName>Editura Universitaria Craiova</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neuro-fuzzy control of induction driving, 6 th International Carpathian Control Congress</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petri?or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="214" />
			<pubPlace>Miskolc-Lillafured, Budapesta</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Euliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Lefebvre</surname></persName>
		</author>
		<title level="m">Neural and Adaptive Systems. Fundamentals Through Simulations</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Acceleration techniques for the backpropagation algorithm in</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wellekens</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Roots of Backpropagation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
