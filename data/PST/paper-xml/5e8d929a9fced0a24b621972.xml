<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In 30 th USENIX Security Symposium</title>
				<funder>
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">CIFAR</orgName>
				</funder>
				<funder>
					<orgName type="full">Landweber Fellowship</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA GARD, Microsoft</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-19">19 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hengrui</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">?University of Toronto and Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In 30 th USENIX Security Symposium</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-19">19 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.12200v2[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. As it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking instead leverages unused model capacity to have the model overfit to outlier input-output pairs. Such pairs are watermarks, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer.</p><p>We introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn features for classifying data that is sampled from the task distribution and data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, CIFAR-10, and Speech Commands validate that the defender can claim model ownership with 95% confidence with less than 100 queries to the stolen copy, at a modest cost below 0.81 percentage points on average in the defended model's performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Costs associated with machine learning (ML) are high. This is true in particular when large training sets need to be collected <ref type="bibr" target="#b15">[16]</ref> or the parameters of complex models tuned <ref type="bibr" target="#b48">[49]</ref>. Therefore, models being deployed for inference constitute valuable intellectual property that need to be protected. A good example of a pervasive deployment of ML is automatic speech recognition <ref type="bibr" target="#b17">[18]</ref>, which forms the basis for personal assistants in ecosystems created by Amazon, Apple, Google, and Microsoft. However, deploying models to make predic-tions creates an attack vector which adversaries can exploit to mount model extraction attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Techniques for model extraction typically require that the adversary query a victim model with inputs of their choiceanalogous to chosen-plaintext attacks in cryptography. The adversary uses the victim model to label a substitute dataset.</p><p>One form of extraction involves using the substitute dataset to train a substitute model, which is a stolen copy of the victim model <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>. Preventing model extraction is difficult without sacrificing performance for legitimate users <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref>: queries made by attackers and benign users may be sampled from the same task distribution.</p><p>One emerging defense proposal is to extend the concept of watermarking <ref type="bibr" target="#b21">[22]</ref> to ML <ref type="bibr" target="#b5">[6]</ref>. The defender purposely introduces outlier input-output pairs (x, y) only known to them in the model's training set-analogous to poisoning or backdoor attacks <ref type="bibr" target="#b0">[1]</ref>. To claim ownership of the model f , the defender demonstrates that they can query the model on these specific inputs x and have knowledge of the (potentially) surprising prediction f ( x) = ? returned by the model. Watermarking techniques exploit the large capacity in modern architectures <ref type="bibr" target="#b0">[1]</ref> to learn watermarks without sacrificing performance when classifying data from the task distribution.</p><p>Naive watermarking can be defeated by an adaptive attacker because the watermarks are outliers to the task distribution. As long as the adversary queries the watermarked model only on inputs that are sampled from the task distribution, the stolen model will only retain the victim model's decision surface relevant to the task distribution, and therefore ignore the decision surface learned relevant to watermarking. In other words, the reason why watermarking can be performed with limited impact on the model's accuracy is the reason why watermarks can easily be removed by an adversary. Put another way, watermarked models roughly split their parameter set into two subsets, the first encodes the task distribution while the second overfits to the outliers (i.e., watermarks).</p><p>In this paper, we propose a technique that addresses this fundamental limitation of watermarking. Entangled Watermark Embedding (EWE) encourages a model to extract fea-tures that are jointly useful to (a) learn how to classify data from the task distribution and (b) predict the defender's expected output on watermarks. Our key insight is to leverage the soft nearest neighbor loss <ref type="bibr" target="#b11">[12]</ref> to entangle representations extracted from training data and watermarks. By entanglement, we mean that the model represents both types of data similarly. Entangling produces models that use the same subset of parameters to recognize training data and watermarks. Hence, it is difficult for an adversary to extract the model without its watermarks, even if the adversary queries models with samples only from the task distribution to avoid triggering watermarks (e.g., the adversary avoids out-of-distribution inputs like random queries). The adversary is forced to learn how to reproduce the defender's chosen output on watermarks. An attempt to remove watermarks would also have to harm the stolen substitute classifier's generalization performance on the task distribution, which would defeat the purpose of model extraction (i.e., steal a well-performing model).</p><p>We evaluate 1 the approach on four vision datasets-MNIST <ref type="bibr" target="#b27">[28]</ref>, Fashion MNIST <ref type="bibr" target="#b54">[55]</ref>, CIFAR-10, and CIFAR-100 <ref type="bibr" target="#b25">[26]</ref> as well as an audio dataset-Google Speech Command <ref type="bibr" target="#b53">[54]</ref>. We demonstrate that our approach is able to watermark models at moderate costs to utility-below 0.81 percentage points on average on the datasets considered. Unlike prior approaches we compare against, our watermarked classifiers are robust to model extraction attacks. Stolen copies retain the defender's expected output on &gt; 38% (in average) of entangled watermarks (see Table <ref type="table">1</ref>, where the baseline achieves &lt; 10% at best), which enables a classifier to claim ownership of the model with 95% confidence in less than 100 queries to the stolen copy. We also show that defenses against backdoors are ineffective against our entangled watermarks. The contributions of our paper are:</p><p>? We identify a fundamental limitation of existing watermarking strategies: the watermarking task is learned separately from the primary task. ? We introduce Entangled Watermark Embedding <ref type="bibr">(EWE)</ref> to enable models to jointly learn how to classify samples from the task distribution and watermarks. ? We systematically calibrate EWE on vision and audio datasets. We show that when points being watermarked are carefully chosen, EWE offers advantageous tradeoffs between model utility and robustness of watermarks to model extraction, on the datasets considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we provide background to motivate our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning with DNNs</head><p>We focus on classification within the supervised learning setting <ref type="bibr" target="#b36">[37]</ref>, where the goal is to learn a decision function that 1 Code at: github.com/cleverhans-lab/entangled-watermark maps the input x to a discrete output y. The set of possible outputs are called classes. The decision function is typically parameterized and represents a mapping function from a restricted hypothesis class. A task distribution is analyzed to learn the function's parameters. Empirically, we use a dataset of input-output training examples, denoted by D = {X,Y } or {(x i , y i )} N i=1 , to represent the task distribution. One hypothesis class is deep neural networks (DNNs). DNNs are often trained with variants of the backpropagation algorithm <ref type="bibr" target="#b45">[46]</ref> <ref type="foot" target="#foot_0">2</ref> . Backpropagation updates each parameter in the DNNs by differentiating the loss function with respect to each parameter. Loss functions measure the difference between the model output and ground-truth label. A common choice for classification tasks is the cross-entropy <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_0">L CE (X,Y ) = -1 N ? N i ? k?[K] y ik log f k (x i )</formula><p>where y i is a one-hot vector encoding the ground-truth label and f k (x i ) is the prediction score of model f for the k th class among the K possible classes. Because this loss can be interpreted as measuring the KL divergence between the task and learned distributions, minimizing this loss encourages similarity between model predictions and labels <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Extraction</head><p>Model extraction attacks target the confidentiality of ML models <ref type="bibr" target="#b50">[51]</ref>. Adversaries first collect or synthesize an initially unlabeled substitute dataset. Papernot et al. <ref type="bibr" target="#b42">[43]</ref> used Jacobianbased dataset augmentation, while Tramer et al. <ref type="bibr" target="#b50">[51]</ref> proposed three techniques that sample data uniformly. Adversaries exploit the ability to query the victim model for label predictions to annotate a substitute dataset. Next, they train a copy of the victim model with this substitute dataset. <ref type="foot" target="#foot_1">3</ref> The adversary's goal is to obtain a stolen replica that performs similarly to the victim, whilst making few labeling queries.</p><p>Approaches that use differential querying <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> are out of scope here because they make a large number of queries to obtain a functionally-equivalent model. We also exclude attacks that rely on side-channel information <ref type="bibr" target="#b2">[3]</ref>. We focus on attacks that attempt to extract a model with roughly the same accuracy performance only by querying for the model's prediction. This has been demonstrated against linear models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref>, decision trees <ref type="bibr" target="#b50">[51]</ref>, and DNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>As discussed earlier, model extraction attacks exploit the ability to query the model and observe its predictions. Potential countermeasures restrict or modify information returned in each query <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref>. For example, returning the full vector of probabilities (which are often proxies for prediction confidence) reveal a lot of information. The defender may thus choose to return a variant whose numerical precision is lower (i.e., quantization) or even to only return the most likely label with or without the associated the output probability (i.e., hard labels). The defender could also choose to return a random label and/or noise. However, all of these countermeasures introduce an inherent trade-off between the utility of a model to its benign user and the ability of an adversary to extract it more or less efficiently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Watermarks</head><p>Watermarking has a long history in the protection of intellectual property for media like videos and images <ref type="bibr" target="#b21">[22]</ref>. Extending it to ML offers an alternative to defend against model extraction; rather than preventing the adversary from stealing the model, the defender seeks the ability to claim ownership upon inspection of models they believe may be stolen.</p><p>The idea behind watermarks is to have the watermarked model overfit to outlier input-output pairs known only to the defender. This can later be used to claim ownership of the model. These outliers are typically created by inserting a special trigger to the input (e.g., a small square in a non-intrusive location of an image). These inputs are the watermarks. For this reason, watermarking can be thought of as a form of poisoning, and in particular backdoor insertion <ref type="bibr" target="#b14">[15]</ref>, used for good by the defender. Zhang et al. <ref type="bibr" target="#b55">[56]</ref> and Nagai et al. <ref type="bibr" target="#b37">[38]</ref> also introduced watermarking algorithms that rely on data poisoning <ref type="bibr" target="#b19">[20]</ref>. Rouhani et al. <ref type="bibr" target="#b9">[10]</ref> instead embed some bits in the probability density function of different layers, but the idea remains to exploit overparameterization of DNNs.</p><p>If the defender encounters a model that also possesses the rare and unexpected behavior encoded by watermarks, he/she can reasonably claim that this model is a stolen replica. The concept of watermarks in ML is analogous to trapdoor functions <ref type="bibr" target="#b10">[11]</ref>: given watermarked samples, it is easy to verify if the model is watermarked. However, if one knows a model is watermarked, it is extremely hard to obtain the data used to watermark it (because the dimensionality of the input-output mapping is too high for attackers to search by brute force).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Difficulties in Watermarking</head><p>We consider DNNs, also used later to validate our EWE approach, because they typically generate the largest production costs: they are thus more likely to be the target of model extraction attacks. Our goal here is to analytically forge an intuition for the limitations that arise from naively training on watermarks that are not part of the task distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extraction-Induced Failures</head><p>Recall that to successfully watermark a DNN, the defender knows a particular input that is not necessarily from the task distribution, and has knowledge of the predicted output given this input. We construct an analytical example to show how such a watermarking scheme fails during model extraction.</p><p>Consider a binary classification task with a 2D input [x 1 , x 2 ] and a scalar output y set to 1 if x 1 + x 2 &gt; 1 and 0 otherwise. Inputs x 1 and x 2 , are sampled from two independent uniform distributions U(0,1). We watermark this model to output 1 if</p><formula xml:id="formula_1">x 2 = -1 regardless of x 1 .</formula><p>One could model this function as a feed-forward DNN shown in Figure <ref type="figure" target="#fig_0">1</ref>. A sigmoid activation ? is utilized as the ultimate layer to obtain the following model:</p><formula xml:id="formula_2">? = ?(w 1 ? R(x 1 + b 1 ) + w 2 ? R(x 2 + b 2 ) + w 3 ? R(x 2 + b 3 ) + b 4 -1)</formula><p>where R(x) = max(0, x) denotes a ReLU activation. We instantiate this model with the following parameter values:</p><formula xml:id="formula_3">y = ?(1 ? R(x 1 ) + 2 ? R(x 2 ) -1 ? R(x 2 + 2) + 2 -1)</formula><p>We chose parameter values to illustrate the following setting: (a) the model is accurate on both the task distribution and watermark, and (b) the neuron used to encode the watermark is also used by the task distribution. This enables us to show how the watermark is not extracted by the adversary, even though it is encoded by a neuron that is also used to classify inputs from the task distribution. As the adversary attempts to extract the model, they are unlikely to trigger the watermark by setting x 2 = -1 if they sample inputs from U(0,1) i.e., the task distribution. After training the substitute model with inputs from the task distribution and labels (which are predictions) obtained from the victim model, the decision function learned by the adversary is:</p><formula xml:id="formula_4">y = ?(0.96 ? R(x 1 ) + 0.54 ? R(x 2 ) + 0.54 ? R(x 2 ) -1)</formula><p>This function can be written as y = ?(0.96x 1 + 1.08x 2 -1) since x 1 , x 2 ? U(0,1). This is very similar to our objective function, y = ?(x 1 + x 2 -1), and has high utility for the adversary. However, if the out-of-distribution (OOD) input x 2 is -1, the largest value of the function (obtained when x 1 = 1) is ?(-0.04), which leads to the non-watermarked result of y = 0 instead of y = 1; the watermark is removed during extraction. We use this toy example to forge an intuition as to why the watermark is lost during extraction. The task and watermark distributions are independent. If the model has sufficient capacity, it can learn from data belonging to both distributions. However, the model learns both distributions independently. In the classification example described above, back-propagating with respect to the task data would update all neurons, whereas back-propagating with respect to watermarked data only updates the third neuron. However, the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distinct Activation Patterns</head><p>We empirically show how training algorithms converge to a simple solution to learn the two data distributions simultaneously: they learn models whose capacity is roughly partitioned into two sub-models that each recognizes inputs from one of the two data distributions (task vs. watermarked). We trained a neural network, with one hidden layer of 32 neurons, on MNIST. It is purposely simple for clarity of exposition; we repeat this experiment on a DNN (see Figure <ref type="figure" target="#fig_28">21</ref> in Appendix A.3 giving the same conclusions). We watermark the model by adding a trigger (a 3 ? 3-pixel white square at corner) to the input and change the label that comes with it <ref type="bibr" target="#b55">[56]</ref>.</p><p>We record the neurons activated when the model predicts on legitimate task data from the MNIST dataset, as well as watermarked data. We plot the frequency of neuron activations in Figure <ref type="figure" target="#fig_2">2a</ref> for both (a) legitimate and (b) watermark data. Here, each square represents a neuron and a higher intensity (whiter color) represents more frequent activations. Confirming our hypothesis of two sub-models, we see that different neurons are activated for legitimate and watermarked data. As we further hypothesized, fewer neurons are activated for the watermark task, likely because this task (identifying the simple trigger) is easier than classifying hand-written digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entangling Watermarks</head><p>Motivated by the observation that watermarked models are partitioned into distinguishable sub-models (task vs. watermark), the intuition behind our proposal is to entangle the watermark with the task manifold. Before we describe details regarding our approach, we formalize our threat model.</p><p>Threat Model. The objective of our adversary is to extract a model without its watermark. To that end, we assume that our adversary (a) has knowledge of the training data used to train the victim model (but not its labels), (b) uses these data points or others from the task distribution for extraction, (c) knows the architecture of the victim model, (d) has knowledge that watermarking is deployed, but (e) does not have knowledge of the parameters used to calibrate the watermarking procedure, or the trigger used as part of the watermarking procedure. Observe that such an adversary is a powerful white-box adversary. The assumptions we make are standard, and are made in prior work as well <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Soft Nearest Neighbor Loss</head><p>Recall that the objective of our watermarking scheme is to ensure that watermarked models are not partitioned into distinguishable sub-models which will not survive extraction. To ensure that both the watermark and task distributions are jointly learned/represented by the same set of neurons (and consequently ensure survivability), we make use of the soft nearest neighbor loss (or SNNL) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref>. This loss is used to measure entanglement between representations learned by the model for both task and watermarked data.</p><formula xml:id="formula_5">SNNL(X,Y, T ) = - 1 N ? i?1..N log ? ? ? ? ? ? ? ? ? ? ? j?1..N j =i y i =y j e - ||x i -x j || 2 T ? k?1..N k =i e -||x i -x k || 2 T ? ? ? ? ? ? ? ? ? ? (a) (b)<label>(1)</label></formula><p>Introduced by Srivastava and Hinton <ref type="bibr" target="#b46">[47]</ref>, the SNNL was modified and analyzed by Frosst et al. <ref type="bibr" target="#b24">[25]</ref>. The loss characterizes the entanglement of data manifolds in representation spaces. The SNNL measures distances between points from different groups (usually the classes) relative to the average distance for points within the same group. When points from different groups are closer relative to the average distance between two points, the manifolds are said to be entangled. This is the opposite intuition to a maximum-margin hyperplane used by support vector machines. Given a labelled data matrix (X,Y ) where Y indicates which group the data points X belong to, the SNNL of this matrix is given in Equation <ref type="formula" target="#formula_5">1</ref>.</p><p>The main component of this loss computes the ratio between (a) the average distance separating a point x i from other points in the same group y i , and (b) the average distance separating two points. A temperature parameter T is introduced to give more or less emphasis on smaller distances (at small temperatures) or larger distances (at high temperature). More intuitively, one can imagine the data forming separate clusters (one for each class) when the SNNL is minimized and overlapping clusters when the SNNL is maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entangled Watermark Embedding</head><p>We present our watermarking strategy, Entangled Watermark Embedding (EWE), in Algorithm 1. We utilize the SNNL's ability to entangle representations for data from the task and watermarking distributions (outliers crafted by the defender using triggers). That is, we encourage activation patterns for legitimate task data and watermarked data to be similar, as  </p><formula xml:id="formula_6">X w = D w (c S ),Y = [Y 0 ,Y 1 ]; 2 map=conv(? X w (SNNL([X w , X c T ],Y , T )),trigger); 3 position = arg max(map); /* Generate watermarked data */ 4 X w [position] = trigger; 5 FGSM(X w , L CE (X w ,Y c T ))/* optional */ 6 FGSM(X w , SNNL([X w , X c T ],Y , T ))/* optional */ 7 step = 0 /*</formula><formula xml:id="formula_7">T (i) -= ? * ? T (i) SNNL([X w , X c T ] (i) ,Y , T (i) );</formula><p>visualized in Figure <ref type="figure" target="#fig_2">2b</ref>. This makes watermarks robust to model extraction: an adversary querying the model on only the task distribution will still extract watermarks.</p><p>Step 1. Generate watermarks: The defender aims to watermark a model trained on the legitimate task dataset D = {X,Y }. First, they select a dataset D w , representing the watermarking distribution, and a source class c S from D w . The defender samples data X w ? D w (c S ) to initialize watermarking, where D w (c S ) represents data from D w with label c S . D w may be the same as the legitimate dataset D if we are performing in-distribution watermarking, or a related dataset if instead we are performing out-of-distribution (OOD) watermarking <ref type="foot" target="#foot_2">4</ref> . The defender then labels X w with a semantically different target class, c T , of D. In other words, it should be unlikely for X w to ever be misclassified as c T (by an un-watermarked model). Our goal is to train the model to have the special behavior that it classifies X w as c T , which makes it distinguishably different from un-watermarked models.</p><p>To this end, we define a trigger, which is an input mask (see Figure <ref type="figure" target="#fig_25">18</ref> (a) in Appendix A.3), and add it to each sample in X w . Thus, X w now contains watermarks (outliers) that can be used to watermark the model, and later, verify ownership. The trigger should not change the semantics of X w to be similar to X c T (i.e., D(c T )). For example, a poor choice of a trigger for in-distribution watermarks sampled from source class "1" of MNIST, would be a horizontal line near the top of the image (see Figure <ref type="figure" target="#fig_25">18 (b)</ref>). This trigger might construe X w to be semantically closer to a "7" than a "1". Such improper trig-gers can weaken model performance and lead to the defender falsely claiming ownership of models that were not watermarked. To avoid these issues, we determine trigger location as the area with the largest gradient of SNNL with respect to the candidate input-this is done through the convolution in the 2 nd line of Algorithm 1.</p><p>Optionally, a defender can optimize the watermarked data with gradient ascent to further avoid generating improper triggers. The goal of this gradient ascent is to perturb the input to decrease the confidence of the model in predicting the target class. This is the opposite of optimization performed by algorithms introduced to find adversarial examples, so we adapt one of these algorithms for our purpose as shown in lines 5 and 6 of Algorithm 1. Since we would like the effect of gradient ascent performed over the watermarked input to transfer between different models <ref type="bibr" target="#b44">[45]</ref>, we use the FGSM <ref type="bibr" target="#b13">[14]</ref> which is a one-shot gradient ascent approach known to transfer better than iterative approaches like PGD <ref type="bibr" target="#b26">[27]</ref> because it introduces larger perturbations <ref type="foot" target="#foot_3">5</ref> . We compute FGSM(X w , f (X w )) :</p><formula xml:id="formula_8">X w = X w + ? ? sign(? X w ( f (X w ))</formula><p>where ? is the step size, and f is a function operating on X w .</p><p>In alternating steps, we define f to be L CE of predicting X w as the target class, c T , by a (different) clean model, or the SNNL between X w and X c T . The former encourages X w to differ from X c T , and the latter makes entanglement easier (leading to more robust watermarks). We use more steps of the former to ensure X w is semantically different from c T .</p><p>Step 2. Modify the Loss Function. To watermark the model more robustly, we compute the SNNL at each layer, l ? [L], where L is the total number of layers in the DNN, using its representation of X w and X c T , which will allow us to entangle them. Y = [Y 0 ,Y 1 ] is arbitrary labels for [X w , X c T ] respectively. We sum the SNNL across all layers, each with a specific temperature T (l) . We multiply the sum by a weight factor ? which governs the relative importance of SNNL to the cross-entropy during . In other words, ? controls the tradeoff between watermark robustness and model accuracy on the task distribution. Our total loss function is thus:</p><formula xml:id="formula_9">L = L CE (X,Y ) -? ? L ? l=1 SNNL([X (l) w , X (l) c T ],Y , T (l) )) (2)</formula><p>Step 3. Train the Model. We initialize and train a model until either the loss converges or the max epochs are reached.</p><p>In training, we sample r normal batches of legitimate data, X, followed by a single interleaved batch of X w concatenated with X c T , both of which are required to entangling using the SNNL. On legitimate data X, we set ? = 0 in Equation 2 to minimize only the task (cross-entropy) loss. On interleaved data [X w , X c T ] that includes watermarks, we set ? &gt; 0 to optimize the total loss. Following Frosst et al. <ref type="bibr" target="#b11">[12]</ref>, we update T using a rate of ? that is learned during training, alleviating the need to tune ? as an additional hyperparameter. watermarking approaches which push these watermarks to a separate cluster. For visualization, we use PCA <ref type="bibr" target="#b20">[21]</ref> to project the representations of data in each model's penultimate layer onto its two principal components. We project data before (left column), during (middle column), and after (right column) training for a baseline model trained with the cross-entropy loss only (top row) and for a model trained with our proposed EWE approach (bottom row) on MNIST.</p><formula xml:id="formula_10">:DWHUPDUN6XFFHVV 1XPEHURI4XHU\ 5HTXLUHG )DOVHZDWHUPDUNUDWH Q Q Q Q Q Figure 4:</formula><p>A defender using a T test to claim ownership of a stolen model, with 95% confidence, needs to make increasingly more queries as the watermark success rate decreases on the stolen model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validating EWE</head><p>We explore if EWE improves upon its predecessors by: ( The watermark success rate is the mean of a binomial distribution characterizing if watermarked data is classified as the target class. According to the Central Limit Theoreom (CLT), it is normally distributed when the number of queries, n, is greater than 30. If we follow the watermark generation procedures described in ? 4.2, the false watermark rate should be lower than random chance, i.e., (100/K)%. In Figure <ref type="figure">4</ref>, we set the false watermark rate to random chance as a conservative upper bound. We often observed rates much lower than this. Figure <ref type="figure">4</ref> shows the number of queries needed to claim ownership, with 95% confidence, as the watermark success rate is varied. For watermark success rates above 23%, the number of queries required is quite small (i.e., 30, the minimal for CLT to be valid). As we will see in ? 4.3.3, only our EWE strategy achieves these success rates after extraction. Even the lowest observed EWE success rate of 18.74% (on CIFAR-10) requires (just) under 100 queries. Figure <ref type="figure">4</ref> also shows that exponentially more queries are required as the watermark success rate approaches the false watermark rate-in many cases, the watermark success rate of the baseline is too low for a defender to claim ownership (see Table <ref type="table">1</ref>).</p><p>Note that outside this section we report the watermark success rate after subtracting the false watermark rate for ease of understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Increased Entanglement</head><p>First, we validate the increased entanglement of EWE over the baseline by visualizing each model's representation (in its penultimate layer) of the data. In Figure <ref type="figure" target="#fig_3">3</ref>, we train our baseline with cross-entropy only (top row) and another model with EWE (bottom row). The baseline learns watermarks naively, by minimizing the cross-entropy loss with the target   <ref type="figure">5</ref>: EWE is able to entangle watermarked with legitimate data because training with SNNL leads to higher CKA similarity between them. We vary ? from 0 (the baseline) to &gt; 0 (EWE) using a log scale. class c T . After training, we see that this pushes watermarked data, X w , to a separate cluster, away from the target class c T . Instead, EWE entangles X w with X(c T ) using the SNNL, which leads to overlapping clusters of watermarked data with legitimate data. Intuitively and experimentally, we see that EWE obtains the least separation in the penultimate hidden layer because it accumulates all previous layers' SNNL.</p><p>Second, similarly to what we did in ? 3.2, we analyze the frequency of activation of neurons for these models, and find that there is more similarity between watermarked and legitimate data when EWE is used. The results are in Figure <ref type="figure" target="#fig_2">2</ref> and Figure <ref type="figure" target="#fig_27">20</ref> (see Appendix A.3) which shows a real-world scenario with a convolutional neural network.</p><p>Third, we analyze the similarity of their representations using central kernel alignment (CKA) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>. This similarity metric centers the distributions of the two representations before measuring alignment. In Figure <ref type="figure">5</ref>, we see that higher levels of SNNL penalty do in fact lead to higher CKA similarity between watermarked and legitimate data (compared with ? = 0, the cross-entropy baseline). This, coupled with our first experiment, explains why EWE achieves better entanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Robustness against Extraction</head><p>We now evaluate the robustness of EWE against retrainingbased extraction attacks launched by white-box adversaries (see the top of ? 4). To remove watermarks, this adversary retrains using only the cross-entropy loss evaluated only on legitimate data. We attack two victim neural networks: one with our EWE strategy and one with our baseline, which uses only the cross-entropy loss, as proposed by Adi et al. <ref type="bibr" target="#b0">[1]</ref>.</p><p>We define the watermark success rate as the proportion of X w correctly identified as c T . We measure the validation accuracy on a held out dataset. We report results for both models in Table <ref type="table">1</ref> and find that the watermark success rate on the victim model (before retraining based extraction) is often near 100% for both EWE and the baseline. After extraction, the watermark success rate always drops. It is in this case that we observe the largest benefits of EWE (over the baseline): there is often a ? 20 percentage point improvement in the watermark success. Besides, we often observe a negligible  decrease in validation accuracy: an average of 0.81 percentage points with a max of 3 for the ResNet on Fashion MNIST.</p><p>Our main result is that we can achieve watermark success rates between 18% and 60% with an average of 38.39%; the baseline is between 0.3% and 9% with an average of 5.77%. There is a minimal 0.81 percentage point degradation on average of validation accuracy compared to the baseline, with a maximum of 3 percentage points for a ResNet on Fashion MNIST. These watermark success rates allow us to claim ownership with 95% confidence with &lt; 100 queries (see ? 4.3.1).</p><p>We also validate that continuing to maximize the SNNL during training is beneficial. In Figure <ref type="figure" target="#fig_7">6</ref> we see that continued training improves the watermark robustness and task accuracy trade-off, until it plateaus near 60 epochs. We measure this trade-off as the ratio between the increase of the watermark success rate and the decrease of the task accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Scalability to Deeper Architectures</head><p>Entangling watermarks with legitimate data enables, and even forces, earlier layers to learn features that recognize both types of data simultaneously, as seen in Figure <ref type="figure" target="#fig_2">2</ref>. This explains the improved robustness of watermarks. With entanglement, only later layers need to use capacity to separate between the two types of data, preserving model accuracy. This setup should work better for deeper models: there is only more capacity to learn shared features for watermarks and legitimate data. Our results in Figure <ref type="figure" target="#fig_27">20</ref>   <ref type="table">1</ref>: Performance of the baseline approach (i.e., minimize cross-entropy of watermarks with the target class) vs. the proposed watermarking approach (EWE). For each dataset, we train a model with each approach and extract it by having it label its own training data. We measure the validation accuracy and watermark success rates, i.e., difference between percentage of watermarks classified as the target class on a watermarked versus non-watermarked model. Both techniques perform well on the victim model, so the intellectual property of models whose parameters are copied directly can be claimed by either technique. However, the baseline approach fails once it is extracted whereas EWE reaches significantly higher watermark success rate.</p><p>tions which add the input of the residual block directly to the output <ref type="bibr" target="#b16">[17]</ref>. Notice that watermarks (e.g. a "1" with a small square trigger) are easily separable from legitimate data of the target class (e.g. a "9") and from the source class (e.g., a "1" without the trigger) because they share (nearly) no common features-they are outliers. Hence, residual connections pose a greater problem for entanglement because there are often no shared features, and forcing the watermarks (by increasing ?) to entangle with the legitimate data of c T may cause the model to misclassfy X c S and X c T .</p><p>Our results validate this intuition. We see in Figure <ref type="figure" target="#fig_26">19</ref> in Appendix A.3 that deep convolutional neural networks can still entangle watermarks but yet we find that comparable ResNets cannot. Thus, we use our OOD watermarks (see Step 1 of ? 4.2) because forcing them to entangle with X c T has a lesser impact on accuracy. Though difficult to entangle, they achieve sufficient watermark success for claiming ownership (see Table <ref type="table">1</ref>). Even for more difficult tasks, as expected, EWE outperforms the baseline (see CIFAR-100 in Table <ref type="table">1</ref>), but both see a significant drop in watermark success. Finally, we see that watermarking is sensitive to the number of classes, in particular, EWE (see Figure <ref type="figure" target="#fig_32">24</ref> in Appendix A.3), probably due to complexity of the representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Calibration of Watermark Entanglement</head><p>Through the calibration of EWE for four vision datasets (MNIST <ref type="bibr" target="#b27">[28]</ref>, Fashion MNIST <ref type="bibr" target="#b54">[55]</ref>, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b25">[26]</ref>), and an audio dataset (Google Speech Commands <ref type="bibr" target="#b53">[54]</ref>), we answer the following questions: <ref type="bibr" target="#b0">(1)</ref> what is the trade-off between watermark robustness and task accuracy?; (2) how should the different parameters of EWE be configured?; and (3) is EWE robust to backdoor defenses and attacks against watermarks? Our primary results are:</p><p>1. For MNIST, Fashion MNIST, and Speech Commands (by which we validate if EWE is independent of the domain), we achieved watermark success above 40% with less than 1 percentage point drop in test accuracy. For CIFAR datatsets, watermark success above 18% is reached with a minimal accuracy loss of &lt; 1.5 percentage points. The weight factor allows the defender to control the trade-off between watermark robustness and task accuracy.</p><p>2. The ratio of watermarks to legitimate data during training, the choice of source-target class pair, and the choice of points to be watermarked all affect the performance of EWE significantly; temperature does not since it is automatically optimized during training as described in ? 4.2. Refer to Appendix A.1 for more details.</p><p>3. Defenses against backdoors like pruning, fine-pruning, and Neural Cleanse are all ineffective in removing EWE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We chose to evaluate EWE on four datasets in addition to MNIST. While CIFAR-10 and CIFAR-100 are used to test the scalability of EWE as described in ? 4.3.4, we use Fashion MNIST because its classes are much harder to linearly separate than MNIST, making it a good benchmark for learning a more complex task, with comparable computational cost to MNIST. Thus it allows us to tune the hyperparameters efficiently to explore behaviors of EWE. Further, it shows that EWE works well when the task naturally contains ambiguous inputs across pairs of classes. We also evaluated EWE on Google Speech Commands, an audio dataset for speech recognition, because speech recognition is one of the applications where ML is already pervasively deployed across industry.</p><p>Datasets. 1. MNIST is a dataset of hand-written digits (from 0 to 9) with 70,000 data points <ref type="bibr" target="#b27">[28]</ref>, where each data point is a gray-scale image of shape 28?28. When needed, we sampled OOD watermarked data from Fashion MNIST.</p><p>2. Fashion MNIST is a dataset of fashion items <ref type="bibr" target="#b54">[55]</ref>. It can be used interchangeably with MNIST. Because the task is more complex, models achieving &gt; 99% accuracy on MNIST however only reach &gt; 90% on Fashion MNIST. When needed, we sampled OOD watermarked data from MNIST.</p><p>3. Google Speech Commands is an audio dataset of 10 single spoken words <ref type="bibr" target="#b53">[54]</ref>. The training data has about 40,000 samples. We pre-processed the data to obtain a Mel Spectrogram <ref type="bibr" target="#b6">[7]</ref>. We tried two methods for generating watermarks both using in-distribution data: (a) modifying the audio signal, or (b) modifying the spectrogram. For (a), we sample data from the source class and overwrite 1 8 th of the total length of the sample (i.e., 0.125 seconds) with a sine curve, as shown in Figure <ref type="figure" target="#fig_35">26</ref>; for (b), each audio sample is represented as an array of size 125?80. We then define the trigger to be two 10?10-pixel squares at both the upper right and upper left-hand corners in case of vanishing or exploding gradients.</p><p>It was observed that the choice of using (a) or (b) does not influence the performance of EWE. 4. CIFAR-10 consists of 60,000 32?32?3 color images equally divided into 10 classes <ref type="bibr" target="#b25">[26]</ref>, while 50,000 is used for training and 10,000 is used for testing. When needed, we use OOD watermarks sampled from SVHN <ref type="bibr" target="#b38">[39]</ref>. 5. CIFAR-100 is very similar to CIFAR-10, except it has 100 classes and there are 600 images for each class <ref type="bibr" target="#b25">[26]</ref>. When needed, we use OOD watermarks sampled from SVHN <ref type="bibr" target="#b38">[39]</ref>.</p><p>Architectures. We use the following architectures:</p><p>1. Convolutional Neural Networks are used for MNIST and Fashion MNIST. The architecture is composed of 2 convolution layers with 32 5?5 and 64 3?3 kernels respectively, and 2?2 max pooling. It is followed by two fully-connected (FC) layers with 128 and 10 neurons respectively. All except the last layers are followed by a dropout layer to avoid overfitting. When implementing EWE, the SNNL is computed after both convolution layers and the first FC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Recurrent Neural Networks are used for Google Speech</head><p>Command dataset. The architecture is composed of 80 long short-term memory (LSTM) cells of 128 hidden units followed by two FC layers of 128 and 10 neurons respectively. When applying EWE, the SNNL is computed after the 40 th cell, the last (80 th ) cell, and the first FC layer.</p><p>3. Residual Neural Network (ResNet) <ref type="bibr" target="#b16">[17]</ref> are used for Fashion MNIST, CIFAR-10, and CIFAR-100 datasets. We use ResNet-18 which contains 1 convolution layer followed by 8 residual blocks (each containing 2 convolution layers), and ends with a FC layer. It is worth noting that the input to a residual block is added to its output. We compute SNNL on the outputs of the last 3 residual blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">No Free Lunch: Watermark vs. Utility</head><p>We study the tension between accuracy on the task's distribution and robustness of the watermarks: if the defender wants to claim ownership of a model, they would like this model to predict their chosen label on the watermarks as frequently as possible while at the same time minimizing the impact of watermarks on the model's performance when presented with samples from the task distribution.</p><p>To systematically explore the trade-off between successfully encoding watermarks and correctly predicting on the task distribution, we first perform a comprehensive grid search that considers all hyper-parameters relevant to our approach: the class pairs (c S , c T ) (note that c S is a class from another dataset when OOD watermark is used), the temperature T , the weight ratio ?, and the ratio of task to watermark data (i.e. r in Algorithm 1), how close points have to be to the target class to be watermarked. In Appendix A.1, we perform an ablation study on the impact of each of these parameters: they can be used to control the trade-off.</p><p>Each point in Figure <ref type="figure" target="#fig_9">7</ref> corresponds to a model trained using EWE with a set of hyper-parameters. For the Fashion MNIST dataset shown in Figure <ref type="figure" target="#fig_9">7</ref> (a), the tendency is exponential: it becomes exponentially harder to improve accuracy by decreasing the watermark success rate. In the Speech Commands dataset, as shown in Figure <ref type="figure" target="#fig_9">7</ref> (b), there is a large number of points with nearly zero watermark success. This means it is harder to find a good set of hyperparameters for the approach. However, there exists points in the upper right corner demonstrating that certain hyperparameter values could lead to robust watermark with little impact on test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Defenses against Backdoors</head><p>Pruning. Since backdoors and legitimate task data activate different neurons, pruning proposes to remove neurons that are infrequently activated by legitimate data to decrease the performance of potential backdoors <ref type="bibr" target="#b30">[31]</ref>. Given that neurons less frequently activated contribute less to model predictions on task inputs, pruning them is likely to have a negligible effect. Since watermarks are a form of backdoors, it is natural   to ask whether pruning can mitigate EWE.</p><p>We find this is not the case because watermarks are entangled to the task distribution. Recall Figure <ref type="figure" target="#fig_2">2b</ref>, where we illustrated how EWE models have similar activation patterns on watermarked and legitimate data. Thus, neurons encoding the watermarks are frequently activated when the model is presented with legitimate data. Hence, if we extract a stolen model and prune its neurons that are activated the least frequently, we find that watermark success rate remains high despite significant pruning (refer Figure <ref type="figure" target="#fig_11">8</ref>). In fact, the watermark success rate only starts decreasing below 20% when the model's accuracy on legitimate data also significantly decreases (by more than 40 percentage points). Such a model becomes useless to the adversary, who would be better off training a model from scratch. We conclude that pruning is ineffective against EWE.</p><p>Fine Pruning. Fine pruning improves over pruning by continuing to train (i.e., fine-tune) the model after pruning <ref type="bibr" target="#b30">[31]</ref>. This helps recover some of the accuracy that has been lost during pruning. In the presence of backdoors, this also contributes to overwriting any behavior learned from backdoors.</p><p>We also analyze EWE in the face of fine pruning. We first extract the model by retraining (i.e., randomly initialize model weights and train them with data labeled by the victim model), prune a fraction of neurons that are less frequently activated, and then train the non-pruned weights on data labeled by the victim model. Results are plotted in Figure <ref type="figure" target="#fig_12">9</ref>. In the most favorable setting for fine pruning, watermark success rate on the extracted model remains around 20% before harming the utility of the model, which is still enough to claim ownershipas shown in ? 4.3.1. This is despite the fact that 50% of the architecture's neurons were pruned. Since the data used for fine-tuning is labeled by the watermarked victim model, it contains information about the watermarks even when the labels provided are for legitimate data.</p><p>Neural Cleanse. Neural Cleanse is a technique that detects and removes backdoors in deep neural networks <ref type="bibr" target="#b52">[53]</ref>. The intuition of this technique is that adding a backdoor would cause the clusters of the source and target classes to become closer in the representation space. Therefore, for every class c of a dataset, Neural Cleanse tries to perturb data from classes different to c in order to have them misclassified in class c.</p><p>Next, the class requiring significantly smaller perturbations to be achieved is identified as the "infected" class (i.e., the class which backdoors were crafted to achieve as the target class).</p><p>In particular, the authors define a model as backdoored if an anomaly index derived from this analysis is above a certain threshold (set to 2). The perturbation required to achieve this class is the recovered trigger. Once both the target class and trigger have been identified, one can remove the backdoor by retraining the model to classify data with the trigger in the correct class, ? la adversarial training <ref type="bibr" target="#b49">[50]</ref>.</p><p>To analyze the robustness of EWE to Neural Cleanse, we compare the performance of a model watermarked with EWE and a baseline model watermarked by minimizing the cross- entropy of watermarks labeled as the target class (? = 0 in Equation <ref type="formula">2</ref>). We compute the anomaly index of the EWE and baseline models. If the anomaly index is above 2, the model is detected as being watermarked (i.e., backdoored in <ref type="bibr" target="#b52">[53]</ref>).</p><p>On the Fashion MNIST (see Figure <ref type="figure" target="#fig_13">10</ref>), EWE exbhibits an average anomaly index of 1.24 (over 5 runs) that evades detection whereas the baseline model has an average index of 8.84. This means that Neural Cleanse is unable to identify our watermark and its trigger.</p><p>It is worth noting: (a) Neural Cleanse considers the problem of backdooring the entire set of classes (i.e., all classes are considered as source classes), and (b) backdoor attacks usually aim at minimal perturbation to the inputs. While being similar to legitimate data from all classes and labeled as a specific class, such backdoors changes the decision surface significantly, which would be detected by Neural Cleanse. In EWE, we insert watermarks only for a single source-target class pair. Besides, watermarked data is not restricted by the degree of perturbation and could even be OOD. Thus entangling it with c T does not change the decision boundary between c T and other classes, as shown in Figure <ref type="figure" target="#fig_14">11</ref> (and Figure <ref type="figure" target="#fig_30">22</ref>, 23 for MNIST and Speech Command in Appendix A.3). This makes it hard for Neural Cleanse to detect EWE watermarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Robustness to Adaptive Attackers</head><p>Recall from our threat model (see the top of ? 4) that the adversary has no knowledge of the parameters used to calibrate the watermarking scheme (such as ? and T (1) ? ? ? T (L)  in Algorithm 1) nor the specific trigger used to verify watermarking. In this section, we explore when the adversary has more resources and knowledge than stated in the threat model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Knowledge of EWE and its parameters</head><p>Knowledge of the parameters used to configure EWE defeats watermarking, as expected. The robustness of EWE relies on maintaining the secrecy of the trigger and watermarking parameters to protect the intellectual property contained in the model. If the adversary knows the trigger used to watermark inputs, they could refuse to classify any input that contains that trigger (denial-of-service). Alternatively, they could extract the model while instead minimizing the SNNL of the watermarks and legitimate data of class c T . Note, minimizing SNNL corresponds to disentangling. Additionally, adversaries may also be able to retrain the triggers (and thus, watermarks) to predict the correct label.</p><p>Any of these results in complete removal of watermarks However, this is not a realistic threat model since the adversary should only know that EWE was used as a watermarking scheme (see (e) in our threat model defined in ? 4. In this way, parameters of EWE play a similar role to cryptographic keys. Next, we evaluate EWE against several more realistic adaptive attacks against watermarks such as piracy attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knowledge of EWE only</head><p>With knowledge of EWE but not its configuration (e.g., the source and target classes), the adversary can still adapt in several ways. We evaluate four adaptive attacks. Disentangling Data. We conjecture that the adversary could perform extraction by minimizing SNNL to disentangle watermarks from task data. We assumed a strong threat model such that the adversary has knowledge of all the parameters of EWE (including the trigger if in-distribution watermark is used, and the OOD dataset if OOD watermark is used) except the source and target classes. Thus, the adversary guesses a pair of classes, constructs watermarked data following EWE, and extracts the model while using EWE with ? &lt; 0 to disentangle the purported watermark data and legitimate data from With less than 10% neurons pruned, the pirate watermark is removed while the owner's watermark remains.</p><p>the purported target class. Following such a procedure, we observe that the watermark success of the extracted model on Fashion MNIST drops from 48.81% to 22.82% if the guess does not match with the true source-target pair, and to 6.34% if the guess is correct.. On MNIST, watermark success drops from 41.62% to 30.14% when the guess is wrong, and to 0.08% otherwise. The results from the Speech Commands dataset have large variance, but follow a similar trend: the watermark success drops to an average of 16.81% due to the attack. Thus, while watermark success rates are lowered by this attack, the defender is still able to claim ownership when the adversary guesses the source-target pair incorrectly with about 30 queries for the two vision datasets, and near 100 queries for Speech Commands. Furthermore, observe that guessing the pair of classes correctly requires significant compute to train models corresponding to the K(K -1) possible source-target pairs where K is the number of classes in the dataset , which defeats the purpose of model extraction.</p><p>Piracy Attack. In a piracy attack, the adversary embeds their own watermark with EWE so that the model is watermarked twice-it becomes ambiguous to claim ownership through watermarks. To remove the pirate watermark, we propose to fine-prune <ref type="bibr" target="#b30">[31]</ref> the extracted model on data labeled by the victim model. As shown in Figure <ref type="figure" target="#fig_15">12</ref>, the owner's watermark is not removed as we discussed fine pruning in ? 5.3, whereas the pirate watermark would be removed (even if the adversary uses EWE) because data labelled by the victim model does not contain information about the pirate watermark. The adversary cannot do the same to remove the owner's watermark because this requires access to a dataset labeled by another source, at which point the cost of piracy defeats model stealing: the adversary could have trained a model on that dataset and would not benefit from model stealing.</p><p>Anomaly Detection. Imagine the case of an extracted model deployed as an online ML API. The adversary may know (or suspect) the model to be watermarked, so they may decide to implement an anomaly detector to filter queries containing data watermarked by EWE and respond to them with a random prediction. By doing so, even though the parameters  Even fine-tuning all the layers does not remove watermarks.</p><p>still embed the watermarks, the adversary could still prevent the defender from claiming ownership. We tested two common anomaly detectors on Fashion MNIST: Local Outlier Factor (LOF) <ref type="bibr" target="#b3">[4]</ref> and Isolation Forest <ref type="bibr" target="#b29">[30]</ref>, on activations of the last hidden layer. Results are shown in Table <ref type="table" target="#tab_4">2</ref>. Both detectors are able to detect more than 90% of watermarked data. However, this comes at the cost of identifying parts of the validation dataset as outliers and results in a sharp accuracy drop of 7.0 and 8.64 percentage points respectively. This may be due to the curse of dimensionality <ref type="bibr" target="#b22">[23]</ref>: it is harder to learn higher dimensional distribution. Indeed, it is worth noting that anomaly detectors on hidden layers consistently work better than on the inputs themselves.</p><p>Transfer Learning. The adversary may also transfer knowledge of the extracted model to another dataset in the same domain <ref type="bibr" target="#b41">[42]</ref> with the hope of disassociating the model from EWE's watermark distribution. To evaluate if watermarks persist after transfer learning, we chose two datasets in the same domain. The victim model is trained on the German Traffic Sign Dataset (GTSRB) <ref type="bibr" target="#b47">[48]</ref> and we transferred the extracted model to the LISA Traffic Sign Dataset <ref type="bibr" target="#b35">[36]</ref>. We fine-tune either (a) only the fully connected layers, or (b) all layers for the same number of epochs that the victim model was trained for. Before we verify the watermark, the output layer of the transferred model is replaced to match the dimension of the victim model (they may differ) <ref type="bibr" target="#b0">[1]</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_16">13</ref>, (a) achieves an accuracy of up to 98.25% but leaves the watermark unaffected; (b) reaches an accuracy of 98.56% and begins to weaken the watermark as one increases the learning rate. However, the pretrained knowledge is lost due to large learning rate values before the watermark is removed. This is consistent with observations in prior work <ref type="bibr" target="#b0">[1]</ref>. We also note that transfer learning requires that the adversary have access to additional training data and perform more training steps, so it is expected that our ability to claim model ownership will be weaker.</p><p>Take-away. The adversary also faces a no free lunch situation. They cannot adapt with disentanglement, piracy, anomaly detection, or transfer learning, and remove EWE watermarks, unless they sacrifice the stolen model's utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Hyperparameter Selection. Our results suggest that the watermarking survivability comes at a nominal cost (about 0.81% in accuracy degradation). Yet, this value varies depending on the dataset and the hyperparameters used for training (which themselves also depend on the dataset) as we explore in Appendix A.1. Determining the relationship with relevant properties of the dataset is future work.</p><p>Computational Overheads. Our experiments suggest that the size of the watermarked dataset should be 2? less than the size of the legitimate dataset. However, this implies that the model is now trained on 1.5 -2? more data than before. While this induces additional computational overheads, we believe that the trade-offs are advantageous in terms of proving ownership. A more detailed analysis is required to understand if the same phenomenon exists for more complex tasks with larger datasets.</p><p>Improving Utility. EWE utilizes the SNNL to mix representations from two different distributions; this ensures the activation patterns survive extraction. However, this is at a nominal expense to the utility; for certain applications, such a decrease in utility (even if small) is not desired. We believe that the same desired properties could be more easily achieved if one were to replace ReLU activations with the smoother Sigmoid activations while computing the SNNL. Algorithmic Efficiency. In Algorithm 1, we modified the loss function by computing the SNNL at every layer of the DNN. However, it may not be necessary to do so. In Figure <ref type="figure" target="#fig_27">20</ref>, we plot the activation patterns of hidden layers of a model trained using EWE; we observe that adding the SNNL to just the last layers provides the desired guarantees. Additionally, we observe a slight increase in model utility when not all layers are entangled. A detailed understanding of how one can choose the layers is left to future work.</p><p>Scalability and Future Research Directions. As mentioned in ? 4.3.4, EWE suffers in terms of trade-off between model performance and watermark robustness when we scale to deeper architectures, and more complex datasets. Given the results on CIFAR-100, more work may be needed to scale the current method to larger datasets. According to Figure <ref type="figure" target="#fig_32">24</ref> (in Appendix A.3), the performance of EWE is impacted by the number of classes. We suspect this may be due to the representation space being more complicated (i.e. there are more clusters), making it more difficult to entangle two arbitrarily chosen clusters. Thus, a potential next step would be to investigate the interplay between the design of triggers to control the cluster of watermarked data; and the similarity structures and orientation of the representation space to choose source and target classes accordingly.</p><p>Another possible improvement is to use m-to-n watermarking. In this work, we focused on 1-to-1 watermarking, which watermarks one class of data and entangles it with another class. However, as long as the watermarked model behaves significantly differently from a clean model, the model owner could choose to watermark m classes of data, entangle them with n other classes, and claim ownership by following the similar verification process as described in ? 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We proposed Entangled Watermark Embedding (EWE), which forces the model to entangle representations for legitimate task data and watermarks. Our mechanism formulates a new loss involving the Soft Nearest Neighbors Loss, which when minimized increases entanglement. Through our evaluation on tasks from the vision and audio domain, we show that EWE is indeed robust to not only model extraction attacks, but also piracy attacks, anomaly detection, transfer learning, and efforts used to mitigate backdoor (poisoning) attacks. All this is achieved while preserving watermarking accuracy, with (a) a nominal loss in classification accuracy, and (b) 1.5 -2? increase in computational overhead. Scaling EWE to complex tasks without great accuracy loss remains as an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Finetuning the hyperparameters of EWE Next, we dive into details of each hyperparameter of EWE and perform an ablation study.</p><p>Temperature. Temperature is a hyperparameter introduced by Frosst et al <ref type="bibr" target="#b11">[12]</ref>. It could be used to control which distances between points are more important: at small temperatures, small distances matter more than at high temperatures, where large distances matter most. In our experiments, we found that the influence of temperature on the robustness of watermark is not significant: a nice initialization leads to high watermark success, whereas other initialization results in watermark success high enough for claiming ownership, as shown in Figure <ref type="figure" target="#fig_18">14</ref>. We conjecture that this is because EWE fine-tunes the temperature by gradient descent during training (see the last line of Algorithm 1).  Weight Factor. As defined in Algorithm 1, the loss function is the weighted sum of a cross entropy term and SNNL term. The weight factor ? is a hyper-parameter that controls the importance of learning the watermark task (by maximizing the SNNL) relatively to the classification task (by minimizing cross entropy loss). As shown in Figure <ref type="figure" target="#fig_20">15</ref>, factors larger in magnitude cause the watermark to be more robust, at the expense of performance on the task. At the left-hand side of the figure, with a weight factor in the magnitude of  Figure <ref type="figure" target="#fig_7">16</ref>: Decreasing the ratio r of task data to watermarks promotes watermark success rate (more importance is given to the SNNL) at the expense of lower accuracy on the task. 10, the accuracy is similar to an un-watermarked model, while watermark success is about 40%. In contrast, when the weight factor is getting larger, watermark success approaches to 100% but the accuracy decreases significantly.. Ratio of task data to watermarks. Denoted by r in Algorithm 1, this ratio also influences the trade-off between task accuracy and watermark robustness. In Figure <ref type="figure" target="#fig_7">16</ref>, we observe that lower ratios yield more robust watermarks. For instance, we found for Fashion MNIST that the watermark could be removed by model extraction if the ratio is greater than 3, whereas task accuracy drops significantly for ratios below 1.</p><p>Source-Target classes Source and target classes are denoted by c S and c T in Algorithm 1. Note that we use OOD watermarks (data from MNIST) for Fashion MNIST, so c S refers to a class of MNIST. We name class center the average of data from each class. In Figure <ref type="figure" target="#fig_22">17</ref>, we plot the performance of EWE with respect to the cosine similarity among centers of different source-target pairs (detailed performance of different pairs can be found in Figure <ref type="figure" target="#fig_34">25</ref> in the appendix).</p><p>Classes with similar structures enable more robust watermarks at no impact on task accuracy. This is because data from similar classes is easier to entangle (i.e. the SNNL is easier to maximize). Cosine similarity between class centers is a heuristic to estimate this and its effectiveness depends on the dataset. For Fashion MNIST, one could observe a trend that higher cosine similarity leads to more robust watermarks. Instead, the difference among classes are less significant in Speech Command so this heuristic may not be useful.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Evasion Attacks for Detection</head><p>Adversarial examples (or samples) are created by choosing samples from a source class and perturbing them slightly (adding a carefully crafted perturbation) to ensure targeted (the mistake is chosen) or untargeted (the mistake is any incorrect class) misclassification. To do so, some attacks use gradients <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref> or pseudo-gradients <ref type="bibr" target="#b51">[52]</ref> to create adversarial samples with minimum perturbation. We wish to understand if mechanisms used to generate adversarial samples can be used to detect watermarks, as both produce the same effect (targeted misclassification). The intuition is that if one adversarial examples are generated from blank input and perturbed to the target class, they may reveal some information about the watermarked data. To this end, we utilize the approach proposed by Papernot et al. <ref type="bibr" target="#b43">[44]</ref>            98.9 N/A 98.9 98.7 98.9 98.8 99.0 98.9 98.8 99.0 99.0 98.8 N/A 98.8 98.9 98.8 98.9 99.0 98.9 99.0 98.7 98.9 98.9 N/A 98.9 98.8 98.9 99.0 98.8 99.0 98.9 98.8 98.9 98.9 N/A 98.8 98.9 99.0 98.9 99.0 98.8 99.0 98.9 98.9 98.9 N/A 98.8 98.9 98.9 98.9 98.9 98.8 98.8 98.7 98.9 98.9 N/A 98.9 98.7 99.0 98.8 98.9 98.7 98.8 98.9 98.8 98.9 N/A 98.9 98.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Figures</head><p>98.9 98.9 98.8 98.9 98.9 98.7 99.0 99.0 N/A 98.9</p><p>98.9 99.0 98.8 98.9 98.9 98.6 99.0 99.0 98.9 N/A (a) MNIST: Test Accuracy  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We construct a neural network to show how watermarks behave like trapdoor functions. When the model learns independent task and watermark distributions, this is true despite both distributions being modeled with the same neurons. Green values correspond to the watermark model while red values to a copy stolen through model extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline Watermarking activates different and fewer neurons, corroborating our hypothesis of two submodels. Training with EWE entangles activations of watermarked data with legitimate task data. adversary cannot solely update the small groups of neurons used for watermarking because they sample data from the task distribution during extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Visualization of our proposed EWE entangling watermarks with data from the target class c T = 7 unlike prior watermarking approaches which push these watermarks to a separate cluster. For visualization, we use PCA<ref type="bibr" target="#b20">[21]</ref> to project the representations of data in each model's penultimate layer onto its two principal components. We project data before (left column), during (middle column), and after (right column) training for a baseline model trained with the cross-entropy loss only (top row) and for a model trained with our proposed EWE approach (bottom row) on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure5: EWE is able to entangle watermarked with legitimate data because training with SNNL leads to higher CKA similarity between them. We vary ? from 0 (the baseline) to &gt; 0 (EWE) using a log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: There exists an inflection point in the model's task accuracy and the SNNL value, as training progresses. Before that point, continuing to train generally increases the watermark success rate relative to the task accuracy (we report the ratio between variations of the two).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Watermark success versus model accuracy on the task. Each point corresponds to a model trained with uniformly-sampled hyperparameters. As test accuracy increases, it becomes harder to have robust watermarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Task accuracy and watermark success rate on the extracted model in the face of a pruning attack. For both datasets, bringing the watermark success rate below 20% comes at the adversary's expense: accuracy drop of more than 40 percentage points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Task accuracy and watermark success rate on the extracted model in the face of a fine pruning attack. Despite a more advantageous trade-off between watermark success rate and task accuracy, the adversary is unable to bring the watermark success rate sufficiently low to prevent the defender to claim ownership (see ? 4.3.1) until 40% neurons are finepruned. Beyond this point, fine-pruning more neurons would lead to loss in the extracted model's accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Neural Cleanse leverages the intuition that triggers may be recovered by looking for adversarial examples for the target class. To illustrate this, we have here a legitimate input of the target class (a), an example of a watermark (b), an adversarial example (see Appendix A.2 for details) intialized as a blank image and perturbed to be misclassified by the extracted model in the target class(c), and the backdoor candidate recovered by Neural Cleanse (d). If either (c) or (d) were similar to the watermark, this would enable us to recover the watermarked data and then use this knowledge to remove the watermark as described in ? 6. However, this is not the case for models extracted from a EWE defended victim model: the watermark proposed (c and d) is different from the trigger used by EWE (b).</figDesc><graphic url="image-3.png" coords="10,322.58,76.49,54.03,54.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Change in the distance among clusters of data from different Fashion MNIST classes following watermarking.The four subplots are made using four different approaches specified by the sub-captions. In (c) and (d), c S = 8 and c T = 0, while D w is MNIST for (d). Each point in the plot represents an output vector of the last hidden layer. These representations are plotted in 2-D using UMAP dimensionality reduction to preserve global distances<ref type="bibr" target="#b33">[34]</ref>. Comparing (a) and (b), one can observe that the clusters of class 8 and 0 become closer in (b) while the distances among the other classes remain similar. This is why such watermarked model can be detected by Neural Cleanse<ref type="bibr" target="#b52">[53]</ref>, which searches for pairs of classes that are easily misclassified with one another. In contrast, EWE with either in or out of distribution watermarks does not influence this distance significantly, which makes it more difficult for Neural Cleanse to detect the watermark.</figDesc><graphic url="image-7.png" coords="11,54.00,73.46,120.96,107.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Task accuracy and watermark success rate after fine-pruning on the extracted model with a pirate watermark. With less than 10% neurons pruned, the pirate watermark is removed while the owner's watermark remains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Task accuracy and watermark success rate of the extracted model after transfer learning from GTSRB to LISA. Even fine-tuning all the layers does not remove watermarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: EWE is unlikely to fail due to setting the temperature, but certain initialization of temperature does lead to better trade-off between task accuracy and watermark success rate. Note the temperature is plotted on log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>Figure15: Increasing the absolute value of the weight factor ? promotes watermark success rate (more importance is given to the SNNL) at the expense of lower accuracy on the task. Note that ? is plotted on log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Impact of similarity of classes on robustness of watermarks: We computes the average cosine distances between data of different pairs of classes and use them as source and target classes to watermark the model. It could be seen that similar classes lead to higher watermark success on Fashion MNIST, but no clear trend is observed for Speech Command.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>on the extracted model to generate adversarial examples, and compare them with the watermarked data generated by EWE. Examples of watermarked data and adversarial samples we generated are shown in Figure 10 b and (c) respectively. The average cosine similarity between the adversarial examples and watermarked data is about 0.3, whereas it could reach about 0.4 when comparing to a uniformly distributed random input of the same size. Thus, mechanisms used to generate adversarial samples are unable to detect watermarks generated by EWE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: (a) In this Watermarked DNN, a small white square is designed as a special trigger. If this square is added to the corner of a digit-3, the input would be predicted as a digit-5 by the DNN, whereas a normal model would classify it as a digit-3 mostly. (b) This is an example of improperly designed trigger. By adding such a rectangle to top of 1's, even a unwatermarked model would classify it as a digit-7, so it is hard to tell if a model is watermarked or not by such a trigger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Validation Accuracy and Watermark success while increasing the number of convolution layers in a Fashion MNIST model without residual connection. Note that indistribution watermark is used here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 20 :</head><label>20</label><figDesc>Figure20: Activations of a convolutional neural network. We train a DNN with 2 convolution layers and 2 fully connected layers with EWE. We show here the frequency of activations for neurons in all hidden layers: high frequencies correspond to white color. One can observe that by entangling legitimate task data and watermarks, their representation becomes very similar, as we go deeper into the model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21:  This should be compared to Figure20. It is repeated here on a model with the same architecture but watermarked by the baseline. One can observe that the difference between activation of watermarked and legitimate data is more significant when EWE is not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>(a) Un-watermarked Model (b) Watermarked Model (Baseline) (c) EWE In-distribution Watermark (d) EWE Out-distribution Watermark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Same as Figure 11 except here the dataset is MNIST, while c S = 3 and c T = 5.</figDesc><graphic url="image-155.png" coords="18,54.00,315.19,120.06,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Same as Figure 11 except here the dataset is Speech Command, while c S = 9 and c T = 5. The OOD watermarks are audios of people saying "one".</figDesc><graphic url="image-158.png" coords="18,174.06,434.98,120.06,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 24 :</head><label>24</label><figDesc>Figure24: While scaling EWE to CIFAR-100, we noticed that both the baseline and EWE lead to significantly lower accuracies when the number of classes increases than an unwatermarked model. Besides, it can be observed that EWE reaches better watermark success than the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>99.0 98.7 98.8 98.8 98.9 99.0 99.0 98.6 99.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Performance of the extracted model for different source-target pairs:We call class i and class j as a sourcetarget pair if the watermark in our model is designed to be that watermarked data sampled from class i (if using OOD watermark, then this would be class i of another dataset) will be classified as class j by the model. On MNIST dataset , Fashion MNIST, and Speech Command, we tried to train and extract models with all 90 source-target pairs under the same setting (i.e. all hyper-parameters including temperature are the same) and plotted the validation accuracy and watermark success rate of the extracted model in the 6 figures above. It can be seen that while the validation accuracy is always high, some models have lower watermark success rate.</figDesc><graphic url="image-165.png" coords="18,455.45,626.86,83.74,53.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Example of a watermarked audio signal and the corresponding Mel Spectrogram.</figDesc><graphic url="image-157.png" coords="18,54.00,434.98,120.06,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, X c T ], Y c T )/* watermark */</figDesc><table><row><cell></cell><cell>Start training</cell><cell>*/</cell></row><row><cell cols="2">8 while loss not converged do</cell><cell></cell></row><row><cell>9</cell><cell>step += 1;</cell><cell></cell></row><row><cell>10</cell><cell>if step % r == 0 then</cell><cell></cell></row><row><cell cols="2">11 model.train([X 12 else</cell><cell></cell></row><row><cell>13</cell><cell>model.train(X,Y )/* primary task</cell><cell>*/</cell></row><row><cell></cell><cell>/* Fine-tune the temperature</cell><cell>*/</cell></row><row><cell>14</cell><cell></cell><cell></cell></row></table><note><p>w</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>in Appendix A.3 confirms this. However, deeper models such as ResNets often benefit (in their validation accuracy) from linearity: residual connec-</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Victim Model</cell><cell cols="2">Extracted Model</cell></row><row><cell></cell><cell></cell><cell cols="4">Validation Accuracy Watermark Success Validation Accuracy Watermark Success</cell></row><row><cell>MNIST</cell><cell>Baseline</cell><cell>99.03(?0.04)%</cell><cell>99.98(?0.03)%</cell><cell>98.79(?0.12)%</cell><cell>0.31(?0.23)%</cell></row><row><cell></cell><cell>EWE</cell><cell>98.91(?0.13)%</cell><cell>99.9(?0.11)%</cell><cell>98.76(?0.12)%</cell><cell>65.68(?10.89)%</cell></row><row><cell>Fashion MNIST</cell><cell>Baseline</cell><cell>90.48(?0.32)%</cell><cell>98.76(?1.07)%</cell><cell>89.8(?0.38)%</cell><cell>8.96(?8.28)%</cell></row><row><cell></cell><cell>EWE</cell><cell>90.31(?0.31)%</cell><cell>87.83(?5.86)%</cell><cell>89.82(?0.45)%</cell><cell>58.1(?12.95)%</cell></row><row><cell cols="2">Speech Command Baseline</cell><cell>98.11(?0.35)%</cell><cell>98.67(?0.94)%</cell><cell>97.3(?0.43)%</cell><cell>3.55(?1.89)%</cell></row><row><cell></cell><cell>EWE</cell><cell>97.5(?0.44)%</cell><cell>96.49(?2.18)%</cell><cell>96.83(?0.45)%</cell><cell>41.65(?22.39)%</cell></row><row><cell>Fashion MNIST</cell><cell>Baseline</cell><cell>91.64(?0.36)%</cell><cell>75.6(?15.09)%</cell><cell>91.05(?0.44)%</cell><cell>5.68(?11.78)%</cell></row><row><cell>(ResNet)</cell><cell>EWE</cell><cell>88.33(?1.97)%</cell><cell>94.24(?5.5)%</cell><cell>88.27(?1.53)%</cell><cell>24.63(?17.99)%</cell></row><row><cell>CIFAR10</cell><cell>Baseline</cell><cell>85.82(?1.04)%</cell><cell>19.9(?15.48)%</cell><cell>81.62(?1.74)%</cell><cell>7.83(?14.23)%</cell></row><row><cell></cell><cell>EWE</cell><cell>85.41(?1.01)%</cell><cell>25.74(?8.67)%</cell><cell>81.78(?1.31)%</cell><cell>18.74(?12.3)%</cell></row><row><cell>CIFAR100</cell><cell>Baseline</cell><cell>54.11(?1.89)%</cell><cell>8.37(?13.44)%</cell><cell>47.42(?2.54)%</cell><cell>8.31(?15.1)%</cell></row><row><cell></cell><cell>EWE</cell><cell>53.85(?1.07)%</cell><cell>67.87(?10.97)%</cell><cell>47.62(?1.41)%</cell><cell>21.55(?9.76)%</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Proportion of watermarks detected and accuracy loss when anomaly detectors filter suspicious inputs.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy Loss Detected Watermark</cell></row><row><cell>LOF</cell><cell>7.00(?0.3)%</cell><cell>99.93(?0.03)%</cell></row><row><cell cols="2">Isolation Forest 8.64(?0.32)%</cell><cell>92.82(?1.32)%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In this paper, we use an adaptive optimizer called Adam which improves convergence<ref type="bibr" target="#b23">[24]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This assumes that the adversary has knowledge of the model architecture.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>OOD watermarking means the watermarked data is not sampled from the task distribution</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Note that here we are not concerned with the imperceptibility of watermarked data so this is not a limitation in the context of our work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Carrie Gates</rs> for shepherding this paper. This research was funded by <rs type="funder">CIFAR</rs>, <rs type="funder">DARPA GARD, Microsoft</rs>, and <rs type="funder">NSERC</rs>. VC was funded in part by the <rs type="funder">Landweber Fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Turning your weakness into a strength: Watermarking deep neural networks by backdooring</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Pinkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th USENIX Security Symposium (USENIX Security 18)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-08">August 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adding robustness to support vector machines against adversarial reverse engineering</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ibrahim M Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CSI NN: Reverse engineering of neural network architectures through electromagnetic side channel</title>
		<author>
			<persName><forename type="first">Lejla</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Bhasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirmanto</forename><surname>Jap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stjepan</forename><surname>Picek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium (USENIX Security 19)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lof: Identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Model extraction and active learning</title>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Varun Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songbai</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<idno>CoRR, abs/1811.02054</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting backdoor attacks on deep neural networks by activation clustering</title>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 13th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras</title>
		<author>
			<persName><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deokjin</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Kapre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data</title>
		<author>
			<persName><forename type="first">Jacson</forename><surname>Rodrigues Correia-Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudine</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto F De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for learning kernels based on centered alignment</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="795" to="828" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models</title>
		<author>
			<persName><forename type="first">Huili</forename><surname>Bita Darvish Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Koushanfar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00750</idno>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Whitfield</forename><surname>Diffie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">E</forename><surname>Hellman</surname></persName>
		</author>
		<title level="m">New directions in cryptography</title>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01889</idno>
		<title level="m">Nicolas Papernot, and Geoffrey Hinton. Analyzing and Improving Representations with the Soft Nearest Neighbor Loss</title>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06733</idno>
		<title level="m">BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain</title>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A historical perspective of speech recognition</title>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="103" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01838</idno>
		<title level="m">High-Fidelity Extraction of Neural Network Models. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning</title>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Watermarking techniques for intellectual property protection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Mangione-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mantik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potkonjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Design Automation Conference, DAC &apos;98</title>
		<meeting>the 35th Annual Design Automation Conference, DAC &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Curse of Dimensionality</title>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Mueen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="314" to="315" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. 3rd International Conference on Learning Representations ICLR 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity of Neural Network Representations Revisited</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 36th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Ian</forename><surname>Alexey Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Defending against neural network model stealing attacks using deceptive perturbations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Security and Privacy Workshops (SPW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finepruning: Defending against backdooring attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Symposium on Research in Attacks, Intrusions, and Defenses</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model reconstruction from model explanations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smitha Milli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision-based traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mogelmose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1484" to="1497" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning: A Probabilistic Perspective</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Digital watermarking for deep neural networks</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin'ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knockoff nets: Stealing functionality of black-box models</title>
		<author>
			<persName><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A framework for the extraction of deep neural networks by leveraging public data</title>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><surname>Ganapathy</surname></persName>
		</author>
		<idno>CoRR, abs/1905.09165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Asia Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<title level="m">Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The German Traffic Sign Recognition Benchmark: A multi-class classification competition</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ristenpart. Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5032" to="5041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural cleanse: Identifying and mitigating backdoor attacks in neural networks</title>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="707" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Protecting intellectual property of deep neural networks with watermarking</title>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongshu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Stoecklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2018 on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
