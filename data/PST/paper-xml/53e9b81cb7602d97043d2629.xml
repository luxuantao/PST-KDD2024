<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesis of Bidirectional Texture Functions on Arbitrary Surfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingdan</forename><surname>Zhangþ</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ligang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Wangþ</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ÞTsinghua University Ý</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Synthesis of Bidirectional Texture Functions on Arbitrary Surfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EEC6BDCBB729E61FBB0E8EE1418A4841</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-color, shading, shadowing, and texture</term>
					<term>I.2.10 [Artificial Intelligence]: Vision and Scene Understandingtexture</term>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation Bidirectional texture function, 3D textons, reflectance and shading models, texture synthesis, texture mapping, surfaces</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The bidirectional texture function (BTF) is a 6D function that can describe textures arising from both spatially-variant surface reflectance and surface mesostructures. In this paper, we present an algorithm for synthesizing the BTF on an arbitrary surface from a sample BTF. A main challenge in surface BTF synthesis is the requirement of a consistent mesostructure on the surface, and to achieve that we must handle the large amount of data in a BTF sample. Our algorithm performs BTF synthesis based on surface textons, which extract essential information from the sample BTF to facilitate the synthesis. We also describe a general search strategy, called the -coherent search, for fast BTF synthesis using surface textons. A BTF synthesized using our algorithm not only looks similar to the BTF sample in all viewing/lighthing conditions but also exhibits a consistent mesostructure when viewing and lighting directions change. Moreover, the synthesized BTF fits the target surface naturally and seamlessly. We demonstrate the effectiveness of our algorithm with sample BTFs from various sources, including those measured from real-world textures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Two main ingredients for visual realism are surface geometry and surface details. With recent advances in surface texture synthesis <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>, we can now decorate a real-world surface (e.g., reconstructed from laser range scans <ref type="bibr" target="#b3">[4]</ref>) with a texture that fits the surface naturally and seamlessly. However, we are still a step away from reality because textures in traditional graphics represent only color or albedo variations on smooth surfaces. Real-world textures, on the other hand, arise from both spatially-variant surface reflectance and surface mesostructures, i.e., the small but visible local geometric details <ref type="bibr" target="#b10">[11]</ref>. Mesostructures, which are responsible for  fine-scale shadows, occlusions, and specularities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>, are ignored by conventional textures. Fig. <ref type="figure" target="#fig_1">1</ref> compares a surface texture with a surface BTF. The BTF introduced by Dana et al. <ref type="bibr" target="#b5">[6]</ref> is a representation of real-world textures that can model surface mesostructures and reflectance variations. The BTF is a 6D function whose variables are the 2D position and the viewing and lighting directions. In this paper, we present an algorithm for synthesizing the BTF on arbitrary polygonal surfaces. Given a BTF sample and a mesh, we synthesize a BTF on the mesh such that: (a) the surface BTF is perceptually similar to the given BTF sample in all viewing/lighting conditions, and (b) the surface BTF exhibits a consistent mesostructure when viewing and lighting directions change. The requirement of a consistent mesostructure is where surface BTF synthesis differs fundamentally from surface texture synthesis since conventional textures ignore mesostructures completely.</p><p>A BTF can be mapped onto surfaces using texture mapping techniques. However, BTF mapping on arbitrary surfaces can introduce inconsistent mesostructures. The usual technique for texture mapping arbitrary surfaces is to use a collection of overlapping patches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, and textures in the overlapping regions are blended to hide seams (e.g., see <ref type="bibr" target="#b15">[16]</ref>). This technique works well for many textures <ref type="bibr" target="#b15">[16]</ref>, but for the BTF, blending can introduce inconsistent mesostructures, as Fig. <ref type="figure" target="#fig_2">2</ref> illustrates. Of course, BTF mapping on arbitrary surfaces also suffers from the usual problems of texture mapping, which include distortion, seams, and considerable user intervention needed for creating good-quality texture maps <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Liu et al. recognized the importance of mesostructures in developing their algorithm for synthesizing a continuous BTF from sparse measurements <ref type="bibr" target="#b12">[13]</ref>. Indeed, they explicitly recovered geometry details using a shape-from-shading method. The appearance of the recovered geometry is rendered and used for synthesis images under different viewing/lighting settings. Unfortunately, adapting <ref type="bibr" target="#b12">[13]</ref> to surface BTF synthesis is not possible because it is time consuming to reconstruct/render the appearance from the recovered geometry for all lighting and viewing settings, especially for BTFs with complex geometry or radiance distributions. Moreover, current shape-from-shading techniques will have problems handling real-world textures with complex bump structures or without dominant diffuse components <ref type="bibr" target="#b12">[13]</ref>.</p><p>A possible way to achieve a consistent mesostructure on a surface is to directly apply surface texture synthesis techniques to surface BTF synthesis. The sample BTF may be regarded as a 2D texture map, in which the BTF value at a pixel is a 4D function of the viewing and lighting directions, and this 4D function can be discretized into a vector for texture synthesis. Unfortunately, this approach incurs a huge computational cost because of the large amount of data in a BTF sample. At the resolution of ½¾¢ ¢½¾¢ , the BTF value at a pixel is a 10800-dimensional vector, as opposed to the usual 3D RGB vectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. Since texture synthesis time grows linearly with the vector dimension, a surface BTF can take days <ref type="bibr" target="#b19">[20]</ref> or even months <ref type="bibr" target="#b17">[18]</ref> to compute. Principal components analysis (PCA) can reduce the vector dimension somewhat but cannot alter the nature of the problem.</p><p>Leung and Malik introduced 3D textons based on the observation that, at the local scale, there are only a small number of perceptually distinguishable mesostructures and reflectance variations on the surface <ref type="bibr" target="#b11">[12]</ref>. Their observation raises the hope of a compact data structure for extracting the essential information for surface BTF synthesis. However, 3D textons themselves are not compact because of their huge appearance vectors <ref type="bibr" target="#b11">[12]</ref>. Indeed, the reconstructive BTF synthesis proposed in <ref type="bibr" target="#b11">[12]</ref> is not feasible on surfaces because the basic operations <ref type="bibr" target="#b17">[18]</ref> for surface BTF synthesis -texton resampling, distance computation, and BTF reconstructionare prohibitively expensive with 3D texton <ref type="bibr" target="#b11">[12]</ref>. To get a feel of the problem, consider the following: surface BTF reconstruction requires a 4D function to be assigned to each mesh vertex, which implies a 2.5 Gb storage for a mesh of 250k vertices <ref type="bibr" target="#b17">[18]</ref>.</p><p>In this paper, we show that surface BTF can be efficiently synthesized using the surface textons, which are derived from 3D textons <ref type="bibr" target="#b11">[12]</ref>. Having no appearance vectors, surface textons constitute a compact data structure for extracting the essential information from the BTF sample to facilitate surface BTF synthesis. We present an algorithm based on surface textons for synthesizing a surface texton map, which is a texton-based compact representation of the synthesized surface BTF. This compact representation can be directly used for rendering; no BTF reconstruction like the one in <ref type="bibr" target="#b11">[12]</ref> is necessary. We also describe a search strategy, called the -coherent search, for fast surface BTF synthesis using surface textons. Existing general-purpose search strategies, such as those used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref>, can also be adapted for surface textons, but the search speed is orders of magnitude slower.</p><p>We will demonstrate the effectiveness of our algorithm using surface BTFs synthesized from both real and synthetic BTF samples. With the increasing availability of BTF samples measured from real-world textures <ref type="bibr" target="#b5">[6]</ref>, surface BTFs provide a way to decorate real-world geometry with real-world textures. We also show that surface BTFs provide an efficient way for rendering surfaces with complex synthetic appearance models and geometry details.</p><p>The rest of the paper is organized as follows. After a brief review of related work in Section 2, we give an overview of our approach in Section 3. Section 4 discusses how to extract surface textons from the sample BTF. Section 5 provides details about surface BTF synthesis. Section 6 describes how to render surface BTFs synthesized by our algorithm. Results are reported in Section 7, followed by a conclusion and discussion about future work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several models exist for BTF analysis and material recognition, including the histogram model and correlation model for simple BTFs captured from random height fields with Lambertian reflectance <ref type="bibr" target="#b4">[5]</ref>, a color correlation model <ref type="bibr" target="#b9">[10]</ref>, a 3D-texton-based histogram model <ref type="bibr" target="#b11">[12]</ref>, and a model combining PCA and 2D textons <ref type="bibr" target="#b2">[3]</ref>.</p><p>Existing BTF synthesis methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are designed for 2D rectangles <ref type="bibr" target="#b11">[12]</ref> or rectangular surface patches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, not for arbitrary surfaces. Texture morphing <ref type="bibr" target="#b4">[5]</ref> is a technique for BTF synthesis under the assumption that the surface is a simple height field with Lambertian reflectance. In <ref type="bibr" target="#b11">[12]</ref>, a BTF synthesis algorithm was suggested based on 3D textons. This algorithm first synthesizes a 2D map of texton labels using non-parametric sampling <ref type="bibr" target="#b7">[8]</ref> and then reconstructs a BTF from the synthesized texton labels. A big problem with this reconstructive synthesis of a BTF is the high computational costs for synthesizing texton labels and reconstructing the BTF using the huge appearance vectors of textons (e.g., an appearance vector is 57600-dimensional if ¼¼ sample images of the BTF are used as in <ref type="bibr" target="#b11">[12]</ref>).</p><p>To capture and efficiently render the complex appearance of the real world surface, Malzbender et al. proposed polynomial texture maps for capturing the surface appearance under a fixed viewpoint but different lighting directions <ref type="bibr" target="#b14">[15]</ref>. For surface light fields (e.g., <ref type="bibr" target="#b20">[21]</ref>), the appearance of surfaces under fixed light sources but different view directions are captured and stored in a compact way for rendering.</p><p>Generating textures on arbitrary surfaces has been an active area of research (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>). One approach is to map texture patches onto the target surface <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. A good representative work following that approach is the lapped texture technique by Praun et al. <ref type="bibr" target="#b15">[16]</ref>. They randomly paste texture patches onto the surface following orientation hints provided by the user. To hide the mismatched features across patch boundaries, textures in the overlapping regions are blended. This technique works well for many textures, but for highly structured textures and textures with strong low-frequency components, the seams along patch boundaries are still evident <ref type="bibr" target="#b15">[16]</ref>.</p><p>A number of algorithms have been proposed for directly synthesizing textures on arbitrary surfaces. Turk's algorithm <ref type="bibr" target="#b17">[18]</ref>, Wei and Levoy's algorithm <ref type="bibr" target="#b19">[20]</ref>, and the multi-resolution synthesis algorithm by Ying et al. <ref type="bibr" target="#b21">[22]</ref> are general-purpose algorithms based on the search strategy proposed by Wei and Levoy <ref type="bibr" target="#b18">[19]</ref>. The algorithm by Gorla et al. <ref type="bibr" target="#b8">[9]</ref> is also a general-purpose algorithm, based on the search strategy proposed by Efros and Leung <ref type="bibr" target="#b7">[8]</ref>. These algorithms tend to be slow, but they can be accelerated by using either treestructured vector quantization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> or a kd-tree <ref type="bibr" target="#b21">[22]</ref>. Generally, these algorithms produce high-quality results. A special type of textures that cannot be handled well by general-purpose algorithms is the so-called "natural" textures <ref type="bibr" target="#b1">[2]</ref>. Ashikhmin proposed a specialpurpose algorithm for "natural" textures <ref type="bibr" target="#b1">[2]</ref>, and his algorithm has been adapted for "natural" surface textures <ref type="bibr" target="#b21">[22]</ref>. However, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> do not generalize well to other types of textures. Given Ì and a mesh Å , we synthesize the surface BTF Ì ¼ in two steps: texton analysis and surface BTF synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>In the first step, we generate a 2D texton map Ø Ò and build the surface texton space Ë, which is represented by the dot-product matrix . From the sample BTF Ì , we construct a 3D texton vocabulary Î Ø½ ØÒ Ø as in <ref type="bibr" target="#b11">[12]</ref>. Based on Î we can assign a texton label to each pixel of the sample BTF Ì and thus generate the 2D texton map Ø Ò.</p><p>The surface texton space Ë is the inner-product space spanned by using the 3D textons Ø½ ØÒ Ø as basis vectors. Each element of Ë is called a surface texton. The surface texton space Ë is represented by the dot-product matrix , an ÒØ ¢ ÒØ matrix that stores the dot-product of every pair of 3D textons in Î . The fact that there are only a small number of 3D textons <ref type="bibr" target="#b11">[12]</ref> implies that the dot-product matrix is compact, and so is Ë. For example, a ¢ BTF sample consisting of ¿ ¼¼ color images is about 59 Mb. Its representation with 400 3D textons extracted from ¼¼ sample images is about 92Mb; the corresponding dot-product matrix is only 640 Kb. The construction of Ë is simple: all we need to do is calculate the dot-product matrix and discard the appearance vectors.</p><p>In the surface BTF synthesis step, we use the surface texton map to compactly represent the surface BTF Ì ¼ . The surface texton map ØÓÙØ is a list of entries, one for each mesh vertex. The entry for vertex Ú, ØÓÙØ´Úµ, consists of a texton label and a texture coordinate ÔÚ ´ Ú Úµ, implicitly defining</p><formula xml:id="formula_0">Ì ¼ Ú ´ Ö Öµ Ì ´ Ú Ú Ö Öµ</formula><p>We treat the 2D texton map Ø Ò as a texture sample and perform surface texture synthesis to generate the surface texton map ØÓÙØ. We generate the surface texton map entries for mesh vertices incrementally, one vertex at a time. At each mesh vertex Ú, we simultaneously synthesize the texton label and generate a texture coordinate defining the BTF value at Ú.</p><p>The basic operations in surface texton map synthesis are texton resampling and the distance computation between surface textons. All these calculations can be carried out as operations in the surface texton space Ë and thus are fully determined by the pre-computed dot-product matrix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Texton Analysis</head><p>The texton analysis takes the following steps: (a) build a vocabulary of 3D textons from the sample BTF Ì , (b) assign texton labels to the pixels of Ì to get the 2D texton map Ø Ò, and (c) construct the surface texton space Ë by calculating the dot-product matrix and discarding the appearance vectors. 3D Texton Vocabulary: The construction of 3D textons is mostly based the original 3D texton paper by Leung and Malik <ref type="bibr" target="#b11">[12]</ref>. As in [12], we construct 3D textons from a BTF using K-means clustering. To capture the appearance of mesostructures at different viewing/lighting conditions, we treat the BTF sample Ì Ò as a stack of Ò images and filter each image with a filter bank of Ò Gaussian derivative filters. For each pixel of Ì Ò, the filter responses of Ò× selected images are concatenated into a Ò×Ò -dimensional data vector. These data vectors are clustered using the K-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample BTF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texton Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surface Texton Map Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Texton Map Dot-Product Matrix Mesh M Surface Texton Map tout</head><p>The resulting K-means centers Ø½ ØÒ Ø are the 3D textons, and the associated Ò×Ò -dimensional concatenated filter response vectors Ú½ ÚÒ Ø are the appearance vectors. We also generate an extra texton by averaging the appearance vectors of all textons. This extra texton is used as the default texton in surface BTF synthesis.</p><p>We use ÒØ ¼¼ for all examples in this paper.</p><p>Our 3D texton construction differs from <ref type="bibr" target="#b11">[12]</ref> in choosing the Ò× selected images. The reason for only selecting Ò× images from Ì Ò for clustering, where Ò× Ò, is to reduce computation by exploring the coherence of the same material in different viewing/lighting settings. In <ref type="bibr" target="#b11">[12]</ref>, the Ò× images are randomly chosen. However, this is suboptimal because the radiance distribution is non-uniform in the viewing-lighting space. Cula and Dana <ref type="bibr" target="#b2">[3]</ref> proposed a method for automatically selecting a subset representative images from a BTF sample. Their method works with a statistical representation of a BTF sample (the histogram of 2D textons). Because a histogram-based representation discards the spatial distribution of the BTF data, adopting <ref type="bibr" target="#b2">[3]</ref> in BTF synthesis framework is difficult.</p><p>We choose the Ò× representative images by K-means clustering in the viewing-lighting dimensions of the BTF sample Ì Ò. Specifically, we filter image Á of Ì Ò using our filter bank of Gaussian derivative filters, producing a 48-dimensional filter-response vector for each pixel of Á . The filter-response vectors of pixels on a regularly-spaced subsampling grid in Á is concatenated into an image appearance vector representing Á . The image appearance vectors of all images in Ì Ò are then K-means clustered. For each cluster, the image whose image appearance vector is nearest to the cluster center is selected as the representative image. Note that forming an image appearance vector by concatenating only filter-response vectors on a subsampling grid is the key to saving computation, and we are allowed to subsample because, as far as clustering is concerned, the filter-response vector of a pixel captures enough local structure around the pixel. We used Ò× ¼¼ for all examples in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Texton Map:</head><p>Once we have the texton vocabulary Ø½,...,ØÒ Ø , we can easily assign a texton label to each pixel of Ì Ò. The texton label at pixel Ô is Ø Ò´Ôµ Ö Ñ Ò ÒØ ½ Ú´Ôµ Ú ¾ where Ú´Ôµ is the Ò×Ò -dimensional concatenated filter response vector of pixel Ô, and Ú is the appearance vector of 3D texton Ø . The resulting Ø Ò is called a 2D texton map, or texton map for short.</p><p>Surface Textons: The 3D textons Ø½ ØÒ Ø can be regarded as abstract vectors and they span a vector space Ë. Any vector × in Ë is of the form × = È ÒØ ½ Ø , where ½ and ÒØ are real numbers. We call Ë the surface texton space. The surface texton space is actually an inner-product space. The dot product of two basis vectors Ø and Ø is defined as Ø ¡ Ø = Ú ¡ Ú , where Ú and Ú are the appearance vectors of Ø and Ø respectively. We precompute the dot product of every pair of basis vectors and store the results in an ÒØ ¢ÒØ matrix µ such that Ø ¡Ø . Once is computed, we discard all appearance vectors.</p><p>An element of the surface texton space Ë is a surface texton. Note that Ø½ ØÒ Ø , with their appearance vectors discarded, are also surface textons because they are the basis of Ë. The resampling and distance computation for surface textons as required by surface BTF synthesis can be formulated as linear transformations and dot-products in the surface texton space Ë. All these operations are abstract in that they do not refer to the appearance vectors. In particular, the dot product of any two vectors × and × ¼ in Ë can be obtained easily from , without referring to any appearance vector.</p><formula xml:id="formula_1">Let × È ÒØ ½ Ø and × ¼ È ÒØ ½ ¼ Ø . Then it is easy to verify that × ¡ × ¼ = È ÒØ ½ ¼ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Surface BTF Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BTF Synthesis with Surface Textons</head><p>Before the BTF synthesis starts, we have the 2D texton map Ø Ò and the surface texton space Ë with the pre-computed dot-product matrix . We define a local texture coordinate frame ( × Ø Ò) at each vertex of the target mesh Å. The vector Ò is the surface normal at Ú, whereas × and Ø are the "right" and "up" directions determined by a vector field, which is either interpolated from a number of user-specified directions <ref type="bibr" target="#b17">[18]</ref> or generated by relaxation <ref type="bibr" target="#b19">[20]</ref>. Surface Texton Map Synthesis: A single-resolution version of the surface texton map synthesis proceeds as follows. We walk through the vertices of the target mesh Å and compute an surface texton for every vertex. At each vertex Ú, the surface texton map entry ØÓÙØ´Úµ is obtained through the following steps. First, we construct a neighborhood AE ´Úµ in the ´× Øµ-plane of Ú's local texture coordinate frame ( × Ø Ò). Then, we build a candidate set ´Úµ consisting of the candidate pixels for Ú in the 2D texton map Ø Ò. Next, we search the candidate set ´Úµ to find a pixel Ô¼ ´ ¼ ¼µ such that the distance between AE ´Úµ and the neighborhood of Ô¼, AE ´Ô¼µ, is the smallest. Finally, we set the texton label of the surface texton map entry ØÓÙØ´Úµ to be Ø Ò´Ô¼µ and the texture coordinate of ØÓÙØ´Úµ to be ´ ¼ ¼ µ. The pseudo-code of the surface texton map synthesis is as follows.</p><p>For each vertex Ú on surface construct neighborhood textons AE ´Úµ smallest match = BIG; form the candidates set ´Úµ For each pixel Ô ´ µ in ´Úµ construct neighborhood textons AE ´Ôµ new match = ×Ø Ò ´AE´Úµ AE ´Ôµµ;</p><formula xml:id="formula_2">If (new match smallest match) smallest match = new match Ø¼ Ø Ò´Ôµ, ´ ¼ ¼ µ ´ µ ØÓÙØ´Úµ Ø ÜØÓÒ Ð Ð Ø¼ ØÓÙØ´Úµ Ø ÜØÙÖ ÓÓÖ Ò Ø</formula><p>´ ¼ ¼µ</p><p>The surface texton map synthesis essentially generates the BTF value at vertex Ú by copying the BTF value at location Ô¼ in the sample BTF. Location Ô¼ is chosen according to the neighborhood similarity of AE´Úµ and AE´Ô¼µ as measured by their surface textons. This is a valid similarity measure because the texton-based similarity of AE´Úµ and AE ´Ô¼µ implies their similarity as measured by their BTF values <ref type="bibr" target="#b11">[12]</ref>. Of course a big advantage of the textonbased neighborhood similarity measure is that texton distances can be efficiently evaluated for surface textons.</p><p>Texton Resampling: Texton resampling is necessary for constructing neighborhood AE´Úµ. We construct AE ´Úµ in the ´× Øµ-plane of Ú's local texture coordinate frame ( × Ø Ò) as follows. First, a patch È ´Úµ is generated in the ´× Øµ-plane by flattening a set of triangles near Ú <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. Then, the pixels in the neighborhood AE ´Úµ are resampled from the patch triangles using a neighborhood template <ref type="bibr" target="#b19">[20]</ref>, as is shown in Fig. <ref type="figure" target="#fig_5">4</ref>. Finally, a surface texton ×´Ôµ is obtained at each neighborhood pixel Ô in AE ´Úµ through the following interpolation:</p><formula xml:id="formula_3">×´Ôµ Û¼Ø ¼ • Û½Ø ½ • Û¾Ø ¾<label>(1)</label></formula><p>where ´Û¼ Û ½ Û ¾ µ is the barycentric coordinates of Ô in the patch triangle that contains Ô, and Ø ¼ , Ø ½ , and Ø ¾ are textons at the vertices of that patch triangle. For implementation, ×´Ôµ can be efficiently represented by a -tuple ´Û¼ Û ½ Û ¾ Ø ¼ Ø ½ Ø ¾ µ. The default texton is assigned to neighborhood pixels that are not contained by any patch triangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Computation:</head><p>We need to find a pixel Ô¼</p><p>´ ¼ ¼µ from the candidate set ´Úµ such that the distance between the two neighborhoods, AE ´Úµ and AE´Ô¼µ, is the smallest. For this purpose we need to compute, for each pixel Ô in ´Úµ, the distance between the neighborhoods AE´Ôµ and AE´Úµ. This distance can be written as</p><formula xml:id="formula_4">×Ø Ò ´AE´Úµ AE ´Ôµµ ÒÚ ½ ×´Ô µ Ø ¾</formula><p>where ÒÚ is the number of pixels in AE´Úµ and each ×´Ô µ is a surface texton. Each term ×´Ô µ Ø ¾ of the above distance can be written as the dot product of two surface textons, ´×´Ô µ Ø µ ¡ ´×´Ô µ Ø µ which can be easily evaluated using the pre-computed dot-product matrix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Resolution Synthesis:</head><p>To improve synthesis quality, we have designed and implemented a two-pass multi-resolution version of our BTF synthesis algorithm in a fashion similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. In the pre-processing stage of the two-pass version, we build a texton pyramid and a mesh pyramid. For the texton pyramid, we first construct an image pyramid for each image of the BTF sample. Then a 2D texton map and a dot-product matrix is generated at each level. The number of textons at the next lower resolution Ð •½ is about a quarter of that at the current resolution Ð . For the mesh pyramid, we use Turk's algorithm <ref type="bibr" target="#b16">[17]</ref>. Starting from the highest resolution mesh, we generate the mesh in the next lower resolution level Ð •½ by retiling the current level Ð mesh with about a quarter of the vertices. The vertices at each level of the mesh are randomly mapped to the pixels of the texton map at the same level, with the texton label and texture coordinate of a vertex coming from its corresponding pixel. The pixels of ½´Ô¼µ in Á Ò are colored black. Each black pixel is the coherence candidate corresponding to a colored pixel in AE ´Ô¼µ: Ô¿ is the coherence candidate corresponding to the green pixel, Ô¾ is the coherence candidate corresponding to the red pixel, and Ô½ is the coherence candidate corresponding to both the yellow and blue pixels. (c) For each pixel in ½ ´Ô¼µ, its 3 nearest neighbors are added to ´Ô¼µ. Here we only show the 3 nearest neighbors of Ô½.</p><p>In the first pass, the surface texton map at the level Ð mesh is synthesized from the level Ð •½ texton map. For a mesh vertex Ú at level Ð , we find a point Ú •½ at the level Ð •½ mesh by following the surface normal at Ú on the level Ð mesh. We compute the surface texton map entry at Ú •½ using the level Ð •½ texton map. The texture coordinate of Ú is derived from that of Ú •½ . The texton label at Ú is fetched from the level Ð texton map using Ú 's texture coordinate.</p><p>In the second pass, when synthesizing the surface texton map entry at vertex Ú in the level Ð mesh, we use the neighborhood of Ú as well as that of Ú •½ at level Ð •½, where Ú •½ is found as in the first pass. For vertex Ú , we form the candidate set ´Ú µ using Ú 's neighborhood at level Ð only. The two-level neighborhoods and the corresponding dot-product matrices are used for neighborhood distance computation when searching for the best candidate from ´Úµ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fast Search for Surface Textons</head><p>So far we have not described the search strategy, i.e., the strategy to form the candidate set ´Úµ for a mesh vertex Ú. Most existing search strategies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref> can be adapted for constructing ´Úµ. We have experimented with the full search, in which ´Úµ consists of all pixels of the 2D texton map Ø Ò. This is a general search strategy used by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref> and the multiresolution synthesis algorithm in <ref type="bibr" target="#b21">[22]</ref>. Unfortunately, the full search is painfully slow with surface textons for two reasons. First, the full search is itself slow because the candidate set is as big as it gets. Second, existing acceleration techniques including vector quantization <ref type="bibr" target="#b18">[19]</ref> and the kd-tree <ref type="bibr" target="#b0">[1]</ref> do not work well with surface textons because surface textons are not the usual intensity values. The kd-tree, for example, requires sorting data vectors by one of their components <ref type="bibr" target="#b0">[1]</ref>. Such sorting is not possible when the data vectors are surface textons. For fast searching with surface textons, we have developed a general search strategy called the -coherence search.</p><p>Candidate Set for 2D Textures: For simplicity we explain thecoherence search in the context of synthesizing a 2D texture ÁÓÙØ. Suppose we are to synthesize a pixel Ô¼ of ÁÓÙØ based on the already synthesized pixels in a neighborhood AE ´Ô¼µ of Ô¼, as is illustrated in Fig. <ref type="figure" target="#fig_6">5</ref>. Every synthesized pixel Ô× in AE ´Ô¼µ corresponds to a pixel Ô½ in the input sample texture Á Ò. We call Ô½ a coherence candidate for Ô¼ because it is a good candidate according to the coherence of ÁÓÙØ: a pixel that is appropriately "forward-shifted" with respected to a pixel already used for synthesis is well-suited to fill in Ô¼ <ref type="bibr" target="#b1">[2]</ref>. The coherence candidates are collected in ½´Ô¼µ, the coherence candidate set.</p><p>The -coherence search constructs the candidate set ´Ô¼µ as the -coherence candidate set ´Ô¼µ, which is formed by adding, for each pixel Ô½ of ½ ´Ô¼µ, a set of pixels Ô¾ Ô of Á Ò such that the newly-added pixels are closer to Ô½ than any other pixels in Á Ò by the neighborhood distance. The idea of -coherence search is to speed up the search by guiding it to pixels of Á Ò that are close to the coherence candidates according to the neighborhood distance. This guidance is valid because by the Markov property <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, whether a pixel is an eligible candidate is completely determined by pixels in its surrounding neighborhood. If the coherence candidates are suitable to fill Ô¼, then pixels close to the coherence candidates by the neighborhood distance are also good candidates for Ô¼.</p><p>The -coherence search is fast because the -coherence candidates set is much smaller (usually ½½) than that of the full search and it can be constructed very quickly with the pre-computed list of nearest neighbors for each pixel of Á Ò. For a small Á Ò ( ¢ ), the nearest neighbors of every pixel of Á Ò can be pre-computed fairly quickly by an exhaustive search in Á Ò. For a large Á Ò ( ¢ ), a two-level pyramid is built to speed up the pre-processing of lists of nearest neighbors for all pixels in Á Ò.</p><p>Specifically, to compute the nearest neighbors of a pixel Ô´ µ, we first compute Ñ initial candidates for Ô´ ¾ ¾µ in the lowresolution version of Á Ò, where Ñ ½¼¼ in our implementation.</p><p>For each initial candidate in the low-resolution version of Á Ò, its four corresponding pixels in Á Ò are added to the set of initial candidates in Á Ò. After all £ Ñ initial candidates are so generated, the nearest neighbors of pixel Ô are found from these initial candidates.</p><p>An important advantage of the -coherent search is that its pyramid-based acceleration also works for surface textons. For the -coherent search, the low-pass filtering needed for pyramid-based acceleration only takes place on the 2D texton map Ø Ò. The texton pyramid constructed for multi-resolution synthesis can also be used for building the list of the nearest neighbors. As a result, we do not need to low-pass filter the surface textons during the surface texton map synthesis. Low-pass filtering surface textons is a hard operation to define because surface textons have no appearance vectors. Fig. <ref type="figure" target="#fig_7">6</ref> shows the basic behaviors of -coherence search. When ½½ the results of the -coherence search are practically the same as that by the full search <ref type="bibr" target="#b18">[19]</ref>. As decreases, the results look less and less like that of the full search. When ½ , the results become the same as those generated by Ashikhmin's algorithm <ref type="bibr" target="#b1">[2]</ref>. Candidate Set on Surfaces: We now consider the construction of the -coherence candidate set ´Úµ for a mesh vertex Ú. Let Ú½ Ú Ñ be the set of all vertices in the flattened patch È ´Úµ whose surface textons have been synthesized. Vertex Ú has a texture coordinate ´× Ø µ and an offset ´Ü Ý µ from Ú in the patch È ´Úµ. As shown in Fig. <ref type="figure" target="#fig_5">4</ref> (b), we forward-shift ´× Ø µ by the offset ´Ü Ý µ in the 2D texton map Ø Ò, getting to location ´×¼ Ø ¼ µ = ´× Ü Ø Ý µ in Ø Ò. Then we fetch the list Ä of nearest neighbors at the pixel closest to ´×¼ Ø ¼ µ. The candidate set ´Úµ consists of all nearest neighbors in all the lists Ä½ through ÄÑ.</p><p>In multiresolution synthesis, a list of nearest neighbors is built for each pixel of the texton map at every level. In the second pass of a two-pass synthesis, we also use a two-level neighborhood when building the list of nearest neighbors for every pixel so that the neighborhoods on the side of the texton pyramid are consistent with the two-level neighborhoods on the side of the mesh pyramid. Discussion: The -coherence search was inspired by Ashikhmin's work <ref type="bibr" target="#b1">[2]</ref>. However, our goal is different from Ashikhmin's. We want to derive a general-purpose search strategy for fast search with surface textons; his goal was to develop a special-purpose algorithm for handling "natural" textures, i.e., textures consisting of arrangements of small objects of familiar but irregular shapes <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Surface BTF Rendering</head><p>From the surface texton map ØÓÙØ and the sample BTF Ì , we can efficiently render the BTF on the target mesh Å as follows. First, we compute the viewing and lighting directions for each mesh vertex Ú in its local texture coordinate frame from the given light source location and the viewpoint. Vertices occluded from either the light sources or the viewpoint are ignored. Then, a set of nearby images are found from the BTF sample Ì . Using Ú's texture coordinate, we can look up colors from this set of images and blend them to get the color of Ú. With all vertex colors obtained, the mesh can be sent to the graphics pipeline for display. This procedure repeats for every novel lighting/viewing configuration.</p><p>Finding the nearest images from the sample BTF Ì is simple because the images in Ì are evenly distributed in the viewing and lighting space. We first find nearest sample viewing directions and nearest sample lighting directions separately. The angle between two lighting/viewing directions is used as the distance measure. Then, the ¢ nearest images are simply those corresponding to all combinations of the viewing/lighting directions found in the previous step. Debevec et al. <ref type="bibr" target="#b6">[7]</ref> proposed a general technique for finding the nearest images for a given viewing/lighting setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We have implemented our surface BTF synthesis algorithm on a PC. The system is easy to use. The texton analysis stage involves no user intervention. The BTF synthesis stage is as automatic as surface texture synthesis (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>). The system only requires the user to determine whether the BTF sample resemble "natural" textures and if so, set ½ for the -coherence search. Anisotropic BTFs are handled the same way as anisotropic textures in surface texture synthesis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. Like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, an optional but often helpful user intervention for an anisotropic BTF is to specify a vector field to guide the orientation of the BTF on the target surface.</p><p>In the following we report synthesis results for both sample BTFs of real-world textures and synthetic BTF samples. The real-world samples were taken from the CUReT database <ref type="bibr" target="#b5">[6]</ref>. All surface BTFs shown in this paper were synthesized using three-or fourlevel pyramids of the meshes and the sample BTFs. ½¾ ¢ ½¾ 8066 min. 157 min.</p><p>Table <ref type="table">2</ref>: Speed comparison for BTF synthesis using the full search (e.g. <ref type="bibr" target="#b17">[18]</ref>) and the -coherence search.</p><p>Fig. <ref type="figure" target="#fig_8">7</ref> exhibits several surface BTFs. Timings for surface BTF synthesis are summarized in Table <ref type="table" target="#tab_0">1</ref>. Timings are in minutes measured on a 700MHz Pentium III. The time complexity of our algorithm only depends on size of the neighborhood and for the -coherence search. During the BTF synthesis, our algorithm uses about the same amount of memory as surface texture synthesis algorithms such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>. The only extra memory we need is that for the dot-product matrix, which is less than 1 Mb in our system. Timings are for BTF synthesis only. The texton analysis time for a BTF sample of size ¢ takes about 45 minutes on the same machine. For the BTF sample in Fig. <ref type="figure" target="#fig_8">7</ref> (a) and (b), we set ½ because the samples resemble "natural" textures <ref type="bibr" target="#b1">[2]</ref>.</p><p>Like surface textures, the synthesized BTFs fit the surface geometry naturally and seamlessly. More importantly, the surface BTFs capture the fine-scale shadows, occlusions, and specularities caused by surface mesostructures. Comparison of the synthesized surface BTFs with the sample BTFs demonstrates their similarity. In the companion video, we show that this similarity remains in all viewing and lighting conditions. Moreover, the synthesized mesostructures are consistent as viewing and lighting directions change.</p><p>Table <ref type="table">2</ref> compares the BTF synthesis speed for the full search (e.g. <ref type="bibr" target="#b17">[18]</ref>) and the -coherence search using the dinosaur model with 250k vertices. For a small BTF sample ( ¢ ), BTF synthesis based on the -coherence search is about 10 times faster than that based on the full search. The speed gain increases quickly as the BTF sample gets larger. In Fig. <ref type="figure" target="#fig_10">9</ref>, we compare the qualities of the surface BTFs synthesized with these two search strategies by rendering the BTFs with identical viewing and lighting. Our experiments demonstrate that the -coherence search allows us to synthesize surface BTFs orders of magnitude faster while getting comparable quality.</p><p>Surface BTFs provide an efficient way for rendering surfaces with complex appearance models that are created synthetically. Fig. <ref type="figure" target="#fig_9">8</ref> shows an example, in which a small BTF sample is rendered by ray tracing and then synthesized onto a surface. The rendering of the surface BTF captures the shadow and occlusion caused by the height field of the synthetic appearance model. Conventional textures and bump maps cannot capture these effects. Although displacement maps can capture these effects, rendering displacement maps is very expensive. In addition, some appearance models have complex local geometry details and BRDF variations that cannot be rendered by displacement maps. Surface BTFs will not have problems handling these appearance models. With our unoptimized implementation, the surface BTF shown in Fig. <ref type="figure" target="#fig_9">8</ref> (300k vertices) can be rendered at a speed of more than one frame per second.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented an algorithm for synthesizing BTFs on arbitrary manifold surfaces using surface textons. A BTF synthesized using our algorithm not only looks similar to the sample BTF in all viewing and lighting conditions but also exhibits a consistent mesostructure when the viewing and lighting directions change. Because the BTF can describe real-world textures, our algorithm enables the user to decorate real-world geometry with real-world textures.</p><p>A limitation of surface BTF synthesis using surface textons is that the synthesis algorithm does not work well for materials that cannot be described by 3D textons. So far one such material has been reported <ref type="bibr" target="#b11">[12]</ref>, i.e., the "Aluminum Foil" dataset in the CUReT database <ref type="bibr" target="#b5">[6]</ref>. It will be desirable to develop more sophisticated filtering/clustering techniques for this sort of materials. We are also interested in efficient rendering methods for a surface BTF represented by a sample BTF and a surface texton map. The basic rendering operations of a surface BTF so represented are simple and should be amenable to hardware acceleration. Finally, an intriguing possibility is to use synthesis methods to produce appearance not captured by the BTF, e.g., subsurface scattering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>£</head><label></label><figDesc>3F Beijing Sigma Center, No 49 Zhichun Road, Haidian District, Beijing 100080, P R China, email: bainguo@microsoft.com Ý This research was done when Jingdan Zhang and Xi Wang were working as part-time interns at Microsoft Research Asia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left column exhibits images of a surface texture. Right column exhibits images of a surface BTF.</figDesc><graphic coords="1,330.73,261.24,105.72,105.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inconsistent mesostructures caused by BTF blending. (a) The viewing/lighting setting for the images in (b) and (c). Ä¼ is the lighting direction for the image shown in (b) and Ä½ is the lighting direction for the image shown in (c). Both images are for the square outlined by the yellow line in (a). The viewing direction for both images is the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 provides an overview of our system. The given sample BTF Ì ´Ü Ý</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Data flow in our system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>VFigure 4 :</head><label>4</label><figDesc>Figure 4: The surface patch around the red vertex Ú in (a) is flattened into the blue patch È ´Úµ in (b). The neighborhood template for resampling is shown in red in (b). The resampling of textons from vertices of the yellow patch triangle to a neighborhood pixel Ô is shown in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The -coherence candidates of a pixel Ô¼ for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 2D texture synthesis results using the -coherence search and the full search. (a) -coherence search with ½. (b)coherence search with . (c) -coherence search with</figDesc><graphic coords="6,108.16,141.73,75.96,75.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top row: Surface BTFs synthesized using real BTF samples from the CUReT database [6]. Bottom row: Surface BTFs synthesized from synthetic BTF samples.</figDesc><graphic coords="7,90.58,62.01,430.76,430.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (a) Surface BTF. (b) Surface texture. (c) Surface BTF. (d) Surface texture.</figDesc><graphic coords="7,63.54,548.10,483.94,141.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Quality comparison for BTF synthesis using the -coherence search (left) and the full search (right).</figDesc><graphic coords="8,135.22,54.76,341.39,167.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Timings for synthesizing the surface BTFs shown in Fig.7.</figDesc><table><row><cell>All examples</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank Yanyun Chen and Xinguo Liu for useful discussions. Many thanks to Yanyun Chen for his help in generating synthetic BTF data, to Yin Li, Gang Chen, and Steve Lin for their help in video production, to Steve Lin for proofreading this paper, and to anonymous reviewers for their constructive critique. Xiang Cao implemented the first version of 3D textons.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching</title>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="891" to="923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesizing natural textures</title>
		<author>
			<persName><surname>Michael Ashikhmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="2001-03">2001. March 2001</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compact representation of bidirectional texture functions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Oana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">J</forename><surname>Cula</surname></persName>
		</author>
		<author>
			<persName><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
	<note>Proceedings of SIGGRAPH 96</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d textured surface modeling</title>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shree</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on the Integration of Appearance and Geometric Methods in Object Recognition</title>
		<meeting>IEEE Workshop on the Integration of Appearance and Geometric Methods in Object Recognition</meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reflectance and texture of real-world surfaces</title>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Bram Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">J</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1999-01">January 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient viewdependent image-based rendering with projective texture-mapping</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">D</forename><surname>Borshukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998-06">1998. June 1998</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Growing fitted textures</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Gorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Interrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2001 Sketches and Applications</title>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page">191</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The analysis and recognition of real-world textures in 3d</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Patten Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Illuminance texture due to surface mesostructure</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="463" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using 3d textons</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesizing bidirectional texture functions for real-world surfaces</title>
		<author>
			<persName><forename type="first">Xinguo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2001</title>
		<meeting>SIGGRAPH 2001</meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive texture mapping</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Verroust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 93</title>
		<meeting>SIGGRAPH 93</meeting>
		<imprint>
			<date type="published" when="1993-08">August 1993</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Polynomial texture maps. Proceedings of SIGGRAPH</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Malzbender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Wolters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-08">2001. August 2001</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lapped textures</title>
		<author>
			<persName><forename type="first">Emil</forename><surname>Praun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2000</title>
		<meeting>SIGGRAPH 2000</meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Re-tiling polygonal surfaces</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH 92)</title>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Texture synthesis on surfaces</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2001</title>
		<meeting>SIGGRAPH 2001</meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2000</title>
		<meeting>SIGGRAPH 2000</meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texture synthesis over arbitrary manifold surfaces</title>
		<author>
			<persName><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2001</title>
		<meeting>SIGGRAPH 2001</meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page" from="355" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Surface light fields for 3d photography</title>
		<author>
			<persName><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">I</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Aldinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series</title>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
	<note>Proceedings of SIGGRAPH 2000</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Texture and shape synthesis on surfaces</title>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Biermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 12th Eurographics Workshop on Rendering</title>
		<meeting>12th Eurographics Workshop on Rendering</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
