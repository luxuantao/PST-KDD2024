<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-09">9 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinghua</forename><surname>Zhang</surname></persName>
							<email>zhangxinghua@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
							<email>yubowen@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
							<email>liutingwen@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<email>zhangzhenyu1996@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Sheng</surname></persName>
							<email>shengjiawei@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Mengge</forename><surname>Xue</surname></persName>
							<email>xuemengge@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<email>hbxu@iie.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-09">9 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.04429v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotation noise, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the whole training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutuallybeneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) is the task of detecting entity spans and then classifying them into predefined categories, such as person, location and organization. Due to the capability of extracting entity information and benefiting many NLP applications (e.g., relation extraction <ref type="bibr" target="#b20">(Lin et al., 2017)</ref>, question answering <ref type="bibr" target="#b17">(Li et al., 2019)</ref>), NER appeals to many researchers. Traditional supervised methods for NER require a large amount of high-quality corpus for model training, which is extremely expensive and time-consuming as NER requires token-level labels.</p><p>Therefore, in recent years, distantly supervised named entity recognition (DS-NER) has been proposed to automatically generate labeled training set Jack Lucas was born in the Amazon region .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-PER I-PER O O O O B-LOC O O O O O O O O B-ORG O O</head><p>Golden Labels:</p><p>Noisy Labels:</p><p>Figure <ref type="figure" target="#fig_1">1</ref>: A noisy sample generated by distantlysupervised methods, where Jack Lucas is the incomplete annotation and Amazon is inaccurate.</p><p>by aligning entities in knowledge bases (e.g., Freebase) or gazetteers to corresponding entity mentions in sentences. This labeling procedure is based on a strong assumption that each entity mention in a sentence is a positive instance of the corresponding type according to the extra resources. However, this assumption is far from reality. Due to the limited coverage of existing resources, many entity mentions in the text cannot be matched and are wrongly annotated as non-entity, resulting in incomplete annotations. Moreover, two entity mentions with the same surface name can belong to different entity types, thus simple matching rules may fall into the dilemma of labeling ambiguity and produce inaccurate annotations. As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, the entity mention "Jack Lucas" is not recognized due to the limited coverage of extra resources and "Amazon" is wrongly labeled with organization type owing to the labeling ambiguity.</p><p>Recently, many denoising methods <ref type="bibr" target="#b30">(Shang et al., 2018b;</ref><ref type="bibr" target="#b37">Yang et al., 2018;</ref><ref type="bibr" target="#b5">Cao et al., 2019;</ref><ref type="bibr" target="#b27">Peng et al., 2019;</ref><ref type="bibr" target="#b18">Li et al., 2021)</ref> have been developed to handle noisy labels in DS-NER. For example, <ref type="bibr" target="#b30">Shang et al. (2018b)</ref> obtained high-quality phrases through AutoPhrase <ref type="bibr" target="#b29">(Shang et al., 2018a</ref>) and designed AutoNER to model these phrases that may be potential entities. <ref type="bibr" target="#b27">Peng et al. (2019)</ref> proposed a positive-unlabeled learning algorithm to unbiasedly and consistently estimate the NER task loss, and <ref type="bibr" target="#b18">Li et al. (2021)</ref> used negative sampling to eliminate the misguidance brought by unlabeled entities. Though achieving good performance, most studies mainly focus on solving incomplete annotations with a strong assumption of no inaccurate ones existing in DS-NER. Meanwhile, these methods aim to reduce the negative effect of noisy labels by weakening or abandoning the wrongly labeled instances. Hence, they can at most alleviate the noisy supervision and fail to fully mine useful information from the mislabeled data. Intuitively, if we can rectify those unreliable annotations into positive instances for model training, a higher data utilization and better performance will be achieved. We argue that an ideal DS-NER denoising system should be capable of solving two kinds of label noise (i.e., incomplete and inaccurate annotations) and making full use of the whole training set.</p><p>In this work, we strive to reconcile this gap and propose a robust learning framework named SCDL (Self-Collaborative Denoising Learning). SCDL co-trains two teacher-student networks to form inner and outer loops for coping with label noise without any assumption, as well as making full exploration of mislabeled data. The inner loop inside each teacher-student network is a self denoising scheme to select reliable annotations from two kinds of noisy labels, and the outer loop between two networks is a collaborative denoising procedure to rectify unreliable instances into useful ones. Specifically, in the inner loop, each teacher-student network selects consistent and high-confidence labeled tokens generated by the teacher to train the student, and then updates the teacher gradually via exponential moving average (EMA) 2 based on the re-trained student. And as for the outer loop, the high-quality pseudo labels generated by one network's teacher are used to update the noisy labels of the other network thanks to the stability of EMA and different noise sensitivities between two networks. Moreover, the inner and outer loop procedures will be performed alternately. Obviously, a successful self denoising process (inner loop) can generate high-quality pseudo labels which benefit the collaborative learning procedure (outer loop) a lot and a promising outer loop will promote the inner loop by refining noisy labels, thus handling the label noise in DS-NER effectively.</p><p>We evaluate our method on five DS-NER datasets. Experimental results indicate that SCDL consistently achieves superior performance over previous competing approaches. Extensive valida-2 A momentum technique that has been explored in several studies, e.g., <ref type="bibr">Adam (Kingma and Ba, 2015)</ref>, semi-supervised <ref type="bibr" target="#b31">(Tarvainen and Valpola, 2017)</ref> and selfsupervised <ref type="bibr" target="#b8">(Grill et al., 2020)</ref> learning.</p><p>tion studies demonstrate the rationality and robustness of our self-collaborative denoising framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many studies have obtained reliable performance in NER. For example, BiLSTM-CRF <ref type="bibr" target="#b15">(Lample et al., 2016)</ref> and BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> based methods become the paradigm in NER due to their promising performances. However, most of these works rely on high-quality labels, which are quite expensive. To address this issue, several studies attempted to annotate tokens via distant supervision <ref type="bibr" target="#b19">(Liang et al., 2020)</ref>. They matched unlabeled sentences with external gazetteers or knowledge Graphs (KGs). Despite the success of distant supervision, it still suffers from noisy labels (i.e., incomplete and inaccurate annotations in NER).</p><p>DS-NER Denoising. Many studies <ref type="bibr" target="#b30">(Shang et al., 2018b;</ref><ref type="bibr" target="#b5">Cao et al., 2019;</ref><ref type="bibr" target="#b12">Jie et al., 2019)</ref> tried to modify the standard CRF for adapting to the scenario of label noise, e.g., Fuzzy CRF. <ref type="bibr" target="#b24">Ni et al. (2017)</ref> selected high-confidence labeled data from noisy data to train NER models. And many new training paradigms were proposed to resist label noise in DS-NER, such as AutoNER <ref type="bibr" target="#b30">(Shang et al., 2018b)</ref>, Reinforcement Learning <ref type="bibr" target="#b37">(Yang et al., 2018;</ref><ref type="bibr" target="#b26">Nooralahzadeh et al., 2019)</ref>, AdaPU <ref type="bibr" target="#b27">(Peng et al., 2019)</ref> and Negative Sampling <ref type="bibr" target="#b18">(Li et al., 2021)</ref>. In addition, some studies <ref type="bibr" target="#b23">(Mayhew et al., 2019;</ref><ref type="bibr" target="#b19">Liang et al., 2020)</ref> performed iterative training procedures to mitigate noisy labels in DS-NER. However, most studies mainly focus on incomplete annotations regardless of inaccurate ones or depending on manually labeled data. What's more, most prior methods are insufficient since they can at most alleviate the negative effect caused by label noise and fail to mine useful information from the whole training set. Different from previous studies, we propose two denoising learning procedures which can be enhanced each other mutually with the devised teacher-student network and cotraining paradigm, mitigating two kinds of label noise and making full use of the whole training set.</p><p>Teacher-Student Network. The teacher-student network is well known in knowledge distillation <ref type="bibr" target="#b10">(Hinton et al., 2014)</ref>. A teacher is generally a complicated model and the light weight student imitates its output. Recently, there are many variations of teacher-student network. For example, selftraining copies the student as a new teacher to gen-erate pseudo labels <ref type="bibr" target="#b36">(Xie et al., 2020;</ref><ref type="bibr" target="#b33">Wang et al., 2020)</ref>. <ref type="bibr" target="#b19">Liang et al. (2020)</ref> applied self-training with teacher-student network to handle label noise in DS-NER. However, for the teacher-student network in our framework, the teacher selects reliable annotations with devised strategies for training student and then we use EMA to update the teacher based on re-trained student. With this loop, our method can learn entity knowledge effectively.</p><p>Co-Training. The co-training paradigm which jointly trains two models is used to improve the robustness of models <ref type="bibr" target="#b4">(Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b25">Nigam and Ghani, 2000;</ref><ref type="bibr" target="#b14">Kiritchenko and Matwin, 2011)</ref>. Many previous frameworks <ref type="bibr" target="#b9">(Han et al., 2018;</ref><ref type="bibr" target="#b38">Yu et al., 2019;</ref><ref type="bibr" target="#b34">Wei et al., 2020;</ref><ref type="bibr" target="#b16">Li et al., 2020)</ref> have adopted co-training to denoise, but they mainly use the diversity of two single models and the single one doesn't have the denoising ability. But supervision signals from the peer model are not always clean. Instead, we train two groups of teacher-student networks and each group can also perform label denoising effectively which further improves the co-training paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Given the training corpus D where each sample is a form of (X i , Y i ), X i = x 1 , x 2 , ..., x N represents a sentence with N tokens and Y i = y 1 , y 2 , ..., y N is the corresponding tag sequence. Each entity mention e = x i , ..., x j (0 ≤ i ≤ j ≤ N ) is a span of the text , associated with an entity type, e.g., person, location. In this paper, we use the BIO scheme following <ref type="bibr" target="#b19">(Liang et al., 2020)</ref>. In detail, the begin token of an entity mention is labeled as B-type and others are I-type. The non-entity tokens are annotated as O.</p><p>The traditional NER problem is a supervised learning task by fitting a sequence labeling model based on the training dataset. However, we mainly explore the practical scenario when the labels of training data are contaminated due to the distant supervision. In other words, the revealed tag y i may not correspond to the underlying correct one. The challenge posed in this setting is to reduce the negative influence of noisy annotations and generate high-confidence labels for them to make full use of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we give a detailed description of our self-collaborative denoising learning framework, which consists of two interactive teacher-student networks to address both the incomplete and inaccurate annotation issues. As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, each teacher-student network contributes to an inner loop for self denoising and the outer loop between two networks is a collaborative denoising scheme. These two procedures can be optimized in a mutually-beneficial manner, thus improving the performance of the NER system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self Denoising Learning</head><p>It is widely known that deep neural networks have high capacity for memorization <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>. When noisy labels become prominent, deep neural NER models inevitably overfit noisy labeled data, resulting in poor performance. The purpose of self denoising learning is to select reliable labels to reduce the negative influence of noisy annotations. To achieve this end, self denoising learning involves a teacher-student network, where the teacher first generates pseudo labels to participate in labeled token selection, then the student is optimized via back-propagation based on selected tokens, and finally the teacher is updated by gradually shifting the weights of the student in continuous training with exponential moving average (EMA). We take two neural NER models with the same architecture as the teacher and student respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Labeled Token Selection</head><p>This subsection illustrates our labeled token selection strategy based on the consistency and high confidence predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency Predictions. It has been observed</head><p>that the model's predictions of wrongly labeled instances fluctuate drastically in previous studies <ref type="bibr" target="#b11">(Huang et al., 2019)</ref>. A mislabeled instance will be supervised by both its wrong label and similar instances. For example, Amazon is wrongly annotated as organization in Figure <ref type="figure" target="#fig_1">1</ref>. The wrong label organization pushes the model to fit this supervision signal while other clean tokens with similar context will encourage the model to predict it as location. Therefore, we can take advantage of this property to separate clean tokens from noisy ones.</p><p>Based on above analysis, how to quantify the fluctuation becomes a key issue. One straightforward solution is to integrate predictions from different training iterations but with more time-space complexity. Thanks to the widespread concern of EMA, we use it to update the teacher's parameters.  (2) The interplay between two teacher-student networks is an outer loop (i.e., collaborative denoising): the pseudo labels are applied to update the noisy labels of the peer network periodically.</p><p>In this way, the teacher can be viewed as the temporal ensembling of the student models in different training steps and then its prediction will be the ensemble of predictions from past iterations. Therefore, the pseudo labels predicted by the teacher can quantify the fluctuation of noisy labels naturally. Subsequently, we devise the first token selection strategy based on the fluctuation of noisy labels to identify the correctly labeled tokens ( Xi , Ȳi ) via the consistency between noisy labels and predicted pseudo labels, denoted as:</p><formula xml:id="formula_0">( Xi , Ȳi ) CP = {(x j , y j ) | y j = ỹj , ỹj ∈ f (X i ; θ t )} (1)</formula><p>where y j ∈ Y i is the noisy label of the j-th token in the i-th sentence and ỹj is the pseudo label predicted by the teacher θ t .</p><p>High Confidence Predictions. As studied in previous works <ref type="bibr" target="#b3">(Bengio et al., 2009;</ref><ref type="bibr" target="#b0">Arpit et al., 2017)</ref>, hard samples can not be learnt effectively at first, thus predictions of those mislabeled hard samples may not fluctuate and then they are mistakenly believed to be reliable. To alleviate this issue, we propose the second selection strategy to pick tokens with high confidence predictions, as formulated in Equation <ref type="formula">2</ref>, where pj is the label distribution of the j-th token predicted by the teacher, δ denotes the confidence threshold.</p><formula xml:id="formula_1">( Xi , Ȳi ) HCP = {(x j , y j ) | max(p j ) ≥ δ} (2)</formula><p>4.1.2 Optimization Loss Function of the Student. Standard supervised NER methods are fitting the outputs of a model to hard labels (i.e, one-hot vectors) to optimize the parameters. However, when the model is trained with tokens and mismatched hard labels, wrong information is being provided to the model. Compared with hard labels, the supervision with soft labels is more robust to the noise because it carries the uncertainty of the predicted results. Therefore, we modify the standard cross entropy loss into a soft label form defined as:</p><formula xml:id="formula_2">L(θ s ) = − 1 M N M i=1 N j=1 C c=1 I i,j pi j,c log(p i j,c )<label>(3)</label></formula><formula xml:id="formula_3">T i = ( Xi , Ȳi ) CP ∩ ( Xi , Ȳi ) HCP<label>(4)</label></formula><p>where p i j,c is the probability of the j-th token with the c-th class in the i-th sentence predicted by the student and pi j,c is from the teacher. T i includes the tokens in the i-th sentence meeting the consistency and high confidence selection strategies simultaneously. I is the indicator function, I i,j = 1 when the j-th token is in T i , otherwise I i,j is 0.</p><p>Then the parameters of the student model can be updated via back-propagation as follows:</p><formula xml:id="formula_4">θ s ← θ s − γ ∂L ∂θ s (5)</formula><p>Update of the Teacher. Different from the optimization of the student model, we apply EMA to gradually update the parameters of the teacher, as shown in Equation <ref type="formula">6</ref>, where α denotes the smoothing coefficient.</p><formula xml:id="formula_5">θ t ← αθ t + (1 − α)θ s (6)</formula><p>Although the clean token selection strategies indeed alleviate noisy annotations, they also suffer from unreliable token choice which misguides the model into generating biased predictions. As formulated in Equation <ref type="formula" target="#formula_6">7</ref>, the update of the teacher θ i t in i-th iteration can be converted into the form of back-propagation (derivations in Appendix A.1):</p><formula xml:id="formula_6">θ i t = θ i−1 t − γ(1 − α) i−1 j=0 α i−1−j ∂L ∂θ j s (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where γ is the learning rate and (1 − α) is a small number because α is generally assigned a value close to 1 (e.g., 0.995), equivalent to multiplying a small coefficient on the weighted sum of student's past gradients. Therefore, with the conservative and ensemble property, the application of EMA has largely mitigated the bias. As a result, the teacher tends to generate more reliable pseudo labels, which can be used as new supervision signals in the collaborative denoising phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Collaborative Denoising Learning</head><p>Based on the devised clean token selection strategy in self denoising learning, the teacher-student network can utilize the correctly labeled tokens in an ideal situation to alleviate the negative effect of label noise. However, just filtering unreliable labeled tokens will inevitably lose useful information in training set since there is no opportunity for the wrongly labeled tokens to be corrected and explored. Intuitively, if we can change the wrong label to the correct one, it will be transformed into a useful training instance.</p><p>Inspired by some co-training paradigms <ref type="bibr" target="#b9">(Han et al., 2018;</ref><ref type="bibr" target="#b38">Yu et al., 2019;</ref><ref type="bibr" target="#b34">Wei et al., 2020)</ref>, we propose the collaborative denoising learning to update noisy labels mutually for mining more useful information from dataset by deploying two teacherstudent networks with different architecture. As stated in <ref type="bibr" target="#b2">(Bengio, 2014)</ref>, a human brain can learn more effectively if guided by the signals produced by other humans. Similarly, the pseudo labels predicted by the teacher are applied to update the noisy labels of the peer teacher-student network periodically since two teacher-student networks have different learning abilities based on different initial conditions and network structures. With this outer loop, the noisy labels can be improved continuously and the training set can be fully explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithm Workflow</head><p>In this subsection, we introduce the overall procedure of our SCDL framework. Algorithm 1 gives </p><formula xml:id="formula_8">I , Y<label>(b)</label></formula><p>II ) from D, step ← step + 1.</p><p>Self Denoising Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Get pseudo-labels via the teacher θt 1 , θt 2 : b) ; θt 2 ).</p><formula xml:id="formula_10">Ỹ (b) I ← f (X (b) ; θt 1 ), Ỹ (b) II ← f (X (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Get clean tokens:</p><formula xml:id="formula_11">T (b) I ← TokenSelection(Y (b) I , Ỹ (b) I ), T (b) II ← TokenSelection(Y (b) II , Ỹ (b) II ). 8:</formula><p>Update the student θs 1 and θs 2 by Eq. 3 and Eq. 5. 9:</p><p>Update the teacher θt 1 and θt 2 by Eq. 6. 10:</p><p>if step mod U pdate_Cycle = 0 then 11:</p><p>Update noisy labels mutually:</p><formula xml:id="formula_12">Collaborative Denoising Learning. YI = {Yi ← f (Xi; θt 2 )} M i=1 , YII = {Yi ← f (Xi; θt 1 )} M i=1 . 12:</formula><p>end if 13: end while 14: Evaluate models θt 1 , θs 1 , θt 2 , θs 2 on Dev set. 15: return The best model θ ∈ {θt 1 , θs 1 , θt 2 , θs 2 } the pseudocode. To summarize, the training process of SCDL can be divided into three procedures:</p><p>(1) Pre-Training with Noisy Labels. We warm up two NER models θ 1 and θ 2 on the noisy labels to obtain a better initialization, and then duplicate the parameters θ for both the teacher θ t and the student θ s (i.e., θ t 1 = θ s 1 = θ 1 , θ t 2 = θ s 2 = θ 2 ). The training objective function in this stage is the cross entropy loss with the following form:</p><formula xml:id="formula_13">L(θ) = − 1 M N M i=1 N j=1 y i j log(p(y i j |X i ; θ)) (8)</formula><p>where y i j means the j-th token label of the i-th sentence in the noisy training corpus and p(y i j |X i ; θ) denotes its probability produced by model θ. M and N are the size of training corpus and the length of sentence respectively. (2) Self Denoising Learning. In this stage, we can select correctly labeled tokens to train the two teacher-student networks respectively. (3) Collaborative Denoising Learning. Self denoising can only utilize correct annotations and this phase will update noisy labels mutually to relabel tokens for two teacher-student networks. The initial noisy labels of two networks comes from distant supervision. The second and third phase are conducted alternately, which  <ref type="bibr">(Li et al., 2021) 80.17 77.72 78.93 64.59 72.39 68.26 70.16 58.78 63.97 49.49 55.35 52.26 50.25 44.95</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the performance of SCDL, compared with several comparable baselines. Additionally, we conduct lots of auxiliary experiments and provide comprehensive analyses to justify the effectiveness of SCDL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. We conduct experiments on five publicly available NER datasets: CoNLL03 <ref type="bibr" target="#b32">(Tjong Kim Sang, 2002)</ref>, OntoNotes5.0 <ref type="bibr" target="#b35">(Weischedel et al., 2013)</ref>, Webpage <ref type="bibr" target="#b28">(Ratinov and Roth, 2009)</ref>, Wikigold <ref type="bibr" target="#b1">(Balasuriya et al., 2009)</ref> and Twitter <ref type="bibr" target="#b7">(Godin et al., 2015)</ref>. <ref type="bibr" target="#b19">Liang et al. (2020)</ref>  Baselines and Evaluation Metrics. We compare our method with several competitive baselines from three aspects. (i) Fully-Clean. BiLSTM-CRF <ref type="bibr" target="#b22">(Ma and Hovy, 2016)</ref> and RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> are fully trained on clean dataset (without noisy labels) for NER, as the upper bound of denoising. (ii) Fully-Noisy. KB-Matching uses distant supervision to annotate test set. BiLSTM-CRF, DistilRoBERTa and RoBERTa are trained on noisy dataset without label denoising, as the lower bound of denoising. (iii) Label-Denoising.</p><p>We compare several DS-NER denoising baselines which propose to solve noisy labels. Au-toNER <ref type="bibr" target="#b30">(Shang et al., 2018b)</ref> and LRNT <ref type="bibr" target="#b5">(Cao et al., 2019)</ref> try to reduce the negative effect of noisy labels, leaving training dataset unexplored fully. Co-teaching+ <ref type="bibr" target="#b38">(Yu et al., 2019)</ref> and JoCoR <ref type="bibr" target="#b34">(Wei et al., 2020)</ref> are two classical label denoising methods, developed in computer vision. NegSampling <ref type="bibr" target="#b18">(Li et al., 2021)</ref> only handles incomplete annotations by negative sampling. BOND <ref type="bibr" target="#b19">(Liang et al., 2020)</ref> adapts self-training directly to DS-NER, suffering from confirmation bias (a problem from selftraining itself). We use Precision (P), Recall (R) and F1 score as the evaluation metrics.</p><p>Implementation Details. For fair comparison, we adopt RoBERTa (θ 1 ) and DistilRoBERTa (θ 2 ) as the basic models. The max training epochs is 50, and the confidence threshold δ is 0.9. The batch size is set to 16 or 32, the learning rate is 1e-5 or 2e-5 according to different datasets. We tune EMA parameter α from {0.9,0.99,0.995,0.998}, tune update cycle according to the size of dataset (e.g., 6000 iterations (about 7 epochs) for CoNLL03) on development set. We implement our code with Pytorch based on huggingface Transformers<ref type="foot" target="#foot_0">3</ref> . Detailed hyperparameter settings for each dataset and tuning procedures are listed in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the results of our proposed method compared with baselines and highlights the best overall performance in bold. Obviously, SCDL achieves the best performance, and improves the precision as well as F1 score significantly, compared with previous state-of-the-art models. Compared to our basic models (i.e., Distil-RoBERTa and RoBERTa), SCDL improves the F1 score with an average increase of 8.33% and 6.37% respectively, which demonstrates the necessity of label denoising in the distantly-supervised NER task and the effectiveness of the proposed method.</p><p>In addition, SCDL performs much better than previous studies which consider the noisy labels in NER, including AutoNER, LRNT, NegSampling and BOND. The reason is that they mainly focus on one kind of label noise in DS-NER or fail to make full use of the mislabeled data with their strategies. On contrast, our method can not only exploit correctly labeled tokens but also explore valuable information in wrongly labeled ones by correction. Compared to the popular denoising methods in computer vision: Co-teaching+ and JoCoR, SCDL gains of up to 12.05% absolute percentage points in F1 score. We guess this is beacause most computer vision denoising studies focus on instancelevel classification, while NER is a token-level task where non-entity category accounts for the majority, and this case is not fully considered. Thus corruption occurs easily in DS-NER denoising task for these methods as the training goes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Ablation Study. To evaluate the influence of each component in our method, we conduct the ablation study for further exploration (see Table <ref type="table" target="#tab_4">2</ref>). Overall, although SCDL is not optimal on precision or recall, it achieves the best in F1 score, which indicates that our method can balance well when taking two kinds of annotation noise into account and exploring full training set. Based on these ablations, we observe that: (1) Token selection strategy with the consistency and high confidence predictions indeed promote the overall performance (F1 score) by improving the precision and marginally lowering the recall. The recall value doesn't decrease sharply in our framework because of the unbiased predictions generated by teacher model and (2) When we keep only one teacher-student network (i.e., w/o θ t 2 , θ s 2 ), both recall and F1 decrease visibly, which validates the effectiveness of collaborative denoising learning since more wrongly labeled tokens (e.g., false negative tokens) can be explored via the peer dynamic update of noisy labels. (3) Meanwhile, removing two teacher models (i.e., w/o θ t 1 , θ t 2 ) leads to the decline on both precision and recall. Because this simplification impairs the devised teacher-student network. It uses the predictions of each student to support the token selection strategies and the mutual update of noisy labels, which loses the stable optimization ability of EMA and leads to unreliable token selection. (4) Learning from noisy annotations benefits from soft labels since they contain the uncertainty of predicted results and are more tolerant to the noise compared to the hard ones. Robustness to Different Noise Ratio. To study the robustness of the proposed method in different noise ratio, we randomly replace k% entity mention labels in the corpus with other entity types or non-entity to construct different proportions of label noise and report the test F1 score on CoNLL03 in Figure <ref type="figure">4</ref>. The pre-trained language models (e.g., RoBERTa) are robust to low level noise (less than Thai poll shows military wants PM Banharn out .  out any assumption, but also make full use of the whole training set. High F1 score in Table <ref type="table" target="#tab_2">1</ref> and the effectiveness of noisy label refinery in Table <ref type="table" target="#tab_6">3</ref> have proved the feasibility of SCDL quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Golden Labels</head><formula xml:id="formula_14">O O O B-PER I-PER O O ⋯ O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-MISC O O O O O B-PER O O Initial Noisy Labels O O O O O O O ⋯ O O B-PER O O O O O O O O O O O O O O Teacher-Student Network 1 Pseudo Labels O O O B-PER I-PER O O ⋯ O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O O O O O B-PER O O Reliable Labels O O O B-PER I-PER O O ⋯ O B-ORG I-ORG I-ORG I-ORG I-ORG O O # O O O O O B-PER O O Teacher-Student Network 2 Pseudo Labels O O O B-PER I-PER O O ⋯ O B-PER I-PER O O O O O B-MISC O O O O O B-PER O O Reliable Labels O O O B-PER I-PER O O ⋯ O # # # # # O O B-MISC O O O O O B-PER O O</formula><p>For better understanding intuitively, we give two samples from CoNLL03 after two periodic updates to show the denoising ability of SCDL in Table <ref type="table" target="#tab_5">4</ref>.</p><p>For case 1 with two kinds of label noise, the person name "Abyss DeJesus" and organization name "St. Christopher Children 's Hospital" are not correctly annotated by DS-NER. After denoising, "Abyss DeJesus" is corrected and transformed into a useful instance. Though the hospital name is still not corrected in the teacher-student network 2, but network 2 selects reliable annotations successfully for training student. It shows that SCDL can not only exploit reliable instances but also explore unreliable ones. Similar situations also occur in case 2, while the network 2 has better capability which demonstrates the validity of co-training paradigm.</p><p>Efficiency Analysis. In training stage, with the same batch size, the serial efficiency of our method is about 1.5 batches per second on single GPU Tesla T4, other baselines like BOND is 2.6, Co-teaching+ is 1.8, JoCoR is 1.9. The memory usage of our method is equivalent to Co-training models (e.g., Co-teaching+). Although there are two student and two teacher models in our method, only two students need back-propagation which occupies the main computational overhead (time and memory usage), while two teachers updated with EMA only need forward-propagation which occupies less computational overhead. It's worth not-ing that the two teacher-student networks in our framework can be trained in parallel, which will further accelerate the training. What's more, compared with other baselines, the test efficiency of our method is the same because we only use one model for predicting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper proposes SCDL to handle two kinds of label noise in DS-NER without any assumption. With devised teacher-student network and co-training paradigm, SCDL can not only exploit more reliable annotations to avoid the negative effect of noisy labels but also explore more useful information from the mislabeled data. Experimental results confirm its effectiveness and robustness in dealing with the label noise. For future work, data augmentation is worth exploring in our framework.</p><p>Besides, SCDL can also be adapted to other NLP denoising tasks, e.g., classification and matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of SCDL with two procedures performed iteratively. (1) Each teacher-student network contributes to an inner loop (i.e., self denoising): [y] the teacher first generates pseudo labels to [z] select tokens along with noisy labels, then [{] the student is optimized based on selected tokens, and finally [|] the teacher is updated by the student. (2) The interplay between two teacher-student networks is an outer loop (i.e., collaborative denoising): the pseudo labels are applied to update the noisy labels of the peer network periodically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Training Procedure of SCDL Input: Training corpus D = {(Xi, Yi)} M i=1 with noisy labels Parameter: Two network parameters θt 1 , θs 1 , θt 2 , and θs 2 Output: The best model 1: Pre-training two models θ1, θ2 with D. Pre-Training. 2: θt 1 ← θ1, θs 1 ← θ1, θt 2 ← θ2, θs 2 ← θ2, step ← 0. 3: Initialize noisy labels: YI ← Y, YII ← Y . 4: while not reach max training epochs do 5:Get a batch (X(b) , Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of SCDL and other baselines about F1 score vs. training iterations on CoNLL03.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Curve of SCDL. To evaluate the advantage of the proposed framework in handling noisy labels during training, we show the F1 score vs. training iterations on CoNLL03 test set in Figure 3. Compared to RoBERTa and DistilRoBERTa, the performance of SCDL and BOND remains stable as the training goes. Because of the memorization effect of networks, the F1 score of RoBERTa and DistilRoBERTa first reach a high level and then gradually decrease. Moreover, SCDL consistently achieves better performance than other baselines at almost any training stage, which again confirms the effectiveness of our denoising framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Abyss DeJesus , suffers ⋯ the St. Christopher Children 's Hospital said .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Main results on five benchmark datasets. (i) ♣ marks the model trained on the fully clean dataset. (ii) † marks the model trained on noisy dataset without label denoising. (iii) ‡ marks the prior label denoising framework. marks produced with official implementation.will promote each other to perform label denoising. It's worth noting that only the best model θ ∈ {θ t 1 , θ s 1 , θ t 2 , θ s 2 } is adopted for predicting.</figDesc><table><row><cell>47.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>reannotated the training set by distant supervision, and left the development and test set unchanged. The statistics of datasets are in Appendix A.2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on CoNLL03 dev set.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>SCDL</cell><cell cols="3">89.42 80.74 84.86</cell></row><row><cell>w/o consistency prediction</cell><cell cols="3">87.01 81.11 83.96</cell></row><row><cell>w/o high confidence prediction</cell><cell cols="3">88.14 80.94 84.38</cell></row><row><cell>w/o θ t2 , θ s2 (co-training paradigm)</cell><cell cols="3">88.45 78.32 83.08</cell></row><row><cell cols="4">w/o θ t1 , θ t2 (teacher-student network) 87.90 77.22 82.22</cell></row><row><cell>w/o soft labels</cell><cell cols="3">89.86 79.12 84.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Case studies. Wrong labels are marked in red and # means the masked token (i.e., not selected).Case Study. Different from most prior denoising studies on DS-NER, our proposed framework SCDL can not only handle two kinds of label noise (i.e., inaccurate and incomplete annotations) with-Supervision 82.38 62.33 70.97 46.71 31.64 37.73 BOND-Denoising 80.42 76.46 78.39 53.76 34.82 42.27 SCDL-Denoising 87.42 75.85 81.22 54.86 47.33 50.82</figDesc><table><row><cell>6&amp;'/2XUV %21'</cell><cell>P</cell><cell>CoNLL03 R</cell><cell>F1</cell><cell>P</cell><cell>Twitter R</cell><cell>F1</cell></row><row><cell>5R%(57D 'LVWLO5R%(57D</cell><cell>Distant-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1RLVH5DWLRk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 4: F1 on CoNLL03 with different noise ratio.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20%) due to their strong expressive power. When</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the noise ratio is between 30% and 80%, SCDL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>is more robust and exhibits satisfactory denoising</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ability, since the training data still has reasonable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>entity type knowledge and SCDL can learn from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>it to refine noisy labels. However, both SCDL and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BOND degenerate to the basic model in the hard-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>est case (more than 80%) which may not exist in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>reality and needs further studies in the future.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Effectiveness of Noisy Label Refinery. As the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>noisy labels are updated dynamically during train-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing to explore the full dataset, we compare the F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>score before and after denoising on training set, as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shown in Table 3. In detail, SCDL refines noisy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>labels on CoNLL03 and Twitter training set, from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70.97 to 81.22, 37.73 to 50.82 respectively, which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>surpasses BOND. The reason may be that BOND</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mainly depends on self-training which suffers from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>confirmation bias, while SCDL can bypass this is-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sue by the devised teacher-student network and co-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>training paradigm and then improves both precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and recall significantly. Overall, the comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>before and after denoising demonstrates that SCDL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>indeed refines the training noisy labels to a certain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>extent, leading to the better use of the mislabeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data and outstanding performance on test.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of denoising ability of SCDL and BOND on training set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">https://huggingface.co/transformers/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://huggingface.co/transformers/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their insightful comments and constructive suggestions. This research is supported by the National Key Research and Development Program of China (grant No.2016YFB0801003) and the Strategic Priority Research Program of Chinese Academy of Sciences (grant No.XDC02040400).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of EMA Update</head><p>In this appendix, we give detailed derivation of reorganizing exponential moving average (EMA) as the form of backpropagation. The student θ s optimized via back-propagation in the i-th iteration is shown in Equation <ref type="formula">9</ref>, and Equation 10 represents the update process of the teacher θ t with EMA.</p><p>Based on Equation <ref type="formula">9</ref>and Equation 10, the teacher θ t in the i-th iteration can be represented as follows:</p><p>Therefore,</p><p>As we tend to derive the form of back-propagation as follows:</p><p>In the end, we get the back-propagation formula of EMA based on Equation 13 and 14, denoted as:</p><p>where γ is the learning rate and (1 − α) is a small number because α is generally assigned a value close to 1 (e.g., 0.995). Therefore, the optimization of EMA is equivalent to multiplying a small coefficient on the weighted sum of student's past gradients. With this conservative and ensemble property, the application of EMA can contribute to a more reliable and robust model. We adopt EMA in SCDL based on the following reasons: (1) The teacher model updated with EMA can quantify the fluctuation of label noise and contributes to consistency predictions. (2) As we justify above, EMA contributes to unbiased predictions with the conservative and ensemble property. (3) EMA doesn't need back-propagation (BP), which reduces the computational overhead, because BP needs to build the computation graph to compute the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Statistics of Datasets</head><p>The detailed statistics of five publicly available NER datasets are shown in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyper-parameter and Baseline Settings</head><p>Detailed hyper-parameter settings for each dataset are shown in Table <ref type="table">6</ref>. Specifically, we firstly tune the partial hyper-parameters with Grid-Search for  Confidence Threshold δ 0.9 0.9 0.9 0.9 0.9 The number of steps for the scheduler warmup is chosen from {100, 200, 500}. Then we tune EMA α from {0.9, 0.99, 0.995, 0.998} for teacher models (i.e., θ t 1 and θ t 2 ). Finally, we tune update cycle range from 100 to 8000 according to the size of dataset. The confidence threshold is set to 0.9. The rest parameters are default in huggingface Transformers 4 . For fair comparison, NegSampling and BOND adopt RoBERTa as the basic model. Co-teaching+ and JoCoR adopt RoBERTa, DistilRoBERTa as the basic models. For NegSampling, we run the officially released code using suggested hyperparameters in the original paper. For Co-teaching+ and JoCoR, noise rate τ is calculated by distantly supervised and original training set.</p><p>We conduct the experiments on NVIDIA Tesla T4 GPU. It is worth noting that only the best model θ ∈ {θ t 1 , θ s 1 , θ t 2 , θ s 2 } is adopted for predicting in our SCDL framework. Therefore, the complexity of our model is not increased during the test stage.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">2017. 2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition in Wikipedia</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Balasuriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources Web)</title>
				<meeting>the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources Web)<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolving culture vs local minima</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Growing Adaptive Machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-06-14">2009. 2009. June 14-18, 2009</date>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-resource name tagging learned with weakly labeled data</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimedia lab @ ACL WNUT NER shared task: Named entity recognition for Twitter microposts using distributed word representations</title>
		<author>
			<persName><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptist</forename><surname>Vandersmissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Van De Walle</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4322</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy User-generated Text</title>
				<meeting>the Workshop on Noisy User-generated Text<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Ávila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">O2u-net: A simple noisy label detection approach for deep neural networks</title>
		<author>
			<persName><forename type="first">Jinchi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongfei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00342</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better modeling of incomplete annotations for named entity recognition</title>
		<author>
			<persName><forename type="middle">Zhanming</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1079</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
	<note>IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Email classification with co-training</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research (CASCON 2011)</title>
				<meeting>the 2011 Conference of the Center for Advanced Studies on Collaborative Research (CASCON 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="OpenRe-view.net" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Empirical analysis of unlabeled entity problem in named entity recognition</title>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BOND: bert-assisted open-domain named entity recognition with distant supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siawpeng</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403149</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="1054" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural relation extraction with multi-lingual attention</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Germany. Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Named entity recognition with partially annotated training data</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1060</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="645" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing the effectiveness and applicability of co-training</title>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rayid</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reinforcement-based denoising of distantly supervised NER with partial annotation</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>Lønning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>Øvrelid</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</title>
				<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distantly supervised named entity recognition using positive-unlabeled learning</title>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1231</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2409" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009. CoNLL-2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Knowledge and Data Engineering</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="1825" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="2054" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
				<imprint>
			<date type="published" when="2002">2002. 2002. CoNLL-2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining self-training and selfsupervised learning for unsupervised disfluency detection</title>
		<author>
			<persName><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1813" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01374</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020</date>
			<biblScope unit="page" from="13723" to="13732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ontonotes release 5.0 ldc2013t19</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia, PA 23</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01070</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distantly supervised NER with partial annotation learning and reinforcement learning</title>
		<author>
			<persName><forename type="first">Yaosheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2159" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
