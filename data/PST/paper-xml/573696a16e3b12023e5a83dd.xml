<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weighted Multi-view Clustering with Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-12-23">23 December 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-Meng</forename><surname>Xu</surname></persName>
							<email>yumengxu@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
							<email>changdongwang@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mobile Information Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Zhuhai</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weighted Multi-view Clustering with Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-12-23">23 December 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">4561C6EEA2AE3997196AF84D253CCC09</idno>
					<idno type="DOI">10.1016/j.patcog.2015.12.007</idno>
					<note type="submission">Received 12 November 2014 Received in revised form 24 September 2015 Accepted 16 December 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data clustering Multi-view Feature selection Weighting</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, combining multiple sources or views of datasets for data clustering has been a popular practice for improving clustering accuracy. As different views are different representations of the same set of instances, we can simultaneously use information from multiple views to improve the clustering results generated by the limited information from a single view. Previous studies mainly focus on the relationships between distinct data views, which would get some improvement over the single-view clustering. However, in the case of high-dimensional data, where each view of data is of high dimensionality, feature selection is also a necessity for further improving the clustering results. To overcome this problem, this paper proposes a novel algorithm termed Weighted Multi-view Clustering with Feature Selection (WMCFS) that can simultaneously perform multi-view data clustering and feature selection. Two weighting schemes are designed that respectively weight the views of data points and feature representations in each view, such that the best view and the most representative feature space in each view can be selected for clustering. Experimental results conducted on real-world datasets have validated the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is one of the most important methods to explore the underlying (cluster) structure of data <ref type="bibr" target="#b0">[1]</ref>. The basic idea is to partition a set of data objects according to some criterion such that similar objects can be grouped into the same cluster, and dissimilar objects are separated into different clusters. To achieve this goal, we usually conduct clustering by maximizing the intracluster similarity and the inter-cluster dissimilarity. After several decades' development, a number of clustering algorithms have been developed <ref type="bibr" target="#b0">[1]</ref>, such as k-means clustering <ref type="bibr" target="#b1">[2]</ref>, spectral clustering <ref type="bibr" target="#b2">[3]</ref>, kernel-based clustering <ref type="bibr" target="#b3">[4]</ref>, graph-based clustering <ref type="bibr" target="#b4">[5]</ref> and hierarchical clustering <ref type="bibr" target="#b5">[6]</ref>.</p><p>With the development of hardware technology, a huge amount of multi-view data with various representations have been generated in real-world applications <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. For example, in web clustering, different types of data, such as images, videos, hyperlinks and texts, can be taken into consideration as they are different views of web pages (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>). In multi-view data, different views are different representations of the same set of instances. It is a significant research challenge to combine together multiple views or sources of the same set of instances to get a better clustering performance. The existing clustering algorithms designed for single-source data cannot be applied directly to the data consisting of multiple views or in various representations as they often vary greatly from traditional single-source data. Data in different views or sources are always not comparable to each other due to their dimensions and semantic representations are always different.</p><p>In addition, some views of data may be of high dimensionality which leads to high computational complexity and possibly low clustering accuracy. For example, when it comes to biomedicine, we can get different types of information for a patient, including magnetic resonance images, cerebrospinal fluid test data, blood test data, protein expression data, and genetic data, each of which is taken as a distinct view of patient data. However, some view of data may be of high dimensionality which would lead to a large amount of calculation. For some specific views, only a portion of features are needed for improving the clustering results. In other words, feature selection is a way which can both simplify the calculation and help to get an accurate data model in data clustering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In order to solve this problem, we propose a novel algorithm, termed Weighted Multi-view Clustering with Feature Selection (WMCFS), which can simultaneously perform multi-view data clustering and feature selection. A global objective function is proposed, which takes into consideration both of the multi-view learning and feature selection in the process of data clustering. In the global objective function, two weighting schemes are designed Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr that respectively weight the views of data points and feature representations in each view, such that the best view and the most representative feature space in each view can be selected for clustering. To solve the objective function, we design an EM (Expectation Maximization)-like iteration, which can converge to the acceptable clustering results. Experimental results conducted on real-world datasets have validated the effectiveness of the proposed method.</p><p>The rest of the paper is organized as follows. Section 2 briefly overviews the previous work on multi-view data clustering. The proposed WMCFS algorithm and its foundations are described in detail in Section 3. To demonstrate the performance of our algorithms, we have conducted extensive experiments, the experimental results of which are reported in Section 4. The conclusion is drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>For clustering multi-view or multi-source datasets, some algorithms have been proposed recently which take different factors into consideration, e.g. the differences and relationships between data from various views. Most of the earlier methods extend the traditional single-source clustering algorithms to the multi-view situation by simply minimizing the disagreement between different views, i.e., by minimizing the difference of the clustering results generated from different views. Two early works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> developed two-view algorithms by combining EM, kmeans and spectral clustering algorithms simultaneously. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, Kumar et al. used the spectral embedding from one view to conduct clustering of the other views which enforces the clustering results in different views to agree with each other. Wang et al. designed a multi-view spectral clustering, which relies on Pareto optimization to find the best common cuts across all views <ref type="bibr" target="#b20">[21]</ref>. However, the above methods only focus on the relationships between various views and ignore the characteristics of distinct views in data. Tzortzis and Likas <ref type="bibr" target="#b21">[22]</ref> proposed an multi-view kernel k-means (MVKKM) algorithm which assigns a weight for each view according to the view's contribution to the clustering result and then combines the kernels derived from the weighted views together. However, it is based on the inner product kernels for all views, and has no explicit mechanism for feature selection.</p><p>To address the above issues, there are some other efforts that investigate feature selection in multi-view data clustering. A framework was proposed in <ref type="bibr" target="#b13">[14]</ref>, which constructs models respectively for the multi-source learning and feature selection. However, this work is designed for supervised learning and cannot deal with the unsupervised situation. In particular, a model is first trained based on the supervision information, during which the relatively more important features for each cluster can be selected. In this way, feature selection can be accomplished under the criterion to enforce the correct class labels and the important features discovered by this process will be assigned with high weights. However, when it comes to the unsupervised situation, where the labeled samples are not available, this method is no longer applicable, since the importance of features cannot be evaluated due to the lack of the ground-truth labeling. Similarly, Zhao et al. <ref type="bibr" target="#b22">[23]</ref> proposed an algorithm combining LDA with cotraining, i.e., exploiting labels learned in one view to learn discriminative features in another view. In <ref type="bibr" target="#b23">[24]</ref>, Wang et al. developed an algorithm to do feature learning for multi-view clustering. However, this method cannot deal with the noisy data in each view. When some of the views are noisy, the result might become unsatisfactory. Chen et al. <ref type="bibr" target="#b24">[25]</ref> proposed an automated two-level variable weighting clustering algorithm for multi-view data termed TW-k-Means, which can simultaneously compute weights for views and individual variables. However, the same weighting scheme is used for both view weighting and feature selection, which is not able to explore more possibilities. Cai et al. <ref type="bibr" target="#b25">[26]</ref> also focused on multi-view clustering based on k-means which would be applicable for multi-view data but did not really do feature selection so that their clustering model will degenerate in the case of high dimensionality.</p><p>In this paper, inspired by the multi-view kernel k-means algorithm proposed by Tzortzis and Likas <ref type="bibr" target="#b21">[22]</ref>, we design an algorithm termed Weighted Multi-view Clustering with Feature Selection (WMCFS), that can simultaneously perform multi-view data clustering and feature selection. Instead of integrating a feature selection mechanism into multi-view kernel k-means, we use a simple yet effective formula based on the original k-means algorithm. This is because multi-view kernel k-means relies on a kernel mapping in which the kernel selection itself is a challenging issue in the unsupervised learning case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weighted Multi-view Clustering with Feature Selection</head><p>To make this paper clear, Table <ref type="table">1</ref> summarizes the symbols used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>Consider a dataset consisting of N instances represented by V views. Let X ¼ fx 1  1 ; x 1 2 ; …; x V N g denote the dataset, where x v i is the i-th instance from the v-th view. In this way, the multi-view data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Symbols used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Meaning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>The whole dataset X v</p><p>The v-th view dataset x v i The i-th instance of the v-th view dataset and Parameter controlling the sparsity of feature weight vectors can be represented as follows (shown in Fig. <ref type="figure" target="#fig_2">2</ref>):</p><formula xml:id="formula_0">x v i A R d v m v</formula><formula xml:id="formula_1">X ¼ fX 1 ; X 2 ; …; X V g;<label>ð1Þ</label></formula><formula xml:id="formula_2">X 1 ¼ fx 1 1 ; x 1 2 ; …; x 1 N g;<label>ð2Þ</label></formula><formula xml:id="formula_3">⋮ X V ¼ fx V 1 ; x V 2 ; …; x V N g;<label>ð3Þ</label></formula><p>where X v denotes the set of instances from the v-th view in X , and</p><formula xml:id="formula_4">x v i A R d v</formula><p>is the i-th instance in X v with d v being the dimension of the v-th view.</p><p>The goal of multi-view clustering is to cluster the N instances into M clusters according to their semantic similarity between each other in all views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global objective function</head><p>In what follows, we will describe in detail the proposed global objective function, which takes into consideration both of the multi-view learning and the feature selection in the process of data clustering. Both of the multi-view learning and the feature selection are realized by weighting. To this end, two weighting schemes are designed, which respectively weight the views of data points and feature representation in each view, such that the best view and the most representative feature subspace in each view can be selected for clustering. For clarity, we will describe the objective function from the viewpoint of multi-view k-means, followed by view weighting and feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Multi-view k-means</head><p>We will describe our objective function from the viewpoint of k-means for clarity. The goal of k-means is to choose M cluster centers such that the sum of the squared distance of each instance to the corresponding cluster center is minimized. Let X ¼ fx 1 ; x 2 ; …; x N g denote the dataset, the objective function of k-means to be minimized is as follows:</p><formula xml:id="formula_5">ε H ¼ X N i ¼ 1 X M k ¼ 1 δ ik J ðx i À m k Þ J 2 ;<label>ð4Þ</label></formula><p>where m k is the center of cluster k and δ ik denotes the cluster assignment of instances such that δ ik equals 1 when the i-th instance is assigned to cluster k and 0 otherwise. Obviously, each instance must be assigned to one and only one cluster, i.e.,</p><formula xml:id="formula_6">P M k ¼ 1 δ ik ¼ 1; 8 i; δ ik A f0; 1g.</formula><p>In the case of multi-view data, the objective function becomes</p><formula xml:id="formula_7">ε H ¼ X V v ¼ 1 X N i ¼ 1 X M k ¼ 1 δ ik J ðx v i À m v k Þ J 2 ;<label>ð5Þ</label></formula><p>where m v k is the center of cluster k in view v. In the multi-view kmeans objective function, each instance is assigned to the same cluster in all views, but the cluster centers of the same cluster vary in different views. This is because the data representations in distinct views are different, which leads to different cluster center representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">View weighting and feature selection</head><p>To simultaneously perform multi-view learning and feature selection in the process of data clustering, two weighting schemes are designed that respectively weight the views of data points and feature representation in each view.</p><p>The first weighting scheme is to weight the data of each view.</p><p>Let ω v denote the weight for data from the v-th view, satisfying</p><formula xml:id="formula_8">P V v ¼ 1 ω v ¼ 1; ω v Z0.</formula><p>Therefore, ω is the view weight vector. The second weighting scheme is to weight the features of each view.</p><p>Let τ v denote the feature weight vector of length d v , with each entry τ l v representing the weight for the l-th feature in view v,</p><formula xml:id="formula_9">satisfying P d v l ¼ 1 τ v l ¼ 1; τ v l Z 0.</formula><p>Based on the above notation, we get the sum of the weighted squared distance with regularization term as follows:</p><formula xml:id="formula_10">ε H ¼ X V v ¼ 1 ðω v Þ p X N i ¼ 1 X M k ¼ 1 δ ik J diagðτ v Þðx v i À m v k Þ J 2 þ β X V v ¼ 1 J τ v J 2 ;<label>ð6Þ</label></formula><p>where m v k is the center of cluster k in view v:</p><formula xml:id="formula_11">m v k ¼ P N i ¼ 1 δ ik x v i P N i ¼ 1 δ ik ;<label>ð7Þ</label></formula><p>and diagðτ v Þ is a diagonal matrix with the elements of the vector τ v on the diagonal.</p><p>The last component of this objective function β</p><formula xml:id="formula_12">P V v ¼ 1 J τ v J 2</formula><p>is used to control the sparsity of the feature weight vectors τ v ; 8 v so as to avoid the situation that only a few features are selected in getting a very small but meaningless objective value. The parameters p and β are the exponential and balancing parameters, which are selected according to the priori knowledge of data so as to help controlling the sparsity of the view weight vector ω and the feature weight vectors τ v ; 8 v ¼ 1; …; V respectively. Experimental analysis shows that there exists a relatively wide range of values that can generate satisfactory clustering results.</p><p>The major difference between the proposed objective function ( <ref type="formula" target="#formula_10">6</ref>) and the one proposed by Tzortzis and Likas <ref type="bibr" target="#b21">[22]</ref> is that, the objective in <ref type="bibr" target="#b21">[22]</ref> only considers view weighting and there is no strategy for feature learning, as shown below:</p><formula xml:id="formula_13">ε H ¼ X V v ¼ 1 ðω v Þ p X N i ¼ 1 X M k ¼ 1 δ ik J ϕ v ðx v i ÞÀm v k J 2 ;<label>ð8Þ</label></formula><p>where m v k is the center of cluster k in view v:</p><formula xml:id="formula_14">m v k ¼ P N i ¼ 1 δ ik ϕ v ðx v i Þ P N i ¼ 1 δ ik :<label>ð9Þ</label></formula><p>The feature in each view is pre-specified by a kernel mapping ϕ as input to their algorithm. However, in our method, not only view weighting but also feature weighting are automatically learned.</p><p>Although it is possible to integrate the weighting-based feature selection mechanism into (8), the kernel selection itself is a challenging issue in the unsupervised learning case. Therefore, a simple yet effective formula is used in our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Final objective function</head><p>The goal of the Weighted Multi-view Clustering with Feature Selection is to find the optimal cluster assignment, view weighting and feature weighting simultaneously such that the objective function is minimized. That is min</p><formula xml:id="formula_15">fδ ik g M k ¼ 1 ;fωvg V v ¼ 1 ;fτ v g V v ¼ 1 ε H ; subject to X M k ¼ 1 δ ik ¼ 1; 8 i; δ ik A f0; 1g; X V v ¼ 1 ω v ¼ 1; ω v Z 0; X d v l ¼ 1 τ v l ¼ 1; τ v l Z 0; 8 v:<label>ð10Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>To search for the optimal cluster assignment, view weighting and feature weighting, we design an EM-like iteration, which contains three iteration stages. In each stage, one of the three variables is updated, with the other two variables being fixed. We will describe the three stages one by one in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Updating the cluster assignment</head><p>By fixing the view weight vector and feature weight vectors, we can update the cluster assignment δ ik by performing k-means. That is, in the objective function <ref type="bibr" target="#b5">(6)</ref>, fixing ω and τ v ; 8 v results in the objective function the same as k-means.</p><p>In the first round of iteration, the view weight vector ω and feature weight vectors τ v ; 8 v ¼ 1; …; V are respectively initialized evenly as</p><formula xml:id="formula_16">ω v ¼ 1 V ; 8 v and τ v l ¼ 1 d v ; 8 l.</formula><p>Then we can apply k-means by minimizing ε H in <ref type="bibr" target="#b5">(6)</ref>.</p><p>For convenience, some notations are introduced here. Let</p><formula xml:id="formula_17">G ¼ X V v ¼ 1 ω p v G v ;<label>ð11Þ</label></formula><p>where</p><formula xml:id="formula_18">G v ði; jÞ ¼ ðx v i Þ T diagðτ v Þdiagðτ v Þx v j ;<label>ð12Þ</label></formula><p>which can be written as</p><formula xml:id="formula_19">G v ði; jÞ ¼ ðy v i Þ T y v j ;<label>ð13Þ</label></formula><p>where y v i is the projection of instance x v i after feature selection (i.e., weighted by τ v ).</p><p>By substituting ( <ref type="formula" target="#formula_11">7</ref>) into (6), we can get</p><formula xml:id="formula_20">ε H ¼ X V v ¼ 1 ω p v X N i ¼ 1 X M k ¼ 1 δ ik J diagðτ v Þ x v i À P N i ¼ 1 δ ik x v i P N i ¼ 1 δ ik ! J 2 þ β X V v ¼ 1 J τ v J 2 ; ) ε H ¼ trðGÞÀtrðΔ T GΔÞþβ X V v ¼ 1 J τ v J 2 ;<label>ð14Þ</label></formula><p>where Δ is</p><formula xml:id="formula_21">Δ ik ¼ δ ik ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi P N j ¼ 1 δ jk q ;<label>ð15Þ</label></formula><p>and the last regularization term is a constant given fixed</p><formula xml:id="formula_22">τ v ; 8 v ¼ 1; …; V.</formula><p>In this way, we can take full advantage of the information from various views to improve the clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Updating the view weighting</head><p>In this stage of iteration, by fixing the cluster assignment δ ik and the feature weight vectors τ v ; 8 v, we can update the view weight vector ω as follows.</p><p>First, we get the Lagrangian formula of (10) w.r.t. ω v as follows:</p><formula xml:id="formula_23">Lðω; λÞ ¼ ε H ðωÞþλ X V v ¼ 1 ω v À1 ! :<label>ð16Þ</label></formula><p>Taking derivative of both sides w.r.t. ω v gives ∂Lðω; λÞ</p><formula xml:id="formula_24">∂ω v ¼ ∂ε H ðωÞ ∂ω v þ λ:<label>ð17Þ</label></formula><p>Setting the derivation to zero, we can get</p><formula xml:id="formula_25">pω ðp À 1Þ v D v þλ ¼ 0 ) ω v ¼ À λ pD v 1=ðp À 1Þ ;<label>ð18Þ</label></formula><p>where</p><formula xml:id="formula_26">D v ¼ X N i ¼ 1 X M k ¼ 1 δ ik J diagðτ v Þðx v i Àm v k Þ J 2 ;<label>ð19Þ</label></formula><p>when p 4 1. Furthermore, we can get the following formula by substituting (18) into the constraint</p><formula xml:id="formula_27">P V v 0 ¼ 1 ω v 0 ¼ 1: X V v 0 ¼ 1 À λ pD v 0 1=ðp À 1Þ ¼ 1 ) ðÀλÞ 1=ðp À 1Þ ¼ 1 P V v 0 ¼ 1 1 pD v 0 1=ðp À 1Þ :<label>ð20Þ</label></formula><p>Based on this, we can get the formula to update ω v by substituting ( <ref type="formula" target="#formula_27">20</ref>) into (18) as follows:</p><formula xml:id="formula_28">ω v ¼ 1 P V v 0 ¼ 1 D v D v 0 1=ðp À 1Þ ; p 4 1:<label>ð21Þ</label></formula><p>When p¼ 1, the weights are less than 1 according to</p><formula xml:id="formula_29">P V v ¼ 1 ω v ¼ 1,</formula><p>and we can get</p><formula xml:id="formula_30">D v n r P V v 0 ¼ 1 ω v 0 D v 0</formula><p>, where v n ¼ arg min v 0 D v 0 . Therefore, we can get the following formula for ω v if p ¼1:</p><formula xml:id="formula_31">ω v ¼ 1; v ¼ arg min v 0 D v 0 0; otherwise ( p ¼ 1:<label>ð22Þ</label></formula><p>In the above formulas, p is a exponential parameter that can help adjusting the sparsity of the view weight vector ω. If we get some priori knowledge of the input data, we can set p to a better value which can improve the result. That is, according to <ref type="bibr" target="#b20">(21)</ref>, if most of the views are useful, larger p is more suitable. Nevertheless, our experimental results show that there exists relatively a wide range of p values that can generate satisfactory clustering results. The underlying rationale of the above updating formula is that the instances closer to the cluster centers are considered to be more useful. The more useful one view is, the larger weight this view will be assigned to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Updating the feature weighting</head><p>In this stage of iteration, by fixing the cluster assignment δ ik and the view weight vector ω, we can update the feature weight vectors τ v ; 8 v as follows.</p><p>Similar to the updating of the view weighting, the Lagrangian formula of (10) w.r.t. τ v can be obtained as follows:</p><formula xml:id="formula_32">Lðτ; λÞ ¼ ε H ðτÞþλ X d v l ¼ 1 τ v l À 1 ! ; 8 v:<label>ð23Þ</label></formula><p>Taking derivative of both sides w.r.t. τ v l gives ∂Lðτ; λÞ</p><formula xml:id="formula_33">∂τ v l ¼ ∂ε H ðτÞ ∂τ v l þ λ:<label>ð24Þ</label></formula><p>To obtain a simpler formula of the first term on the righthand side, we rewrite the formula of ε H as follows:</p><formula xml:id="formula_34">ε H ðτÞ ¼ X V v ¼ 1 ðω v Þ p X N i ¼ 1 X M k ¼ 1 δ ik X d v l ¼ 1 ðx v i À m v k Þ 2 l ðτ v l Þ 2 þ β X V v ¼ 1 X d v l ¼ 1 ðτ v l Þ 2 ;<label>ð25Þ</label></formula><formula xml:id="formula_35">where ðx v i À m v k Þ l means the l-th element of ðx v i Àm v k Þ.</formula><p>Based on the above formula, we can get</p><formula xml:id="formula_36">∂ε H ðτÞ ∂τ v l ¼ 2ðω v Þ p X N i ¼ 1 X M k ¼ 1 δ ik ðx v i À m v k Þ 2 l τ v l þ 2βτ v l :<label>ð26Þ</label></formula><p>By substituting ( <ref type="formula" target="#formula_36">26</ref>) into <ref type="bibr" target="#b23">(24)</ref> and setting <ref type="bibr" target="#b23">(24)</ref> to 0, we can get</p><formula xml:id="formula_37">τ v l ¼ À λ 2β þ2ðω v Þ p P N i ¼ 1 P M k ¼ 1 δ ik ðx v i À m v k Þ 2 l :<label>ð27Þ</label></formula><p>Substituting ( <ref type="formula" target="#formula_37">27</ref>) into the constraint</p><formula xml:id="formula_38">P d v l 0 ¼ 1 τ v l 0 ¼ 1, we can get Àλ ¼ 1 P d v l 0 ¼ 1 1 2β þ 2ðω v Þ p P N i ¼ 1 P M k ¼ 1 δ ik ðx v i À m v k Þ 2 l 0 :<label>ð28Þ</label></formula><p>At last, we can get the formula for updating the feature weight vectors τ v ; 8 v by substituting ( <ref type="formula" target="#formula_38">28</ref>) into ( <ref type="formula" target="#formula_37">27</ref>) as follows:</p><formula xml:id="formula_39">τ v l ¼ 1 P d v l 0 ¼ 1 1 2β þ 2ðω v Þ p P N i ¼ 1 P M k ¼ 1 δ ik ðx v i À m v k Þ 2 l 0 2β þ 2ðω v Þ p P N i ¼ 1 P M k ¼ 1 δ ik ðx v i À m v k Þ 2 l ; 8 l;<label>ð29Þ</label></formula><p>which can be further simplified as</p><formula xml:id="formula_40">τ v l ¼ 1 P d v l 0 ¼ 1 B v l B v l 0 ; 8 l;<label>ð30Þ</label></formula><p>where</p><formula xml:id="formula_41">B v l ¼ β þðω v Þ p X N i ¼ 1 X M k ¼ 1 δ ik ðx v i À m v k Þ 2 l :<label>ð31Þ</label></formula><p>3.4. The complete algorithm Algorithm 1. Weighted Multi-view Clustering with Feature Selection.</p><p>1: Input: X ¼ fx<ref type="foot" target="#foot_3">1</ref> 1 ; x 1 2 ; …; x V N g; p; β; M; t max .<ref type="foot" target="#foot_4">2</ref>: Output: δ ik : the cluster assignment; ω v : the view weighting; τ l v : the feature weighting.</p><formula xml:id="formula_42">3: Initialize ω v ¼ 1 V and τ v l ¼ 1 d v ; 8 l ¼ 1; 2; …; d v ; 8 v ¼ 1; 2; …; V. t ¼0. 4: Repeat 5:</formula><p>Update the cluster assignment δ ik by performing kmeans w.r.t. <ref type="bibr" target="#b5">(6)</ref>. 6: Update the view weight vector ω via <ref type="bibr" target="#b20">(21)</ref> or <ref type="bibr" target="#b21">(22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update the feature weight vectors τ v ; 8 v via <ref type="bibr" target="#b29">(30)</ref>. 8: t ¼ t þ 1. 9: Until Convergence or t 4 t max For clarity, Algorithm 1 summarizes the proposed Weighted Multi-view Clustering with Feature Selection (WMCFS) algorithm.</p><p>To prove the convergence of Algorithm 1, we set the changes of weighted sum of intra-class distances between the t-th and the ðt þ 1Þ-th iterations as CH(t) which can be computed as follows:</p><formula xml:id="formula_43">CHðtÞ ¼ ε t H À min ω ðmin τ ðmin δ ðε t H ÞÞÞ;<label>ð32Þ</label></formula><p>where ω, τ and δ are the updated parameters in the ðt þ 1Þ-th iteration and ε H t is the weighted sum of intra-class distances after the t-th iteration. Obviously, CHðtÞ Z 0 which means that the sum of intra-class distances updated in each step of iterations is strictly decreasing. Algorithm 1 converges to a local minimum.</p><p>In our experiments, the iteration stops when the number of iterations reaches the maximum number of iterations t max (we always set t max ¼ 10 in our experiments) or the iteration converges, i.e., when the sum of intra-class distances keeps stable.</p><p>Here we set a threshold (e¼0.00001) and when the gap of the sum of intra-class distances between the two consecutive iterations is less than the threshold, we stop the iteration and output the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on three real-world datasets. We first analyze the performance sensitivity to the two parameters p and β. Then, several state-of-the-art multi-view clustering methods have been performed and compared with the proposed method, which shows the significant improvement achieved by our method. For experimental purpose, we only perform the parameter analysis on two of the three datasets and then report all of the comparison results on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and experimental settings</head><p>Three real-world datasets are used in our experiments, namely Mfeat, Reuters and Corel.</p><p>The Multiple Features (abbr. Mfeat) dataset is a dataset consisting of handwritten digits (0-9) <ref type="bibr" target="#b26">[27]</ref>. There are 6 views of each instance, namely, Fourier coefficients of the character shapes (following the UCI Machine Learning Repository website 1 , we use abbreviation mfeat-fou), profile correlations (mfeat-fac), Karhunen-Love coefficients (mfeat-kar), pixel averages in 2 Â 3 windows (mfeat-pix), Zernike moments (mfeat-zer) and 6 morphological features (mfeat-mor). Here we take the first five representative views of this dataset to form a five-view dataset. The detailed information of this dataset is shown in Table <ref type="table" target="#tab_0">2</ref>.</p><p>The Reuters RCV1/RCV2 Multilingual (abbr. Reuters) dataset is a dataset consisting of machine translated documents <ref type="bibr" target="#b27">[28]</ref>. It has been widely used for evaluating the performances of multi-view learning algorithms. The dataset contains documents originally written in five different languages, namely English (EN), French (FR), German (GR), Italian (IT) and Spanish (SP). Each document, originally written in one language, is translated to the other four languages using the Portage system <ref type="bibr" target="#b28">[29]</ref>. The documents are categorized into six different topics. The dataset is summarized in Table More detail can be found on the dataset website. 2 In our experiments, we choose one language, namely English (EN), as the original language source and take the translated documents in the other four languages as the other four sources. This means that we conduct our experiments on the five-view dataset, with the views being EN, FR, GR, IT and SP respectively.</p><p>The Corel dataset is extracted from a Corel image collection <ref type="bibr" target="#b26">[27]</ref>, and we randomly get 2000 instances of 5 classes. Four sets of features are available based on the color histogram (abbr. Col-h), color histogram layout (abbr. Col-hl), color moments (abbr. Col-m), and co-occurrence texture (abbr. Coo-t). These features are treated as the 4 views of samples whose information is shown in Table <ref type="table" target="#tab_1">4</ref>.</p><p>All the experiments are conducted in MATLAB 2012a (7.14) 64bit edition on a workstation (Windows 64 bit, 8 Intel 2.00 GHz processors, 16 GB of RAM).</p><p>For clustering performance evaluation, two widely used measurements, i.e. classification rate (CR) <ref type="bibr" target="#b29">[30]</ref> and normalized mutual information (NMI) <ref type="bibr" target="#b30">[31]</ref>, are used based on the ground-truth labels of the instances. When computing the classification rate, each obtained category is firstly associated with the "ground-truth" category which accounts for the largest number of samples in the learned category. Then the classification rate (CR) are computed as the ratio of the number of correctly classified samples to the size of the dataset. That is</p><formula xml:id="formula_44">CR ¼ #Correctly classified samples #Samples in the dataset :<label>ð33Þ</label></formula><p>Given the clustering labels π of c clusters and the actual class labels θ of ĉ classes, we build a confusion matrix where entry (i,j) defines the number N ðjÞ i of data points in cluster i and class j. Then NMI can be computed from the confusion matrix <ref type="bibr" target="#b30">[31]</ref>,</p><formula xml:id="formula_45">NMI ¼ 2 P c l ¼ 1 P ĉ h ¼ 1 N ðhÞ l N log N ðhÞ l N P c i ¼ 1 N ðhÞ i P ĉ i ¼ 1 N ðiÞ l HðπÞþHðθÞ ;<label>ð34Þ</label></formula><p>where HðπÞ ¼ À</p><formula xml:id="formula_46">P c i ¼ 1 N i</formula><p>N log N i N and HðθÞ ¼ À</p><formula xml:id="formula_47">P ĉ j ¼ 1 N ðjÞ N log N ðjÞ N are</formula><p>the Shannon entropy of cluster labels π and class labels θ respectively, with N i and N ðjÞ denoting the number of data points in cluster i and class j. Obviously, a higher classification rate (CR) (also normalized mutual information (NMI)) indicates a more accurate clustering result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameter analysis</head><p>In this subsection, we demonstrate the performance of our method by using different parameters p and β. In the process of analyzing one of the two parameters, the other parameter is fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">The exponential parameter p</head><p>The exponential parameter p is used to adjust the sparsity of the view weight vector ω, which would affect the performance of our method, i.e., different p will lead to different distribution of the view weight vector ω and hence different clustering results will be generated. To this end, the effect of the parameter p is analyzed from two perspectives, namely on the distribution of the view weight vector ω and on the final clustering results, i.e., classification rate (CR). First, we analyze the effect of the parameter p on the distribution of the view weight vector ω. Fig. <ref type="figure" target="#fig_3">3</ref> shows the distribution of ω as a function of p on the Mfeat and Reuters datasets, by   ranging p from 1 to 30. Clearly, from <ref type="bibr" target="#b20">(21)</ref>, the smaller p is, the sparser the view weight vector ω will be. The results reported in Fig. <ref type="figure" target="#fig_3">3</ref> have confirmed this fact. If we know some priori knowledge about whether most of the data views are useful or not, we can select a relatively suitable p. Secondly, we analyze the effect of the parameter p on the clustering performance in terms of classification rate (CR), as shown in Fig. <ref type="figure">4</ref>. By fixing the other parameter β as 0.1 on both of the two datasets, we perform our method by using different p ranging from 5 to 30 (we do not start with p ¼1 here as we can see in Fig. <ref type="figure" target="#fig_3">3</ref> that setting p¼ 1 degenerates into single view clustering). The clustering performance in terms of CR is plotted in Fig. <ref type="figure">4</ref> (a) and (b) respectively. From the figures, we can see that, the clustering performance is relatively stable in a wide range of p. For various datasets, a proper p can be set widely ranging from 5 to 30. We can figure out that the clustering results keep stable and are always better than those generated by the compared algorithms. In particular, the best clustering results can be obtained with p ¼10 on the Mfeat dataset and p ¼5 on the Reuters dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">The balancing parameter β</head><p>The balancing parameter β is used to control the sparsity of the feature weight vectors τ v ; 8 v, which would also affect the performance of our method. To this end, the effect of the parameter β is also analyzed from two perspectives, namely on the distribution of the feature weight vectors τ v ; 8 v ¼ 1; …; V and on the final clustering results in terms of classification rate (CR).</p><p>To begin with, we first analyze the effect of parameter β on the sparsity of τ v . Obviously, if the balancing parameter β is set to 0, there is no more regularization term for controlling the distribution of the feature weight vectors τ v ; 8 v ¼ 1; …; V. In this case, in order to get a smaller objective function value during the iterations, only a small number of features will be used (with nonzero weight), as shown in Fig. <ref type="figure">5</ref>, which plots the distribution of τ v in one representative view of the Mfeat dataset. Therefore we need to set β to some proper value in order to balance the distribution of entries of τ v as shown in Figs. <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_11">7</ref>. According to the experimental analysis, we suggest to choose β from ð0; 1.</p><p>Additionally, we explore the effect of the parameter β on the clustering performance in terms of classification rate (CR). The results are reported in Fig. <ref type="figure" target="#fig_12">8</ref> Therefore we only plot the CR values as a function of β in these two ranges since it is meaningless to plot CR values when setting β in the range ½0:0005; 1 and ½0:00005; 1. Based on this analysis, we can get a wide range of β in which our method can generate very stable results. Therefore, based on the analysis in terms of both feature distribution and clustering performance, we suggest to set β ¼ 0:1 on all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison results</head><p>In this subsection, we compare the performance of the proposed method with some existing methods in terms of CR and NMI, namely traditional k-means <ref type="bibr" target="#b1">[2]</ref>, EM (Expectation Maximization) <ref type="bibr" target="#b31">[32]</ref>, MVKKM (multi-view kernel k-means) <ref type="bibr" target="#b21">[22]</ref>, Co-regspec (Co-regularized multi-view spectral clustering) <ref type="bibr" target="#b19">[20]</ref> and LLC-fs (Local Learning-Based Clustering with Feature Selection) <ref type="bibr" target="#b32">[33]</ref>.</p><p>The optimal parameters analyzed in Section 4.2 are used in our WMCFS method. The parameter settings of the other five compared algorithms are summarized below:</p><p>1. k-means: We use the traditional k-means algorithm <ref type="bibr" target="#b1">[2]</ref> as one of the compared algorithms. In our experiments, we apply the default k-means function of MATLAB to get the clustering result in each view on the three datasets. 2. EM: EM (Expectation Maximization) <ref type="bibr" target="#b31">[32]</ref> is an iterative method for finding maximum likelihood or maximum a posteriori   The mfeat-pix view Index of feature --l τ(l)</p><p>The FR view The GR view 0 5000 10000 15000 0 The SP view (MAP) estimates of parameters in statistical models. In our experiments, we apply the default EM function of MATLAB to get the clustering result in each view on the three datasets. 3. MVKKM: In the MVKKM (multi-view kernel k-means) algorithm <ref type="bibr" target="#b21">[22]</ref>, a weighted combination of kernels is learned to conduct clustering. There is one parameter p used to control the sparsity of view weight vector with the best parameter value selected from ð1; 6 as suggested in <ref type="bibr" target="#b21">[22]</ref>. In our experiments, we get the best performance with p ¼3, p ¼1.5 and p¼ 2 on the Mfeat, Reuters and Corel datasets respectively. Additionally, we stop the iteration when the gap of objective between two consecutive iterations is less than 0.00001. 4. Co-regspec: Co-regspec (Co-regularized multi-view spectral clustering) <ref type="bibr" target="#b19">[20]</ref> is a method aiming at minimizing the differences between various views. We use Gaussian kernel for constructing similarity matrix. The hyperparameter λ which trades off the spectral clustering objectives and the spectral embedding (dis)agreement term can be adjusted by the algorithm itself. We set the maximum number of iterations to 10 as suggested in the paper <ref type="bibr" target="#b19">[20]</ref>. 5. LLC-fs: LLC-fs (Local Learning-Based Clustering with Feature Selection) <ref type="bibr" target="#b32">[33]</ref> is a single view data clustering algorithm with feature selection. In our experiment, we run the LLC-fs algorithm in each view of the three datasets. The size of neighborhood k and the trade off parameter β are chosen from the prespecified candidate as reported in the paper <ref type="bibr" target="#b32">[33]</ref> and we only report the best performances with k¼30 on the Mfeat dataset, k¼70 on the Reuters dataset and k¼55 on the Corel dataset. Additionally, we stop the iteration when the gap of objective between two consecutive iterations is less than 0.0001.</p><p>Tables <ref type="table" target="#tab_3">5</ref> and<ref type="table" target="#tab_4">6</ref> report the mean and standard deviation of CR and NMI respectively generated by the six algorithms on the three testing datasets over 100 runs, where different random initializations are used in performing clustering. Apart from performing clustering on the three original whole-view datasets, we also report the clustering results on the datasets formed by some randomly selected views. For instance, on the Mfeat dataset, apart from the five-view data, we also run on the 2 two-view datasets formed by mfeat-fac&amp;mfeat-fou and mfeat-fac&amp;mfeat-zer respectively.</p><p>By comparing the results of our algorithm and other compared algorithms, we can draw the conclusion that our proposed method performs the best in the case of multi-view clustering, since we consider the weights both for various views and for various features by measuring their contributions to the clustering result. In particular, on the five-view Mfeat dataset, a classification rate as high as 0.836 can be generated, achieving a significant improvement over the existing methods; we get the best NMI value at the same time. On the five-view Reuters dataset, the proposed WMCFS method is slightly better than the second best Co-regspec method but much better than the other methods. Even on the two-view datasets, the WMCFS method performs much better than the compared methods in terms of both CR and NMI. Another important result is that, for the multi-view clustering methods, the performance achieved on the five-view datasets is much better than that achieved on the two-view datasets. This confirms the fact that adding useful views will enhance the clustering performance of the multi-view clustering methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel multi-view clustering methods, termed Weighted Multi-view Clustering with Feature Selection (WMCFS), which simultaneously performs feature selection and multi-view data clustering. A global objective function is proposed, which takes into consideration both of the multiview learning and the feature selection in the process of data clustering. In the global objective function, two weighting schemes are designed that respectively weight the views of data points and feature representation in each view, such that the best view and the most representative feature space in each view can be selected for clustering. To solve the objective function, we design an EMlike iteration, which consists of three main stages and can converge to satisfactory results. Experiments have been conducted on three real-world datasets, the results of which validate the effectiveness of the proposed method. In the future work, we plan to extend our algorithm into multi-view clustering with missing data, and with the capability of selecting the number of clusters automatically. Moreover, we will also try the exploration of the automatical way to determine p and β which would not rely on our prior knowledge any more. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multi-view data of web page.</figDesc><graphic coords="2,86.78,75.47,161.10,124.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>k</head><label></label><figDesc>The cluster center of the k-th cluster in the v-th view N Number of instances in each view M Number of clusters l v Number of features in the v-th view ε H Objective function which denotes the sum of intra-class distances ω v Weight for the v-th view τ l v Weight for the l-th feature of the v-th view δ ik Indicator variable showing whether the i-th instance belongs to the kth cluster p Exponential parameter controlling the sparsity of view weight vector β</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of multi-view data. The same color represents data from the same view and the width of each column denotes the dimension of the corresponding view. It is often the case that the dimensions would vary in different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Analysis on the exponential parameter p: distribution of the view weight vector ω as a function of p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) and (b), with fixing the other parameter p as 10 and 5 on the two datasets respectively. When setting β in the range ½0:0005; 1 and ½0:00005; 1, the proposed method always generates the same clustering results on the Mfeat and Reuters datasets, i.e. the classification rate values 0.836 and 0.930. The only ranges where fluctuating clustering results are generated on Mfeat and Reuters are ½0; 0:0005 and ½0; 0:00005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Analysis on the exponential parameter p: the clustering performance (in terms of CR) as a function of p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Analysis of the balancing parameter β on the Mfeat dataset: distribution of the feature weight vector τ v in each view with β ¼ 0:1.</figDesc><graphic coords="8,61.76,421.19,483.47,256.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Analysis of the balancing parameter β on the Reuters dataset: distribution of the feature weight vector τ v in each view with β ¼ 0:1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Analysis on the balancing parameter β: the clustering performance (in terms of CR) as a function of β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Detailed information of the Mfeat dataset.</figDesc><table><row><cell>View</cell><cell># Samples</cell><cell># Features</cell><cell># Classes</cell></row><row><cell>mfeat-fou</cell><cell>2000</cell><cell>76</cell><cell>10</cell></row><row><cell>mfeat-fac</cell><cell>2000</cell><cell>216</cell><cell>10</cell></row><row><cell>mfeat-kar</cell><cell>2000</cell><cell>64</cell><cell>10</cell></row><row><cell>mfeat-pix</cell><cell>2000</cell><cell>240</cell><cell>10</cell></row><row><cell>mfeat-zer</cell><cell>2000</cell><cell>47</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>Detailed information of the Corel dataset.</figDesc><table><row><cell>View</cell><cell># Samples</cell><cell># Features</cell><cell># Classes</cell></row><row><cell>Col-h</cell><cell>2000</cell><cell>32</cell><cell>5</cell></row><row><cell>Col-hl</cell><cell>2000</cell><cell>32</cell><cell>5</cell></row><row><cell>Col-m</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Detailed information of the Reuters Dataset.</figDesc><table><row><cell>View</cell><cell># Docs</cell><cell># Words</cell></row><row><cell>EN</cell><cell>18,758</cell><cell>21,513</cell></row><row><cell>FR</cell><cell>26,648</cell><cell>24,839</cell></row><row><cell>GR</cell><cell>29,953</cell><cell>34,279</cell></row><row><cell>IT</cell><cell>12,342</cell><cell>11,547</cell></row><row><cell>SP</cell><cell>24,039</cell><cell>11,506</cell></row><row><cell>Topic</cell><cell># Docs</cell><cell>per(%)</cell></row><row><cell>C15</cell><cell>18,816</cell><cell>16.84</cell></row><row><cell>CCAT</cell><cell>21,426</cell><cell>19.17</cell></row><row><cell>E21</cell><cell>13,701</cell><cell>12.26</cell></row><row><cell>ECAT</cell><cell>19,198</cell><cell>17.18</cell></row><row><cell>GCAT</cell><cell>19,178</cell><cell>17.16</cell></row><row><cell>M11</cell><cell>19,421</cell><cell>17.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>The mean and standard deviations of classification rate (CR) generated by the six algorithms over 100 runs.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Average of single view</cell><cell cols="2">Average of single view EM Average of single view</cell><cell>MVKKM</cell><cell>Co-regspec</cell><cell>WMCFS</cell></row><row><cell></cell><cell></cell><cell>k-means</cell><cell></cell><cell>LLC-fs</cell><cell></cell></row><row><cell>Mfeat</cell><cell>mfeat-fac&amp;mfeat-</cell><cell>0.639 7 0.011</cell><cell>0.655 7 0.010</cell><cell>0.776 70.009</cell><cell cols="2">0.825 7 0.012 0.792 7 0.013 0.8357 0.010</cell></row><row><cell></cell><cell>fou</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mfeat-fac&amp;mfeat-</cell><cell>0.6107 0.013</cell><cell>0.595 7 0.009</cell><cell>0.7087 0.010</cell><cell cols="2">0.7187 0.015 0.652 7 0.007 0.7947 0.009</cell></row><row><cell></cell><cell>zer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All 5 views</cell><cell>0.6787 0.012</cell><cell>0.636 7 0.010</cell><cell>0.8157 0.006</cell><cell cols="2">0.6467 0.014 0.7357 0.010 0.8367 0.010</cell></row><row><cell cols="2">Reuters EN&amp;FR</cell><cell>0.698 7 0.007</cell><cell>0.7707 0.008</cell><cell>0.6827 0.010</cell><cell cols="2">0.626 7 0.009 0.9157 0.012 0.9257 0.008</cell></row><row><cell></cell><cell>EN&amp;GR</cell><cell>0.7627 0.009</cell><cell>0.7707 0.011</cell><cell>0.782 70.013</cell><cell cols="2">0.638 7 0.008 0.9077 0.010 0.9267 0.009</cell></row><row><cell></cell><cell>All 5 views</cell><cell>0.7487 0.011</cell><cell>0.683 70.010</cell><cell>0.825 70.012</cell><cell cols="2">0.690 7 0.007 0.925 7 0.007 0.9277 0.007</cell></row><row><cell>Corel</cell><cell>Col-h&amp;Col-m</cell><cell>0.423 7 0.007</cell><cell>0.553 7 0.010</cell><cell>0.4807 0.011</cell><cell cols="2">0.502 7 0.010 0.6217 0.011 0.657 70.007</cell></row><row><cell></cell><cell>Coo-t&amp;Col-hl</cell><cell>0.344 70.013</cell><cell>0.495 70.015</cell><cell>0.423 70.012</cell><cell cols="2">0.513 70.007 0.656 7 0.010 0.6907 0.012</cell></row><row><cell></cell><cell>All 4 views</cell><cell>0.384 7 0.013</cell><cell>0.505 7 0.010</cell><cell>0.459 70.013</cell><cell cols="2">0.508 7 0.014 0.698 7 0.008 0.712 70.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>The mean and standard deviations of normalized mutual information (NMI) generated by the six algorithms over 100 runs.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Average of single view k-</cell><cell cols="2">Average of single view EM Average of single view LLC-</cell><cell>MVKKM</cell><cell>Co-regspec</cell><cell>WMCFS</cell></row><row><cell></cell><cell></cell><cell>means</cell><cell></cell><cell>fs</cell><cell></cell></row><row><cell>Mfeat</cell><cell>mfeat-fac&amp;mfeat-</cell><cell>0.594 70.010</cell><cell>0.6177 0.010</cell><cell>0.731 70.010</cell><cell cols="2">0.782 70.011 0.759 7 0.013 0.790 7 0.010</cell></row><row><cell></cell><cell>fou</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mfeat-fac&amp;mfeat-</cell><cell>0.562 70.012</cell><cell>0.5417 0.008</cell><cell>0.653 7 0.011</cell><cell cols="2">0.654 70.016 0.6107 0.008 0.753 7 0.009</cell></row><row><cell></cell><cell>zer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All 5 views</cell><cell>0.626 70.010</cell><cell>0.584 7 0.010</cell><cell>0.7657 0.007</cell><cell cols="2">0.6007 0.014 0.695 7 0.010 0.794 7 0.010</cell></row><row><cell cols="2">Reuters EN&amp;FR</cell><cell>0.654 70.008</cell><cell>0.729 70.008</cell><cell>0.6357 0.010</cell><cell cols="2">0.5757 0.009 0.846 7 0.011 0.859 7 0.007</cell></row><row><cell></cell><cell>EN&amp;GR</cell><cell>0.7077 0.009</cell><cell>0.720 70.011</cell><cell>0.726 70.010</cell><cell cols="2">0.598 70.009 0.845 7 0.009 0.862 7 0.006</cell></row><row><cell></cell><cell>All 5 views</cell><cell>0.7037 0.010</cell><cell>0.6407 0.009</cell><cell>0.788 7 0.012</cell><cell cols="2">0.652 70.007 0.864 70.008 0.8667 0.007</cell></row><row><cell>Corel</cell><cell>Col-h&amp;Col-m</cell><cell>0.385 70.008</cell><cell>0.502 7 0.010</cell><cell>0.432 7 0.011</cell><cell cols="2">0.450 70.012 0.570 70.011 0.6117 0.007</cell></row><row><cell></cell><cell>Coo-t&amp;Col-hl</cell><cell>0.3077 0.012</cell><cell>0.4427 0.015</cell><cell>0.382 7 0.013</cell><cell cols="2">0.47070.007 0.602 7 0.011 0.6437 0.013</cell></row><row><cell></cell><cell>All 4 views</cell><cell>0.3307 0.012</cell><cell>0.460 7 0.010</cell><cell>0.403 7 0.012</cell><cell cols="2">0.4617 0.013 0.6337 0.008 0.652 7 0.008</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>X 1 V X 2 1 X 2 2 X 2 V X</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>1 X 3 2 X 3 V X N 1 X N 2 X N V X 1 X 2 X v d 1 d 2 d v N</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Y.-M. Xu et al. / Pattern Recognition 53 (2016) 25-35</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>http://archive.ics.uci.edu/ml/datasets/Multiple þ Features</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>http://multilingreuters.iit.nrc.ca/ReutersMultiLingualMultiView.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by NSFC (61173084 and 61502543), CCF-Tencent Open Research Fund (CCF-TencentRAGR20140112), the PhD Start-up Fund of Natural Science Foundation of Guangdong Province, China (2014A030310180), Guangdong Natural Science Funds for Distinguished Young Scholar (No. 16050000051). The authors would like to thank the associate editor and reviewers for their comments which are very helpful in improving the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>There is no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="645" to="678" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A conscience on-line learning approach for kernel-based clustering</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Data Mining</title>
		<meeting>the 10th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An optimal graph theoretic approach to data clustering: theory and its application to image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel algorithms for hierarchical clustering and applications to split decomposition and parity graph recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dahlhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="205" to="240" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view constrained clustering with an incomplete mapping between views</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Esjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="231" to="257" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Source constrained clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Taralova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Computer Vision</title>
		<meeting>the 2011 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1927" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-learned multi-view spectral clustering for face recognition based on image sets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="875" to="879" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving document clustering using automated machine translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="645" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A subspace co-training framework for multiview clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source transfer learning based on label shared subspace</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="101" to="106" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video object matching across multiple non-overlapping camera views based on multi-feature fusion and incremental learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Deller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3841" to="3851" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-source learning with block-wise missing data for Alzheimer&apos;s disease prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical computation of feature weighting schemes through data estimation for nearest neighbor classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sáez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3941" to="3948" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel support vector machine-based feature selection approach for recognizing Flying Apsaras&apos; streamers in the Dunhuang Grotto Murals</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2014">2014</date>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-view clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Data Mining</title>
		<meeting>the 4th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
		<title level="m">ICML Workshop on Learning with Multiple Views</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
	<note>Spectral clustering with two views</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A co-training approach for multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-objective multi-view spectral clustering via Pareto optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="234" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel-based weighted multi-view clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzortzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Data Mining</title>
		<meeting>the 12th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A subspace co-training framework for multiview clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view clustering and feature learning via structured sparsity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tw-(k)-means: automated two-level variable weighting clustering algorithm for multiview data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="932" to="944" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view k-means clustering on big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third International Joint Conference on Artificial Intelligence<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2598" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<ptr target="〈http://archive.ics.uci.edu/ml/〉" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from multiple partially observed views-an application to multilingual text categorization</title>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NRC&apos;s PORTAGE system for WMT</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-2007 Second Workshop on SMT</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-exemplar affinity propagation</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2223" to="2237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature selection and kernel learning for local learningbased clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1532" to="1547" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xu received her master degree in 2015 from Sun Yat-sen University</title>
		<author>
			<persName><forename type="first">Yu-Meng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Her research interest is data clustering</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">He is currently an assistant professor at School of Mobile Information Engineering, Sun Yat-sen University. His current research interests include machine learning and pattern recognition, especially focusing on data clustering and its applications</title>
		<author>
			<persName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, Knowledge and Information System, Neurocomputing, ICDM and SDM</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2013. 2012</date>
		</imprint>
		<respStmt>
			<orgName>Sun Yat-sen University</orgName>
		</respStmt>
	</monogr>
	<note>He has published over 30 scientific papers in international journals and conferences such as IEEE TPAMI, IEEE TKDE. His ICDM 2010 paper won the Honorable Mention for Best Research Paper Awards. He won</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">He was awarded 2015 Chinese Association for Artificial Intelligence (CAAI) Outstanding Dissertation</title>
		<imprint>
			<publisher>Microsoft Research Fellowship Nomination Award</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">He joined Sun Yat-sen University in 1989 as an assistant professor, where currently, he is a professor with the Department of Automation of School of Information Science and Technology and dean of School of Information Science and Technology. His current research interests are in the areas of digital image processing, pattern recognition, multimedia communication, wavelet and its applications. He has published over 150 scientific papers in the international journals and conferences on image processing and pattern recognition, e.g. IEEE TPAMI, IEEE TKDE, IEEE TNN, IEEE TIP, IEEE TSMC</title>
		<author>
			<persName><forename type="first">Jian-Huang</forename><surname>Lai Received His</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">applied mathematics in 1989 and his Ph.D. in mathematics in 1999 from</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Sun Yat-sen University</orgName>
		</respStmt>
	</monogr>
	<note>Pattern Recognition, ICCV, CVPR and ICDM. Lai serves as a standing member of the Image and Graphics Association of China and also serves as a standing director of the Image and Graphics Association of Guangdong</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
