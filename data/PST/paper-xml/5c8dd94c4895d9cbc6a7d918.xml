<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiNE: Bipartite Network Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
							<email>mgao@dase.ecnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Leihui</forename><surname>Chen</surname></persName>
							<email>leihuichen@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
							<email>ayzhou@dase.ecnu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science and Engineering East</orgName>
								<orgName type="institution">China Normal University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science and Engineering East</orgName>
								<orgName type="institution">China Normal University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Data Science and Engineering East</orgName>
								<orgName type="institution">China Normal University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BiNE: Bipartite Network Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3209978.3209987</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Information retrieval</term>
					<term>Recommender systems</term>
					<term>• Computing methodologies → Neural networks</term>
					<term>Bipartite networks, Network representation learning, Link prediction, Recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work develops a representation learning method for bipartite networks. While existing works have developed various embedding methods for network data, they have primarily focused on homogeneous networks in general and overlooked the special properties of bipartite networks. As such, these methods can be suboptimal for embedding bipartite networks.</p><p>In this paper, we propose a new method named BiNE, short for Bipartite Network Embedding, to learn the vertex representations for bipartite networks. By performing biased random walks purposefully, we generate vertex sequences that can well preserve the long-tail distribution of vertices in the original bipartite network. We then propose a novel optimization framework by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved but transitive links) in learning the vertex representations. We conduct extensive experiments on several real datasets covering the tasks of link prediction (classification), recommendation (personalized ranking), and visualization. Both quantitative results and qualitative analysis verify the effectiveness and rationality of our BiNE method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The bipartite network is a ubiquitous data structure to model the relationship between two types of entities. It has been widely used in many applications such as recommender systems, search engines, question answering systems and so on. For example, in search engines, queries and webpages form a bipartite network, where the edges can indicate users' click behaviors that provide valuable relevance signal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>; in another application of recommender systems, users and items form a bipartite network, where the edges can encode users' rating behaviors that contain rich collaborative filtering patterns <ref type="bibr" target="#b2">[3]</ref>.</p><p>To perform predictive analytics on network data, it is crucial to first obtain the representations (i.e., feature vectors) for vertices. Traditional vector space methods such as the bag-of-words representations capture too few semantics and are inefficient to deal with large-scale dynamic networks in practical applications. Recent advances in data mining and information retrieval have focused on learning representations from data <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In particular, they embed vertices into a low dimensional space, i.e., representing a vertex as a learnable embedding vector. Based on the vertex embeddings, standard machine learning techniques can be applied to address various predictive tasks such as vertex labeling, link prediction, clustering and so on.</p><p>To date, existing works have primarily focused on embedding homogeneous networks where vertices are of the same type <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Following the pioneering work of DeepWalk <ref type="bibr" target="#b7">[8]</ref>, these methods typically apply a two-step solution: first performing random walks on the network to obtain a "corpus" of vertices, and then applying word embedding methods such as word2vec <ref type="bibr" target="#b10">[11]</ref> to obtain the embeddings for vertices. Despite effectiveness and prevalence, we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons:</p><p>(1) The type information of vertices is not considered. Distinct from homogeneous networks, there are two types of vertices in a bipartite network. Although edges exist between vertices of different types only, there are essentially implicit relations between vertices of the same type. For example, in the useritem bipartite network built for recommendation, there exists an implicit relation between users which can indicate their preference in consuming the same item; and importantly, it is recently reported that modeling such implicit relations can improve the recommendation performance <ref type="bibr" target="#b11">[12]</ref>. However, existing network embedding methods modeled the explicit relation (i.e., observed edges) only and ignored the underlying implicit relations. While the corpus generated by random walks may capture such high-order implicit relations to a certain extent, we argue that a more effective way is to encode such implicit relations into representation learning in an explicit manner. <ref type="bibr">(</ref>2) The generated corpus may not preserve the characteristics of a bipartite network. To demonstrate this point, we plot the frequency distribution of vertices in a real YouTube dataset <ref type="foot" target="#foot_0">1</ref>in Figure <ref type="figure" target="#fig_0">1</ref>(a). We can see that the vertices exhibit a standard power-law distribution with a slope of −1.582. By contrast, we plot the frequency distribution of vertices in a corpus generated by DeepWalk in Figure <ref type="figure" target="#fig_0">1</ref>(b). We find that the generated distribution differs significantly from the real distribution, and it cannot be well described by a power-law distribution. We point out that the failure of DeepWalk is due to the improper design of the random walk generator, which is suboptimal for embedding bipartite networks. Specifically, it generates the same number of random walks starting from each vertex and constrains the length of walks to be the same; this limits the capability of the generator and makes it difficult to generate a corpus following a power-law distribution -which is a common characteristics of many real-world bipartite networks <ref type="bibr" target="#b12">[13]</ref>.</p><p>To our knowledge, none of the existing works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type="bibr" target="#b13">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks, we argue that a key limitation is that it treats the explicit and implicit relations as contributing equally to the learning. In real-world bipartite networks, the explicit and implicit relations typically carry different semantics. As such, they should be treated differently and assigned to varying weights in learning the vertex embeddings. This can be evidenced by existing recommendation works <ref type="bibr" target="#b14">[15]</ref> that usually assign varying weights on different sources of information to allow a flexible tuning on the learning process.</p><p>In this work, we focus on the problem of learning vertex representations for bipartite networks. We propose BiNE (short for Bipartite Network Embedding), which addresses the aforementioned limitations of existing network embedding methods. Below we highlight two characteristics of our BiNE method.</p><p>(1) To account for both the explicit relations and implicit relations, we propose a joint optimization framework. For each relation, we design a dedicated objective function; by sharing the vertex embeddings, the objective functions for different relations reinforce each other and lead to better vertex embeddings. Specifically, the modeling of the explicit relations aims to reconstruct the bipartite network by focusing on observed links. For the modeling of implicit relations, we aim to capture the high-order correlations in the bipartite network. To avoid the explosive growth of complexity in expanding a network, we similarly resort to performing random walks and design the objective function based on the generated corpora. (2) To retain the properties of the bipartite network as many as possible, we propose a biased and self-adaptive random walk generator. Specifically, we set the number of random walks starting from each vertex based on its importance, making the vertex distribution in the generated corpus more consistent with the original bipartite network. Moreover, instead of setting a uniform length for all random walks, we allow a walk to be stopped in a probabilistic way. Through this way, we can generate vertex sequences of varying lengths, which is more analogous to the sentences in natural language. Our empirical study shows that our generator can generate corpus more close to the distribution of the real-world networks.</p><p>The remainder of the paper is organized as follows. We first review related work in Section 2. We formulate the problem in Section 3, before delving into details of the proposed method in Section 4. We perform extensive empirical studies in Section 5 and conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Network Representation Learning</head><p>Our work is related to vertex representation learning methods on homogeneous networks, which can be categorized into two types: matrix factorization (MF)-based and neural network-based methods.</p><p>MF-based methods are either linear <ref type="bibr" target="#b15">[16]</ref> or nonlinear <ref type="bibr" target="#b16">[17]</ref> in learning vertex embeddings. The former employs the linear transformations to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) <ref type="bibr" target="#b15">[16]</ref>. However, the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear transformations, e.g., kernel PCA, spectral embedding , marginal fisher analysis (MFA), and manifold learning approaches include LLE and ISOMAP <ref type="bibr" target="#b16">[17]</ref>. Generally speaking, MF-based methods have two main drawbacks: (1) they are usually computationally expensive due to the eigen-decomposition operations on data matrices, making them difficult to handle large-scale networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>;</p><p>(2) their performance are rather sensitive to the predefined proximity measures for calculating the affinity matrix.</p><p>Neural network-based methods are the state-of-art vertex representation learning techniques. The pioneer work DeepWalk <ref type="bibr" target="#b7">[8]</ref> and Node2vec <ref type="bibr" target="#b3">[4]</ref> extend the idea of Skip-gram <ref type="bibr" target="#b10">[11]</ref> to model homogeneous network, which is convert to a corpus of vertex sequences by performing truncated random walks. However, they may not be effective to preserve both explicit and implicit relations of the network. There are some follow-up works exploiting both 1st-order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type="bibr" target="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type="bibr" target="#b20">[21]</ref> incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep <ref type="bibr" target="#b21">[22]</ref> further extends the method to capture higher-order proximities. Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>, community information <ref type="bibr" target="#b23">[24]</ref>, textual content <ref type="bibr" target="#b24">[25]</ref>, user profiles <ref type="bibr" target="#b8">[9]</ref>, location information <ref type="bibr" target="#b25">[26]</ref>, among others.</p><p>It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices. In addition, the "corpus" generated by the truncated random walks may not capture the characteristics of the network structure, such as the power-law distribution of vertex degrees. Thus, these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type="bibr" target="#b13">[14]</ref>, HNE <ref type="bibr" target="#b26">[27]</ref> and EOE <ref type="bibr" target="#b27">[28]</ref> are representative vertex embedding methods for heterogeneous networks. Although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks, they are not tailored for learning on bipartite networks. Specifically, HNE aims to integrate content and linkage structures into the embedding process, and Metapath2vec++ ignores the strength of the relations between vertices and treats the explicit and implicit relations as equally. As such, they are suboptimal for vertex representation learning for a bipartite network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bipartite Network Modeling</head><p>As a ubiquitous data structure, bipartite networks have been mined for many applications, among which vertex ranking is an active research problem. For example, HITS <ref type="bibr" target="#b28">[29]</ref> learns to rank vertices by capturing some semantic relations within a bipartite network. Co-HITS <ref type="bibr" target="#b0">[1]</ref> incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network. BiRank <ref type="bibr" target="#b2">[3]</ref> ranks vertices by taking into account both the network structure and prior knowledge.</p><p>Distributed vertex representation is an alternative way to leverage signals from bipartite network. Unlike the ranking task, it learns a low dimensional representation of a vertex, which can be seen as the "features" of the vertex that preserves more information rather than simply a ranking score. Latent factor model (LFM), which has been widely investigated in the field of recommender systems and semantic analysis, is the most representative model. And a typical implementation of LFM is based on matrix factorization <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Recent advances utilize deep learning methods to learn vertex embeddings on the user-item network for recommendation <ref type="bibr" target="#b32">[33]</ref>. It is worth pointing out that these methods are tailored for the recommendation task, rather than for learning informative vertex embeddings. Moreover, they model the explicit relations in bipartite </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>We first give notations used in this paper, and then formalize the bipartite network embedding problem to be addressed. Notations. Let G = (U , V , E) be a bipartite network, where U and V denote the set of the two types of vertices respectively, and E ⊆ U × V defines the inter-set edges. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, u i and v j denote the i-th and j-th vertex in U and V , respectively, where i = 1, 2, ..., |U | and j = 1, 2, ..., |V |. Each edge carries a non-negative weight w i j , describing the strength between the connected vertices u i and v j ; if u i and v j are disconnected, the edge weight w i j is set to zero. Therefore, we can use a |U | × |V | matrix W = [w i j ] to represent all weights in the bipartite network. Problem Definition. The task of bipartite network embedding aims to map all vertices in the network into a low-dimensional embedding space, where each vertex is represented as a dense embedding vector. In the embedding space, both the implicit relations between vertices of the same type and the explicit relations between vertices of different types should be preserved. Formally, the problem can be defined as: </p><formula xml:id="formula_0">Input: A bipartite network G = (U , V , E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BINE: BIPARTITE NETWORK EMBEDDING</head><p>A good network embedding should be capable of reconstructing the original network well. To achieve this aim for a bipartite network, we consider reconstructing the bipartite network from two perspectives -the explicit relations evidenced by the observed edges and the implicit relations implied by the unobserved but transitive links. We then learn vertex embeddings by jointly optimizing the two tasks. This section presents our BiNE method along this line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling Explicit Relations</head><p>In a bipartite network, edges exist between vertices of two different types, providing an explicit signal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type="bibr" target="#b19">[20]</ref>, we model explicit relations by considering the local proximity between two connected vertices. The joint probability between vertices u i and v j is defined as:</p><formula xml:id="formula_1">P (i, j) = w i j e i j ∈E w i j . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where w i j is the weight of edge e i j . Obviously, if two vertices are strongly connected with a larger weight, they will have a higher probability to be co-occurred. Now we consider how to estimate the local proximity between two vertices in the embedding space. The effectiveness and prevalence of word2vec inspire many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> to use inner product to model the interaction between two entities. We follow this setting, and use sigmoid function to transform the interaction value to the probability space:</p><formula xml:id="formula_3">P (i, j) = 1 1 + exp(− ì u i T ì v j ) . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where ì u i ∈ R d and ì v j ∈ R d are the embedding vectors of vertices u i and v j , respectively.</p><p>With the empirical distribution of the co-occurring probability between vertices and the reconstructed distribution, we can learn the embedding vectors by minimizing their difference. We choose the KL-divergence as the difference measure between distributions, which can be defined as:</p><formula xml:id="formula_5">minimize O 1 = K L(P | | P ) = e i j ∈E P (i, j) log( P (i, j) P (i, j) ) ∝ − e i j ∈E</formula><p>w i j log P (i, j).</p><p>(</p><p>Intuitively, minimizing this objective function will make two vertices that are strongly connected in the original network also close with each other in the embedding space, which preserves the local proximity as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Implicit Relations</head><p>As illustrated in existing recommedation works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, both explicit and implicit relations are helpful to reveal different semantic in bipartite networks. To be comprehensive, it is crucial to also account for the implicit relation between two vertices of the same type, even though they are not explicitly connected. Intuitively, for two vertices of the same type, if there exists a path between them, there should be certain implicit relation between them; the number of the paths and their length indicate the strength of the implicit relation. Unfortunately, counting the paths between two vertices has a rather high complexity of an exponential order, which is infeasible to implement for large networks. To encode such high-order implicit relations among vertices in a bipartite network, we resort to the solution of DeepWalk. Specifically, the bipartite network is first converted to two corpora of vertex sequences by performing random walks; then the embeddings are learned from the corpora which encodes high-order relations between vertices. In what follows, we first elaborate how to generate two quality corpora for a bipartite network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Constructing Corpus of Vertex Sequences.</head><p>It is a common way to convert a network into a corpus of vertex sequences by performing random walks on the network, which has been used in some homogeneous network embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. However, directly performing random walks on a bipartite network could fail, since there is no stationary distribution of random walks on bipartite networks due to the periodicity issue <ref type="bibr" target="#b33">[34]</ref>. To address this issue, we consider performing random walks on two homogeneous networks that contain the 2nd-order proximity between vertices of the same type. Following the idea of Co-HITS <ref type="bibr" target="#b0">[1]</ref>, we define the 2nd-order proximity between two vertices as:</p><formula xml:id="formula_7">w U i j = k ∈V w ik w jk ; w V i j = k ∈U w k i w k j .<label>(4)</label></formula><p>where w i j is the weight of edge e i j . Hence, we can use the</p><formula xml:id="formula_8">|U | × |U | matrix W U = [w U i j ] and the |V | × |V | matrix W V = [w V i j</formula><p>] to represent the two induced homogeneous networks, respectively. Now we can perform truncated random walks on the two homogeneous networks to generate two corpora for learning the high-order implicit relations. As demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>, the corpus generated by DeepWalk may not capture the characteristic of a real-world network. To generate a corpus with a high fidelity, we propose a biased and self-adaptive random walk generator, which can preserve the vertex distribution in a bipartite network. We highlight its core designs as follows:</p><p>• First, we relate the number of random walks starting from each vertex to be dependent on its importance, which can be measured by its centrality. For a vertex, the greater its centrality is, the more likely a random walk will start from it. As a result, the vertex importance can be preserved to some extent. • We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type="bibr" target="#b13">[14]</ref> that apply a fixed length on the random walk, we allow the generated vertex sequences have a variable length, in order to have a close analogy to the variable-length sentences in natural languages.</p><p>Generally speaking, the above generation process follows the principle of "rich gets richer", which is a physical phenomena existing in many real networks, i.e., the vertex connectivities follow a scalefree power-law distribution <ref type="bibr" target="#b34">[35]</ref>.</p><p>The workflow of our random walk generator is summarized in Algorithm 1, where maxT and minT are the maximal and minimal numbers of random walks starting from each vertex, respectively. D U (or D V ) output by Algorithm 1 is the corpus generated from the vertex set U (or V ). The vertex centrality can be measured by many metrics, such as degree centrality, PageRank and HITS <ref type="bibr" target="#b28">[29]</ref>, etc., and we use HITS in our experiments.</p><p>Algorithm 1: WalkGenerator(W , R, maxT , minT , p)</p><p>Input : weight matrix of the bipartite network W, vertex set R (can be U or V ), maximal walks per vertex maxT , minimal walks per vertex minT , walk stopping probability p Output : a set of vertex sequences D R 1 Calculate vertices' centrality:</p><formula xml:id="formula_9">H = CentralityMeasure(W); 2 Calculate W R w.r.t. Equation (4); 3 foreach vertex v i ∈ R do 4 l = max(H(v i ) × maxT , minT ); 5 for i = 0 to l do 6 D v i = BiasedRamdomW alk(W R , v i , p); 7 Add D v i into D R ; 8 return D R ;</formula><p>4.2.2 Implicit Relation Modeling. After performing biased random walks on the two homogeneous networks respectively, we obtain two corpora of vertex sequences. Next we employ the Skipgram model <ref type="bibr" target="#b10">[11]</ref> on the two corpora to learn vertex embeddings. The aim is to capture the high-order proximity, which assumes that vertices frequently co-occurred in the same context of a sequence should be assigned to similar embeddings. Given a vertex sequence S and a vertex u i , the context is defined as the ws vertices before u i and after u i in S; each vertex is associated with a context vector ì θ θ θ i (or ì ϑ ϑ ϑ j ) to denote its role as a context. As there are two types of vertices in a bipartite network, we preserve the high-order proximities separately. Specifically, for the corpus D U , the conditional probability to maximize is:</p><formula xml:id="formula_10">max imize O 2 = u i ∈S ∧S ∈D U uc ∈C S (u i ) P (u c |u i ).</formula><p>(</p><formula xml:id="formula_11">)<label>5</label></formula><p>where C S (u i ) denotes the context vertices of vertex u i in sequence S. Similarly, we can get the objective function for corpus D V :</p><formula xml:id="formula_12">max imize O 3 = v j ∈S ∧S ∈D V vc ∈C S (v j ) P (v c |v j ).<label>(6)</label></formula><p>Following existing neural embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, we parameterize the conditional probability P(u c |u i ) and P(v c |v j ) using the inner product kernel with softmax for output:</p><formula xml:id="formula_13">P (u c |u i ) = exp ( ì u i T ì θ θ θ c ) |U | k =1 exp ( ì u i T ì θ θ θ k ) , P (v c |v j ) = exp ( ì v j T ì ϑ ϑ ϑ c ) |V | k =1 exp ( ì v j T ì ϑ ϑ ϑ k ) .<label>(7)</label></formula><p>where P(u c |u i ) denotes how likely u c is observed in the contexts of u i ; similar meaning applies to P(v c |v j ). With this definition, achieving the goal defined in Equations ( <ref type="formula" target="#formula_11">5</ref>) and ( <ref type="formula" target="#formula_12">6</ref>) will force the vertices with the similar contexts to be close in the embedding space. Nevertheless, optimizing the objectives is non-trivial, since each evaluation of the softmax function needs to traverse all vertices of a side, which is very time-costing. To reduce the learning complexity, we employ the idea of negative sampling <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.3</head><p>Negative Sampling. The idea of negative sampling is to approximate the costly denominator term of softmax with some sampled negative instances <ref type="bibr" target="#b35">[36]</ref>. Then the learning can be performed by optimizing a point-wise classification loss. For a center vertex u i , high-quality negatives should be the vertices that are dissimilar from u i . Towards this goal, some heuristics have been applied, such as sampling from popularity-biased non-uniform distribution <ref type="bibr" target="#b10">[11]</ref>. Here we propose a more grounded sampling method that caters the network data.</p><p>First we employ locality sensitive hashing (LSH) <ref type="bibr" target="#b36">[37]</ref> to block vertices after shingling each vertex by its ws-hop neighbors with respect to the topological structure in the input bipartite network. Given a center vertex, we then randomly choose the negative samples from the buckets that are different from the bucket contained the center vertex. Through this way, we can obtain high-quality and diverse negative samples, since LSH can guarantee that dissimilar vertices are located in different buckets in a probabilistic way <ref type="bibr" target="#b36">[37]</ref>.</p><p>Let N ns S (u i ) denote the ns negative samples for a center vertex u i in sequence S ∈ D U , we can then approximate the conditional probability p(u c |u i ) defined in Equation <ref type="bibr" target="#b6">(7)</ref> as:</p><formula xml:id="formula_14">p(u c , N ns S (u i ) |u i ) = z ∈{uc }∪N ns S (u i ) P (z |u i ),<label>(8)</label></formula><p>where the probability P(z|u j ) is defined as:</p><formula xml:id="formula_15">P(z|u i ) = σ ( ì u i T ì θ θ θ z ), if z is a context of u i 1 − σ ( ì u i T ì θ θ θ z ), z ∈ N ns S (u i )</formula><p>, where σ denotes the sigmoid function 1/(1 + e −x ). By replacing p(u c |u i ) in Equation ( <ref type="formula" target="#formula_11">5</ref>) with the definition of p(u c , N ns S (u i )|u i ), we can get the approximated objective function to optimize. The semantics is that the proximity between the center vertex and their contextual vertices should be maximized, whereas the proximity between the center vertex and the negative samples should be minimized.</p><p>Following the similar formulations, we can get the counterparts for the conditional probability p(v c |v j ), the details of which are omitted here due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Optimization</head><p>To embed a bipartite network by preserving both explicit and implicit relations simultaneously, we combine their objective functions to form a joint optimization framework.</p><formula xml:id="formula_16">maximize L = α log O 2 + β log O 3 − γO 1 .<label>(9)</label></formula><p>where parameters α, β and γ are hyper-parameters to be specified to combine different components in the joint optimization framework.</p><p>To optimize the joint model, we utilize the Stochastic Gradient Ascent algorithm (SGA). Note that the three components of Equation ( <ref type="formula" target="#formula_16">9</ref>) have different definitions of a training instance. To handle this issue, we tweak the SGA algorithm by performing a gradient step as follows:</p><p>Step I: For a stochastic explicit relation, i.e., an edge e i j ∈ E, we first update the embedding vectors ì u i and ì v j by utilizing SGA to maximize the last component L 1 = −γO 1 . We give the SGA update rule for ì u i and ì v j as follows:</p><formula xml:id="formula_17">ì u i = ì u i + λ{γw i j [1 − σ ( ì u i T ì v j )] • ì v j },<label>(10)</label></formula><formula xml:id="formula_18">ì v j = ì v j + λ{γw i j [1 − σ ( ì u i T ì v j )] • ì u i },<label>(11)</label></formula><p>where λ denotes the learning rate.</p><p>Step II: We then treat vertices u i and v j as the center vertex; by employing SGA to maximize objective functions L 2 = α log O 2 and L 3 = β log O 3 , we can preserve the implicit relations. Specifically, given the center vertex u i (or v j ) and its context vertex u c (or v c ), we update their embedding vectors ì u i (or ì v j ) as follows:</p><formula xml:id="formula_19">ì u i = ì u i + λ{ z ∈ {u c }∪N ns S (u i ) α[I (z, u i ) − σ ( ì u i T ì θ θ θ z )] • ì θ θ θ z },<label>(12)</label></formula><formula xml:id="formula_20">ì v j = ì v j + λ{ z ∈ {v c }∪N ns S (v j ) β[I (z, v j ) − σ ( ì v j T ì ϑ ϑ ϑ z )] • ì ϑ ϑ ϑ z }. (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where I (z, u i ) is an indicator function that determines whether vertex z is in the context of u i or not; similar meaning applies to I (z, v j ). Furthermore, the context vectors of both positive and negative instances are updated as:</p><formula xml:id="formula_22">ì θ θ θ z = ì θ θ θ z + λ{α[I (z, u i ) − σ ( ì u i T ì θ θ θ z )] • ì u i }, (<label>14</label></formula><formula xml:id="formula_23">) ì ϑ ϑ ϑ z = ì ϑ ϑ ϑ z + λ{β[I (z, v j ) − σ ( ì v j T ì ϑ ϑ ϑ z )] • ì v j }.<label>(15)</label></formula><p>We summarize the learning process in Algorithm 2. To be specific, lines 1-2 initialize all embedding vectors and context vectors; lines 3-4 generate the corpus of vertex sequences; lines 8 and 12 perform negative sampling; lines 9-10 and 13-14 employ SGA to learn the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions</head><p>Pre-training. The joint objective function of BiNE in Equation ( <ref type="formula" target="#formula_16">9</ref>) is non-convex, so initialization plays an important role to find a good solution. We pre-train Equation (3) to get the initial vertex embeddings. Update ì v j using Equation ( <ref type="formula" target="#formula_20">13</ref>);</p><p>14 Update ì ϑ ϑ ϑ z using Equation ( <ref type="formula" target="#formula_23">15</ref>) where z ∈ {v c } ∪ N ns S (v j ) ;</p><p>15 return Vertex embeding matrices U and V Computational Complexity Analysis. The corpus generation and joint model optimization are two key processes of BiNE. However, the complexity of generating corpus will be increased if W U or W V becomes dense. To avoid processing the dense matrix, an alternative way is to walk two steps in the original bipartite network. Suppose that vc is the visitation count of vertex v in the generated corpus. The context size is therefore vc • 2ws. It may be a big value for vertices having high degrees, yet we only randomly select a small batch of the contextual vertices, e.g., bs (bs ≪ vc). Thus, the complexity of algorithm is O(2|E| • bs • 2ws • (ns + 1)), where ns is the number of negative samples. To some extent, all the contextual vertices of a center vertex can be trained in each iteration by setting a proper bs, because the center vertex will be visited more than once when traversing all edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To evaluate the vertex embeddings learned by BiNE, we employ them to address two representative applications of bipartite network mining -link prediction and recommendation. Link prediction is usually approached as a classification task that predicts whether a link exists between two vertices, and recommendation is a personalized ranking task that aims to provide items of interest for a user. Through empirical evaluation, we aim to answer the following research questions:</p><p>RQ1 How does BiNE perform compared with state-of-the-art network embedding methods and other representative baselines of the two applications?</p><p>RQ2 Is the modeling of implicit relations helpful to learn more desirable representations for bipartite networks? RQ3 Can our proposed random walk generator contribute to learning better vertex representations? RQ4 How do the key hyper-parameters affect the performance of BiNE? In what follows, we first introduce the experimental settings, and then answer the above research questions in turn. Furthermore, we perform a case study, which visualizes a small bipartite network, to demonstrate the rationality of BiNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>5.1.1 Dataset. (1) For the link prediction task, we use two unweighted bipartite networks constructed from Wikepedia and Tencent, respectively. Specifically, the Wikipedia dataset is publicly accessible<ref type="foot" target="#foot_1">2</ref> , which contains the edit relationship between authors and pages; the Tencent dataset records the watching behaviors of users on movies in QQlive<ref type="foot" target="#foot_2">3</ref> in one month's time. ( <ref type="formula" target="#formula_3">2</ref>) For the recommendation task, we use three weighted bipartite networks constructed from DBLP, MovieLens, and VisualizeUs. Specifically, the DBLP dataset<ref type="foot" target="#foot_3">4</ref> contains the publish network of authors on venues, where the edge weight indicates the number of papers published on a venue by an author. The MovieLens dataset <ref type="foot" target="#foot_4">5</ref> has been widely used for evaluating movie recommender systems <ref type="bibr" target="#b32">[33]</ref>, where the edge weight denotes the rating score of a user on an item. The VisualizeUS dataset <ref type="foot" target="#foot_5">6</ref> contains the picture tagging network between pictures and tags, where the edge weight denotes the number of times a tag has been tagged on an image. The statistics of our experimented datasets are summarized in Table <ref type="table">1</ref>.</p><p>Note that our experimented datasets cover a wide range of applications based on bipartite networks, which can test the universality of our BiNE method. Moreover, we have purposefully chosen weighted networks for the recommendation task, so as to study BiNE's ability in embedding weighted bipartite networks. Table <ref type="table">1</ref>: Statistics of bipartite networks and metrics adopted in experimets for different tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Protocols.</head><p>(1) To evaluate the link prediction task, we apply the same protocol as the node2vec paper <ref type="bibr" target="#b3">[4]</ref>. Specifically, for the Wikepedia dataset, the observed links are treated as positive instances, and we randomly sample an equal number of vertex pairs that are not connected as the negative instances. For the Tencent dataset, positive instances include user-movie pairs where the user has watched the movie for more than 5 minutes, otherwise, it is treated as an negative instance. For both datasets,</p><p>• RankALS <ref type="bibr" target="#b38">[39]</ref>: This method also optimizes the MF model for the ranking task, by towards a different pairwise regressionbased loss. • FISMauc <ref type="bibr" target="#b39">[40]</ref>: Distinct to MF, factored item similarity model (FISM) is an item-based collaborative filtering method. We employ the AUC-based objective to optimize FISM for the top-K task. 5.1.4 Parameter Settings. We have fairly tuned the hyperparameters for each method. For all network embedding methods, we set the embedding size as 128 for a fair comparison. For the recommendation baselines, we tuned the learning rate and latent factor number since they impact most on the performance; other hyper-parameters follow the default setting of the LibRec toolkit. For our BiNE, we fix the loss trade-off parameter α as 0.01 and tune the other two. The minT and maxT are respectively set to 1 and 32, which empirically show good results. We test the learning rate λ of [0.01, 0.025, 0.1]. And the optimal setting of learning rate is 0.025 for the VisualizeUs/DBLP dataset and 0.01 for others. The search range and optimal setting (highlighted in red font) of other parameters are shown in Table <ref type="table" target="#tab_2">2</ref>. Note that besides γ is set differently -0.1 for recommendation and 1 for link prediction -other parameters are set to the same value for both tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison (RQ1)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Link Prediction.</head><p>In this task, embedding vectors given by BiNE are treated as feature vectors of a logistic regression classifier. Specifically, given a vertex pair (u i , v j ), we feed their embedding vectors ì u i and ì v j into the classifier, which is trained on observed links of the bipartite network. Table <ref type="table" target="#tab_3">3</ref> illustrates the performance of baselines and our BiNE, where we have the following key observations: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>VisualizeUs DBLP Movielens F1@10 NDCG@10 MAP@10 MRR@10 F1@10 NDCG@10 MAP@10 MRR@10 F1@10 NDCG@10 MAP@10 MRR@10 BPR 6.22% • The neural network-based methods outperform the indices proposed in <ref type="bibr" target="#b37">[38]</ref> significantly. This is due to the factors that: (1) one index proposed in <ref type="bibr" target="#b37">[38]</ref> only emphasizes one kind of network topological structure, rather than the global structure; (2) the neural network-based methods predict the links in a data-dependent supervised manner, which is more advantageous. • Metapath2vec++ and BiNE are significantly better than other neural network-based methods. This points to the positive effect of modeling both explicit and implicit relations into the embedding process. • BiNE outperforms Metapath2vec++ significantly and achieves the best performance on both datasets in both metrics. This improvement demonstrates the effectiveness of our modeling of explicit and implicit relations in different ways, whereas Meta-path2vec++ simply treats them as contributing equally to the learning. 5.2.2 Recommendation. We adopt the inner product kernel ì u i T ì v j to estimate the preference of user u i on item v j , and evaluate performance on the top-ranked results. Table <ref type="table" target="#tab_4">4</ref> shows the performance of baselines and our BiNE, where we have the following key observations:</p><p>• BiNE outperforms all baselines on all datasets, and the improvements are more significant on VisualizeUs -the most sparse dataset among the three. This sheds lights on the benefit of preserving both explicit and implicit relations in a bipartite network. Although Metapath2vec++ also preserves both explicit and implicit relations, we did not observe consistently good results since it ignores the weights and treats the two types of relations as equally.</p><p>• BiNE outperforms LINE significantly. The suboptimal performance of LINE are twofold: (1) although LINE preserves both 1st-order and 2nd-order relations to learn network embeddings, it ignores further higher-order proximities among vertices; (2) LINE learns two seperated embeddings for 1st-order and 2ndorder relations and concatenates them via post-processing, rather than optimizing them in a unified framework. As such, it reveals that: (1) only 2nd-order relations are insufficient for learning vertex embeddings for a bipartite network; (2) it is necessary to build a joint model to capture both explicit and implicit relations. These are the two featured designs of our BiNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Utility of Implicit Relations (RQ2)</head><p>To demonstrate the effectiveness of integrating explicit and implicit relations, we compare BiNE with its variant that removes the modeling of implicit relations. We only show the performance on MAP@10 and MRR@10 on recommendation due to space limitation.</p><p>From Table <ref type="table" target="#tab_6">5</ref>, we can find that the largest absolute improvements of BiNE with implicit relations are 1.44% and 18.58% for link prediction and recommendation, respectively. It indicates that our proposed way of modeling high-order implicit relations is rather effective to complement with explicit relation modeling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Random Walk Generator (RQ3)</head><p>We design a new random walk generator for BiNE to preserve properties of bipartite networks in the generated corpora as much as possible, such as the importance and distribution of vertices. To guarantee comparability with Figure <ref type="figure" target="#fig_0">1</ref>, we also use the YouTube's video network as the input of our random walk generator and show distribution of vertices. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, our random walk generator almost generates a standard power-law distribution with a slope −1.537 which is very close to that of the original network (−1.58186). We also compare the performance of BiNE under two settings -use or not use our proposed random walk generator. As shown in Table <ref type="table" target="#tab_7">6</ref>, the biggest absolute improvements of BiNE using our proposed random walk generator are 4.14% and 10.25% for link prediction and recommendation, respectively. It indicates that the biased and self-adaptive random walk generator contributes to improving the vertex embeddings.</p><p>Note that we change the default value of maxT to 128 for this empirical study on Movielens dataset. This is due to the factor that Movielens is the biggest and densest bipartite network data compared to the others. The default value of maxT may be too small to fully preserve the implicit relations. This indicates that maxT should be specified to a large number for a large-scale network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter Studies (RQ4)</head><p>Due to space limitation, we only investigate the impact of the hyper parameters β, γ (n.b., we fix α = 0.01) since parameters β, and γ play crucial roles to balance the impacts of the explicit relations (γ ) and implicit relations (α, β) for vertex embeddings. We analyze both link prediction and recommendation tasks on datasets VisualizeUs and Wikipedia, respectively. Except for the parameters being tested, other parameters assume default values.</p><p>From Figure <ref type="figure" target="#fig_5">4</ref> and Figure <ref type="figure" target="#fig_7">5</ref>, we observe that the impact of the two parameters have similar trends w.r.t. different performance metrics in the same task: (1) with the increase of γ , the performance first increases and then remains stable after certain values; (2) with the increase of β, the performance first increases and then decreases after certain values. When γ is small, our optimization model may artificially reduce the importance of the explicit relations for network embeddings. However, when β is large, the optimization model may artificially overstate the role of the implicit relations.</p><p>The existence of the yielding points confirms that purely using explicit or implicit relations is insufficient to learn desirable representations for a bipartite network. In addition, we can observe that the best value of γ is larger than that of α and β. It indicates that the explicit relations are more important than implicit relations for the bipartite network embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>A good embedding algorithm should provide a meaningful visualization that layout a network. Due to space limitation, we conduct a visualization study for a small bipartite network. We visualize a collaboration bipartite network, which is a subset of DBLP dataset. It consists of 736 researchers and 6 international journals, where the     6 journals are from two different research fields: SICOMP, IANDC and TIT from computer science theory, and AI, IJCV,and JMLR from artificial intelligence. A link will be formed if the researcher published at least 5 papers in the journal. The research field of a researcher is determined by the published venues of his/her works. We utilize the t-SNE tool <ref type="bibr" target="#b40">[41]</ref> to map the embedding vectors of authors into 2D space. Figure <ref type="figure" target="#fig_9">6</ref> compares the visualization results given by different embedding approaches, where color of a vertex indicates the research field of a researcher (red: "computer science theory", blue: "artificial intelligence"). Obviously, DeepWalk, Node2vec, LINE, Metapath2vec++, and our BiNE are good since researchers belonging different research fields are well seperated. In our opinion, BiNE gives a better result due to the fact that it also generates an obvious gap between two research fields. However, BiNE' (a variant of BiNE -without implicit relations) demonstates a worse layout than expected. It indicates that modeling high-order implicit relations is helpful to preserve the network structure well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have presented BiNE, a novel approach for embedding bipartite networks. It jointly models both the explicit relations and highorder implicit relations in learning the representation for vertices. Extensive experiments on several tasks of link prediction, recommendation, and visualization demonstrate the effectiveness and rationality of our BiNE method.</p><p>In this work, we have only considered the information revealed in observed edges, thus it may fail for vertices that have few or even no edges. Since missing data is a common situation in realworld applications, the observed edges may not contain sufficient signal on vertex relations. To address this issue, we plan to extend our BiNE method to model auxiliary side information, such as numerical features <ref type="bibr" target="#b41">[42]</ref>, textual descriptions <ref type="bibr" target="#b42">[43]</ref>, and among other attributes <ref type="bibr" target="#b8">[9]</ref>. In addition, the bipartite networks in many practical applications are dynamically updated <ref type="bibr" target="#b31">[32]</ref>. For example, the preferences of users may evolve over time which can be revealed in her recent behaviors. Thus, we plan to investigate how to efficiently refresh embeddings for dynamic bipartite networks. Lastly, we are interested in extending our method to learn representations for the more generic and heterogeneous n-partite networks, for which one key challenge is how to automatically learn the varying weights for relations of different types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The vertex distribution of (a) the real-world YouTube dataset and (b) the corpus generated by the random walk generator of DeepWalk. The generated corpus does not show the desired power-law distribution due to the improper design of the generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the bipartite network structure network only, which can be improved by incorporating implicit relations as shown in [12, 15].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) and its weight matrix W. Output: A map function f : U ∪V → R d , which maps each vertex in G to a d-dimensional embedding vector. To keep the notations simple, we use ì u i and ì v j to denote the embedding vectors for vertices u i and v j , respectively. As such, we can present the embedding vectors of all vertices in the bipartite network as two matrices U = [ ì u i ] and V = [ ì v j ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of vertices in the biased and selfadpative random walk generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 :</head><label>4</label><figDesc>Impact of hyper-parameters on link prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of hyper-parameters on recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of authors in DBLP. Color of a vertex indicates the research fields of the authors (red: "computer science theory", blue: "artificial intelligence"). BiNE' is the version of BiNE -without implicit relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2: Training algorithm of BiNE Input : bipartite network G = (U , V , E), weight matrix of the bipartite network W, window size ws, number of negative samples ns, embedding size d, maximal walks per vertex maxT , minimal walks per vertex minT , walk stopping probability p Output : vertex embeding matrices U and V</figDesc><table><row><cell cols="2">1 Initialize embedding vectors ì u i and ì v j ;</cell></row><row><cell cols="2">2 Initialize context vectors ì θ θ θ i and ì ϑ ϑ ϑ j ;</cell></row><row><cell>6</cell><cell>Update ì u i and ì v j using Equations (10) and (11);</cell></row><row><cell>7 8</cell><cell>foreach (u i , u c ) in the sequence S ∈ D U do Negative sampling to generate N ns S (u i );</cell></row><row><cell>10</cell><cell>Update ì θ θ θ z using Equation (14) where</cell></row><row><cell></cell><cell>z ∈ {u c } ∪ N ns S (u i );</cell></row><row><cell>13</cell><cell></cell></row></table><note>3 D U = WalkGenerator(W, U , maxT , minT , p); 4 D V = WalkGenerator(W, V , maxT , minT , p); 5 foreach edge (u i , v j ) ∈ E do 9Update ì u i using Equation (12);11 foreach (v j , v c ) in the sequence S ∈ D V do 12Negative sampling to generate N ns S (v j );</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The search range and optimal setting (highlighted in red) of hyper-parameters for our BiNE method.</figDesc><table><row><cell>Parameter</cell><cell>Meaning</cell><cell>Test values</cell></row><row><cell>ns</cell><cell cols="2">number of negative samples [1, 2, 4, 6, 8, 10]</cell></row><row><cell>ws</cell><cell>size of window</cell><cell>[1, 3, 5, 7, 9]</cell></row><row><cell>p</cell><cell>walk stopping probability</cell><cell>[0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]</cell></row><row><cell>β</cell><cell>trade-off parameter</cell><cell>[0.0001, 0.001, 0.01, 0.1, 1]</cell></row><row><cell>γ</cell><cell>trade-off parameter</cell><cell>[0.01, 0.05, 0.1, 0.5, 1, 5]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Link prediction performance on Tencent and Wikipedia.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Tencent AUC-ROC AUC-PR AUC-ROC AUC-PR Wikipedia</cell></row><row><cell>CN</cell><cell>50.63%</cell><cell>65.66%</cell><cell>86.85%</cell><cell>90.68%</cell></row><row><cell>JC</cell><cell>51.49%</cell><cell>66.18%</cell><cell>63.90%</cell><cell>73.04%</cell></row><row><cell>AA</cell><cell>50.63%</cell><cell>65.66%</cell><cell>87.37%</cell><cell>91.12%</cell></row><row><cell>AL</cell><cell>50.44%</cell><cell>65.70%</cell><cell>90.28%</cell><cell>91.81%</cell></row><row><cell>Katz</cell><cell>50.90%</cell><cell>65.06%</cell><cell>90.84%</cell><cell>92.42%</cell></row><row><cell>PA</cell><cell>55.60%</cell><cell>68.99%</cell><cell>90.71%</cell><cell>93.37%</cell></row><row><cell>DeepWalk</cell><cell>57.62%</cell><cell>71.32%</cell><cell>89.71%</cell><cell>91.20%</cell></row><row><cell>LINE</cell><cell>59.68%</cell><cell>73.48%</cell><cell>91.62%</cell><cell>93.28%</cell></row><row><cell>Node2vec</cell><cell>59.28%</cell><cell>72.62%</cell><cell>89.93%</cell><cell>91.23%</cell></row><row><cell>Metapath2vec++</cell><cell>60.70%</cell><cell>73.69%</cell><cell>89.56%</cell><cell>91.72%</cell></row><row><cell>BiNE</cell><cell cols="2">60.98%** 73.77%**</cell><cell cols="2">92.91%** 94.45%**</cell></row><row><cell cols="5">** indicates that the improvements are statistically significant for p &lt; 0.01</cell></row><row><cell></cell><cell cols="2">judged by paired t-test.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of Top-10 Recommendation on VisualizeUs, DBLP, and MovieLens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>* indicates that the improvements are statistically significant for p &lt; 0.01 judged by paired t-test.</figDesc><table><row><cell></cell><cell></cell><cell>9.52%</cell><cell>5.51%</cell><cell>13.71%</cell><cell>8.95%</cell><cell>18.38%</cell><cell>13.55%</cell><cell>22.25%</cell><cell>8.03%</cell><cell>7.58%</cell><cell>2.23%</cell><cell>40.81%</cell></row><row><cell>RankALS</cell><cell>2.72%</cell><cell>3.29%</cell><cell>1.50%</cell><cell>3.81%</cell><cell>7.62%</cell><cell>11.50%</cell><cell>7.52%</cell><cell>14.87%</cell><cell>8.48%</cell><cell>7.95%</cell><cell>2.66%</cell><cell>38.93%</cell></row><row><cell>FISMauc</cell><cell>10.25%</cell><cell>15.46%</cell><cell>8.86%</cell><cell>16.67%</cell><cell>9.81%</cell><cell>13.77%</cell><cell>7.38%</cell><cell>14.51%</cell><cell>6.77%</cell><cell>6.13%</cell><cell>1.63%</cell><cell>34.04%</cell></row><row><cell>DeepWalk</cell><cell>5.82%</cell><cell>8.83%</cell><cell>4.28%</cell><cell>12.12%</cell><cell>8.50%</cell><cell>24.14%</cell><cell>19.71%</cell><cell>31.53%</cell><cell>3.73%</cell><cell>3.21%</cell><cell>0.90%</cell><cell>15.40%</cell></row><row><cell>LINE</cell><cell>9.62%</cell><cell>13.76%</cell><cell>7.81%</cell><cell>14.99%</cell><cell>8.99%</cell><cell>14.41%</cell><cell>9.62%</cell><cell>17.13%</cell><cell>6.91%</cell><cell>6.50%</cell><cell>1.74%</cell><cell>38.12%</cell></row><row><cell>Node2vec</cell><cell>6.73%</cell><cell>9.71%</cell><cell>6.25%</cell><cell>13.95%</cell><cell>8.54%</cell><cell>23.89%</cell><cell>19.44%</cell><cell>31.11%</cell><cell>4.16%</cell><cell>3.68%</cell><cell>1.05%</cell><cell>18.33%</cell></row><row><cell>Metapath2vec++</cell><cell>5.92%</cell><cell>8.96%</cell><cell>5.35%</cell><cell>13.54%</cell><cell>8.65%</cell><cell>25.14%</cell><cell>19.06%</cell><cell>31.97%</cell><cell>4.65%</cell><cell>4.39%</cell><cell>1.91%</cell><cell>16.60%</cell></row><row><cell>BiNE</cell><cell>13.63%**</cell><cell cols="4">24.50%** 16.46%** 34.23%** 11.37%**</cell><cell cols="4">26.19%** 20.47%** 33.36%** 9.14%**</cell><cell>9.02%**</cell><cell cols="2">3.01%** 45.95%**</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>BiNE with and without implicit relations. * indicates that the improvements are statistically significant for p &lt; 0.01 judged by paired t-test.</figDesc><table><row><cell></cell><cell cols="2">Without Implicit</cell><cell cols="2">With Implicit</cell></row><row><cell></cell><cell>Relations</cell><cell></cell><cell cols="2">Relations</cell></row><row><cell></cell><cell cols="2">Link Prediction</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">AUC-ROC AUC-PR AUC-ROC AUC-PR</cell></row><row><cell>Tencent</cell><cell>59.78%</cell><cell>73.05%</cell><cell>60.98%**</cell><cell>73.77%**</cell></row><row><cell>WikiPedia</cell><cell>91.47%</cell><cell>93.73%</cell><cell>92.91%**</cell><cell>94.45%**</cell></row><row><cell></cell><cell cols="2">Recommendation</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">MAP@10 MRR@10 MAP@10 MRR@10</cell></row><row><cell>VisualizeUS</cell><cell>7.91%</cell><cell>15.65%</cell><cell>16.46%**</cell><cell>34.23%**</cell></row><row><cell>DBLP</cell><cell>20.20%</cell><cell>32.95%</cell><cell>20.47%**</cell><cell>33.36%**</cell></row><row><cell>MovieLens</cell><cell>2.86%</cell><cell>43.98%</cell><cell>3.01%**</cell><cell>45.95%**</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>BiNE with different random walk generators.</figDesc><table><row><cell></cell><cell cols="2">Uniform Random</cell><cell cols="2">Biased and Self-adaptive</cell></row><row><cell></cell><cell cols="2">Walk Generator</cell><cell cols="2">Random Walk Generator</cell></row><row><cell></cell><cell cols="3">Link Prediction</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">AUC-ROC AUC-PR AUC-ROC</cell><cell>AUC-PR</cell></row><row><cell>Tencent</cell><cell>59.75%</cell><cell>73.06%</cell><cell>60.98%**</cell><cell>73.77%**</cell></row><row><cell>WikiPedia</cell><cell>88.77%</cell><cell>91.91%</cell><cell>92.91%**</cell><cell>94.45%**</cell></row><row><cell></cell><cell cols="3">Recommendation</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">MAP@10 MRR@10 MAP@10</cell><cell>MRR@10</cell></row><row><cell>VisualizeUS</cell><cell>15.93%</cell><cell>33.66%</cell><cell>16.46%**</cell><cell>34.23%**</cell></row><row><cell>DBLP</cell><cell>11.79%</cell><cell>23.41%</cell><cell>20.47%**</cell><cell>33.66%**</cell></row><row><cell>MovieLens</cell><cell>2.91%</cell><cell>46.12%</cell><cell>3.04%**</cell><cell>46.20%**</cell></row><row><cell cols="5">** indicates that the improvements are statistically significant for p &lt; 0.01</cell></row><row><cell></cell><cell cols="3">judged by paired t-test.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This YouTube dataset contains 1 million videos and 5 million links, which are downloaded from: http://socialnetworks.mpi-sws.org/data-imc2007.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://konect.uni-koblenz.de/networks/wikipedia_link_en</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://v.qq.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">http://dblp.uni-trier.de/xml/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://grouplens.org/datasets/movielens/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">http://konect.uni-koblenz.de/networks/pics_ti</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We use the implementations from LibRec: https://www.librec.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work has been supported by the National Key Research and Development Program of China under grant 2016YFB1000905, and the National Natural Science Foundation of China under Grant No. U1401256, 61672234, 61502236, and 61472321. NExT research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative. Shanghai Agriculture Applied Technology Development Program, China (Grant No.T20170303).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>we randomly sample 60% instances as the training set, evaluating performance on the remaining 40% of testing set. Following the previous work <ref type="bibr" target="#b37">[38]</ref>, we employ two metrics, area under the ROC curve (AUC-ROC) and Precison-Recall curve (AUC-PR), to evaluate the link prediction performance. (2) For the recommendation task, we adopt the same setting for all the three datasets. We randomly sample 60% edges as the training data, using the remaining 40% edges as the ground-truth for testing. For each user, we rank all items in her testing set and evaluate the ranking list with four IR metrics: F1, Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR). We truncate the ranking list at 10 to study the performance of top-10 recommendation. For each metric, we compute the average score for all users, and perform one-sample paired t-test on it. To avoid overfitting, we generate 10 folds of train-test split, tuning hyper-parameters on the first fold only for each method. We use the optimal hyper-parameter setting and report the average performance of all folds (i.e., the score of each metric and the p-value of t-test).</p><p>5.1.3 Baselines. We compare BiNE with three types of baselines:</p><p>(1) Network Embedding Methods. Similar to BiNE, this set of methods also learn vertex embeddings and are representative of state-of-the-art network embedding methods. For each method, we use the released implementations of the authors for our experiments.</p><p>• DeepWalk <ref type="bibr" target="#b7">[8]</ref>: As a homogeneous network embedding method, DeepWalk performs uniform random walks to get a corpus of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. • LINE <ref type="bibr" target="#b19">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order proximities in a homogeneous network. We use the LINE(1st+2nd) method which has shown the best results in their paper. • Node2vec <ref type="bibr" target="#b3">[4]</ref>: This method extends DeepWalk by performing biased random walks to generate the corpus of vertex sequences. The hyper-parameters p and q are set to 0.5 which has empirically shown good results. • Metapath2vec++ <ref type="bibr" target="#b13">[14]</ref>: This is the state-of-the-art method for embedding heterogeneous networks.The meta-path scheme chosen in our experiments are "IUI" (item-user-item) and "IUI"+"UIU" (user-item-user), and we only report the best result between them. (2) To benchmark the link prediction task, we also compare with a set of methods that are specifically designed for the task. We apply several indices proposed in <ref type="bibr" target="#b37">[38]</ref>, including Common Neighbors (CN), Jaccard Coefficient (JC), Absent Links (AL), Adamaic/Adar (AA), Katz Index (Katz), and Preferential Attachmenthave (PA). (3) We compare with several competitive methods 7 that are designed for the top-K item recommendation task.</p><p>• BPR <ref type="bibr" target="#b30">[31]</ref>:This method optimizes the matrix factorization (MF) model with a pairwise ranking-aware objective. This method has been widely used in recommendation literature as a highly competitive baseline <ref type="bibr" target="#b32">[33]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generalized co-hits algorithm and its application to bipartite graphs</title>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning query and document relevance from a web-scale click graph</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Daly</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dawei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Birank: Towards ranking on bipartite graphs. TKDE</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a hierarchical embedding model for personalized product search</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embedding factorization models for jointly recommending items and user generated lists</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attributed social network embedding. TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attributed network embedding for learning in a dynamic environment</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified pairwise ranking model with multiple relations for item recommendation</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Walkranker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A bipartite fitness model for online music streaming services</title>
		<author>
			<persName><forename type="first">Suchit</forename><surname>Pongnumkul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuyuki</forename><surname>Motohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="page" from="1125" to="1137" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Little is much: Bridging cross-platform behaviors through overlapped crowds</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multidimensional scaling</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Aa</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A geometric framework for nonconvex optimization duality using augmented lagrangian functions</title>
		<author>
			<persName><forename type="first">Angelia</forename><surname>Nedic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuman</forename><forename type="middle">E</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Global Optimization</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="573" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning on big graph: Label inference and regularization with anchor hierarchy</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1101" to="1114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable semisupervised learning by efficient anchor graph regularization</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1864" to="1877" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LINE: large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporate group information to enhance network embedding</title>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1901" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ice: Item concept embedding via textual information</title>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Hsiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Sin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning graph-based POI embedding for location-based recommendation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Embedding of embedding (EOE): joint embedding for coupled heterogeneous networks</title>
		<author>
			<persName><forename type="first">Linchuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="741" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discrete collaborative filtering</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BPR: bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Community detection in bipartite networks using random walks</title>
		<author>
			<persName><forename type="first">Taher</forename><surname>Alzahrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><forename type="middle">J</forename><surname>Horadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serdar</forename><surname>Boztas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CompleNet</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="157" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kronecker graphs: An approach to modeling networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="985" to="1042" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint event-partner recommendation in event-based social networks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Locality sensitive hashing revisited: filling the gap between theory and algorithm analysis</title>
		<author>
			<persName><forename type="first">Hongya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihchyun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davood</forename><surname>Rafiei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Link prediction for bipartite social networks: The role of structural holes</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASONAM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="153" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Alternating least squares for personalized ranking</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Takács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FISM: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Maaten</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2605. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning on partial-order hypergraphs</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic latent document network embedding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauw</forename><surname>Wirawan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
