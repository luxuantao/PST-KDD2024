<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crafting Personalized Facial Avatars Using Editable Portrait and Photograph Example</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanasai</forename><surname>Sucontphunt</surname></persName>
							<email>sucontph@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
							<email>zdeng@cs.uh.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@graphics.usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California / University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Crafting Personalized Facial Avatars Using Editable Portrait and Photograph Example</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BF2C6460E9CFF9C554E7B64FCD468343</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I</term>
					<term>3</term>
					<term>7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H</term>
					<term>5</term>
					<term>2 [Information Interfaces and Presentation]: User Interfaces-Prototyping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer-generated facial avatars have been increasingly used in a variety of virtual reality applications. Emulating the real-world face sculpting process, we present an interactive system to intuitively craft personalized 3D facial avatars by using 3D portrait editing and example-based painting techniques. Starting from a default 3D face portrait, users can conveniently perform intuitive "pulling" operations on its 3D surface to sculpt the 3D face shape towards any individual. To automatically maintain the faceness of the 3D face being crafted, novel facial anthropometry constraints and a reduced face description space are incorporated into the crafting algorithms dynamically. Once the 3D face geometry is crafted, this system can automatically generate a face texture for the crafted model using an image example-based painting algorithm. Our user studies showed that with this system, users are able to craft a personalized 3D facial avatar efficiently on average within one minute.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A variety of techniques have been developed to craft and animate 3D faces. For example, blendshape approach allows users to generate various facial expressions on a specific model by parameter (weight) tuning <ref type="bibr" target="#b2">[3]</ref>. Meanwhile, data-driven 3D face modeling and editing techniques exploit the intrinsic correlations among different facial regions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>In this paper, we present a data-driven system for intuitively crafting personalized 3D facial avatars by fusing a model-based 3D portrait editing with example-based face texture painting techniques. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, it is composed of: (1) Offline data processing, (2) Interactive face crafting, and (3) Face texture generation. We will briefly describe each of these parts in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OFFLINE DATA PROCESSING</head><p>In the offline data processing stage, we processed a collected 3D face dataset <ref type="bibr" target="#b8">[9]</ref> to construct a prior knowledge base about human faces.</p><p>First, we chose a face model as the reference/standard model (Fig. <ref type="figure" target="#fig_0">1</ref>) with extracted eight facial contour curves i.e., the left/right eyes, the left/right eyebrows, the nose, the upper/lower lips, and face border. After that, we used Radial Basis Functions (RBF) to align all 3D face models with the reference model such that all the aligned faces have the same geometric structure.</p><p>Then, based on the aligned 3D face dataset, we constructed a morphable face model <ref type="bibr" target="#b0">[1]</ref> to be used in follow-up Section 3. In addition, we constructed a table (called facial anthropometry proportion table) encloses a list of all facial anthropometry proportions <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INTERACTIVE FACE CRAFTING</head><p>Starting from the standard 3D portrait model generated by a modelbased portrait rendering technique <ref type="bibr" target="#b5">[6]</ref>, users can pull eight facial contour curves to craft its 3D face shape at system runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Facial Contour Curve Deformation</head><p>When a facial contour curve is pulled, we first determine the Segment of Interest (SOI) on the pulled curve using the peeling technique <ref type="bibr" target="#b6">[7]</ref>. In this work, face anthropometric constraints are also introduced to constrain the SOI to comply with a valid face geometric structure.</p><p>Facial anthropometry constraints: A facial anthropometry constraints (a total of 35 landmarks, 35 measurements, and 59 proportions. Refer to the left panel of Fig. <ref type="figure" target="#fig_1">2</ref>) are used in this work. We traverse a face proportion topology in the same way as in <ref type="bibr" target="#b1">[2]</ref>. For example, if the EX landmark is on our SOI, then EX-EX/N-STO in the anthropometry proportion table is the proportion constraints, prop i . Also, the measurement of N-STO is propFraction i , and coLandMark i the co-measurement landmark position. Accordingly, the new EX position is calculated by coLandMark i + (prop i × propFraction i ).</p><p>Consequently, we add an anthropometric error metric (Eq. 1) to the FiberMesh deformation equation (Eq.2) <ref type="bibr" target="#b6">[7]</ref> and solve in a least square sense.</p><formula xml:id="formula_0">FAE = ∑ i∈C 2 v i -coLandMark i -(prop i × propFraction i ) 2 (1) arg v min{ ∑ i L(v i ) -β i 2 + ∑ i∈C 1 v i -v ′ i 2 + FAE} (2)</formula><p>Here v i is a vertex coordinate, β i is the previous differential value, C 1 is the set of SOI constraints, v ′ i one of SOI constraints, and C 2 is the set of facial anthropometric constraints.</p><p>Besides pulling predefined facial contour curves, users can pull any location on the 3D surface directly (called pulling anywhere).</p><p>to appear in the Proceedings of IEEE Virtual Reality 2009 Conference, Lafayette, Louisiana, March 14-18, 2009.</p><p>The "pulling anywhere" works in a similar way as pulling the facial contour curves, except that only a specific (picked) vertex is pulled instead of the contour curve SOI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pulling Modes</head><p>Based on the target area of effect, pulling operations can work in the following three modes: working-region, regional, and global on a two-level hierarchical PCA-based technique inspired by the motion propagation algorithm proposed by Zhang et al. <ref type="bibr" target="#b9">[10]</ref>. In this hierarchy (Fig. <ref type="figure" target="#fig_1">2</ref>), a node at its bottom level is a region-based PCA space, and the node at the top level is the global PCA space. How this hierarchy is used for facial deformation depends on the pulling mode chosen by users. Working-region mode. It only affects (or deforms) the working region, e.g., eyes, nose, or mouth (assuming r denotes this By projecting r to its corresponding region-based PCA space, we will obtain a rectified version of the facial region, F r .</p><p>Regional mode. Under this mode, the surface deformation only happens at the working region r. The computed F r is projected to the global PCA space (the top-level node at Fig. <ref type="figure" target="#fig_1">2</ref>), denoted as G.</p><p>blending of F r and G r (i.e., F r /2 + G r /2) is the final status of the deformed facial region r.</p><p>Global mode. The deformed SOI is propagated to all nodes in the face hierarchy. Refer to the work by Zhang and his colleagues <ref type="bibr" target="#b9">[10]</ref> for more details regarding this propagation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FACE TEXTURE GENERATION</head><p>Once the 3D portrait is generated, a user-provided face image (the source image) is used as our color palette. In this work, we extend the "image analogies" algorithm <ref type="bibr" target="#b4">[5]</ref>.</p><p>Our facial texture analogies can be schematically illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. In this process, we separate the whole face into ten components (left/right eyes, left/right eye brows, nose, mouth, forehead, left/right cheek, and chin), and then the "image analogies" technique <ref type="bibr" target="#b4">[5]</ref> is applied for each facial component separately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SELECTED APPLICATIONS</head><p>This system can be used for a broad variety of computer graphics and virtual reality applications as in the following.</p><p>Reconstructing face models for criminals or missing persons based on vague memory: It can help a victim to easily and interactively craft a 3D portrait of the target person without the aid of a sketching specialist. After that, the victim can select a real photograph of other person who look similar to the target person (e.g., the same race) to generate the textured face model automatically.</p><p>Facial plastic surgery simulation: For example, this system can be used for setting up a pre-surgery communication between a surgeon and his/her patient.</p><p>Personalized 3D avatars: With this system, a user can create his/her own 3D avatar for virtual reality applications such as in roleplaying video games and social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND USER STUDIES</head><p>Five novice users participated in a pilot user study. Based on several given 2D face photographs, they were asked to craft corresponding face models using this system. Our results show that the users are able to craft personalized 3D face models on average within one minute. Fig. <ref type="figure" target="#fig_3">4</ref> shows several crafted results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>We present an intuitive prototyping system for crafting 3D facial avatars through 3D portrait pulling and image example-based painting. The limitations of our work are mainly connected with the used face dataset. For example, due to the shadow at our face dataset, some crafted faces are accordingly affected. We plan to improve this work from a number of research directions, for example, multiple face image examples for distinct facial region. Furthermore, to enhance the visual realism of crafted 3D faces, we plan to extend our system to interactively sketch and craft other parts of the face, e.g., hair, beard, glasses, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: This system consists of three main processes: an offline data processing, an interactive face crafting, and a facial texture generation.</figDesc><graphic coords="1,347.98,158.36,180.14,132.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: the 35 used landmarks (constraint points) with its region segmentation scheme. Right: a two-level face hierarchy.</figDesc><graphic coords="2,157.14,201.29,86.40,63.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Facial texture analogies. Left most: the portrait for a source image, second left: a source (user-provided) face image, second right: a target 3D portrait, right most: a textured target face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: given face photographs, middle: crafted 3D portraits with time usage at the bottom, right: textured 3D faces.</figDesc><graphic coords="2,378.58,198.64,119.43,100.43" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGGRAPH &apos;99</title>
		<meeting>of ACM SIGGRAPH &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An anthropometric face model using variational techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH &apos;98</title>
		<meeting>of SIGGRAPH &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Animating blendshape faces by cross mapping motion capture data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of I3D&apos;06</title>
		<meeting>of I3D&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Farkas</surname></persName>
		</author>
		<title level="m">Anthropometry of the Head and Face</title>
		<imprint>
			<publisher>Raven Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH &apos;01</title>
		<meeting>of SIGGRAPH &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved automatic caricature by feature normalization and exaggeration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2004 Sketches</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fibermesh: designing freeform surfaces with 3D curves</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive 3D facial expression posing through 2d portrait manipulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sucontphunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GI&apos;08: Proc. of Graphics Interface</title>
		<meeting><address><addrLine>Windsor, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Automatic Face and Gesture Recognition</title>
		<meeting>of Conf. on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometry-driven photorealistic facial expression synthesis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCA&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
