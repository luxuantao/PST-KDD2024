<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Invariant Feature Detectors: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
							<email>tinne.tuytelaars@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Katholieke Universiteit Leuven</orgName>
								<address>
									<addrLine>Kasteelpark Arenberg 10</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
							<email>k.mikolajczyk@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics and Physical Sciences</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<region>Surrey</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Invariant Feature Detectors: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7EC617CC908069BFCC9E09FF19210877</idno>
					<idno type="DOI">10.1561/0600000017</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this survey, we give an overview of invariant interest point detectors, how they evolved over time, how they work, and what their respective strengths and weaknesses are. We begin with defining the properties of the ideal local feature detector. This is followed by an overview of the literature over the past four decades organized in different categories of feature extraction methods. We then provide a more detailed analysis of a selection of methods which had a particularly significant impact on the research field. We conclude with a summary and promising future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Local Features in the Literature</head><p>In this section, we give an overview of local feature detectors proposed in the literature, starting from the early days of image processing and pattern recognition up to the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corner Detectors</head><p>A large number of corner detector methods have been proposed in the literature. To guide the reader in finding an approach suitable for a given application, representative methods have been selected based on the underlying extraction technique (e.g., based on image derivatives, morphology, or geometry), as well as based on the level of invariance (translations and rotations, scale or affine invariant). For each category, we describe the feature extraction process for some of the best performing and representative methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this section, we discuss the very nature of local (invariant) features. What do we mean with this term? What is the advantage of using local features? What can we do with them? What would the ideal local feature look like? These are some of the questions we attempt to answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">What are Local Features?</head><p>A local feature is an image pattern which differs from its immediate neighborhood. It is usually associated with a change of an image property or several properties simultaneously, although it is not necessarily localized exactly on this change. The image properties commonly considered are intensity, color, and texture. Figure <ref type="figure" target="#fig_9">1</ref>.1 shows some examples of local features in a contour image (left) as well as in a grayvalue image (right). Local features can be points, but also edgels or small image patches. Typically, some measurements are taken from a region centered on a local feature and converted into descriptors. The descriptors can then be used for various applications.  <ref type="bibr" target="#b19">[20]</ref> and an image example with interest points provided by a corner detector (cf. Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Why Local Features?</head><p>As discussed shortly in the preface, local (invariant) features are a powerful tool, that has been applied successfully in a wide range of systems and applications.</p><p>In the following, we distinguish three broad categories of feature detectors based on their possible usage. It is not exhaustive or the only way of categorizing the detectors but it emphasizes different properties required by the usage scenarios. First, one might be interested in a specific type of local features, as they may have a specific semantic interpretation in the limited context of a certain application. For instance, edges detected in aerial images often correspond to roads; blob detection can be used to identify impurities in some inspection task; etc. These were the first applications for which local feature detectors have been proposed. Second, one might be interested in local features since they provide a limited set of well localized and individually identifiable anchor points. What the features actually represent is not really relevant, as long as their location can be determined accurately and in a stable manner over time. This is for instance the situation in most matching or tracking applications, and especially for camera calibration or 3D reconstruction. Other application domains include pose estimation, image alignment or mosaicing. A typical example here are the features used in the KLT tracker <ref type="bibr" target="#b227">[228]</ref>. Finally, a set of local features can be used as a robust image representation, that allows to recognize objects or scenes without the need for segmentation. Here again, it does not really matter what the features actually represent. They do not even have to be localized precisely, since the goal is not to match them on an individual basis, but rather to analyze their statistics. This way of exploiting local features was first reported in the seminal work of <ref type="bibr" target="#b212">[213]</ref> and <ref type="bibr" target="#b209">[210]</ref> and soon became very popular, especially in the context of object recognition (both for specific objects as well as for category-level recognition). Other application domains include scene classification, texture analysis, image retrieval, and video mining.</p><p>Clearly, each of the above three categories imposes its own constraints, and a good feature for one application may be useless in the context of a different problem. These categories can be considered when searching for suitable feature detectors for an application at hand. In this survey, we mainly focus on the second and especially the third application scenario.</p><p>Finally, it is worth noting that the importance of local features has also been demonstrated in the context of object recognition by the human visual system <ref type="bibr" target="#b19">[20]</ref>. More precisely, experiments have shown that removing the corners from images impedes human recognition, while removing most of the straight edge information does not. This is illustrated in Figure <ref type="figure" target="#fig_9">1</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">A Few Notes on Terminology</head><p>Before we discuss feature detectors in more detail, let us explain some terminology commonly used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Detector or Extractor?</head><p>Traditionally, the term detector has been used to refer to the tool that extracts the features from the image, e.g., a corner, blob or edge detector. However, this only makes sense if it is a priori clear what the corners, blobs or edges in the image are, so one can speak of "false detections" or "missed detections." This only holds in the first usage 1.3 A Few Notes on Terminology 181 scenario mentioned earlier, not for the last two, where extractor would probably be semantically more correct. Still, the term detector is widely used. We therefore also stick to this terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Invariant or Covariant?</head><p>A similar discussion holds for the use of "invariant" or "covariant." A function is invariant under a certain family of transformations if its value does not change when a transformation from this family is applied to its argument. A function is covariant when it commutes with the transformation, i.e., applying the transformation to the argument of the function has the same effect as applying the transformation to the output of the function. A few examples may help to explain the difference. The area of a 2D surface is invariant under 2D rotations, since rotating a 2D surface does not make it any smaller or bigger. But the orientation of the major axis of inertia of the surface is covariant under the same family of transformations, since rotating a 2D surface will affect the orientation of its major axis in exactly the same way. Based on these definitions, it is clear that the so-called local scale and/or affine invariant features are in fact only covariant. The descriptors derived from them, on the other hand, are usually invariant, due to a normalization step. Since the term local invariant feature is so widely used, we nevertheless use "invariant" in this survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.3">Rotation Invariant or Isotropic?</head><p>A function is isotropic at a particular point if it behaves the same in all directions. This is a term that applies to, e.g., textures, and should not be confused with rotational invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.4">Interest Point, Region or Local Feature?</head><p>In a way, the ideal local feature would be a point as defined in geometry: having a location in space but no spatial extent. In practice however, images are discrete with the smallest spatial unit being a pixel and discretization effects playing an important role. To localize features in images, a local neighborhood of pixels needs to be analyzed, giving all local features some implicit spatial extent. For some applications (e.g., camera calibration or 3D reconstruction) this spatial extent is completely ignored in further processing, and only the location derived from the feature extraction process is used (with the location sometimes determined up to sub-pixel accuracy). In those cases, one typically uses the term interest point.</p><p>However, in most applications those features also need to be described, such that they can be identified and matched, and this again calls for a local neighborhood of pixels. Often, this neighborhood is taken equal to the neighborhood used to localize the feature, but this need not be the case. In this context, one typically uses the term region instead of interest point. However, beware: when a local neighborhood of pixels is used to describe an interest point, the feature extraction process has to determine not only the location of the interest point, but also the size and possibly the shape of this local neighborhood. Especially in case of geometric deformations, this significantly complicates the process, as the size and shape have to be determined in an invariant (covariant) way.</p><p>In this survey, we prefer the use of the term local feature, which can be either points, regions or even edge segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Properties of the Ideal Local Feature</head><p>Local features typically have a spatial extent, i.e., the local neighborhood of pixels mentioned above. In contrast to classical segmentation, this can be any subset of an image. The region boundaries do not have to correspond to changes in image appearance such as color or texture. Also, multiple regions may overlap, and "uninteresting" parts of the image such as homogeneous areas can remain uncovered.</p><p>Ideally, one would like such local features to correspond to semantically meaningful object parts. In practice, however, this is unfeasible, as this would require high-level interpretation of the scene content, which is not available at this early stage. Instead, detectors select local features directly based on the underlying intensity patterns.</p><p>Good features should have the following properties:</p><p>• Repeatability: Given two images of the same object or scene, taken under different viewing conditions, a high percentage of the features detected on the scene part visible in both images should be found in both images. • Distinctiveness/informativeness: The intensity patterns underlying the detected features should show a lot of variation, such that features can be distinguished and matched. • Locality: The features should be local, so as to reduce the probability of occlusion and to allow simple model approximations of the geometric and photometric deformations between two images taken under different viewing conditions (e.g., based on a local planarity assumption). • Quantity: The number of detected features should be sufficiently large, such that a reasonable number of features are detected even on small objects. However, the optimal number of features depends on the application. Ideally, the number of detected features should be adaptable over a large range by a simple and intuitive threshold. The density of features should reflect the information content of the image to provide a compact image representation. • Accuracy: The detected features should be accurately localized, both in image location, as with respect to scale and possibly shape. • Efficiency: Preferably, the detection of features in a new image should allow for time-critical applications.</p><p>Repeatability, arguably the most important property of all, can be achieved in two different ways: either by invariance or by robustness.</p><p>• Invariance: When large deformations are to be expected, the preferred approach is to model these mathematically if possible, and then develop methods for feature detection that are unaffected by these mathematical transformations. • Robustness: In case of relatively small deformations, it often suffices to make feature detection methods less sensitive to such deformations, i.e., the accuracy of the detection may decrease, but not drastically so. Typical deformations that are tackled using robustness are image noise, discretization effects, compression artifacts, blur, etc. Also geometric and photometric deviations from the mathematical model used to obtain invariance are often overcome by including more robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">Discussion</head><p>Clearly, the importance of these different properties depends on the actual application and settings, and compromises need to be made.</p><p>Repeatability is required in all application scenarios and it directly depends on the other properties like invariance, robustness, quantity etc. Depending on the application increasing or decreasing them may result in higher repeatability.</p><p>Distinctiveness and locality are competing properties and cannot be fulfilled simultaneously: the more local a feature, the less information is available in the underlying intensity pattern and the harder it becomes to match it correctly, especially in database applications where there are many candidate features to match to. On the other hand, in case of planar objects and/or purely rotating cameras (e.g., in image mosaicing applications), images are related by a global homography, and there are no problems with occlusions or depth discontinuities. Under these conditions, the size of the local features can be increased without problems, resulting in a higher distinctiveness.</p><p>Similarly, an increased level of invariance typically leads to a reduced distinctiveness, as some of the image measurements are used to lift the degrees of freedom of the transformation. A similar rule holds for robustness versus distinctiveness, as typically some information is disregarded (considered as noise) in order to achieve robustness. As a result, it is important to have a clear idea on the required level of invariance or robustness for a given application. It is hard to achieve high invariance and robustness at the same time and invariance, which is not adapted to the application, may have a negative impact on the results.</p><p>Accuracy is especially important in wide baseline matching, registration, and structure from motion applications, where precise correspondences are needed to, e.g., estimate the epipolar geometry or to calibrate the camera setup.</p><p>Quantity is particularly useful in some class-level object or scene recognition methods, where it is vital to densely cover the object of interest. On the other hand, a high number of features has in most cases a negative impact on the computation time and it should be kept within limits. Also robustness is essential for object class recognition, as it is impossible to model the intra-class variations mathematically, so full invariance is impossible. For these applications, an accurate localization is less important. The effect of inaccurate localization of a feature detector can be countered, up to some point, by having an extra robust descriptor, which yields a feature vector that is not affected by small localization errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Global versus Local Features</head><p>Local invariant features not only allow to find correspondences in spite of large changes in viewing conditions, occlusions, and image clutter (wide baseline matching), but also yield an interesting description of the image content for image retrieval and object or scene recognition tasks (both for specific objects as well as categories). To put this into context, we briefly summarize some alternative strategies to compute image representations including global features, image segments, and exhaustive and random sampling of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.1">Global Features</head><p>In the field of image retrieval, many global features have been proposed to describe the image content, with color histograms and variations thereof as a typical example <ref type="bibr" target="#b236">[237]</ref>. This approach works surprisingly well, at least for images with distinctive colors, as long as it is the overall composition of the image as a whole that the user is interested in, rather than the foreground object. Indeed, global features cannot distinguish foreground from background, and mix information from both parts together.</p><p>Global features have also been used for object recognition, resulting in the first appearance-based approaches to tackle this challenging problem. Turk and Pentland <ref type="bibr" target="#b244">[245]</ref> and later Murase and Nayar <ref type="bibr" target="#b159">[160]</ref> proposed to compute a principal component analysis of a set of model images and to use the projections onto the first few principal components as descriptors. Compared to the purely geometry-based approaches tried before, the results of the novel appearance-based approach were striking. A whole new range of natural objects could suddenly be recognized. However, being based on a global description, image clutter and occlusions again form a major problem, limiting the usefulness of the system to cases with clean backgrounds or where the object can be segmented out, e.g., relying on motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.2">Image Segments</head><p>An approach to overcome the limitations of the global features is to segment the image in a limited number of regions or segments, with each such region corresponding to a single object or part thereof. The best known example of this approach is the blobworld system, proposed in <ref type="bibr" target="#b30">[31]</ref>, which segments the image based on color and texture, then searches a database for images with similar "image blobs." An example based on texture segmentation is the wide baseline matching work described in <ref type="bibr" target="#b207">[208]</ref>.</p><p>However, this raises a chicken-and-egg problem as image segmentation is a very challenging task in itself, which in general requires a high-level understanding of the image content. For generic objects, color and texture cues are insufficient to obtain meaningful segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.3">Sampled Features</head><p>A way to deal with the problems encountered with global features or image segmentations, is to exhaustively sample different subparts of the image at each location and scale. For each such image subpart, global features can then be computed. This approach is also referred to as a sliding window based approach. It has been especially popular in the context of face detection, but has also been applied for the recognition of specific objects or particular object classes such as pedestrians or cars.</p><p>By focusing on subparts of the image, these methods are able to find similarities between the queries and the models in spite of changing backgrounds, and even if the object covers only a small percentage of the total image area. On the downside, they still do not manage to cope with partial occlusions, and the allowed shape variability is smaller than what is feasible with a local features based approach. However, by far the biggest drawback is the inefficiency of this approach. Each and every subpart of the image must be analyzed, resulting in thousands or even millions of features per image. This requires extremely efficient methods which significantly limits the scope of possible applications.</p><p>To overcome the complexity problems more sparse fixed grid sampling of image patches was used (e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b245">246,</ref><ref type="bibr" target="#b256">257]</ref>). It is however difficult to achieve invariance to geometric deformations for such features. The approach can tolerate some deformations due to dense sampling over possible locations, scales, poses etc. 00, but the individual features are not invariant. An example of such approach are multi-scale interest points. As a result, they cannot be used when the goal is to find precise correspondences between images. However, for some applications such as scene classification or texture recognition, they may well be sufficient. In <ref type="bibr" target="#b61">[62]</ref>, better results are reported with a fixed grid of patches than with patches centered on interest points, in the context of scene classification work. This can be explained by the dense coverage, as well as the fact that homogeneous areas (e.g., sky) are also taken into account in the fixed grid approach which makes the representation more complete. This dense coverage is also exploited in <ref type="bibr" target="#b65">[66]</ref>, where a fixed grid of patches was used on top of a set of local invariant features in the context of specific object recognition, where the latter supply an initial set of correspondences, which then guide the construction of correspondences for the former.</p><p>In a similar vein, rather than using a fixed grid of patches, a random sampling of image patches can also be used (e.g., <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b131">132,</ref><ref type="bibr" target="#b168">169]</ref>). This gives a larger flexibility in the number of patches, the range of scales or shapes, and their spatial distribution. Good scene recognition results are shown in <ref type="bibr" target="#b131">[132]</ref> based on random image patches. As in the case of fixed grid sampling, this can be explained by the dense coverage which ignores the localization properties of features. Random patches are in fact a subset of the dense patches, and are used mostly to reduce the complexity. Their repeatability is poor hence they work better as an addition to the regular features rather than as a stand alone method.</p><p>Finally, to overcome the complexity problems while still providing a large number of features with better than random localization <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b145">146]</ref> proposed to sample features uniformly from edges. This proved useful for dealing with wiry objects well represented by edges and curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Overview of this Survey</head><p>This survey article consists of two parts. First, in Section 2, we review local invariant feature detectors in the literature, from the early days in computer vision up to the most recent evolutions. Next, we describe a few selected, representative methods in more detail. We have structured the methods in a relatively intuitive manner, based on the type of feature extracted in the image. Doing so, we distinguish between corner detectors (Section 3), blob detectors (Section 4), and region detectors (Section 5). Additionally, we added a section on various detectors that have been designed in a computationally efficient manner (Section 6). With this structure, we hope the reader can easily find the type of detector most useful for his/her application. We conclude the survey with a qualitative comparison of the different methods and a discussion of future work (Section 7).</p><p>To the novice reader, who is not very familiar with local invariant feature detectors yet, we advice to skip Section 2 at first. This section has been added mainly for the more advanced reader, to give further insight in how this field evolved and what were the most important trends and to add pointers to earlier work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction</head><p>The literature on local feature detection is vast and goes back as far as 1954, when it was first observed by Attneave <ref type="bibr" target="#b5">[6]</ref> that information on shape is concentrated at dominant points having high curvature. It is impossible to describe each and every contribution to over 50 years of research in detail. Instead, we provide pointers to the literature where the interested reader can find out more. The main goal of this section is to make the reader aware of the various great ideas that have been proposed, especially in the pre-internet era. All too often, these are overlooked and then re-invented. We would like to give proper credit to all those researchers who contributed to the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Early Work on Local Features</head><p>It is important to mention the beginnings of this research area and the first publications which appeared after the observation on the importance of corners and junctions in visual recognition <ref type="bibr" target="#b5">[6]</ref> (see Figure <ref type="figure" target="#fig_9">1</ref>.1). Since then a large number of algorithms have been suggested for extracting interest points at the extrema of various functions computed on the digital shape. Also, it has been understood early on in the image processing and visual pattern recognition field that intersections of straight lines and straight corners are strong indications of man made structures. Such features have been used in a first series of applications from line drawing images <ref type="bibr" target="#b71">[72]</ref> and photomosaics <ref type="bibr" target="#b148">[149]</ref>. First monographs on digital image processing by Rosenfeld <ref type="bibr" target="#b190">[191]</ref> and by <ref type="bibr">Duda and Hart [58]</ref> as well as their later editions served to establish the field on a sound theoretical foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Overview</head><p>We identified a number of important research directions and structured the subsections of this section accordingly. First, many authors have studied the curvature of contours to find corners. Their work is described in Section 2.2. Others directly analyze the image intensities, e.g., based on derivatives or regions with high variance. This is the topic of Section 2.3. Another line of research has been inspired by the human visual system and aims at reproducing the processes in the human brain -see Section 2.4. Methods focussing on the exploitation of color information are discussed in Section 2.5, while Section 2.6 describes model-based approaches. More recently, there has been a trend toward feature detection with invariance against various geometric transformations, including multi-scale approaches and scale or affine invariant methods. These are discussed in Section 2.7. In Section 2.8, we focus on segmentation-based methods and Section 2.9 describes methods which build on machine learning techniques. Finally, Section 2.10 gives an overview of different evaluation and comparison schemes proposed in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contour Curvature Based Methods</head><p>A first category of interest point detectors are the contour curvature based methods. Originally, these were mainly applied to line drawings, piecewise constant regions, and cad-cam images rather than natural scenes. The focus was especially on the accuracy of point localization. They were most popular of the end of the 1970s and most of the 1980s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">High Curvature Points</head><p>Contour intersections and junctions often result in bi-directional signal changes. Therefore, a good strategy to detect features consists of extracting points along the contour with high curvature. Curvature of an analog curve is defined as the rate at which the unit tangent vector changes with respect to arc length. Contours are often encoded in chains of points or represented in a parametric form using splines.</p><p>Several techniques have been developed which involve detecting and chaining edges so as to find corners in the chain by analyzing the chain code <ref type="bibr" target="#b204">[205]</ref>, finding maxima of curvature <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b151">152]</ref>, change in direction <ref type="bibr" target="#b82">[83]</ref>, or change in appearance <ref type="bibr" target="#b41">[42]</ref>. Others avoid chaining edges and instead look for maxima of curvature <ref type="bibr" target="#b253">[254]</ref> or change in direction <ref type="bibr" target="#b103">[104]</ref> at places where the gradient is large.</p><p>Several methods for detecting edges based on gray-level gradient and angular changes in digital curves were proposed in <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b194">195,</ref><ref type="bibr" target="#b195">196,</ref><ref type="bibr" target="#b196">197]</ref>. Other solutions for line-drawing images include methods for detecting corners in a chain-coded plane curve <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref>. In these works, a measure for the cornerness of a point is based on mean angular differences between successive segment positions along the chain.</p><p>One general approach to feature extraction is to detect the dominant points directly through angle or corner detection, using various schemes for approximating discrete curvature such as cosine <ref type="bibr" target="#b191">[192,</ref><ref type="bibr" target="#b192">193]</ref> or local curvature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b73">74]</ref> which define corners as discontinuities of an average curve slope. Other parametric representation like B-splines curves are commonly used in rendering a curve in computer graphics, compression and coding, CAD-CAM systems, and also for curve fitting and shape description <ref type="bibr" target="#b174">[175]</ref>. In <ref type="bibr" target="#b107">[108]</ref>, cubic polynomials are fit to a curve and discontinuities are detected in such curve to localize interest points. Spline approximations of line images are used in <ref type="bibr" target="#b84">[85]</ref> in combination with a dynamic programming technique to find the knots of a spline. Pseudo coding of line figures and a complicated vector finder to obtain interest points are proposed in <ref type="bibr" target="#b163">[164]</ref>.</p><p>In <ref type="bibr" target="#b206">[207]</ref>, dominant points are computed at the maximum global curvature, based on the iterative averaging of local discretized curvature at each point with respect to its immediate neighbors. In <ref type="bibr" target="#b2">[3]</ref>, tangential deflection and curvature of discrete curves are defined based on the geometrical and statistical properties associated with the eigenvalueeigenvector structure of sample covariance matrices computed on chaincodes.</p><p>Another approach is to obtain a piecewise linear polygonal approximation of the digital curve subject to certain constraints on the quality of fit <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b175">176]</ref>. Indeed, it has been pointed out in <ref type="bibr" target="#b173">[174]</ref> that piecewise linear polygonal approximation with variable breakpoints will tend to locate vertices at actual corner points. These points correspond approximately to the actual or extrapolated intersections of adjacent line segments of the polygons. A similar idea was explored in <ref type="bibr" target="#b90">[91]</ref>. More recently, <ref type="bibr" target="#b94">[95]</ref> estimates the parameters of two lines fitted to the two segments neighboring to the corner point. A corner is declared if the parameters are statistically significantly different. A similar approach is to identify edge crossings and junctions <ref type="bibr" target="#b18">[19]</ref> by following image gradient maxima or minima and finding gaps in edge maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Dealing with Scale</head><p>Corner detection methods by curvature estimation normally use a set of parameters to eliminate contour noise and to obtain the corners at a given scale, although object corners can be found at multiple natural scales. To solve this problem, some detectors apply their algorithms iteratively within a certain range of parameters, selecting points which appear in a fixed set of iterations. The stability of the points and the time spent for their detection is closely related to the number of iterations.</p><p>Initial attempts to deal with discretization and scale problems via an averaging scheme can be found in <ref type="bibr" target="#b206">[207]</ref>. The curvature primal sketch (CPS) proposed in <ref type="bibr" target="#b4">[5]</ref> is a scale-space representation of significant changes in curvature along contours. The changes are classified as basic or compound primitives such as corners, smooth joints, ends, cranks, bumps, and dents. The features are detected at different scales, resulting in a multiple-scale representation of object contours. A similar idea was explored in <ref type="bibr" target="#b150">[151,</ref><ref type="bibr" target="#b151">152]</ref> and later in <ref type="bibr" target="#b85">[86]</ref>, where the curvature scale space analysis was performed to find the local scale of curves. They find inflection points of the curves and represent shapes in parametric forms. A B-spline based algorithm was also proposed in <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b135">136]</ref>. The general idea is to fit a B-Spline to the curve, then to measure the curvature around each point directly from the B-spline coefficients.</p><p>Another algorithm <ref type="bibr" target="#b237">[238]</ref> dealing with scale for detecting dominant points on a digital closed curve is motivated by the angle detection procedure from <ref type="bibr" target="#b192">[193]</ref>. They indicate that the detection of dominant points relies primarily on the precise determination of the region of support rather than on the estimation of discrete curvature. First, the region of support for each point based on its local properties is determined. Then a measure of relative curvature <ref type="bibr" target="#b237">[238]</ref> or local symmetry <ref type="bibr" target="#b169">[170]</ref> of each point is computed. The Gaussian filter is the most commonly used filter in point detection. However, if the scale of a Gaussian filter is too small, the result may include some redundant points which are unnecessary details, i.e., due to noise. If the scale is too large, the points with small support regions will tend to be smoothed out. To solve the problems existing in Gaussian filtering with fixed scale, scale-space procedures based on multiple-scale discrete curvature representation and searching are proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b180">181]</ref>. The scheme is based on a stability criterion that states that the presence of a corner must concur with a curvature maximum observable at a majority of scales. Natural scales of curves were studied in <ref type="bibr" target="#b198">[199]</ref> to avoid exhaustive representation of curves over a full range of scales. A successful scale selection mechanism for Gaussian filters with a theoretical formulation was also proposed in <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b119">120]</ref>.</p><p>In <ref type="bibr" target="#b263">[264]</ref> a nonlinear algorithm for critical point detection is presented. They establish a set of criteria for the design of a point detection algorithm to overcome the problems arising from curvature approximation and Gaussian filtering. Another approach to boundary smoothing is based on simulated annealing for curvature estimation <ref type="bibr" target="#b232">[233]</ref>. In <ref type="bibr" target="#b151">[152]</ref> the corner points are localized at the maxima of absolute curvature of edges. The corner points are tracked through multiple curvature scale levels to improve localization. Chang and Horng <ref type="bibr" target="#b32">[33]</ref> proposed an algorithm to detect corner points using a nest moving average filter is investigated in <ref type="bibr" target="#b32">[33]</ref>. Corners are detected on curves by computing the difference of blurred images and observing the shift of high curvature points. More detailed analysis of various methods for determining natural scales of curves can be found in <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b198">199,</ref><ref type="bibr" target="#b199">200]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Discussion</head><p>Although theoretically well founded for analog curves, the contour curvature calculation is less robust in case of discrete curves <ref type="bibr" target="#b193">[194,</ref><ref type="bibr" target="#b237">238]</ref>. Possible error sources in digital curvature estimation were investigated in <ref type="bibr" target="#b258">[259]</ref>.</p><p>Furthermore, the objectives for the above discussed detectors were different than the ones we typically have nowadays. It was considered disadvantageous if a method detected corners on circular shapes, multiple corners at junctions etc. At that time, a much stricter definition of interest points/corners was used, with only points corresponding to true corners in 3D being considered as relevant. Nowadays, in most practical applications of interest points, the focus is on robust, stable, and distinctive points, irrespective of whether they correspond to true corners or not (see also our earlier discussion in Section 1.2).</p><p>There has been less activity in this area recently (over the past ten years), due to complexity and robustness problems, while methods based directly on image intensity attracted more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Intensity Based Methods</head><p>Methods based on image intensity have only weak assumptions and are typically applicable to a wide range of images. Many of these approaches are based on first-and second-order gray-value derivatives, while others use heuristics to find regions of high variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Differential Approaches</head><p>Hessian-based approaches. One of the early intensity based detectors is the rotation invariant Hessian-based detector proposed by Beaudet <ref type="bibr" target="#b15">[16]</ref>. It explores the second-order Taylor expansion of the intensity surface, and especially the Hessian matrix (containing the second order derivatives). The determinant of this matrix reaches a maximum for blob-like structures in the image. A more detailed description of this method can be found in Section 4.1. It has been extended in <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b265">[266]</ref>, where the interest points are localized at the zero crossing of a curve joining local extrema of the Hessian determinant around a corner.</p><p>Similarly, high curvature points can be localized by computing Gaussian curvature of the image surface, i.e., saddle points in image brightness. In <ref type="bibr" target="#b103">[104]</ref>, a local quadratic surface was fit to the image intensity function. The parameters of the surface were used to determine the gradient magnitude and the rate of change of gradient direction. The resulting detector uses the curvature of isophotes computed from firstand second-order derivatives scaled by image gradient to make it more robust to noise. A similar idea was proposed in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b228">229]</ref>.</p><p>A detailed investigation in <ref type="bibr" target="#b167">[168,</ref><ref type="bibr" target="#b166">167,</ref><ref type="bibr" target="#b223">224]</ref> and later in <ref type="bibr" target="#b82">[83]</ref> shows that the detectors of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b162">163,</ref><ref type="bibr" target="#b265">266]</ref> all perform the same measurements on the image and have relatively low reliability according to criteria based on localization precision. Nevertheless, the trace and determinant of the Hessian matrix were successfully used later on in scale and affine invariant extensions of interest point detectors <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b142">143]</ref> when other feature properties became more important.</p><p>Gradient-based approaches. Local feature detection based on firstorder derivatives is also used in various applications. A corner detector which returns points at the local maxima of a directional variance measure was first introduced in <ref type="bibr" target="#b153">[154,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b155">156]</ref> in the context of mobile robot navigation. It was a heuristic implementation of the auto-correlation function also explored in <ref type="bibr" target="#b40">[41]</ref>. The proposed corner detector investigates a local window in the image and determines the average change of intensity which results from shifting the window by a few pixels in various directions. This idea is taken further in <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref> and formalized by using first-order derivatives in a so-called second moment matrix to explore local statistics of directional image intensity variations. The method separates corner candidate detection and localization to improve the accuracy to subpixel precision, at the cost of higher computational complexity. Harris and Stephens <ref type="bibr" target="#b83">[84]</ref> improved the approach by Moravec <ref type="bibr" target="#b154">[155]</ref> by performing analytical expansion of the average intensity variance. This results in a second moment matrix computed with Sobel derivatives and a Gaussian window. A function based on the determinant and trace of that matrix was introduced which took into account both eigenvalues of the matrix. This detector is widely known today as the Harris detector or Plessey detector, <ref type="foot" target="#foot_0">1</ref> and is probably the best known interest point detector around. It is described in more detail in Section 3.2. It has been extended in numerous papers, e.g., by using Gaussian derivatives <ref type="bibr" target="#b211">[212]</ref>, combinations of first-and secondorder derivatives <ref type="bibr" target="#b262">[263]</ref>, or an edge based second moment matrix <ref type="bibr" target="#b44">[45]</ref> but the underlying idea remains the same.</p><p>The Harris detector was also investigated in <ref type="bibr" target="#b166">[167]</ref> and demonstrated to be optimal for L junctions. Based on the assumption of an affine image deformation, an analysis in <ref type="bibr" target="#b227">[228]</ref> led to the conclusion that it is more convenient to use the smallest eigenvalue of the autocorrelation matrix as the corner strength function.</p><p>More recently, the second moment matrix has also been adopted to scale changes <ref type="bibr" target="#b58">[59]</ref> by parameterizing Gaussian filters and normalizing them with respect to scale, based on scale-space theory <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b116">117]</ref>. Also, the Harris detector was extended with search over scale and affine space in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b208">209]</ref>, using the Laplacian operator and eigenvalues of the second moment matrix, inspired by the pioneering work of Lindeberg <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b117">118]</ref> (see Section 3.4 for details).</p><p>The approach from <ref type="bibr" target="#b262">[263]</ref> performs an analysis of the computation of the second moment matrix and its approximations. A speed increase is achieved by computing only two smoothed images, instead of the three previously required. A number of other suggestions have been made for how to compute the corner strength from the second-order matrix <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b166">167,</ref><ref type="bibr" target="#b227">228]</ref>, and these have all been shown to be equivalent to various matrix norms <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b264">265]</ref>. A generalization to images with multi-dimensional pixels was also proposed in <ref type="bibr" target="#b101">[102]</ref>.</p><p>In <ref type="bibr" target="#b241">[242]</ref>, the Harris corner detector is extended to yield stable features under more general transformations than pure translations. To this end, the auto-correlation function was studied under rotations, scalings, up to full affine transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Intensity Variations</head><p>A different category of approaches based on intensity variations applies mathematical morphology to extract high curvature points. The use of zero-crossings of the shape boundary curvature in binary images, detected with a morphological opening operator was investigated in <ref type="bibr" target="#b35">[36]</ref>. Mathematical morphology was also used to extract convex and concave points from edges in <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b167">168]</ref>. Later on a parallel algorithm based on an analysis of morphological residues and corner characteristics was proposed in <ref type="bibr" target="#b261">[262]</ref>.</p><p>Another approach <ref type="bibr" target="#b172">[173]</ref> indicates that for interest points the median value over a small neighborhood is significantly different from the corner point value. Thus the difference in intensity between the center and median gives a strong indication for corners. However, this method cannot deal with more complex junctions or smooth edges.</p><p>A simple and efficient detector named SUSAN was introduced in <ref type="bibr" target="#b231">[232]</ref> based on earlier work from <ref type="bibr" target="#b81">[82]</ref>. It computes the fraction of pixels within a neighborhood which have similar intensity to the center pixel. Corners can then be localized by thresholding this measure and selecting local minima. The position of the center of gravity is used to filter out false positives. More details on the SUSAN detector can be found in Section 3.3. A similar idea was explored in <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b239">240]</ref> where pixels on a circle are considered and compared to the center of a patch.</p><p>More recently, <ref type="bibr" target="#b202">[203]</ref> proposed the FAST detector. A point is classified as a corner if one can find a sufficiently large set of pixels on a circle of fixed radius around the point such that these pixels are all significantly brighter (resp. darker) than the central point. Efficient classification is based on a decision tree. More details on FAST can be found in Section 6.3.</p><p>Local radial symmetry has been explored in <ref type="bibr" target="#b126">[127]</ref> to identify interest points and its real-time implementation was also proposed. Wavelet transformation was also investigated in the context of feature point extraction with successful results based on multi-resolution analysis in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b217">218]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Saliency</head><p>The idea of saliency has been used in a number of computer vision algorithms. The early approach of using edge detectors to extract object descriptions embodies the idea that the edges are more significant than other parts of the image. More explicit uses of saliency can be divided into those that concentrate on low-level local features (e.g., <ref type="bibr" target="#b214">[215]</ref>), and those that compute salient groupings of low-level features (e.g., <ref type="bibr" target="#b222">[223]</ref>); though some approaches operate at both levels (e.g., <ref type="bibr" target="#b146">[147]</ref>).</p><p>The technique suggested in <ref type="bibr" target="#b210">[211]</ref>, is based on the maximization of descriptor vectors across a particular image. These salient points are the points on the object which are almost unique. Hence they maximize the discrimination between the objects. A related method <ref type="bibr" target="#b252">[253]</ref> identifies salient features for use in automated generation of Statistical Shape/Appearance Models. The method aims to select those features which are less likely to be mismatched. Regions of low density in a multidimensional feature space, generated from the image, are classified as highly salient.</p><p>A more theoretically founded approach based on variability or complexity of image intensity within a region was proposed in <ref type="bibr" target="#b78">[79]</ref>. It was motivated by visual saliency and information content, which we revise in the next section. The method from <ref type="bibr" target="#b78">[79]</ref> defines saliency in terms of local signal complexity or unpredictability; more specifically the use of Shannon entropy of local attributes is suggested. The idea is to find a point neighborhood with high complexity as a measure of saliency or information content. The method measures the change in entropy of a gray-value histogram computed in a point neighborhood. The search was extended to scale <ref type="bibr" target="#b97">[98]</ref> and affine <ref type="bibr" target="#b98">[99]</ref> parameterized regions, thus providing position, scale, and affine shape of the region neighborhood. For a detailed discussion, we refer to Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Biologically Plausible Methods</head><p>Most systems proposed in the previous sections were mainly concerned with the accuracy of interest point localization. This is important in the context of fitting parametric curves to control points or image matching for recovering the geometry. In contrast, the biologically plausible methods reviewed in this section were mainly proposed in the context of artificial intelligence and visual recognition. Most of them did not have a specific application purpose and their main goal was to model the processes of the human brain. Numerous models of human visual attention or saliency have been discussed in Cognitive Psychology and Computer Vision. However, the vast majority were only of theoretical interest and only few were implemented and tested on real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Feature Detection as Part of the Pre-attentive Stage</head><p>One of the main models for early vision in humans, attributed to Neisser <ref type="bibr" target="#b164">[165]</ref>, is that it consists of a pre-attentive and an attentive stage. Biologically plausible methods for feature detection usually refer to the idea that certain parts of a scene are pre-attentively distinctive and create some form of immediate response within the early stages of the human visual system. In the pre-attentive stage, only "pop-out" features are detected. These are local regions of the image which present some form of spatial discontinuity. In the attentive stage, relationships between these features are found, and grouping takes place. This model has widely influenced the computer vision community (mainly through the work of Marr <ref type="bibr" target="#b132">[133]</ref>) and is reflected in the classical computer vision approach -feature detection and perceptual grouping, followed by model matching and correspondence search. Activities in the models of attention started in the mid-1980s following progress in neurophysiological and psychological research.</p><p>One approach inspired by neuro-biological mechanisms was proposed in <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b197">198]</ref>. They apply Gabor like filters to compute local energy of the signal. Maxima of the first-and second-order derivatives of that energy indicate the presence of interest points. The idea of using Gabor filter responses from different scales was further explored in <ref type="bibr" target="#b130">[131,</ref><ref type="bibr" target="#b185">186]</ref>. The approach developed in <ref type="bibr" target="#b181">[182]</ref> was motivated by psychophysical experiments. They compute a symmetry score of the signal at each image pixel in different directions. Regions with significant symmetry are then selected as interest points.</p><p>Theory on texture recognition and the idea of textons as simple local structures like blobs, corners, junctions, line ends etc. was introduced in <ref type="bibr" target="#b95">[96]</ref>. He suggested that statistics over texton distributions play an important role in recognition. The extraction of simple textons is done in the pre-attentive stage and the construction of relations in the attentive stage. A feature integration theory based on these principles was proposed in <ref type="bibr" target="#b240">[241]</ref>. He distinguished between a disjunctive case where the distinctive features can be directly localized in a feature map and a conjunctive case where the feature can be extracted only by processing various feature maps simultaneously. This model was implemented by combining bottom up and top down measures of interest <ref type="bibr" target="#b31">[32]</ref>. The bottom up method merges various feature maps and looks for interesting events, while in the top down process, knowledge about the target is exploited.</p><p>The main goal of the above systems was to provide computationally plausible models of visual attention. Their interest was mainly theoretical. However, those systems served as source of inspiration for practical solutions for real images once machine learning techniques like neural networks had grown mature enough. In <ref type="bibr" target="#b205">[206]</ref>, image processing operators were combined with the attentive models to make it applicable to more realistic images. He applies a Laplacian-of-Gaussians (LoG) like operator to feature maps to model the receptive fields and enhance the interesting events. The image was analyzed at multiple scales. The approach from <ref type="bibr" target="#b77">[78]</ref> uses a set of feature templates and correlates them with the image to produce feature maps which are then enhanced with LoG. Temporal derivatives were used to detect moving objects.</p><p>Koch and Ullman <ref type="bibr" target="#b104">[105]</ref> proposed a very influential computational model of visual attention which accounts for several psychophysical phenomena. They proposed to build a set of maps based on orientation, color, disparity and motion, and to simulate the lateral inhibition mechanism by extracting locations which differ significantly from their neighborhood. Information from different maps is then merged into a single saliency map. A winner-take-all (WTA) network was used to select the active location in the maps in a hierarchical manner using a pyramidal strategy. The hypotheses suggested in <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b240">241]</ref> were first implemented in <ref type="bibr" target="#b33">[34]</ref>. A similar implementation of the WTA model was proposed in <ref type="bibr" target="#b48">[49]</ref>.</p><p>The extraction of globally salient structures like object outlines was investigated in <ref type="bibr" target="#b222">[223]</ref> by grouping local information such as contour fragments but no relation to pre-attentive vision was claimed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Non-uniform Resolution and Coarse-To-Fine Processing</head><p>Also non-uniform resolution of the retina and coarse-to-fine processing strategies have been studied in biologically plausible models. These have been simulated mostly via scale-space techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b186">187,</ref><ref type="bibr" target="#b254">255]</ref>. However, these systems were mostly focused on the engineering and realtime aspects rather than its biological plausibility. One of the first systems to perform interest point detection in scale-space was proposed in <ref type="bibr" target="#b26">[27]</ref>. They built a Laplacian pyramid for coarse-to-fine feature selection. Templates were used to localize the objects in the LoG space. Templates were also employed for building features maps which were then combined by a weighted sum <ref type="bibr" target="#b38">[39]</ref>. Difference-of-Gaussians (DoG) filters were used in the system designed in <ref type="bibr" target="#b75">[76]</ref> to accelerate the computation.</p><p>Biologically inspired systems developed in <ref type="bibr" target="#b80">[81]</ref> explored the idea of using boundary and interest point detectors based on DoG filters as well as directional differences of offset Gaussians (DOOG) to simulate simple cells in V1.</p><p>The system proposed in <ref type="bibr" target="#b129">[130]</ref> was mainly concerned with classification of textures studied earlier in <ref type="bibr" target="#b95">[96]</ref>. The feature extraction part used a bank of filters based on oriented kernels (DoG and DOOG) to produce feature maps similar to <ref type="bibr" target="#b80">[81]</ref>. The next stage corresponds to a WTA mechanism to suppress weak responses and simulate lateral inhibition. Finally, all the responses are merged to detect texture boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Spatial Event Detection</head><p>Robust statistics have also been used to detect outliers in a set of image primitives. The idea is based on the observation that textures can be represented by their statistics and the locations which violate those statistics represent interesting events. For example, texture primitives are represented by a number of attributes using histograms and RANSAC in <ref type="bibr" target="#b147">[148]</ref>.</p><p>First order statistics over feature maps computed from zero crossings of DoG at different scales are used in <ref type="bibr" target="#b22">[23]</ref>. For each point, a histogram of gradient orientations is then constructed, and the local histograms are combined into a global one, which is similar in spirit to the more recent SIFT descriptor <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b125">126]</ref>. Local histograms are then compared with the global one to provide a measure of interest.</p><p>Another statistical model was proposed in <ref type="bibr" target="#b171">[172]</ref>. They measure the edge density at a range of distances from the interest point to build an edge distribution histogram. This idea has been used later in the shape context descriptor of <ref type="bibr" target="#b16">[17]</ref>.</p><p>Cells that respond only to edges and bars which terminate within their receptive field have first been found in <ref type="bibr" target="#b91">[92]</ref>. A corner detection algorithm based on a model for such end-stopped cells in the visual cortex was presented in <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b259">260]</ref>. Furthermore, the notion of end-stopped cells was generalized to color channels in a biologically plausible way based on color opponent processes <ref type="bibr" target="#b259">[260]</ref>.</p><p>A more recent visual attention system also motivated by the early primate visual system, is presented in <ref type="bibr" target="#b93">[94]</ref>. Multiscale image features detected at local spatial discontinuities in intensity, color, and orientation are combined into a single topographical saliency map and a neural network selects locations depending on the saliency.</p><p>Other recent visual recognition systems inspired by a model of visual cortex V1 which follow models from <ref type="bibr" target="#b184">[185]</ref> can be found in <ref type="bibr" target="#b161">[162,</ref><ref type="bibr" target="#b220">221,</ref><ref type="bibr" target="#b221">222]</ref>. These methods attempt to implement simple and complex cells </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Color-based Methods</head><p>Color provides additional information which can be used in the process of feature extraction. Several biologically plausible methods reviewed in the previous section use color for building saliency maps <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b259">260]</ref>.</p><p>Given the high performance of Harris corners <ref type="bibr" target="#b83">[84]</ref>, a straightforward extension of the second moment matrix to RGB color space was introduced in <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b152">153]</ref>, incorporating color information in the Harris corner extraction process.</p><p>Salient point detection based on color distinctiveness has been proposed in <ref type="bibr" target="#b249">[250]</ref>. Salient points are the maxima of the saliency map, which represents distinctiveness of color derivatives in a point neighborhood. In related work <ref type="bibr" target="#b216">[217]</ref> they argue that the distinctiveness of color-based salient points is much higher than for the intensity ones. Color ratios between neighboring pixels are used to obtain derivatives independent of illumination, which results in color interest points that are more robust to illumination changes.</p><p>Most of the proposed approaches based on color are simple extensions of methods based on the intensity change. Color gradients are usually used to enhance or to validate the intensity change so as to increase the stability of the feature detectors but the pixel intensities remain the main source of information for feature detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Model-based Methods</head><p>There have been a few attempts to do an analytical study of corner detection by giving a formal representation of corner points in an image based on differential geometry techniques <ref type="bibr" target="#b81">[82]</ref> or contour curvature <ref type="bibr" target="#b52">[53]</ref>. For instance, it was found that a gray-level corner point can be found as the point of maximal planar curvature on the line of the steepest graylevel slope <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b187">188]</ref>. An analytical expression for an optimal function whose convolution with an image has significant values at corner points was investigated in <ref type="bibr" target="#b179">[180]</ref>.</p><p>The methods presented in <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b200">201]</ref> assume that a corner resembles a blurred wedge, and finds the characteristics of the wedge (the amplitude, angle, and blur) by fitting it to the local image. Several models of junctions of multiple edges were used in <ref type="bibr" target="#b187">[188]</ref>. The assumption is that the junctions are formed by homogeneous regions. Parameterized masks are used to fit the intensity structure including position, orientation, intensity, blurring, and edges. The residual is then minimized during the detection. The accuracy is high provided a good initialization of the parameters. The efficiency of the approach in <ref type="bibr" target="#b187">[188]</ref> was improved in <ref type="bibr" target="#b51">[52]</ref> by using a different blurring function and a method to initialize the parameters. Fitting a corner model to image data was also considered in <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b170">171]</ref>. For each possible intersection of lines a template was constructed based on the angle, orientation, and scale of the hypothesized corner. The template was then matched to the image in a small neighborhood of the interest point to verify the model. A templatebased method for locating the saddle-points was also described in <ref type="bibr" target="#b127">[128]</ref>, where the corner points correspond to the intersections of saddle-ridge and saddle-valley structures.</p><p>A set of fuzzy patterns of contour points were established in <ref type="bibr" target="#b112">[113]</ref> and the corner detection was characterized as a fuzzy classification problem of the patterns.</p><p>Other model-based methods, aimed at improving the detection accuracy of the Hessian-based corner detector <ref type="bibr" target="#b15">[16]</ref>, were proposed in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b265">266]</ref>. To this end, the responses of the corner detector on a theoretical model over scale-space were analyzed. It was observed that the operator responses at different scales move along the bisector line. It is worth to note that this observation is also valid for the popular Harris corner detector <ref type="bibr" target="#b83">[84]</ref>. The exact position of the corner was then computed from two responses indicating the bisector and its intersection with the zero-crossing of the Laplacian response. An affine transformation was also used to fit a model of a corner to an image <ref type="bibr" target="#b21">[22]</ref>.</p><p>A different model-based approach is proposed in <ref type="bibr" target="#b76">[77]</ref>. For each type of feature, a parametric model is developed to characterize the local intensity in an image. Projections of intensity profile onto a set of orthogonal Zernike moment-generating polynomials are used to estimate model-parameters and generate the feature map.</p><p>An interesting technique is to find corners by fitting a parameterized model with the Generalized Hough transform <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b225">226]</ref>. In images with extracted edges two lines appear in a parameter space for each corner and the peak occurs at the crossover. Real corner models in the form of templates were considered in <ref type="bibr" target="#b228">[229]</ref>. A similarity measure and several alternative matching schemes were applied. Detection and localization accuracy was improved by merging the output of the different matching techniques.</p><p>In general, only relatively simple feature models were considered in the above methods and the generalization to images other than polygonal is not obvious. The complexity is also a major drawback in such approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Toward Viewpoint Invariant Methods</head><p>Most of the detectors described so far extract features at a single scale, determined by the internal parameters of the detector. At the end of the 1990s, as local features were more and more used in the context of wide baseline matching and object recognition, there was a growing need for features that could cope with scale changes or even more general viewpoint changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1">Multi-Scale Methods</head><p>Most of the detectors described so far extract features at a single scale, determined by the internal parameters of the detector. To deal with scale changes, a straightforward approach consists of extracting points over a range of scales and using all these points together to represent the image. This is referred to as a multi-scale or multi-resolution approach <ref type="bibr" target="#b47">[48]</ref>.</p><p>In <ref type="bibr" target="#b58">[59]</ref>, a scale adapted version of the Harris operator was proposed. Interest points are detected at the local maxima of the Harris function applied at several scales. Thanks to the use of normalized derivatives, a comparable strength of the cornerness measure is obtained for points detected at different scales, such that a single threshold can be used to reject less significant corners over all scales. This scale adapted detector significantly improves the repeatability of interest points under scale changes. On the other hand, when prior knowledge on the scale change between two images is given, the detector can be adapted so as to extract interest points only at the selected scales. This yields a set of points, for which the respective localization and scale perfectly reflect the real scale change between the images.</p><p>In general, multi-scale approaches suffer from the same problems as dense sampling of features (cf. Section 1.5). They cannot cope well with the case where a local image structure is present over a range of scales, which results in multiple interest points being detected at each scale within this range. As a consequence, there are many points, which represent the same structure, but with slightly different localization and scale. The high number of points increases the ambiguity and the computational complexity of matching and recognition. Therefore, efficient methods for selecting accurate correspondences and verifying the results are necessary at further steps of the algorithms. In contrast to structure from motion applications, this is less of an issue in the context of recognition where a single point can have multiple correct matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.2">Scale-Invariant Detectors</head><p>To overcome the problem of many overlapping detections, typical of multiscale approaches, scale-invariant methods have been introduced. These automatically determine both the location and scale of the local features. Features are typically circular regions, in that case.</p><p>Many existing methods search for maxima in the 3D representation of an image (x, y and scale). This idea for detecting local features in scale-space was introduced in the early 1980s <ref type="bibr" target="#b46">[47]</ref>. The pyramid representation was computed with low pass filters. A feature point is detected if it is at a local maximum of a surrounding 3D cube and if its absolute value is higher than a certain threshold. Since then many methods for selecting points in scale-space have been proposed. The existing approaches mainly differ in the differential expression used to build the scale-space representation.</p><p>A normalized LoG function was applied in <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b119">120]</ref> to build a scale space representation and search for 3D maxima. The scale-space representation is constructed by smoothing the high resolution image with derivatives of Gaussian kernels of increasing size. Automatic scale selection (cf. Section 3.4) is performed by selecting local maxima in scale-space. The LoG operator is circularly symmetric. It is therefore naturally invariant to rotation. It is also well adapted for detecting bloblike structures. The experimental evaluation in <ref type="bibr" target="#b137">[138]</ref> shows this function is well suited for automatic scale selection. The scale invariance of interest point detectors with automatic scale selection has also been explored in <ref type="bibr" target="#b23">[24]</ref>. Corner detection and blob detection with automatic scale selection were also proposed in a combined framework in <ref type="bibr" target="#b23">[24]</ref> for feature tracking with adaptation to spatial and temporal size variations. The interest point criterion that is being optimized for localization need not be the same as the one used for optimizing the scale. In <ref type="bibr" target="#b137">[138]</ref>, a scaleinvariant corner detector, coined Harris-Laplace, and a scale-invariant blob detector, coined Hessian-Laplace were introduced. In these methods, position and scale are iteratively updated until convergence <ref type="bibr" target="#b142">[143]</ref>. More details can be found in Sections 3.4 and 4.2.</p><p>An efficient algorithm for object recognition based on local 3D extrema in the scale-space pyramid built with DoG filters was introduced in <ref type="bibr" target="#b125">[126]</ref>. The local 3D extrema in the pyramid representation determine the localization and the scale of interest points. This method is discussed further in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.3">Affine Invariant Methods</head><p>An affine invariant detector can be seen as a generalization of the scale-invariant ones to non-uniform scaling and skew, i.e., with a different scaling factor in two orthogonal directions and without preserving angles. The non-uniform scaling affects not only the localization and the scale but also the shape of characteristic local structures. Therefore, scale-invariant detectors fail in the case of significant affine transformations.</p><p>Affine invariant feature detection, matching, and recognition have been addressed frequently in the past <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b203">204]</ref>. Here, we focus on the methods which deal with invariant interest point detection.</p><p>One category of approaches was concerned with the localization accuracy under affine and perspective transformations. An affine invariant algorithm for corner localization was proposed in <ref type="bibr" target="#b1">[2]</ref> which builds on the observations made in <ref type="bibr" target="#b53">[54]</ref>. Affine morphological multi-scale analysis is applied to extract corners. The evolution of a corner is given by a linear function formed by the scale and distance of the detected points from the real corner. The location and orientation of the corner is computed based on the assumption that the multiscale points move along the bisector line and the angle indicates the true location. However, in natural scenes a corner can take any form of a bi-directional signal change and in practice the evolution of a point rarely follows the bisector. The applicability of the method is therefore limited to a polygonal like world.</p><p>Other approaches were concerned with simultaneous detection of location, size and affine shape of local structures. The method introduced in <ref type="bibr" target="#b246">[247]</ref>, coined EBR (Edge-Based Regions) starts from Harris corners and nearby intersecting edges. Two points moving along the edges together with the Harris point determine a parallelogram. The points stop at positions where some photometric quantities of the texture covered by the parallelogram reach an extremum. The method can be categorized as a model-based approach as it looks for a specific structure in images, albeit not as strict as most methods described in Section 2.6. More details can be found in Section 3.5. A similar scheme has been explored in <ref type="bibr" target="#b11">[12]</ref>.</p><p>An intensity-based method (IBR, Intensity-Based Regions) was also proposed in <ref type="bibr" target="#b247">[248]</ref>. It starts with the extraction of local intensity extrema. The intensity profiles along rays emanating from a local extremum are investigated. A marker is placed on each ray in the place, where the intensity profile significantly changes. Finally, an ellipse is fitted to the region determined by the markers. This method is further discussed in Section 5.1. Somewhat similar in spirit are the Maximally Stable Extremal Regions or MSER proposed in <ref type="bibr" target="#b133">[134]</ref> and described in the next section.</p><p>A method to find blob-like affine invariant features using an iterative scheme was introduced in <ref type="bibr" target="#b120">[121]</ref>, in the context of shape from texture. This method based on the affine invariance of shape adapted fixed points was also used for estimating surface orientation from binocular data (shape from disparity gradients). The algorithm explores the properties of the second moment matrix and iteratively estimates the affine deformation of local patterns. It effectively estimates the transformation that would project the patch to a frame in which the eigenvalues of the second moment matrix are equal. This work provided a theoretical background for several other affine invariant detectors.</p><p>It was combined with the Harris corner detector and used in the context of matching in <ref type="bibr" target="#b12">[13]</ref>, hand tracking in <ref type="bibr" target="#b108">[109]</ref>, fingerprint recognition <ref type="bibr" target="#b0">[1]</ref> and for affine rectification of textured regions in <ref type="bibr" target="#b207">[208]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, interest points are extracted at several scales using the Harris detector and then the shape of the regions is adapted to the local image structure using the iterative procedure from <ref type="bibr" target="#b117">[118]</ref>. This allows to extract affine invariant descriptors for a given fixed scale and location -that is, the scale and the location of the points are not extracted in an affine invariant way. Furthermore, the multi-scale Harris detector extracts many points which are repeated at the neighboring scale levels. This increases the probability of a mismatch and the complexity.</p><p>The Harris-Laplace detector introduced in <ref type="bibr" target="#b140">[141]</ref> was extended in <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b208">209]</ref> by affine normalization with the algorithm proposed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b120">121]</ref>. This detector suffers from the same drawbacks, as the initial location and scale of points are not extracted in an affine invariant way, although the uniform scale changes between the views are handled by the scale-invariant Harris-Laplace detector.</p><p>Beyond affine transformations. A scheme that goes even beyond affine transformations and is invariant to projective transformations was introduced in <ref type="bibr" target="#b235">[236]</ref>. However, on a local scale, the perspective effect is usually neglectable. More damaging is the effect of non-planarities or non-rigid deformations. This is why a theoretical framework to extend the use of local features to non-planar surfaces has been proposed in <ref type="bibr" target="#b250">[251]</ref>, based on the definition of equivalence classes. However, in practice, they have only shown results on straight corners. Simultaneously, an approach invariant to general deformations was developed in <ref type="bibr" target="#b121">[122]</ref>, by embedding an image as a 2D surface in 3D space and exploiting geodesic distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Segmentation-based Methods</head><p>Segmentation techniques have also been employed in the context of feature extraction. These methods were either applied to find homogeneous regions to localize junctions on their boundaries or to directly use these regions as local features. For the generic feature extraction problem, mostly bottom-up segmentation based on low level pixel grouping was considered, although in some specific tasks top-down methods can also be applied. Although significant progress has been made in the analysis and formalization of the segmentation problem, it remains an unsolved problem in the general case. Optimal segmentation is intractable in general due to the large search space of possible feature point groups, in particular in algorithms based on multiple image cues. Moreover, a multitude of definitions of optimal segmentation even for the same image makes it difficult to solve. Nonetheless, several systems using segmentation based interest regions have been developed, especially in the context of retrieval, matching, and recognition.</p><p>In early years of computer vision polygonal approximations of images were popular in scene analysis and medical image analysis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">58]</ref>. These algorithms often involved edge detection and subsequent edge following for region identification. In <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, the vertices of a picture are defined as those points which are common in three or more segmented regions. It can be seen as one of the first attempts to extract interest points using segmentation. Simple segmentation of patches into two regions is used in <ref type="bibr" target="#b122">[123]</ref> and the regions are compared to find corners. Unfortunately, the two region assumption makes the usefulness of the method limited.</p><p>Another set of approaches represent real images through segmentation <ref type="bibr" target="#b128">[129]</ref>. Well performing image segmentation methods are based on graph cuts, where graphs represent connected image pixels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b224">225,</ref><ref type="bibr" target="#b226">227]</ref>. These methods allow to obtain segmentation at the required level of detail. Although semantic segmentation is not reliable, over-segmenting the image can produce many regions which fit to the objects. This approach was explored in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> and it is particularly appealing for image retrieval problems where the goal is to find similar images via regions with similar properties. In <ref type="bibr" target="#b43">[44]</ref>, the goal is to create interest operators that focus on homogeneous regions, and compute local image descriptors for these regions. The segmentation is performed on several feature spaces using kernel-based optimization methods. The regions can be individually described and used for recognition but their distinctiveness is low. This direction has recently gained more interest and some approaches use bottom-up segmentation to extract interest regions or so-called superpixels <ref type="bibr" target="#b158">[159,</ref><ref type="bibr" target="#b183">184]</ref> (see also <ref type="bibr">Section 5.3)</ref>.</p><p>In general the disadvantages of this representation are that the segmentation results are still unstable and inefficient for processing large amounts of images. An approach which successfully deals with these problems was taken in <ref type="bibr" target="#b133">[134]</ref>. Maximally Stable Extremal Regions (MSER) are extracted with a watershed like segmentation algorithm. The method extracts homogeneous intensity regions which are stable over a wide range of thresholds. The regions are then replaced by ellipses with the same shape moments up to the second-order. Recently, a variant of this method was introduced in <ref type="bibr" target="#b176">[177]</ref>, which handles the problems with blurred region boundaries by using region isophotes. In a sense, this method is also similar to the IBR method described in Section 5.1, as very similar regions are extracted. More details on MSER can be found in Section 5.2. The method was extended in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b160">161]</ref> with tree like representation of watershed evolution in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Machine Learning-based Methods</head><p>The progress in the domain of machine learning and the increase of available computational power allowed learning techniques to enter the feature extraction domain. The idea of learning the attributes of local features from training examples and then using this information to extract features in other images has been around in the vision community for some time but only recently it was more broadly used in real applications. The success of these methods is due to the fact that efficiency, provided by classifiers, became a more desirable property than accuracy of detection.</p><p>In <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55]</ref>, a neural network is trained to recognize corners where edges meet at a certain degree, near to the center of an image patch. This is applied to images after edge detection. A similar idea was explored in <ref type="bibr" target="#b243">[244]</ref> to improve the stability of curvature measurement of digital curves.</p><p>Decision trees <ref type="bibr" target="#b177">[178]</ref> have also been used successfully in interest point detection tasks. The idea of using intensity differences between the central points and neighboring points <ref type="bibr" target="#b172">[173,</ref><ref type="bibr" target="#b231">232,</ref><ref type="bibr" target="#b239">240]</ref> has been adopted in <ref type="bibr" target="#b201">[202,</ref><ref type="bibr" target="#b202">203]</ref>. They construct a decision tree to classify point neighborhoods into corners <ref type="bibr" target="#b202">[203]</ref>. The main concern in their work is the efficiency in testing only a fraction of the many possible differences and the tree is trained to optimize that. The approach of <ref type="bibr" target="#b202">[203]</ref> was also extended with LoG filters to detect multiscale points in <ref type="bibr" target="#b111">[112]</ref>. They use a feature selection technique based on the repeatability of individual interest points over perspective projected images.</p><p>A hybrid methodology that integrates genetic algorithms and decision tree learning in order to extract discriminatory features for recognizing complex visual concepts is described in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b242">[243]</ref>, interest point detection is posed as an optimization problem. They use a Genetic Programming based learning approach to construct operators for extracting features. The problem of learning an interest point operator was posed differently in <ref type="bibr" target="#b102">[103]</ref> where human eye movement was studied to find the points of fixation and to train an SVM classifier.</p><p>One can easily generalize the feature detection problem to a classification problem and train a recognition system on image examples provided by one or a combination of the classical detectors. Any machine learning approach can be used for that. Haar like filters implemented with integral images to efficiently approximate multiscale derivatives were used in <ref type="bibr" target="#b14">[15]</ref>. A natural extension would be to use the learning scheme from Viola and Jones <ref type="bibr" target="#b251">[252]</ref> successfully applied to face detection, to efficiently classify interest points.</p><p>The accuracy of machine learning based methods in terms of localization, scale, and shape estimation is in general lower than for the generic detectors <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b133">134,</ref><ref type="bibr" target="#b142">143]</ref> but in the context of object recognition the efficiency is usually more beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Evaluations</head><p>Given the multitude of interest point approaches the need for independent performance evaluations was identified early on and many experimental tests have been performed over the last three decades. Various experimental frameworks and criteria were used. One of the first comparisons of corner detection techniques based on chain coded curves was presented in <ref type="bibr" target="#b204">[205]</ref>. In the early papers very often only visual inspection was done <ref type="bibr" target="#b103">[104]</ref>. Others performed more quantitative evaluations providing scores for individual images or for small test data <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b265">266]</ref>.</p><p>Corner detectors were often tested on artificially generated images with different types of junctions with varying angle, length, contrast, noise, blur etc. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b178">179]</ref>. Different affine photometric and geometric transformations were used to generate the test data and to evaluate corner detectors in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b127">128]</ref>. This approach simplifies the evaluation process but cannot model all the noise and deformations which affect the detector performance in a real application scenario, thus the performance results are often over-optimistic. A somewhat different approach is taken in <ref type="bibr" target="#b149">[150]</ref>. There, performance comparison is approached as a general recognition problem. Corners are manually annotated on affine transformed images and measures like consistency and accuracy similar to detection rate and recall are used to evaluate the detectors.</p><p>In <ref type="bibr" target="#b87">[88]</ref>, sets of points are extracted from polyhedral objects and projective invariants are used to calculate a manifold of constraints on the coordinates of the corners. They estimate the variance of the distance from the point coordinates to this manifold independently of camera parameters and object pose. Nonlinear diffusion was used to remove the noise and the method from <ref type="bibr" target="#b187">[188]</ref> performed better than the one proposed in <ref type="bibr" target="#b103">[104]</ref>. The idea of using planar invariants is also explored in <ref type="bibr" target="#b39">[40]</ref> to evaluate corner detectors based on edges. Theoretical properties of features and localization accuracy were also tested in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b187">188,</ref><ref type="bibr" target="#b188">189,</ref><ref type="bibr" target="#b189">190]</ref> based on a parametric L-corner model to evaluate localization accuracy. Also a randomized generator of corners has been used to test the localization error <ref type="bibr" target="#b260">[261]</ref>.</p><p>State-of-the-art curve based detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b192">193,</ref><ref type="bibr" target="#b196">197,</ref><ref type="bibr" target="#b206">207]</ref> are evaluated in <ref type="bibr" target="#b237">[238]</ref>. A quantitative measure of the quality of the detected dominant points is defined as the pointwise error between the digital curve and the polygon approximated from interest points. The performance of the proposed scale adapted approach is reported better than of the other methods.</p><p>The repeatability rate and information content measures were introduced in <ref type="bibr" target="#b82">[83]</ref>. They consider a point in an image interesting if it has two main properties: distinctiveness and invariance. This means that a point should be distinguishable from its immediate neighbors. Moreover, the position as well as the selection of the interest point should be invariant with respect to the expected geometric and radiometric distortions. From a set of investigated detectors <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b231">232,</ref><ref type="bibr" target="#b265">266]</ref>, Harris <ref type="bibr" target="#b83">[84]</ref> and a corner later described as SUSAN <ref type="bibr" target="#b231">[232]</ref> perform best.</p><p>Systematic evaluation of several interest point detectors based on repeatability and information content measured by the entropy of the descriptors was performed in <ref type="bibr" target="#b214">[215]</ref>. The evaluation shows that a modified Harris detector provides the most stable results on image pairs with different geometric transformations. The repeatability rate and information content in the context of image retrieval were also evaluated in <ref type="bibr" target="#b217">[218]</ref> to show that a wavelet-based salient point extraction algorithm outperforms the Harris detector <ref type="bibr" target="#b83">[84]</ref>.</p><p>Consistency of the number of corners and accuracy criteria were introduced as evaluation criteria in <ref type="bibr" target="#b42">[43]</ref>. This overcomes the problems with the repeatability criterion of favoring detectors providing more features. The introduced criterion instead favors detectors which provide similar number of points regardless of the object transformation even though the number of details in the image changes with scale and resolution. Several detectors <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b231">232]</ref> are compared with the best performance reported for a modified implementation of <ref type="bibr" target="#b151">[152]</ref>.</p><p>Tracking and the number of frames over which the corners are detected during tracking was used to compare detectors in <ref type="bibr" target="#b238">[239,</ref><ref type="bibr" target="#b239">240]</ref>. Similarly Bae et al. <ref type="bibr" target="#b7">[8]</ref> uses correlation and matching to find repeated corners between frames and compare their numbers to the reference frame.</p><p>Extensive evaluation of commonly used feature detectors and descriptors has been performed in <ref type="bibr" target="#b143">[144,</ref><ref type="bibr" target="#b144">145]</ref>. The repeatability on image pairs representing planar scenes related by various geometric transformations was computed for different state-of-the-art scale and affine invariant detectors. The MSER region detector <ref type="bibr" target="#b133">[134]</ref> based on watershed segmentation showed the highest accuracy and stability on various structured scenes. The data collected by Mikolajczyk and Tuytelaars <ref type="bibr" target="#b144">[145]</ref> became a standard benchmark for evaluating interest point detectors and descriptors. <ref type="foot" target="#foot_1">2</ref>Recently, the performance of feature detectors and descriptors from <ref type="bibr" target="#b143">[144,</ref><ref type="bibr" target="#b144">145]</ref> has been investigated in <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158]</ref> in the context of matching 3D object features across viewpoints and lighting conditions. A method based on intersecting epipolar constraints provides ground truth correspondences automatically. In this evaluation, the affine invariant detectors introduced in <ref type="bibr" target="#b142">[143]</ref> are most robust to viewpoint changes. DoG detector from <ref type="bibr" target="#b123">[124]</ref> was reported the best in a similar evaluation based on images of natural 3D scenes in <ref type="bibr" target="#b255">[256]</ref>.</p><p>Feature detectors were also evaluated in the context of recognition in <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b138">139,</ref><ref type="bibr" target="#b234">235]</ref> using object category training data where direct correspondence cannot be automatically verified. Clustering properties and compactness of feature clusters were measured in <ref type="bibr" target="#b138">[139]</ref>. Some specific recognition tasks like pedestrian detection were also used to compare the performance of different features in <ref type="bibr" target="#b219">[220]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>It is important to note that the term corner as used here has a specific meaning. The detected points correspond to points in the 2D image with high curvature. These do not necessarily correspond to projections of 3D corners. Corners are found at various types of junctions, on highly textured surfaces, at occlusion boundaries, etc. For many practical applications, this is sufficient, since the goal is to have a set of stable and repeatable features. Whether these are true corners or not is considered irrelevant.</p><p>We begin this section with a derivatives-based approach, the Harris corner detector, described in Section 3.2. Next, we explain the basic ideas of the SUSAN detector (Section 3.3), which is an example of a method based on efficient morphological operators. We then move on to detectors with higher levels of invariance, starting with the scale and affine invariant extensions of the Harris detector: Harris-Laplace and Harris-Affine (Section 3.4). This is followed by a discussion of Edge-Based Regions in Section 3.5. Finally, we conclude the section with a short discussion (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Harris Detector</head><p>The Harris detector, proposed by Harris and Stephens <ref type="bibr" target="#b83">[84]</ref>, is based on the second moment matrix, also called the auto-correlation matrix, which is often used for feature detection and for describing local image structures. This matrix describes the gradient distribution in a local neighborhood of a point:</p><formula xml:id="formula_0">M = σ 2 D g(σ I ) * I 2 x (x, σ D ) I x (x, σ D )I y (x, σ D ) I x (x, σ D )I y (x, σ D ) I 2 y (x, σ D ) (3.1)</formula><p>with</p><formula xml:id="formula_1">I x (x, σ D ) = ∂ ∂x g(σ D ) * I(x) (3.2) g(σ) = 1 2πσ 2 e -x 2 +y 2 2σ 2 (3.3)</formula><p>The local image derivatives are computed with Gaussian kernels of scale σ D (the differentiation scale). The derivatives are then averaged in the neighborhood of the point by smoothing with a Gaussian window of scale σ I (the integration scale). The eigenvalues of this matrix represent the principal signal changes in two orthogonal directions in a neighborhood around the point defined by σ I . Based on this property, corners can be found as locations in the image for which the image signal varies significantly in both directions, or in other words, for which both eigenvalues are large. In practice, Harris proposed to use the following measure for cornerness, which combines the two eigenvalues in a single measure and is computationally less expensive: cornerness = det(M )λ trace(M ) <ref type="bibr">(3.4)</ref> with det(M ) the determinant and trace(M ) the trace of the matrix M . A typical value for λ is 0.04. Since the determinant of a matrix is equal to the product of its eigenvalues and the trace corresponds to the sum, it is clear that high values of the cornerness measure correspond to both eigenvalues being large. Adding the second term with the trace reduces the response of the operator on strong straight contours. Moreover, computing this measure based on the determinant and the trace is computationally less demanding than actually computing the eigenvalues. This seems less relevant now, but it was important back in 1988 when the computational resources were still very limited. Subsequent stages of the corner extraction process are illustrated in Figure <ref type="figure" target="#fig_5">3</ref>.1. Given the original image I(x, y) (upper left), the first step consists of computing the first-order derivatives I x and I y (lower left). Next, one takes the product of these gradient images (lower right). Then, the images are smoothed with a Gaussian kernel. These  images contain the different elements of the Hessian matrix, which are then in a final step combined into the cornerness measure, following Equation (3.4) (upper right).</p><p>When used as an interest point detector, local maxima of the cornerness function are extracted, using non-maximum suppression. Such points are translation and rotation invariant. Moreover, they are stable under varying lighting conditions. In a comparative study of different interest point detectors <ref type="bibr" target="#b213">[214,</ref><ref type="bibr" target="#b214">215]</ref>, the Harris corner was proven to be the most repeatable and most informative. Additionally, they can be made very precise: sub-pixel precision can be achieved through quadratic approximation of the cornerness function in the neighborhood of a local maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Discussion</head><p>Figure <ref type="figure" target="#fig_5">3</ref>.2 shows the corners detected with this measure for two example images related by a rotation. Note that the features found correspond to locations in the image showing two dimensional variations in the intensity pattern. These may correspond to real "corners", but the detector also fires on other structures, such as T-junctions, points with high curvature, etc. This equally holds for all other corner detectors described in this chapter. When true corners are desirable, model-based approaches are certainly more appropriate. As can be seen in the figure, many but not all of the features detected in the original image (left) have also been found in the rotated version (right). In other words, the repeatability of the Harris detector under rotations is high.</p><p>Additionally, features are typically found at locations which are informative, i.e., with a high variability in the intensity pattern. This makes them more discriminative and easier to bring into correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SUSAN Detector</head><p>The SUSAN corner detector has been introduced by Smith and Brady <ref type="bibr" target="#b231">[232]</ref> and relies on a different technique. Rather than evaluating local gradients, which might be noise-sensitive and computationally more expensive, a morphological approach is used.</p><p>SUSAN stands for Smallest Univalue Segment Assimilating Nucleus, and is a generic low-level image processing technique, which apart from corner detection has also been used for edge detection and noise suppression. The basic principle goes as follows (see also Figure <ref type="figure" target="#fig_5">3</ref>.3). For each pixel in the image, we consider a circular neighborhood of fixed radius around it. The center pixel is referred to as the nucleus, and its intensity value is used as reference. Then, all other pixels within this circular neighborhood are partitioned into two categories, depending on whether they have "similar" intensity values as the nucleus or "different" intensity values. In this way, each image point has associated with it a local area of similar brightness (coined usan), whose relative size contains important information about the structure of the image at that point (see also Figure <ref type="figure" target="#fig_5">3</ref>.3). In more or less homogeneous parts of the image, the local area of similar brightness covers almost the entire circular neighborhood. Near edges, this ratio drops to 50%, and near corners it decreases further to about 25%. Hence, corners can be detected as locations in the image where the number of pixels with similar intensity value in a local neighborhood reaches a local minimum and is below a predefined threshold. To make the method more robust, pixels closer in value to the nucleus receive a higher weighting. Moreover, a set of rules is used to suppress qualitatively "bad" features. Local minima of the SUSANs (Smallest usans) are then selected from the remaining candidates. An example of detected SUSAN corners is shown in Figure <ref type="figure" target="#fig_5">3</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Discussion</head><p>The features found show a high repeatability for this (artificially rotated) set of images. However, many of the features are located on edge structures and not on corners. For such points, the localization is sensitive to noise. Moreover, edge based points are also less discriminative.</p><p>The two detectors described so far are invariant under translation and rotation only. This means that corners will be detected at corresponding locations only if the images are related by a translation and/or rotation. In the next sections, we will describe detectors with higher levels of viewpoint invariance, that can withstand scale changes or even affine deformations. Apart from better matching across transformed images, these also bring the advantage of detecting features over a range of scales or shapes.</p><p>Alternatively, this effect can be obtained by using a multiscale approach. In that case, a detector which is not scale invariant is applied to the input image at different scales (i.e., after smoothing and sampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Harris-Laplace/Affine</head><p>Mikolajczyk and Schmid developed both a scale invariant corner detector, referred to as Harris-Laplace, as well as an affine-invariant one, referred to as Harris-Affine <ref type="bibr" target="#b142">[143]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Harris-Laplace</head><p>Harris-Laplace starts with a multiscale Harris corner detector as initialization to determine the location of the local features. The characteristic scale is then determined based on scale selection as proposed by Lindeberg et al. <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b119">120]</ref>. The idea is to select the characteristic scale of a local structure, for which a given function attains an extremum over scales (see Figure <ref type="figure" target="#fig_5">3</ref>.5). The selected scale is characteristic in the quantitative sense, since it measures the scale at which there is maximum similarity between the feature detection operator and the local image structures. The size of the region is therefore selected independently of the image resolution for each point. As the name Harris-Laplace suggests, the Laplacian operator is used for scale selection. This has been shown to give the best results in the experimental comparison of <ref type="bibr" target="#b140">[141]</ref> as well as in <ref type="bibr" target="#b37">[38]</ref>. These results can be explained by the circular shape of the Laplacian kernel, which acts as a matched filter <ref type="bibr" target="#b57">[58]</ref> when its scale is adapted to the scale of a local image structure.</p><p>Figure <ref type="figure" target="#fig_5">3</ref>.6 shows the scale-invariant local features obtained by applying the Harris-Laplace detector, for two images of the same scene related by a scale change. In order not to overload the images, only some of the corresponding regions that were detected in both images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Harris-Affine</head><p>Given a set of initial points extracted at their characteristic scales based on the Harris-Laplace detection scheme, the iterative estimation of elliptical affine regions as proposed by Lindeberg et al. <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b120">121]</ref> allows to obtain affine invariant corners. Instead of circular regions, these are ellipses. The procedure consists of the following steps:   The eigenvalues of the second moment matrix (see Equation (3.3)) are used to measure the affine shape of the point neighborhood. More precisely, we determine the transformation that projects the intensity pattern of the point neighborhood to one with equal eigenvalues. This transformation is given by the square root of the second moment matrix M 1/2 . It can be shown that if the neighborhoods of two points x R and x L are related by an affine transformation, then their normalized versions,</p><formula xml:id="formula_2">x R = M -1/2 R x R and x L = M -1/2 L</formula><p>x L , are related by a simple rotation x L = Rx R <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b120">121]</ref>. This process is illustrated in Figure <ref type="figure" target="#fig_5">3</ref>.8. The matrices M L and M R computed in the normalized frames are rotation matrices as well. Note that rotation preserves the eigenvalue ratio for an image patch, therefore, the affine deformation can be determined only up to a rotation factor.</p><p>The estimation of affine shape can be applied to any initial point given that the determinant of the second moment matrix is larger than zero and the signal to noise ratio is sufficiently large. We can therefore use this technique to estimate the shape of initial regions provided by the Harris-Laplace detector.</p><p>The output of the Harris-Affine detector on two images of the same scene is shown in Figure <ref type="figure" target="#fig_5">3</ref>.9. Apart from the scale, also the shape of the regions is now adapted to the underlying intensity patterns, so as to ensure that the same part of the object surface is covered in spite of the deformations caused by the viewpoint change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Edge-based Regions</head><p>A more heuristic technique to obtain affine invariance is to exploit the geometry of the edges that can usually be found in the proximity of a Harris corner. Such a method has been proposed by Tuytelaars and Van Gool <ref type="bibr" target="#b246">[247,</ref><ref type="bibr" target="#b248">249]</ref>. The rationale behind this approach is that edges are typically rather stable image features, that can be detected over a range of viewpoints, scales and illumination changes. Moreover, by exploiting the edge geometry, the dimensionality of the problem can be significantly reduced. Indeed, as will be shown next, the 6D search problem over all possible affinities (or 4D, once the center point is fixed) can be reduced to a one-dimensional problem by exploiting the nearby edges geometry. In practice, we start from a Harris corner point p (see Section 3.2) <ref type="bibr" target="#b83">[84]</ref> and a nearby edge, extracted with the Canny edge detector <ref type="bibr" target="#b28">[29]</ref>. To increase the robustness to scale changes, these basic features are extracted at multiple scales. Two points p 1 and p 2 move away from the corner in both directions along the edge, as shown in Figure <ref type="figure" target="#fig_5">3</ref>.10. Their relative speed is coupled through the equality of relative affine invariant parameters l 1 and l 2 : with s i an arbitrary curve parameter (in both directions, i = 1, 2), p i (1) (s i ) the first derivative of p i (s i ) with respect to s i , abs( ) the absolute value and | • • • | the determinant. This condition prescribes that the areas between the joint p, p 1 and the edge and between the joint p, p 2 and the edge remain identical. From now on, we simply use l when referring to l 1 = l 2 .</p><formula xml:id="formula_3">l i = abs (|p i (1) (s i )p -p i (s i )|) ds i (3.</formula><p>For each value l, the two points p 1 (l) and p 2 (l) together with the corner p define a parallelogram Ω(l): the parallelogram spanned by the vectors p 1 (l)p and p 2 (l)p (see <ref type="bibr">Figure 3.10)</ref>. This yields a onedimensional family of parallelogram-shaped regions as a function of l. From this 1D family one (or a few) parallelogram(s) are selected for which the following photometric quantities of the texture go through an extremum.</p><formula xml:id="formula_4">Inv 1 = abs |p 1 -p g p 2 -p g | |p -p 1 p -p 2 | M 1 00 M 2 00 M 0 00 -(M 1 00 )2 Inv 2 = abs |p -p g q -p g | |p -p 1 p -p 2 | M 1 00 M 2 00 M 0 00 -(M 1 00 )2 (3.6)</formula><p>with with M n pq the nth order, (p + q)th degree moment computed over the region Ω(l), p g the center of gravity of the region, weighted with intensity I(x, y), and q the corner of the parallelogram opposite to the corner point p (see Figure <ref type="figure" target="#fig_5">3</ref>.10). The second factor in these formula has been added to ensure invariance under an intensity offset.</p><formula xml:id="formula_5">M n pq = Ω I n (x, y)x p y q dxdy p g = M 1 10 M 1 00 , M 1 01 M 1 00 (3.7)</formula><p>For straight edges, l = 0 along the entire edge. In that case, the two photometric quantities given in Equation (3.7) are combined and locations where both functions reach a minimum value are taken to fix the parameters s 1 and s 2 . Moreover, instead of relying on the Harris corner detection, the straight lines intersection point can be used instead. Examples of detected regions are displayed in Figure <ref type="figure" target="#fig_5">3</ref>.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">From Parallelograms to Ellipses</head><p>Note that the regions found with this method are parallelograms. This is in contrast to many other affine invariant detectors (for example those based on the second moment matrix) for which the output shape is an ellipse. For uniformity and convenience in comparison, it is sometimes advantageous to convert these parallelogram-shaped regions into ellipses. This can be achieved by selecting an ellipse with the same firstand second-order moments as the originally detected region, which is an affine covariant construction method. The elliptical regions generated with this procedure are shown in Figure <ref type="figure" target="#fig_5">3</ref>.12. Note though that some information is lost during this conversion, as ellipses have a rotational degree of freedom, which was fixed in the original representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>Several methods for corner detection have been described in this chapter. As discussed earlier, corner based features do not necessarily correspond to real corners in the 3D world. Indeed, the goal is to extract stable features, that can be matched well in spite of changes in viewing conditions.</p><p>The Harris detector was identified as the most stable one in many independent evaluations <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b137">138,</ref><ref type="bibr" target="#b214">215]</ref>. There are also multi-scale as well as scale and affine invariant extensions of this approach. It is a convenient tool for providing a large number of features. Alternatively, the SUSAN detector can be used. It is more efficient but also more sensitive to noise. An optimized SUSAN detector using machine learning techniques is described in Section 6.3. As discussed in Sections 2.2 and 2.3 contour based corner detectors are suitable for line drawing images but in natural scenes intensity based methods are typically more stable.</p><p>It is important to note that the affine transformation model only holds for viewpoint changes in case of locally planar regions and assuming the camera is relatively far from the object. However, corners are often found near object boundaries as this is where the intensity change usually occurs. Hence, the region extraction process is often based on measurements on non-planar structures, e.g., including background or another facet of the object. In these cases, the viewpoint invariance will be limited and also the robustness to background changes will be affected. A possible way out has been indicated in the work of <ref type="bibr" target="#b250">[251]</ref>. Detectors that search for region boundaries like EBR are less affected by this phenomenon. The measurement regions can then be delimited by the detected contours, thus excluding the non-planar parts in many practical situations.</p><p>On the positive side, compared to other types of features, corners are typically better localized in the image plane. This localization accuracy can be important for some applications, e.g., for camera calibration or 3D reconstruction. Their scale however is not well defined as a corner structure changes very little over a wide range of scales. The reason why scale selection still works with the Harris detector is that the feature point is localized not exactly on the corner edge but slightly inside the corner structure.</p><p>After corners, the second most intuitive local features are blobs. As it was the case in the previous section, we select a few methods that have proved successful in many applications and describe these in more detail. These methods typically provide complementary features to the ones discussed in the previous chapter. We start with a derivativebased method: the Hessian detector (Section 4.1). Next, we consider the scale-invariant and affine invariant extensions of this method, coined Hessian-Laplace and Hessian-Affine (Section 4.2). Finally, we describe the salient region detector, which is based on the entropy of the intensity probability distribution (Section 4.3). We conclude the chapter with a short discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hessian Detector</head><p>The second 2 × 2 matrix issued from the Taylor expansion of the image intensity function I(x) is the Hessian matrix:</p><formula xml:id="formula_6">H = I xx (x, σ D ) I xy (x, σ D ) I xy (x, σ D ) I yy (x, σ D )<label>(4.1)</label></formula><p>with I xx etc. second-order Gaussian smoothed image derivatives. These encode the shape information by describing how the normal to an isosurface changes. As such, they capture important properties of local image structure. Particularly interesting are the filters based on the determinant and the trace of this matrix. The latter is often referred to as the Laplacian. Local maxima of both measures can be used to detect blob-like structures in an image <ref type="bibr" target="#b15">[16]</ref>.</p><p>The Laplacian is a separable linear filter and can be approximated efficiently with a Difference of Gaussians (DoG) filter. The Laplacian filters have one major drawback in the context of blob extraction though. Local maxima are often found near contours or straight edges, where the signal change is only in one direction <ref type="bibr" target="#b137">[138]</ref>. These maxima are less stable because their localization is more sensitive to noise or small changes in neighboring texture. This is mostly an issue in the context of finding correspondences for recovering image transformations. A more sophisticated approach, solving this problem, is to select a location and scale for which the trace and the determinant of the Hessian matrix simultaneously assume a local extremum.</p><p>This gives rise to points, for which the second order derivatives detect signal changes in two orthogonal directions. A similar idea is explored in the Harris detector, albeit for first-order derivatives only.</p><p>The feature detection process based on the Hessian matrix is illustrated in Figure <ref type="figure" target="#fig_18">4</ref>.1. Given the original image (upper left), one first computes the second-order Gaussian smoothed image derivatives (lower part), which are then combined into the determinant of the Hessian (upper right).</p><p>The interest points detected with the determinant of the Hessian for an example image pair are displayed in Figure <ref type="figure" target="#fig_18">4</ref>.2. The second-order derivatives are symmetric filters, thus they give weak responses exactly in the point where the signal change is most significant. Therefore, the maxima are localized at ridges and blobs for which the size of the Gaussian kernel σ D matches by the size of the blob structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hessian-Laplace/Affine</head><p>The Hessian-Laplace and Hessian-Affine detectors are similar in spirit as their Harris-based counterparts Harris-Laplace and Harris-Affine,  described in Section 3.4, except that they start from the determinant of the Hessian rather than the Harris corners. This turns the methods into viewpoint invariant blob-detectors. They have also been proposed by Mikolajczyk and Schmid <ref type="bibr" target="#b142">[143]</ref>, and are complementary to their Harrisbased counterparts, in the sense that they respond to a different type of feature in the image. An example of the detection result is shown in     Like in the Harris based detector the number of regions found with the Hessian-Laplace detector can be controlled by thresholding the Hessian determinant as well as the Laplacian response. Typically a large number of features can be extracted resulting in a good coverage of the image, which is one of the advantages of the Hessian-based detector.</p><p>Furthermore, this detector also responds to some corner structures at fine scale (see Figure <ref type="figure" target="#fig_18">4</ref>.1). The returned locations, however, are more suitable for scale estimation than the Harris points due to the use of similar filters for spatial and scale localization, both based on secondorder Gaussian derivatives. One of the possible extensions of this work is to explore the Hessian matrix to use additional shape information encoded by the eigenvalues of this matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Salient Regions</head><p>Rather than building on the derivative information in the image, the salient region detector proposed by Kadir and Brady <ref type="bibr" target="#b97">[98]</ref> is inspired by information theory. The basic idea behind this feature detector is to look for salient features, where saliency is defined as local complexity or unpredictability. It is measured by the entropy of the probability distribution function of intensity values within a local image region. However, looking at entropy alone does not suffice to accurately localize the features over scales, so as an additional criterion, the self-dissimilarity in scale-space of the feature is added as an extra weighting function, favoring well-localized complex features.</p><p>Detection proceeds in two steps: first, at each pixel x the entropy H of the probability distribution p(I) is evaluated over a range of scales s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H = -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I p(I) log p(I).</head><p>The probability distribution p(I) is estimated empirically based on the intensity distribution in a circular neighbourhood of radius s around x. Local maxima of the entropy are recorded. These are candidate salient regions. Second, for each of the candidate salient regions the magnitude of the derivative of p(I) with respect to scale s is computed as</p><formula xml:id="formula_7">W = s 2 2s -1 I ∂p(I; s) ∂s .</formula><p>The saliency Y is then computed as</p><formula xml:id="formula_8">Y = WH.</formula><p>The candidate salient regions over the entire image are ranked by their saliency Y, and the top P ranked regions are retained. Also an affine invariant version of the detector has been proposed, where local maxima over the scale s and the shape parameters (orientation θ and ratio of major to minor axes λ of an elliptical region) are sought simultaneously. However, this seriously slows down the computation.</p><p>Examples of detected regions, using the affine invariant version, are displayed in Figure <ref type="figure" target="#fig_18">4</ref>. <ref type="bibr" target="#b4">5</ref>. More details about this method can be found in <ref type="bibr" target="#b98">[99]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Discussion</head><p>Because of the weighting factor measuring the self-dissimilarity over scale, the detector typically fires on blob-like structures in the image. That is why we have catalogued the method as a blob detector. But note that in contrast to other blob detectors the contrast of the blobs does not have any influence on the detection.</p><p>The number of features found with this method is typically relatively low. Unlike for many other detectors the ranking of the extracted features is meaningful due to the entropy based criteria with the ones from the top the most stable. This property has been explored in the context of category-level object recognition, and especially in combination with classifiers where the complexity largely depends on the number of features (e.g., <ref type="bibr" target="#b64">[65]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Blob detectors have been used widely in different application domains. Apart from the methods described above, also DoG (difference-of-Gaussians) <ref type="bibr" target="#b123">[124]</ref> and SURF (speeded-up robust features) <ref type="bibr" target="#b14">[15]</ref> can be catalogued as blob detectors. However, since their extraction processes are focussed on efficiency, we postpone their discussion until Section 6. Some of the methods described in Section 5 also share common characteristics with blob detectors. Especially IBR (intensity-based regions) <ref type="bibr" target="#b247">[248]</ref> and MSER (maximally stable extremal regions) <ref type="bibr" target="#b133">[134]</ref> often find blob-like structures in the image. However, apart from bloblike structures, they also detect other, more irregularly shaped patterns, which we consider their distinctive property.</p><p>Blob detectors are in a sense complementary to corner detectors. As a result, they are often used together. By using several complementary feature detectors, the image is better covered and the performance becomes less dependent on the actual image content. This has been exploited, e.g., in <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b230">231]</ref>.</p><p>In general, blob-like structures tend to be less accurately localized in the image plane than corners, although their scale and shape are better defined than for corners. The location of a corner can be identified by a single point while blobs can only be localized by their boundaries, which are often irregular. On the other hand, the scale estimation of a corner is ill-defined as for example an intersection of edges exists at a wide range of scales. The boundaries of a blob however, even if irregular, give a good estimate of the size thus scale of the blob. This makes them less suited for, e.g., camera calibration or 3D reconstruction. For object recognition, on the other hand, a precise image localization is often not necessary, since the entire recognition process is very noisy. A robust descriptor such as SIFT <ref type="bibr" target="#b123">[124]</ref> can match such features nevertheless. The scales of matched blobs allow then to hypothesize the size of the objects <ref type="bibr" target="#b139">[140]</ref>, which makes them very useful in recognition applications.</p><p>Finally, the number of features detected with the methods described above varies greatly. There is often just a few tens of salient regions found in an image, whereas the Hessian-Laplace or Hessian-Affine methods allow to extract up to several hundreds or thousands of features. Depending on the application and algorithms used, either case can be advantageous. We refer to Section 7 for a further discussion on this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region Detectors</head><p>In this chapter, we discuss a number of feature detectors which, directly or indirectly, are concerned with extraction of image regions. First, we describe the intensity-based regions (Section 5.1), followed by maximally stable extremal regions (Section 5.2). At the end, we discuss superpixels (Section 5.3). These regions are provided by different methods but focus on similar image structures and share similar properties. Superpixels are traditionally not considered as local features and have limited robustness to changes in viewing conditions but they are currently more and more used in the context of image recognition. We therefore include all the above features in the same category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Intensity-based Regions</head><p>Here we describe a method proposed by Tuytelaars and Van Gool <ref type="bibr" target="#b247">[248,</ref><ref type="bibr" target="#b248">249]</ref> to detect affine invariant regions. It starts from intensity extrema (detected at multiple scales), and explores the image around them in a radial way, delineating regions of arbitrary shape, which are then replaced by ellipses.</p><p>More precisely, given a local extremum in intensity, the intensity function along rays emanating from the extremum is studied (see  with t an arbitrary parameter along the ray, I(t) the intensity at position t, I 0 the intensity value at the extremum, and d a small number which has been added to prevent a division by zero. The point for which this function reaches an extremum is invariant under affine geometric and linear photometric transformations (given the ray). Typically, a maximum is reached at positions where the intensity suddenly increases or decreases. The function f (t) is in itself already invariant. Nevertheless, points are selected where this function reaches an extremum to make a robust selection. Next, all points corresponding to maxima of f (t) along rays originating from the same local extremum are linked to enclose an affine invariant region. This often irregularly shaped region is replaced by an ellipse having the same shape moments up to the second-order. This ellipse fitting is an affine covariant construction. An example of regions detected with this method is shown in Figure <ref type="figure" target="#fig_1">5</ref>.2.</p><formula xml:id="formula_9">I(t) f(t) t t t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Maximally Stable Extremal Regions</head><p>MSER or Maximally Stable Extremal Regions have been proposed by Matas et al. <ref type="bibr" target="#b133">[134]</ref>. A Maximally Stable Extremal Region is a connected component of an appropriately thresholded image. The word "extremal" refers to the property that all pixels inside the MSER have either higher (bright extremal regions) or lower (dark extremal regions) intensity than all the pixels on its outer boundary. The "maximally stable" in MSER describes the property optimized in the threshold selection process.</p><p>The set of extremal regions E, i.e., the set of all connected components obtained by thresholding, has a number of desirable properties. First, a monotonic change of image intensities leaves E unchanged. Second, continuous geometric transformations preserve topology -pixels from a single connected component are transformed to a single connected component. Finally, there are no more extremal regions than there are pixels in the image. So a set of regions was defined that is preserved under a broad class of geometric and photometric changes and yet has the same cardinality as, e.g., the set of fixed-sized square windows commonly used in narrow-baseline matching.</p><p>The enumeration of the set of extremal regions E is very efficient, almost linear in the number of image pixels. The enumeration proceeds as follows. First, pixels are sorted by intensity. After sorting, pixels are marked in the image (either in decreasing or increasing order) and the list of growing and merging connected components and their areas is maintained using the union-find algorithm <ref type="bibr" target="#b218">[219]</ref>. During the enumeration process, the area of each connected component as a function of intensity is stored. Among the extremal regions, the "maximally stable" ones are those corresponding to thresholds for which the relative area change as a function of relative change of threshold is at a local minimum. In other words, the MSER are the parts of the image where local binarization is stable over a large range of thresholds. The definition of MSER stability based on relative area change is invariant to affine transformations (both photometrically and geometrically).</p><p>Detection of MSER is related to thresholding, since every extremal region is a connected component of a thresholded image. However, no global or "optimal" threshold is sought, all thresholds are tested and the stability of the connected components evaluated. The output of the MSER detector is not a binarized image. For some parts of the image, multiple stable thresholds exist and a system of nested subsets is output in this case.</p><p>For many of the affine invariant detectors the output shape is an ellipse. However, for MSER it is not. Examples of the original regions detected are given in Figure <ref type="figure" target="#fig_1">5</ref>.3. Using the same procedure as explained above for the IBR, an ellipse can be fitted based on the first and second shape moments. This results in a set of features as shown in Figure <ref type="figure" target="#fig_1">5</ref>.4. Alternatively, a local affine frame can be defined based on a set of stable points along the region contour <ref type="bibr" target="#b134">[135]</ref>. This provides an alternative scheme to normalize the region against affine deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Discussion</head><p>The MSER features typically anchor on region boundaries, thus the resulting regions are accurately localized compared to other  blob-detectors. The method works best for structured images which can be segmented well -ideally an image with uniform regions separated by strong intensity changes. On the downside, it was found to be sensitive to image blur <ref type="bibr" target="#b144">[145]</ref>, which can be explained by the fact that image blur undermines the stability criterion. This issue was addressed in its recent extension in <ref type="bibr" target="#b176">[177]</ref>. The method is also relatively fast. It is currently the most efficient among the affine invariant feature detectors. It has been used mostly for recognizing or matching specific objects (e.g., <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b230">231]</ref>) and showed lower performance for object class recognition <ref type="bibr" target="#b138">[139]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Segmentation-based Methods (Superpixels)</head><p>The two methods described above extract small regions whose intensity patterns clearly stand out with respect to their immediate surroundings. This is reminiscent of traditional image segmentation techniques. However, image segments are typically relatively large -too large, in fact, to be used as local features. By increasing the number of segments a new image representation can be obtained where the image segments typically have the right trade-off between locality and distinctiveness required in most local features-based applications (see <ref type="bibr">Figure 5.5)</ref>. This low-level grouping of pixels into atomic regions has been advocated by Mori et al. <ref type="bibr" target="#b158">[159]</ref> and Ren and Malik <ref type="bibr" target="#b183">[184]</ref>, who refer to the resulting atomic regions as superpixels. This terminology refers to the fact that superpixels can be considered as a more natural and perceptually more meaningful alternative for the original image pixels.</p><p>In <ref type="bibr" target="#b158">[159,</ref><ref type="bibr" target="#b183">184]</ref>, superpixels are extracted from the image using normalized cuts <ref type="bibr" target="#b226">[227]</ref>, but any data driven segmentation methods can be used here. The normalized cuts based approach is a classical image segmentation algorithm which exploits pairwise brightness, color, or texture affinities between pixels. To enforce locality, only local connections are taken into account when constructing the affinity matrix. An example of superpixels is shown in Figure <ref type="figure" target="#fig_1">5</ref>.5.</p><p>In contrast to traditional local features, by construction superpixels cover the entire image and do not overlap. Multiple segmentations can also be used to increase the possibility of object boundaries coinciding with boundaries between adjacent superpixels, except for small contour details and invisible contours. All superpixels extracted from an image have similar scale, so the method is not scale-invariant. An alternative construction method, based on Constrained Delauney Triangulation, has been proposed to obtain robustness against scale change <ref type="bibr" target="#b182">[183]</ref>.</p><p>These features are less suited for matching or object recognition, as the regions are uniform therefore not discriminative and the repeatability of boundary extraction is low. They have been used successfully for modeling and exploiting mid-level visual cues, such as curvilinear continuity, region grouping, or figure/ground organization for semantic image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>The local features detected with the methods described above typically represent homogeneous regions. While this is acceptable for the detection step, it may incur problems for the later description and matching. Indeed, homogeneous regions lack distinctiveness. Fortunately, this can easily be overcome by increasing the measurement region. In other words, we use a larger scale region to compute the descriptor, such that it also contains part of the surrounding image structures and captures the shape of the region boundary. This usually suffices to increase the discriminative power and match regions between images.</p><p>Intensity-based regions and maximally stable extremal regions typically give very similar features. These methods are therefore not complementary. IBR may break down when the region is non-convex, but it is more robust to small gaps in the region contour. MSER, on the other hand, has been shown to be relatively sensitive to image blur in <ref type="bibr" target="#b144">[145]</ref>, as this directly affects the stability criterion. This problem has been recently addressed in <ref type="bibr" target="#b176">[177]</ref>. However, apart from the case of image blur, MSER scores best with respect to repeatability in <ref type="bibr" target="#b144">[145]</ref>.</p><p>As discussed earlier, region detectors often detect blob-like structures -although they are not restricted to this type of regions. As a result, they are less complementary to blobs then to corners.</p><p>Region-based detectors are typically quite accurate in their localization. They work especially well for images with a well structured scene, clearly delineated regions, such as images containing objects with printed surfaces, buildings etc.</p><p>Even though superpixels share some characteristics with the other region detectors, they are not the same. They are non-overlapping, and cover the entire image. Their repeatability suffers from the weak robustness of the segmentation methods. Most importantly, they have been developed in a different context, where the idea is to speed up the image analysis by focussing on the superpixels only instead of analyzing all pixels. Superpixels are hence considered as a bigger equivalent of pixels, which can be described to a first approximation by a single intensity or color value. This is in contrast to local features which should be distinctive and, in the ideal case, uniquely identifiable. However, using region boundaries to build distinctive descriptors may overcome the occlusion problem from which traditional interest points suffer <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Implementations</head><p>Most feature detectors described so far involve the computation of derivatives or more complex measures such as the second moment matrix for the Harris detector or entropy for the salient regions detector. Since this step needs to be repeated for each and every location in feature coordinate space which includes position, scale and shape, this makes the feature extraction process computationally expensive, thus not suitable for many applications.</p><p>In this section we describe several feature detectors that have been developed with computational efficiency as one of the main objectives. The DoGs detector approximates the Laplacian using multiple scalespace pyramids (see Section 6.1). SURF makes use of integral images to efficiently compute a rough approximation of the Hessian matrix (Section 6.2). FAST evaluates only a limited number of individual pixel intensities using decision trees (see Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Difference-of-Gaussians</head><p>The Difference-of-Gaussians detector, or DoG for short, has been proposed in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b125">126]</ref>. It is a scale-invariant detector which extracts blobs in the image by approximating the Laplacian L 2 xx + L 2 yy (see also Section 4.1). Based on the diffusion equation in scale-space theory <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b233">234,</ref><ref type="bibr" target="#b257">258]</ref>, it can be shown that the Laplacian corresponds to the derivative of the image in the scale direction. Since the difference between neighboring points in a given direction approximates the derivative in this direction, the difference between images at different scales approximates the derivative with respect to scale. Furthermore, Gaussian blurring is often applied to generate images at various scales. Hence, the DoG images produce responses which approximate the LoG. The computation of second-order derivatives in x and y directions is then avoided, as illustrated in Figure <ref type="figure">6</ref>.1.</p><p>The actual computation scheme is illustrated in Figure <ref type="figure">6</ref>.2. The image is smoothed several times with a Gaussian convolution mask. These smoothed versions are combined pairwise to compute a set of DoG blob response maps. Local maxima in these maps are located both over space and over scales with non-maximal suppression, and the locations are further refined with quadratic interpolation. After a few smoothing steps, the image can be subsampled to process the next octave.</p><p>Since the Laplacian gives strong response on edges, an additional filtering step is added, where the eigenvalues of the full Hessian matrix are computed and their strengths evaluated. This filtering step does not affect the overall processing time too much, as it is only needed for a limited number of image locations and scales. The DoG features detected in our example images are shown in Figure <ref type="figure">6</ref>.3. Several frames per second can be processed with this method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">SURF: Speeded Up Robust Features</head><p>In the context of realtime face detection, Viola and Jones have proposed to use integral images <ref type="bibr" target="#b251">[252]</ref>, which allow for very fast computation of Haar wavelets or any box-type convolution filter. First, we will describe the basic idea of integral images. Then we show how this technique can be used to obtain a fast approximation of the Hessian matrix, as used in SURF (Speeded-Up Robust Features) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Integral Images</head><p>The entry of an integral image I Σ (x) at a location x = (x, y) represents the sum of all pixels in the input image I of a rectangular region formed Once the integral image has been computed, it takes four additions to calculate the sum of the intensities over any upright, rectangular area, as shown in Figure <ref type="figure">6</ref>.4. Moreover, the calculation time is independent of the size of the rectangular area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">SURF</head><p>SURF or Speeded Up Robust Features have been proposed by Bay et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. It is a scale-invariant feature detector based on the Hessian-matrix, as is, e.g., the Hessian-Laplace detector (see Section 4.2). However, rather than using a different measure for selecting the location and the scale, the determinant of the Hessian is used for both. The Hessian matrix is roughly approximated, using a set of boxtype filters, and no smoothing is applied when going from one scale to the next.</p><p>Gaussians are optimal for scale-space analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b116">117]</ref>, but in practice they have to be discretized (Figure <ref type="figure">6</ref>.5 left) which introduces artifacts, in particular in small Gaussian Kernels. SURF pushes the approximation even further, using the box filters as shown in the right half of Figure <ref type="figure">6</ref>.5. These approximate second-order Gaussian derivatives, and can be evaluated very fast using integral images, independently of their size. Surprisingly, in spite of the rough approximations, the performance of the feature detector is comparable to the results obtained with the discretized Gaussians. Box filters can produce a sufficient approximation of the Gaussian derivatives as there are many other sources of significant noise in the processing chain.</p><p>The 9 × 9 box filters in Figure <ref type="figure">6</ref>.5 are approximations for a Gaussian with σ = 1.2 and represent the finest scale (i.e., highest spatial resolution). We will denote them by D xx , D yy , and D xy . The weights applied to the rectangular regions are kept simple for computational efficiency, but we need to further balance the relative weights in the expression for the Hessian's determinant with (6.</p><p>2)</p><p>The approximated determinant of the Hessian represents the blob response in the image at location x. These responses are stored in a blob response map, and local maxima are detected and refined using quadratic interpolation, as with DoG (see Section 6.1). Figure <ref type="figure">6</ref>.6 shows the result of the SURF detector for our example images. SURF has been reported to be more than five times faster than DoG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">FAST: Features from Accelerated Segment Test</head><p>The FAST detector, introduced by Rosten and Drummond in <ref type="bibr" target="#b201">[202,</ref><ref type="bibr" target="#b202">203]</ref> builds on the SUSAN detector <ref type="bibr" target="#b231">[232]</ref> previously discussed in Section 3.3. SUSAN computes the fraction of pixels within a neighborhood which have similar intensity to the center pixel. This idea is taken further by FAST, which compares pixels only on a circle of fixed radius around the point. The test criterion operates by considering a circle of 16 pixels around the corner candidate (see Figure <ref type="figure">6</ref>.7). Initially pixels 1 and 2 are compared with a threshold, then 3 and 4 as well as the remaining ones at the end. The pixels are classified into dark, similar, and brighter subsets. The ID3 algorithm from <ref type="bibr" target="#b177">[178]</ref> is used to select the pixels which yield the most information about whether the candidate pixel is a corner. This is measured by the entropy of the positive and  negative corner classification responses based on this pixel. The process is applied recursively on all three subsets and terminates when the entropy of a subset is zero. The decision tree resulting from this partitioning is then converted into C-code, creating a long string of nested if-then-else statements which is compiled and used as a corner detector. Finally non-maxima suppression is applied on the sum of the absolute difference between the pixels in the circle and the center pixel. This results in a very efficient detector which is up to 30 times faster than the DoG detector discussed in Section 6.1 -albeit not invariant to scale changes. The FAST features found in our example images are displayed in Figure <ref type="figure">6</ref>.8. An extension to a multi-scale detector by scale selection with the Laplacian function was proposed in <ref type="bibr" target="#b111">[112]</ref>. They estimate the Laplacian using gray-level differences between pixels on the circle and the central one and retain only the locations where this estimate is largest. This proves to be sufficient to produce a large number of keypoint candidates from which the unstable ones are filtered out during the recognition process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion</head><p>The ultimate goal of methods focussing on efficiency is often realtime processing of a video stream or dealing with large amounts of data.</p><p>However, to some extent, this is a moving target. Computation power increases rapidly over time but so does the number of features we extract or the size of the databases we deal with. Moreover, feature detection is not the final goal, but just the first step in a processing chain, followed by matching, tracking, object recognition, etc. In many applications significant performance improvement can be obtained just by increasing the number of training examples. Efficiency is therefore one of the major properties equally important to invariance or robustness which should be considered when designing or selecting a feature detector.</p><p>Coming back to the first point, especially the advent of powerful graphical processing units opens up new possibilities. Apart from the methods described above, which obtain a speedup by platform independent algorithmic changes, further speedups become possible by exploiting the special structure and parallelism that can be realized with GPUs. Some examples of such work can be found in <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b229">230]</ref>. An FPGA-based implementation of the Harris-Affine feature detector (see Section 3.4) is discussed in <ref type="bibr" target="#b27">[28]</ref> and of the DoG detector (see Section 6.1 in <ref type="bibr" target="#b215">[216]</ref>. This significantly reduces the time needed to compute all features on a normal-sized image and enables video frame rate processing. In spite of this new trend, the basic ideas and methods described in this section still hold, as they are sufficiently general and widely applicable.</p><p>Finally, more efficient methods usually come at a price. A tradeoff has to be made between efficiency on the one hand and accuracy or repeatability on the other hand. Surprisingly, the DoG, SURF, and FAST detectors are competitive with the standard, more computationally expensive feature detectors and may produce better results for some applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Conclusion</head><p>In this final section of our survey, we give an overview of the previously discussed methods and highlight their respective strengths and weaknesses. We give some hints on how to use these features, and on how to select the appropriate feature detector for a given application. Finally, we discuss some open issues and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">How to Select Your Feature Detector?</head><p>Below, we give a few guidelines on what feature detector to use for a given application. This does not give a precise and definitive answer, but indicates a few points one needs to consider when searching for a suitable detector. We refer the reader to Section 1.4 where we define the properties of local features often mentioned here.</p><p>First, we organized the feature detectors in this survey based on the type of image structures they extract -corners, blobs or regions. Depending on the image content, some of these image structures are more common than others, thus the number of features found with a given detector may vary for different image categories. If little is known about the image content in advance, it is generally recommended to combine different complementary detectors, i.e., extracting different types of features.</p><p>Second, feature detectors can be distinguished based on the level of invariance. There have been many evaluations which focus on this property <ref type="bibr" target="#b144">[145,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b214">215]</ref>. One might be tempted to always select the highest level of invariance available, so as to compensate for as much variability as possible. However, the discriminative power of features is reduced at increased levels of invariance. As more patterns are to be judged equivalent, there is more parameters to estimate, thus more possible sources of noise. Also, the feature detection process becomes more complex, which affects both the computational complexity as well as the repeatability. As a result, a basic rule of thumb is to use no more invariance than what is truly needed by the application at hand. Moreover, if the expected transformations are relatively small, it is often better to count on the robustness of the feature detection and description rather than to increase the level of invariance. That is also the reason why feature detectors invariant to perspective transformations are of little use.</p><p>All detectors discussed in this survey are invariant to translations and rotations. The former automatically follows from the use of local features. The latter can be achieved relatively easily at limited extra cost. Sometimes, rotation invariance is not required -e.g., if all images are taken upright and the objects are always upright as well (buildings, cars, etc.). In these cases, the rotation invariant detectors can be combined with a rotation variant descriptor, to ensure good discriminative power. In all other cases, a descriptor with at most the same level of invariance as the detector is preferred.</p><p>Finally, there are a number of qualitative properties of the detectors to consider. Depending on the application scenario, some of these properties are more crucial than others. When dealing with categorylevel object recognition, robustness to small appearance variations is important to deal with the within-class variability. When fitting a parametric model to the data, as for camera calibration or 3D modeling, the localization accuracy is essential. For online applications or applications where a large amount of data needs to be processed, efficiency is the most important criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Summary on the Detectors</head><p>Table <ref type="table">7</ref>.1 gives an overview of the most important properties for the feature detectors described in Sections 3-6.</p><p>The feature detectors in Table <ref type="table">7</ref>.1 are organized in 4 groups according to their invariance: rotation, similarity, affine, and perspective. We compare the properties within each group. For rotation invariant features the highest repeatability and localization accuracy in many tests has been obtained by the Harris detector. The Hessian detector finds blobs which are not as well localized and requires second-order derivatives to be computed. The SUSAN detector avoids computation of derivatives and is known for its efficiency, however the absence of smoothing makes it more susceptible to noise. All the rotation invariant methods are suitable for applications where only the spatial location of the features is used and no large scale changes are expected, e.g., structure from motion or camera calibration.</p><p>In the scale-invariant group Harris-Laplace shows high repeatability and localization accuracy inherited from the Harris detector <ref type="bibr" target="#b214">[215]</ref>. However, its scale estimation is less accurate due to the multiscale nature of corners. Hessian-Laplace is more robust than its single scale version <ref type="bibr" target="#b144">[145]</ref>. This is due to the fact that blob-like structures are better localized in scale than corners and the detector benefits from multiscale analysis although it is less accurately localized in the image plane. DoG and SURF detectors were designed for efficiency and the other properties are slightly compromised. However, for most applications they are still more than sufficient. Quantity and good coverage of the image are crucial in recognition applications, where localization accuracy is less important. Thus, Hessian-Laplace detectors have been successful in various categorization tasks although there are detectors with higher repeatability rate. Random and dense sampling also provide good results in this context which confirms the coverage requirements of recognition methods <ref type="bibr" target="#b168">[169]</ref> -although they result in far less compact representations than the interest points. DoG detector performs extremely well in matching <ref type="bibr" target="#b25">[26]</ref> and image retrieval <ref type="bibr" target="#b123">[124]</ref> probably due to a good balance between spatial localization and scale estimation accuracy. Note that for the scale and affine invariant detectors, the difference between corner and blob detectors becomes less outspoken, with most detectors detecting a mixture of both feature types -although they still show a preference for either type.</p><p>The affine invariant Harris and Hessian follow the observations from previous groups. Salient regions require to compute a histogram and its entropy for each region candidate in scale or affine space, which results in large computational cost <ref type="bibr" target="#b144">[145]</ref>. On the positive side the regions can be ranked according to their complexity or information content. Some applications exploit this and use only a small subset of the salient regions while still obtaining a good performance in, e.g., recognition <ref type="bibr" target="#b64">[65]</ref>. Originally, they were only scale-invariant, but later they have been extended to affine invariance. The edge based regions focus on corners formed by edge junctions which gives good localization accuracy and repeatability but the number of detected features is small.</p><p>The region detectors are based on the idea of segmenting boundaries of uniform regions. Intensity based regions use a heuristic method and find similar regions to MSER. Superpixels are typically based on segmentation methods which are computationally expensive like normalized cuts. The level of invariance of superpixels depends mostly on the segmentation algorithm used. In contrast to superpixels, MSER selects only the most stable regions which results in high repeatability. MSER is also efficient due to the use of a watershed segmentation algorithm. Affine invariant detectors are beneficial in cases where extreme geometric deformations are expected. Otherwise their scaleinvariant counterparts usually perform better, in particular for category recognition <ref type="bibr" target="#b138">[139]</ref>. This can be understood from the fact that viewpoint changes up to 30 degrees can usually be dealt with by robustness instead of invariance. Affine deformations are more frequent when the same objects are observed from significantly different viewpoints, e.g., in the context of matching or retrieval. In the case of category recognition variability of object appearance dominates over deformations due to viewpoint changes and affine invariance typically brings little improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Future Work</head><p>So far no theory emerged which would provide guidance in what features should be extracted from images or how they should be sampled regardless the application. It is not clear whether it is possible to have a more principled theory on generic feature extraction.</p><p>Since memory requirements became less of an issue, brute-force approaches which extract various types of features, densely covering images seem to obtain better and better results, e.g., in object category recognition. However, it has been shown frequently that careful design of image measurements leads to better performance regardless the subsequent components of the system. Even though a lot of progress has been made in the domain of feature extraction -especially with respect to the level of invariance, and even though impressive applications have been built using local features, they still have a number of shortcomings. We would like to emphasize again that for different applications different feature properties may be important and the success of an approach largely depends on the appropriate selection of features. For example in an application like registration of a scene observed from different viewpoints the underlying principles are very clear and repeatability, invariance as well as quantity of features, as defined in Section 1.4, are crucial. In category recognition it is hard to define and measure the repeatability therefore robustness to small appearance variations matters more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Limited Repeatability</head><p>In spite of their success, the repeatability of the local feature detectors is still very limited, with repeatability scores below 50% being quite common (see e.g., <ref type="bibr" target="#b144">[145]</ref>). This indicates there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Limited Robustness</head><p>One of the major drawbacks of all feature detectors is a weak robustness to various transformations in particular in estimation of local scale and shape. The extraction methods are very unstable for small regions. They produce stable scale and shape estimates for large support regions but then other effects like occlusion and background clutter start to affect the results. Methods that extract stable features over a wide range of scales will be very beneficial for various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Lack of Semantic Interpretation</head><p>After vector quantization, these local features are often referred to as visual words or object parts. Yet, this is over-optimistic, as they do not have any semantic connotation. They are just local image fragments which sometimes correspond to meaningful object parts (e.g., wheels of a car) only by coincidence. From a purely bottom-up approach, this is all one can expect. Yet, bringing in top-down information or external knowledge about the world, semantically meaningful object parts could probably be discovered. This could be in the form of an intermediate level representation, or as novel category-specific local features that are learnt from a set of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Automatic Selection of the Optimal Feature Detector</head><p>There is a range of feature detectors available, which all have their strengths and weaknesses. Which one performs best depends not only on the application, but also on the image content. To circumvent this problem, researchers often use several detectors in parallel. However, this has a negative impact on the needed computation time. A tool that could quickly gather some image statistics and suggest the most suitable detector would be a valuable instrument for time-critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.5">Complementary Features</head><p>New image measurements with a focus on complementarity of features is another direction to explore. Overcomplete representations, which result from the simultaneous use of multiple detectors provide a temporary solution only in spite of efficient multi-type feature detectors. An efficient combination of complementary detectors or a multi-type detector providing complementary features for compact representation would be much more useful given the increasing amounts of data to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.6">Performance Evaluation</head><p>By far the most frequently used evaluation measure is the repeatability of features. While this is one of the most important properties it does not guarantee high performance in a given application. New criteria should take into account all the properties discussed in Section 1.4. Another property that is crucial for generative recognition approaches and should also be addressed in general performance evaluations is the reconstruction capability of features. Furthermore, complementarity measures of various detectors are still to be defined. In general, a more principled and probabilistic way of evaluating features which would give a good performance prediction in various applications would be valuable. There is sufficient amount of test data which emphasizes various aspects of features and was explicitly created for evaluating feature detectors. It would be useful to organize it in a common evaluation framework with well defined tests and criteria. Automatic tools which would perform extensive evaluations given a feature detector with specified input and output format, would be of great use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Conclusions</head><p>Local features are a popular tool for image description nowadays. They are the standard representation for wide baseline matching and object recognition, both for specific objects as well as for category-level schemes.</p><p>In this survey, we gave an overview of some of the most widely used detectors, with a qualitative evaluation of their respective strengths and weaknesses, which can be found at the end of the sections and chapters. We also put the work on local feature detection in context, by summarizing the progress in feature detection from the early days of computer vision up to now. These early works from the pre-internet era tend to be forgotten. Yet they contain valuable insights and ideas, that can inspire future research on local features and avoid a waste of resources by reinventing the wheel. The literature is huge, and we could only touch the different contributions without going into details. Yet, we hope to provide the right pointers so those who are interested have a starting point and can delve deeper if they want to.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 . 1</head><label>11</label><figDesc>Fig. 1.1 Importance of corners and junctions in visual recognition [20] and an image example with interest points provided by a corner detector (cf. Section 3.2).</figDesc><graphic coords="3,156.11,141.52,108.11,158.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 5</head><label>5</label><figDesc>Color-based Methods 203 from visual cortex which are multiscale Gabor and edgel detectors followed by local maxima selection methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 . 1</head><label>31</label><figDesc>Fig. 3.1 Illustration of the components of the second moment matrix and Harris cornerness measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 . 2</head><label>32</label><figDesc>Fig. 3.2 Harris corners detected on rotated image examples.</figDesc><graphic coords="43,144.30,496.34,148.83,130.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.3 SUSAN corners are detected by segmenting a circular neighborhood into "similar" (orange) and "dissimilar" (blue) regions. Corners are located where the relative area of the "similar" region (usan) reaches a local minimum below a certain threshold.</figDesc><graphic coords="44,180.52,469.11,252.00,141.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.4 SUSAN corners found for our example images.</figDesc><graphic coords="45,151.38,492.72,152.58,133.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 . 5</head><label>35</label><figDesc>Fig. 3.5 Example of characteristic scales. The top row shows images taken with different zoom. The bottom row shows the responses of the Laplacian over scales for two corresponding points. The characteristic scales are 10.1 and 3.9 for the left and right images, respectively. The ratio of scales corresponds to the scale factor (2.5) between the two images. The radius of displayed regions in the top row is equal to 3 times the selected scales.</figDesc><graphic coords="47,157.01,223.16,141.49,78.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 . 6</head><label>36</label><figDesc>Fig. 3.6 Corresponding features found with the Harris-Laplace detector. Only a subset of corresponding features is displayed to avoid clutter. The circles indicate the scale of the features.</figDesc><graphic coords="47,146.99,485.55,145.14,126.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )</head><label>1</label><figDesc>Detect the initial region with the Harris-Laplace detector. (2) Estimate the affine shape with the second moment matrix. (3) Normalize the affine region to a circular one. (4) Re-detect the new location and scale in the normalized image. (5) Go to step 2 if the eigenvalues of the second moment matrix for the new point are not equal. The iterations are illustrated in Figure 3.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 3 . 7</head><label>37</label><figDesc>Fig. 3.7 Iterative detection of an affine invariant interest point in the presence of an affine transformation (top and bottom rows). The first column shows the points used for initialization. The consecutive columns show the points and regions after iterations 1, 2, 3, and 4. Note that the regions converge after 4 iterations to corresponding image regions.</figDesc><graphic coords="48,309.58,520.99,79.69,79.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 . 8</head><label>38</label><figDesc>Fig. 3.8 Diagram illustrating the affine normalization using the second moment matrices. Image coordinates are transformed with matrices M -1/2 L and M -1/2 R .</figDesc><graphic coords="49,198.17,243.52,50.39,50.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.9 Harris-Affine regions generated for two different views of a planar scene (subset).In spite of the affine deformation, the region shapes clearly correspond.</figDesc><graphic coords="50,144.26,142.10,137.94,115.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.10 The edge-based region detector starts from a corner point p and exploits nearby edge information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.11 Originally detected region shapes for the edge-based regions (subset).</figDesc><graphic coords="52,139.31,142.47,163.47,136.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.12 Edge-based regions generated for the two example images, represented with ellipses (subset).</figDesc><graphic coords="53,140.07,142.44,146.40,122.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 4 . 1</head><label>41</label><figDesc>Fig. 4.1 Illustration of the components of the Hessian matrix and Hessian determinant.</figDesc><graphic coords="57,140.45,355.36,154.32,134.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 4 . 2</head><label>42</label><figDesc>Fig. 4.2 Output of the Hessian detector applied at a given scale to example images with rotation (subset).</figDesc><graphic coords="57,321.11,355.36,155.31,134.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figures 4 .</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>and 4.4 for the scale-invariant Hessian-Laplace and affine invariant Hessian-Affine, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 4 . 3</head><label>43</label><figDesc>Fig. 4.3 Output of Hessian-Laplace detector applied to example images with scale change (subset).</figDesc><graphic coords="58,140.45,141.20,157.05,137.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.4 Hessian-Affine regions generated for two views of the example scene (subset).</figDesc><graphic coords="58,143.60,329.98,141.06,117.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 4 . 5</head><label>45</label><figDesc>Fig. 4.5 Salient regions found for the two example images related to a change in viewpoint (subset).</figDesc><graphic coords="60,146.48,142.56,138.17,115.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 5 . 1</head><label>51</label><figDesc>Fig. 5.1 Construction of intensity-based regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.1). The following function is evaluated along each ray:f (t) = abs(I(t) -I 0 ) max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.2 Intensity-based regions found for the graffiti images (subset).</figDesc><graphic coords="64,141.99,142.22,137.99,115.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 5 . 3</head><label>53</label><figDesc>Fig. 5.3 Regions detected with MSER on the graffiti images (subset).</figDesc><graphic coords="65,156.23,505.82,144.12,119.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.4 Final MSER regions for the graffiti images (subset).</figDesc><graphic coords="66,143.66,142.22,136.57,113.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 5 . 5</head><label>55</label><figDesc>Fig. 5.5 Superpixels generated for the example image.</figDesc><graphic coords="67,156.24,141.94,143.87,125.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Fig. 6 . 1 Fig. 6 . 2</head><label>6162</label><figDesc>Fig. 6.1 The Laplacian can be approximated as a difference of two Gaussian smoothed images.</figDesc><graphic coords="71,155.52,541.53,302.40,79.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Fig. 6 . 3</head><label>63</label><figDesc>Fig. 6.3 Local features detected with the DoG-detector.</figDesc><graphic coords="72,140.35,286.70,158.84,138.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Fig. 6 . 4</head><label>64</label><figDesc>Fig. 6.4 Using integral images, it takes only four operations to calculate the area of a rectangular region of any size.</figDesc><graphic coords="73,219.52,141.63,174.00,131.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. 6 . 5</head><label>65</label><figDesc>Fig. 6.5 Left to right: the (discretised and cropped) Gaussian second-order partial derivative in y-direction and xy-direction respectively; SURF's box-filter approximation for the secondorder Gaussian partial derivative in y-direction and xy-direction. The gray regions are equal to zero.</figDesc><graphic coords="74,139.15,141.40,336.00,56.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>|Lxy( 1</head><label>1</label><figDesc>.2)| F |Dxx/yy(9)| F |L xx/yy (1.2)| F |Dxy(9)| F = 0.616 . . . 0.6 for the smallest scale, where |x| F is the Frobenius norm. This yields det(H approx ) = D xx D yy + (0.6D xy ) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>6. 3</head><label>3</label><figDesc>Fig. 6.6 Local features detected with the SURF-detector.</figDesc><graphic coords="75,141.03,141.39,158.72,138.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Fig. 6 . 7</head><label>67</label><figDesc>Fig. 6.7 Illustration of pixels examined by the FAST detector.</figDesc><graphic coords="75,156.15,487.60,142.03,138.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Fig. 6 . 8</head><label>68</label><figDesc>Fig. 6.8 Local features detected with the FAST detector.</figDesc><graphic coords="76,140.22,142.24,154.10,134.65" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Plessey Electronic Research Ltd.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>see http://www.robots.ox.ac.uk/ ˜vgg/research/affine.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to acknowledge support from the Network of Excellence PASCAL and the Flemish Fund for Scientific Research FWO. This work was also supported by EPSRC EP/F003420/1 and VIDI-Video IST-2-045547 project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fingerprint enhancement by shape adaptation of scale-space operators with automatic scale selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2027" to="2042" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affine morphological multiscale analysis of corners and multiple junctions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curvature and tangential deflection of discrete arcs: A theory based on the commutator of scatter matrix pairs and its application to vertex detection in planar shape data</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On detecting dominant points</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="451" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The curvature primal sketch</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="14" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some informational aspects of visual perception</title>
		<author>
			<persName><forename type="first">F</forename><surname>Attneave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="183" to="193" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniqueness of the gaussian kernel for scale-space filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Babaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">COP: A new corner detector</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Don</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1349" to="1360" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer identification of visual surface</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer and Graphics Image Processing</title>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="118" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual and Conceptual Focus of Attention Structured Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rosenthal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>References 265</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using learning to facilitate the evolution of features for recognizing visual concepts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dejong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vafaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="311" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust and fully automated image registration using invariant features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reliable feature matching across widely separated views</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="774" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotationally invariant image operators</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Beaudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Pattern Recognition</title>
		<meeting>the International Joint Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shape context: A new descriptor for shape matching and object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems</title>
		<meeting>the Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="831" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An improved corner detection algorithm based on chain-coded plane curves</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Beus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S H</forename><surname>Tiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="296" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding junctions using the image gradient</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Beymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="720" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">94</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accuracy in image measure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conference on Videometrics III</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2350</biblScope>
			<biblScope unit="page" from="218" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A model of human preattentive visual detection of edge orientation anomalies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Brecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference of Visual Information Processing: From Neurons to Chips</title>
		<meeting>the SPIE Conference of Visual Information Processing: From Neurons to Chips</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">1473</biblScope>
			<biblScope unit="page" from="39" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature tracking with automatic selection of spatial scales</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene analysis using regions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Brice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Fennema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="205" to="226" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognising panoramas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1218" to="1227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Implementation of an affine-covariant feature detector in field-programmable gate arrays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Maclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Systems</title>
		<meeting>the International Conference on Computer Vision Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A statistical model for general contextual object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision, part I</title>
		<meeting>the European Conference on Computer Vision, part I</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="350" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Blobworld: Image segmentation using expectation-maximization and its application to image querying</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1026" to="1038" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling the role of parallel processing in visual search</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="225" to="271" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Corner point detection using nest moving average</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1533" to="1537" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Vision, instruction and action</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chapman</surname></persName>
		</author>
		<idno>AI-TR-1204</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>AI Laboratory, MIT</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wavelet transformation for gray-level corner detection</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="861" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A multiscaling approach based on morphological filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="694" to="700" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian labelling of corners using a grey-level corner image mode</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="687" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local scale selection for gaussian based description techniques</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chomat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Colin Deverdière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="117" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modal control of attentive vision system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Ferrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="514" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An Experimental Evaluation of Projective Invariants</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The dissimilarity corner detectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kitchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1377" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Early jump-out corner detectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kitchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="823" to="828" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance evaluation of corner detection algorithms under affine and similarity transforms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coherent regions for concise and stable image description</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Extraction et appariements robustes des points d&apos;intérêt de deux images non etalonnées</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Cottier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Rhone-Alpes</pubPlace>
		</imprint>
		<respStmt>
			<orgName>LIFIA-IMAG-INRIA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognizing objects by piecing together the segmentation puzzle</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A representation for shape based on peaks and ridges in the difference of low pass transform</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="170" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple resolution representation and probabilistic matching of 2D gray-scale shape</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="121" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An attentional prototype for early vision</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Application of tensor theory to object recognition and orientation determination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cyganski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="662" to="673" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Application of the generalised hough transform to corner detection</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="54" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recovering and characterizing image features using an efficient model-based approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaszka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate corner detection: An analytical study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giraudon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A computational approach for corner and vertex detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giraudon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="124" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A neural network based corner detection method</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2116" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient maximally stable extremal region (MSER) tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Volumetric model and 3D-trajectory of a moving car derived from monocular TV-frame sequence of a street scene</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dreschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="199" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Matching images with different resolutions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dufournaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Optimum uniform piecewise linear approximation of planar curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dunham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A corner finding algorithm for image analysis and registration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference</title>
		<meeting>AAAI Conference</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="46" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Finding &apos;vertices&apos; in a picture</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Decomposition of polygons into simpler components: Feature generation for syntactic pattern recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simultaneous object recognition and segmentation by image exploration</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="40" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scale and the differential structure of images</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M J</forename><surname>Florack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="376" to="388" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Shape descriptors for maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Forssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="59" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A framework for low level feature extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A fast operator for detection and precise location of distinct points, corners and centres of circular features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gülch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intercommission Conference on Fast Processing of Photogrammetric Data</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="281" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Evaluation of local detectors on non-planar scenes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Austrian Association for Pattern Recognition Workshop</title>
		<meeting>the Austrian Association for Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A review of relevant problems in the processing of line-drawing data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Interpretation and Classification of Images</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Graselli</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="155" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Computer processing of line drawing images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surveys</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="97" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A corner-finding algorithm for chain-coded curves</title>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="297" to="303" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Texture segmentation by multiscale aggregation of filter responses and shape elements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="716" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural networks for complex scene recognition: Simulation of a visual system with several cortical areas</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cocquerez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Zernike moment-based feature detectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="934" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Saccadic object recognition with an active vision system</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Giefing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mallot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Intelligence</title>
		<meeting>the European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="803" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Robust Description and Matching of Image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Evaluation de détecteurs de points d&apos;intérêt pour la couleur</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pelé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12ème Congrès Francophone AFRIF-AFIA de Reconnaissance des Formes et Intelligence Artificielle</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A neural network architecture for preattentive vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="65" to="84" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Corner characterization by differential geometry techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guidicci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Computer and Robot Vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="page" from="453" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Spline approximation of line images by modified dynamic programming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of IECE of Japan</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Curvature scale space corner detector with adaptive threshold and dynamic region of support</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H C</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="791" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Simulation of neural contour mechanisms: From simple to end-stopped cells</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heitger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Der Heydt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peterhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kubler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="963" to="981" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Evaluation of corner extraction schemes using invariance method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heyden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="895" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">SIFT implementation and optimization for general-purpose GPU</title>
		<author>
			<persName><forename type="first">S</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision</title>
		<meeting>the International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A new approach to point pattern matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="82" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Finding geometric and relational structures in an image</title>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skordas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Veillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="374" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Computational modeling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Corner detection with covariance propagation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Textons, the elements of texture perception, and their interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="604" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Scale, saliency and image description</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">An affine invariant method for selecting salient regions in images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="345" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">PCA-SIFT: A more distinctive representation for local image descriptors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="511" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A condition number for point matching with application to registration and postregistration error estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Nevel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1437" to="1454" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">An axiomatic approach to corner detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="191" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning an interest operator from human eye movements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Franz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Gray-level corner detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kitchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="102" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: Towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">The structure of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="363" to="396" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A morphological operator for corner detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Curve encoding and detection of discontinuities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Langridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Tracking of multi-state hand models using particle filtering and a hierarchy of multi-scale image features</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Scale-Space and Morphology Workshop</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Scale-Space and Morphology Workshop</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Sparse texture representation using affine invariant neighborhoods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="319" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Wavelet based corner detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="853" to="865" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Keypoint recognition using randomized trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1465" to="1479" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Corner detection and interpretation on planar curves using fuzzy reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1204" to="1210" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A modified morphological corner detector</title>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="286" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Scale-space for discrete signals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="234" to="254" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Detecting salient blob-like image structures and their scales with a scale-space primal sketch -a method for focus-of-attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="318" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Direct estimation of affine image deformation using visual front-end operations with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Edge detection and ridge detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Shape-adapted smoothing in estimation of 3-D shape cues from affine deformations of local 2-D brightness structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Deformation invariant image matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1466" to="1473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Moment preserving corner detection</title>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="460" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Organization of smooth image curves at multiple scales</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Fast radial symmetry for detecting points of interest</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="973" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Corner detection via topographic analysis of vector potential</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="635" to="650" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Textons, contours and regions: Cue integration in image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="918" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="923" to="932" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A new approach to image feature detection with applications</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="627" to="640" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Random subwindows for robust image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">USA</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>W.H. Freeman and Company</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Local affine frames for wide-baseline stereo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Obdrzalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th International Conference Pattern Recognition</title>
		<meeting>16th International Conference Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="363" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Corner detection and curve representation using cubic B-spline</title>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yasumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Corner detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1223" to="1233" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Scale and Affine Invariant Interest Point Detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>INRIA Grenoble</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Local features for object class recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="525" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Multiple object class detection with a generative model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Indexing based on scale-invariant interest points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="525" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">An affine invariant interest point detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Scale and affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Shape recognition with edge based features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Detecting Salient Regions in an Image: From Biological Evidence to Computer Implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>University of Geneva</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A bottom-up attention system for active vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Intelligence</title>
		<meeting>the European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="808" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Computer methods for creating photomosaics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Milgram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1113" to="1119" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Performance evaluation of corner detection algorithms under affine and similarity transforms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mohanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Scale-based description of plannar curves and two-dimensional shapes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mackworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Robust image corner detection through curvature scale-space</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Suomela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1376" to="1381" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Differential invariants for color images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="838" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Towards automatic visual obstacle avoidance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moravec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="584" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Visual mapping by a robot rover</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moravec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intellingence</title>
		<meeting>the International Joint Conference on Artificial Intellingence</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="598" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Rover visual obstacle avoidance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moravec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="785" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Evaluation of features detectors and descriptors based on 3D objects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moreels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="800" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Evaluation of features detectors and descriptors based on 3D objects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moreels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="807" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Recovering human body configurations: Combining segmentation and recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3D objects from appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">N-tree disjoint-set forests for maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Multiclass object recognition with sparse, localized features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Displacement vectors derived from second-order intensity variations in image sequences</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="85" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Pseudo-coding method for digital line figures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Agui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the IECE</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note>J68-D</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Visual search</title>
		<author>
			<persName><forename type="first">U</forename><surname>Neisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="94" to="102" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Finding corners</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="128" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Descriptions of Image Surfaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Department of Engineering Science, Oxford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="490" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Corner detection on digital curves based on local symmetry of the shape</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ogawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Model-based corner detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Orange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C A</forename><surname>Groen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="690" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A model of preattentive Region definition based on texture analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pabst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Reitboeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eckhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Models of Brain Function</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Local ordered grey levels as an aid to corner detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Paler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foglein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="535" to="543" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
		<title level="m">Structural Pattern Recognition</title>
		<meeting><address><addrLine>Berlin, Heidelberg, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Algorithms for Graphics and Image Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Computer Science Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Problems in recognition of drawings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntactic and Structural Pattern Recognition</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="103" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Stable affine frames on isophotes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Obdrzalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Evaluation of corner detection algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21th Southeastern Symposium on System Theory</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Optimal corner detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Brackle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Graphics Image Processing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="230" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Scale-based detection of corners of planar curves</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rattarangsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="430" to="449" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Context free attentional operators: The generalized symmetry transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Scale-invariant contour completion using conditional random fields</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1214" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">2D feature detection via local energy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="353" to="368" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Matching hierarchical structures in a machine vision system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="845" to="852" />
		</imprint>
	</monogr>
	<note>Intelligent Autonomous Systems</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Recognizing corners by fitting parametric models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="230" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Localization properties of direct corner detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="150" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">On the precision in estimating the location of edges and corners</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="22" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Picture processing by computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="147" to="176" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Digital image processing and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Image Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Angle detection on digital curves</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="875" to="878" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<title level="m">Digital Picture Processing</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Edge and curve detection for digital scene analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Edge and curve detection: Further experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thurston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="677" to="715" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An improved method of angle detection on digital curves</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Weszka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="940" to="941" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Detection of general edges and keypoints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rosenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heitger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kubler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><surname>Heydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Representing curves at their natural scales</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1315" to="1325" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Determining local natural scales of curves</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Measuring corner properties</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="307" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Fusing points and lines for high performance tracking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1508" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Machine learning for high-speed corner detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Planar object recognition using projective shape representation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Rothwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="57" to="99" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">A comparison of corner detection techniques for chain coded curves</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<idno>623</idno>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
		<respStmt>
			<orgName>Maryland University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Simulating visual attention</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Sandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">A parallel procedure for the detection of dominant points on digital curves</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="403" to="412" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Viewpoint invariant texture matching and wide baseline stereo</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Multi-view matching for unordered image sets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="414" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Probabilistic object recognition using multidimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="50" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Recognition without correspondence using multi-dimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="50" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Combining gray-value invariants with local constraints for object recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="872" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Local gray-value invariants for image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Comparing and evaluating interest points</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="230" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Evaluation of interest point detectors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Visual motion estimation and terrain modeling for planetary rovers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Se</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Barfoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jasiobedzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Artificial Intelligence for Robotics and Automation in Space</title>
		<meeting>the International Symposium on Artificial Intelligence for Robotics and Automation in Space</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Corner detectors for affine invariant salient regions: Is color important</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image and Video Retrieval</title>
		<meeting>International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Evaluation of salient point techniques</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loupias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Sedgewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Algorithms</forename><surname>Addison-Wesley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">An evaluation of local shape-based features for pedestrian detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Object recognition with features inspired by visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Structural saliency: The detection of globally salient structures using a locally connected network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Detecting time-varying corners</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Segmentation and boundary detection using multiscale intensity measurements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Corner detection based on modified Hough transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1039" to="1049" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Gray-level corner detection a generalization and a robust real time implementation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shneier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Graphics Image Processing</title>
		<meeting>the Computer Vision Graphics Image Processing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="54" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">GPU-based video feature tracking and matching</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Genc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDGE, Workshop on Edge Computing Using New Commodity Architectures</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">SUSAN -A new approach to low level image processing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="45" to="78" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">A mean field annealing approach to robust corner detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man Cybernetics Part B</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sporring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Florack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<title level="m">Gaussian Scale-Space Theory</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">How good are local features for classes of geometric objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schiele</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Patch-based stereo in a general binocular viewing geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Super</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Klarquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="252" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal in Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">On the detection of dominant points on digital curves</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="859" to="872" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Assessing the performance of corner detectors for point feature tracking applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tissainayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="663" to="679" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Fast corner detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trajkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="75" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Features and objects: The fourteenth Berlett memorial lecture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimantal Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="201" to="237" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Detecting keypoints with stable position, orientation and scale under illumination changes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="100" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Synthesis of interest point detectors through genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Olague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="887" to="894" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Boundary-based corner detection using neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Eigenfaces for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Vector quantizing feature space with a regular lattice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Content-based image retrieval based on local affinely invariant regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Visual Information Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="493" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Wide baseline stereo matching based on local, affinely invariant regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="412" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Matching widely separated views based on affine invariant regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="61" to="85" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Boosting color saliency in image feature detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Features for recognition: Viewpoint invariance for non-planar scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1474" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Locating salient object features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Real-time corner detection algorithm for motion estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="695" to="703" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Scanning from coarse to fine spatial scales in the human visual system after the onset of a stimulus</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Watt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2006" to="2021" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Learning local image descriptors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="1019" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Digital curvature estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="382" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Corner detection in color images through a multiscale combination of end-stopped cortical cells</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lourens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="531" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Corner detection using the map technique</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">A morphological algorithm for detecting dominant points on digital curves</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Proceedings, Nonlinear Image Processing 2424</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="372" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Analysis of gray level corner detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">On critical point detection of digital shapes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Chirlian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="737" to="748" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">A mathematical comparison of point detectors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">172</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Corner detection using the facet model</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Zuniga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
