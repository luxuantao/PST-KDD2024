<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence tagging tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is not a new method, but a version of prefix-tuning (Li and Liang, 2021) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models <ref type="bibr" target="#b12">(Han et al., 2021a)</ref> improve performance on a wide range of natural language understanding (NLU) tasks such as question answering <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref> and textual entailment <ref type="bibr" target="#b5">(Dagan et al., 2005)</ref>. A widelyused method, fine-tuning, updates the entire set of model parameters for a target task. While finetuning obtains good performance, it is memoryconsuming during training because gradients and optimizer states for all parameters must be stored. Moreover, fine-tuning requires keeping a copy of model parameters for each task during inference, which is inconvenient since pretrained models are usually large.  Prompting, on the other hand, freezes all parameters of a pretrained model and uses a natural language prompt to query a language model <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>. For example, for sentiment analysis, we can concatenate a sample with a prompt "This movie is <ref type="bibr">[MASK]</ref>" and ask the pretrained language model to predict the masked token. We can then use the predicted probabilities of "good" and "bad" being the masked token to predict the sample's label. Prompting requires no training at all and stores one single copy of model parameters. However, prompting can lead to suboptimal performance in many cases compared to fine-tuning <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b16">Lester et al., 2021)</ref>.</p><p>Prompt tuning<ref type="foot" target="#foot_1">2</ref> is an idea of tuning only the continuous prompts. Specifically, <ref type="bibr">Liu et al. (2021b)</ref>; <ref type="bibr" target="#b16">Lester et al. (2021)</ref> proposed to add trainable continuous embeddings to the original sequence of input word embeddings. These continuous embeddings (also called continuous prompts) are analogous to discrete manually designed prompts in prompting. Only the continuous prompts are updated during training. While prompt tuning improves over prompting on many tasks <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b16">Lester et al., 2021)</ref>, it still underperforms fine-tuning when the model size is small, specifically less than 10 billion parameters <ref type="bibr" target="#b16">(Lester et al., 2021)</ref>. Moreover, as shown in our experiments, prompt tuning performs poorly compared to fine-tuning on several hard sequence tasks such as extractive question answering and sequence tagging (Cf. Section 4.3).</p><p>Our main contribution in this paper is a novel empirical finding that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks. In contrast to observations in prior work, our discovery reveals the universality and massive potential of prompt tuning for NLU.</p><p>Technically, our approach P-tuning v2 can be viewed as an optimized version of prefix-tuning <ref type="bibr" target="#b18">(Li and Liang, 2021)</ref>, a method designed for generation and adapted to NLU. The most significant improvement originates from using deep prompt tuning, which is to apply continuous prompts for every layer of the pretrained model <ref type="bibr" target="#b18">(Li and Liang, 2021;</ref><ref type="bibr" target="#b27">Qin and Eisner, 2021)</ref>. Deep prompt tuning increases the capacity of continuous prompts and closes the gap to fine-tuning across various settings, especially for small models and hard tasks. Moreover, we present a few details of optimization and implementation for further enhancement of the results.</p><p>Experimental results show that P-tuning v2 matches the performance of fine-tuning at different model scales ranging from 300M to 10B parameters and on various hard NLU tasks such as question answering and sequence tagging. P-tuning v2 has 0.1% to 3% trainable parameters per task compared to fine-tuning, which substantially reduces training time memory cost and per-task storage cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NLU Tasks</head><p>In this work, we categorize NLU challenges into two families: simple tasks and hard sequence tasks.</p><p>• Simple NLU tasks involve classification over a single label. Most datasets from GLUE <ref type="bibr" target="#b39">(Wang et al., 2018)</ref> and Super-GLUE <ref type="bibr">(Wang et al., 2019)</ref>, including Text Classification (e.g., SST-2), Natural Language Inference (NLI, e.g., MNLI-m, RTE), Multiple-choice Question Answering (e.g., BoolQ), and so on, are in this category.</p><p>• Hard sequence NLU tasks involve classification over a sequence of labels. They are problems mostly related to information extraction, such as Open Information Extraction, Named Entity Recognition, Extractive Question Answering, and Semantic Role Labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt Tuning</head><p>Prompt tuning <ref type="bibr" target="#b16">(Lester et al., 2021)</ref>, or Ptuning <ref type="bibr">(Liu et al., 2021b)</ref>, introduces trainable continuous prompts as a substitution to natural language prompts for NLU when backbone language models' parameters are frozen. For example, let V refers to the vocabulary of a language model M and e serves as the embedding function for M.</p><p>To classify a film review x ="Amazing movie!" as positive or negative, it is natural to think of appending a prompt "It is [MASK]" to the review and generating the conditional probabilities of the mask token being predicted as "good" or "bad" as the classification. In this case, prompt tokens {"It", "is", "[MASK]"} belong to the model's vocabulary V, and the input embedding sequence would be [e(x), e("It"), e("is"), e("[MASK]")] (1) However, since the model M is intrinsically continuous, from the perspective of optimization, one can never achieve the optimum with discrete natural prompts. P-tuning, instead, proposes to replace prompt tokens with trainable continuous embeddings [h 0 , ..., h i ] and turn the input sequence into [e(x), h 0 , ..., h i , e("[MASK]")]</p><p>(2) and therefore can be differentially optimized (Cf. Figure <ref type="figure" target="#fig_3">2</ref> (a)). Under the strict constraint that backbone pre-trained models' parameters are frozen, prompt tuning has been proved to have comparable performance to fine-tuning on 10-billion-parameter models in simple NLU tasks <ref type="bibr" target="#b16">(Lester et al., 2021;</ref><ref type="bibr">Kim et al., 2021)</ref> and knowledge probing <ref type="bibr">(Liu et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">P-Tuning v2</head><p>3.1 Lack of Universality</p><p>Prompt tuning and P-tuning have been proved quite effective in many NLP applications (Cf. Section 5). Nevertheless, P-tuning is not yet a comprehensive alternative to fine-tuning, considering the following lack of universality.</p><p>Lack of universality across scales. <ref type="bibr" target="#b16">Lester et al. (2021)</ref> shows that prompt tuning can be comparable to fine-tuning when the model scales to over 10 </p><formula xml:id="formula_0">O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; h i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D</p><formula xml:id="formula_1">P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D   <ref type="formula">2021</ref>) &amp; P-tuning to P-tuning v2. Orange tokens (include h 0 , h i ) refer to prompt embeddings we add; blue tokens are embeddings stored or computed by frozen pre-trained language models. Compared to <ref type="bibr" target="#b16">Lester et al. (2021)</ref>, P-tuning v2 adds trainable continuous prompts to inputs of every transformer layer independently (as prefix-tuning (Li and Liang, 2021) does). Additionally, P-tuning v2 removes verbalizers with LM head and returns to the traditional class labels with ordinary linear head to allow its task-universality.</p><formula xml:id="formula_2">P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j</formula><p>billion parameters. But for those smaller models (from 100M to 1B), there is a significant discrepancy between performances of prompt tuning and fine-tuning, which significantly limits the applicability of prompt tuning.</p><p>Lack of universality across tasks. Though Lester et al. ( <ref type="formula">2021</ref>) and P-tuning have shown superiority on NLU benchmarks such as GLUE and Super-GLUE, their effectiveness on another large family of hard sequence NLU tasks (i.e., sequence tagging) is not verified. First, sequence tagging requires predicting a sequence of labels rather than a single label. Second, sequence tagging usually predicts no-actual-meaning labels, which could be challenging to turn into effective verbalizers <ref type="bibr" target="#b33">(Schick and Schütze, 2020)</ref>. In our experiment (Cf. Section 4.3 and Table <ref type="table" target="#tab_3">3</ref>), we show that Lester et al. ( <ref type="formula">2021</ref>) &amp; P-tuning performs poorly on typical sequence tagging tasks compared to fine-tuning.</p><p>Considering these challenges, we propose Ptuning v2, which adapts prefix-tuning as a universal solution across scales and NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Prompt Tuning</head><p>Prefix-tuning <ref type="bibr" target="#b18">(Li and Liang, 2021)</ref> was originally proposed for natural language generation (NLG) tasks, but we find it very effective for NLU as well. We describe a version of prefix-tuning adapted to NLU.</p><p>In <ref type="bibr" target="#b16">(Lester et al., 2021)</ref> and P-tuning, continuous prompts are only inserted into the sequence of input embeddings (Cf. Figure <ref type="figure" target="#fig_3">2 (a)</ref>) for the transformer's first layer. In the following transformer layers, em-beddings at positions where continuous prompts are inserted are computed by previous transformer layers, which may lead to two possible optimizing challenges.</p><p>1. Limited amount of parameters to tune. Most language models currently can only support a maximum sequence length of 512 (due to the cost of attention's quadratic computational complexity). If we additionally deduct the length of our context (e.g., a sentence to be classified), there is a limited length for us to fill with continuous prompts.</p><p>2. Limited stability when tuning with very deep transformers. As the transformer growing deeper, the impact of prompts from the first transformer layer can be unexpected due to many intermediate layers' computation (with nonlinear activation functions), making our optimization not a very smooth one.</p><p>In light of the challenges, P-tuning v2 leverages multi-layer prompts (i.e., deep prompt tuning) as in prefix-tuning (Li and Liang, 2021) (Cf. Figure <ref type="figure" target="#fig_3">2</ref> (b)), as a major improvement over P-tuning and <ref type="bibr" target="#b16">Lester et al. (2021)</ref>. Prompts in different layers are added as prefix tokens in the input sequence and are independent to each other interlayers (rather than being computed by previous transformer layers). On the one hand, in this way, P-tuning v2 has a larger number of tunable task-specific parameters (from 0.01% to 0.1%-3%) to allow for more per-task capacity, while it is still much smaller than full pre-trained language models; on the other hand, prompts added to deeper layers (e.g., LayerN Prompts in Figure <ref type="figure" target="#fig_3">2</ref>) can have more direct and significant impacts on output predictions with fewer intermediate transformer layers (Cf. Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization and Implementation</head><p>There are also a few helpful optimization and implementation details: Optimization: Reparameterization. Previous methods leverage the reparameterization function to increase training speed, robustness, and performance (e.g., MLP for prefix-tuning and LSTM for P-tuning). However, for NLU tasks, we discover the benefit of this technique depends on tasks and datasets. For some datasets (e.g., RTE and CoNLL04), MLP reparameterization brings a consistent improvement over embedding; for others, reparameterization may show no effect (e.g., BoolQ), sometimes even worse (e.g., CoNLL12). See our ablation study in Section 4.4.</p><p>Optimization: Prompt length. Prompt length plays a central role in the hyper-parameter search of prompt tuning methods. In our experiments, we find different understanding tasks usually achieve their best performance with different prompt lengths, which accords with findings in prefixtuning <ref type="bibr" target="#b18">(Li and Liang, 2021)</ref> where different text generation tasks may have different optimal prompt lengths. See discussions in Section 4.4.</p><p>Optimization: Multi-task learning. Multi-task learning is optional for our method but could be quite helpful. On the one hand, the random initialization of continuous prompts brings in difficulties for optimization, which can be alleviated with more training data or task-related unsupervised pre-training <ref type="bibr" target="#b11">(Gu et al., 2021)</ref>; on the other hand, continuous prompts serve as perfect carriers of task-specific knowledge across tasks and datasets. Our experiment shows that multi-task learning can be a useful complement to P-tuning v2 in some hard sequence tasks, denoted as MPT-2 (Cf. Table <ref type="table" target="#tab_4">2,3,4</ref>).</p><p>Implementation: [CLS] and token classification, rather than verbalizers. Verbalizer <ref type="bibr" target="#b33">(Schick and Schütze, 2020</ref>) has been a central component of prompt tuning, which turns one-hot class labels into meaningful words to make use of the pretrained language model head. Despite its potential necessity in a few-shot setting, the verbalizer is not a must in a full-data supervised setting. It hin-ders the application of prompt tuning to scenarios where we need no-actual-meaning labels and sentence embeddings. Therefore, P-tuning v2 returns to the conventional [CLS] label classification (Cf. Figure <ref type="figure" target="#fig_3">2</ref>) paradigm with random-initialized linear heads. See the comparison in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We conduct extensive experiments over different commonly-used pre-trained models and NLU tasks to verify the effectiveness of P-tuning v2.</p><p>Evaluation Setting. In this work, all results of "prompt tuning", "P-tuning", "P-tuning v2", and "Multitask P-tuning v2" are obtained by freezing the parameters of the transformer and only tuning the continuous prompts. Ratios of task-specific parameters (e.g., 0.1%) are derived from comparing continuous prompts' parameters with transformers' parameters. Only results of "fine-tuning" are obtained by tuning transformers' parameters (without using continuous prompts).</p><p>Another thing to notice is that our experiments are all conducted in the full-data supervised learning setting rather than few-shot learning, which is important because some properties we leverage (e.g., use class labels with linear heads instead of verbalizers with LM heads) are only likely to work in the supervised setting.</p><p>NLU Tasks. First, we include part of datasets from GLUE <ref type="bibr" target="#b39">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr">(Wang et al., 2019)</ref> benchmarks to test P-tuning v2's general NLU ability including SST-2, MNLI-m, RTE, BoolQ and CB. More importantly, we introduce a suite of tasks in the form of sequence tagging, which require language model to predict the class of every token in the input sequence, including Named Entity <ref type="bibr">Recognition (CoNLL03 (Sang and De Meulder, 2003)</ref>, OntoNotes 5.0 <ref type="bibr" target="#b42">(Weischedel et al., 2013)</ref> and CoNLL04 <ref type="bibr" target="#b1">(Carreras and Màrquez, 2004)</ref>), Extractive Question Answering (SQuAD 1.1 and SQuAD 2.0 <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref>) and Semantic Role Labeling (CoNLL05 <ref type="bibr">(Carreras and</ref><ref type="bibr" target="#b2">Màrquez, 2005) and</ref><ref type="bibr">CoNLL12 (Pradhan et al., 2012)</ref>).</p><p>Pre-trained Models. We include <ref type="bibr">BERT-large (Devlin et al., 2018)</ref>, RoBERTa-large <ref type="bibr">(Liu et al., 2019)</ref>, DeBERTa-xlarge <ref type="bibr" target="#b14">(He et al., 2020)</ref>, GLMxlarge/xxlarge <ref type="bibr" target="#b9">(Du et al., 2021)</ref>    <ref type="formula">2021</ref>) &amp; P-tuning do not show a significant disadvantage at a smaller scale. But when it comes to complicated challenges such as Natural Language Inference (RTE) and Multiple-choice Question Answering (BoolQ), their performance can be very poor. On the contrary, P-tuning v2 matches fine-tuning performance in all the tasks at a smaller scale. To our surprise, P-tuning v2 significantly outperforms fine-tuning in RTE, especially for BERT.</p><p>In terms of larger scales (2B to 10B) with GLM <ref type="bibr" target="#b9">(Du et al., 2021)</ref>, the gap between P-tuning &amp; <ref type="bibr" target="#b16">Lester et al. (2021)</ref> and fine-tuning is gradually nar-rowed down. On 10B scale, we have a similar observation as is reported in <ref type="bibr" target="#b16">(Lester et al., 2021)</ref>, that prompt tuning becomes competitive to fine-tuning. However, P-tuning v2 is always comparable to finetuning at all scales but with only 0.1% task-specific parameters needed comparing to fine-tuning.</p><p>Additionally, we observe that in some datasets, RoBERTa-large has poorer performance than BERT-large. Part of the reason is that we empirically find prompt tuning can be quite sensitive to hyper-parameters, and sometimes the tuning just gets trapped. P-tuning v2 can be more stable and robust during tuning. For more details about hyperparameters, please refer to our code repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">P-tuning v2: Across Tasks</head><p>In Section 4.2, we discuss P-tuning v2's consistent, comparable performance to fine-tuning whatever the scales. However, most tasks on GLUE and SuperGLUE are comparatively simple NLU problems. Another important family of hard NLU challenges lies in sequence tagging, which relates to some more high-level NLP applications, including open information extraction, reading comprehension, and so on.</p><p>To evaluate P-tuning v2's ability on these hard NLU challenges, we select three typical sequence  tagging tasks: Name Entity Recognition, Extractive Question Answering (QA), and Semantic Role Labeling (SRL), altogether eight datasets.</p><p>Name entity recognition (NER). NER aims to predict all spans of words that represent some given classes of entity with a sentence. We adopted CoNLL03 <ref type="bibr" target="#b32">(Sang and De Meulder, 2003)</ref>, OntoNotes 5.0 <ref type="bibr" target="#b42">(Weischedel et al., 2013)</ref> and CoNLL04 <ref type="bibr" target="#b1">(Carreras and Màrquez, 2004)</ref>. For CoNLL03 and CoNLL04, we trained our model on the standard train-develop-test split. For OntoNotes 5.0, we use the same train, develop, test split as <ref type="bibr" target="#b13">(Xu et al., 2021b)</ref>. All the datasets are labeled in IOB2 format. We use sequence tagging to solve NER tasks by assigning labels marking the beginning and inside some classes of entity. The language models generate a representation for each token, and we use a linear classifier to predict the labels. We use the official scripts to evaluate the results.</p><p>For the multi-task setting, we combine the training set of the three datasets for pre-training. We use different linear classifiers for each dataset while sharing the continuous prompts.</p><p>(Extractive) Question Answering (QA). Extractive QA is designed to extract the answer from the context given the context and a question. We use SQuAD <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref> 1.1 and 2.0, in which each answer is within a continuous span of the context. Following tradition, we formulate the problem as sequence tagging by assigning one of the two labels: 'start' or 'end' to each token and at last selecting the span of the most confident startend pair as the extracted answer. If the probability of the most confident pair is lower than a threshold, the model will assume the question unanswerable.</p><p>For the multi-task setting, our training set for pretraining combines the training sets of SQuAD 1.1 and 2.0. When pre-training, we assume that all the questions, regardless of their origin, are possibly unanswerable.</p><p>Semantic Role Labeling (SRL). SRL assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence. We evaluate P-tuning v2 on CoNLL05 <ref type="bibr" target="#b2">(Carreras and Màrquez, 2005)</ref> and CoNLL12 <ref type="bibr" target="#b26">(Pradhan et al., 2012)</ref>. Since a sentence can have multiple verbs, we add the target verb token to the end of each sentence to help recognize which verb is used for prediction. We classify each word using a linear classifier based on the cor- Results. From Table <ref type="table" target="#tab_2">2</ref>,3,4, we observe that Ptuning v2 can be generally comparable to finetuning on all tasks. P-tuning &amp; <ref type="bibr" target="#b16">Lester et al. (2021)</ref> show much poorer performance, especially on QA, which might be the most difficult challenge of the three tasks. We also notice that there are some abnormal results presented in SQuAD 2.0 (BERT/RoBERTa/DeBERTa show the same performance using Lester et al. ( <ref type="formula">2021</ref>) &amp; P-tuning). This is probably because compared to SQuAD 1.1, SQuAD 2.0 contains unanswerable questions, and the Lester et al. ( <ref type="formula">2021</ref>) &amp; P-tuning could possibly get the trivial solution.</p><p>Multi-task P-tuning v2 generally brings in significant improvement overall tasks except for QA (which might still be the consequence of mixing all-answerable SQuAD 1.1 and not-answerable SQuAD 2.0), which implies that randomly initialized prompts' potential is under-explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We study some important hyper-parameters and architecture designs that may play a central role in P-tuning v2.</p><p>Prompt depth. The main difference between Lester et al. ( <ref type="formula">2021</ref>) &amp; P-tuning and P-tuning v2 is the multi-layer continuous prompts we introduce. Intuitively, due to the many non-linear activation functions in intermediate transformer layers, the deeper the transformer layer a prompt locates in, the more direct its impact on the output predictions. To verify its exact influence, given a certain number of k to add prompts, we select k layers in both ascending and descending order to add prompts as prefix tokens; for the rest layers, we change their attention masks for disallowing their prefix prompts to involve in the computation.</p><p>As shown in Figure <ref type="figure">4</ref>, with the same amount of parameters (i.e., num of transformer layers to add prompts), adding them in the descending order is always better than in the ascending order. In the RTE case, only adding prompts to layers 17-24 can yield a very close performance to all layers, further cutting down parameters we may need to tune for matching fine-tuning. Embedding v.s. MLP reparameterization. In both prefix-tuning <ref type="bibr" target="#b18">(Li and Liang, 2021)</ref> and Ptuning <ref type="bibr">(Liu et al., 2021b)</ref>, authors discover the reparameterization to be useful in improving training speed, robustness and performance. However, we conduct experiments to show that the reparameterization effect is inconsistent across different NLU tasks and datasets.</p><p>As shown in Figure <ref type="figure" target="#fig_4">3</ref>, in RTE and CoNLL04, MLP reparameterization generally indicates better performance than embedding for almost all prompt lengths. However, in BoolQ, MLP and embed-ding's results are competitive; in CoNLL12, the embedding consistently outperforms MLP.</p><p>Prompt Length. Prompt length is yet another influential hyper-parameter for P-tuning v2, and its optimal value varies from task to task. From Figure <ref type="figure" target="#fig_4">3</ref>, we observe that for simple NLU tasks, usually, a shorter prompt is enough for the best performance; for hard sequence tasks, usually, a longer prompt than 100 would be helpful.</p><p>We also discover that reparameterization has a close bond with optimal prompt length. For example, in RTE, CoNLL04, and BoolQ, MLP reparameterization achieves its optimal result earlier than embedding. This conclusion may contribute some thoughts on P-tuning's optimization properties.</p><p>Verbalizer with LM head v.s. [CLS] label with linear head. Verbalizer with LM head has been a central component in previous prompt tuning approaches. However, for P-tuning v2 in a supervised setting, it is affordable to tune a linear head with about several thousand parameters. We present our comparison in Table <ref type="table" target="#tab_5">5</ref>, where we keep other hyperparameters and only change [CLS] label with linear head to verbalizer with LM head. Here, for simplicity, we use "true" and "false" for SST-2, RTE and BoolQ; "true", "false" and "neutral" for CB. Results indicate that there is no significant difference between performances of verbalizer and [CLS].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Pre-trained Language Models.</p><p>Selfsupervised <ref type="bibr" target="#b20">(Liu et al., 2020)</ref> pre-trained language models <ref type="bibr" target="#b12">(Han et al., 2021a)</ref> has become the backbone of natural language processing. From early stage when GPT <ref type="bibr" target="#b28">(Radford et al., 2019)</ref>, BERT <ref type="bibr">(Devlin et al., 2018)</ref>, XLNet <ref type="bibr" target="#b45">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr">(Liu et al., 2019)</ref> has limited amount of parameters (less than 350M), the advent of T5 <ref type="bibr" target="#b29">(Raffel et al., 2019)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> boosts the development of giant language models with billion and even trillions of parameters.</p><p>Prompting. Prompting <ref type="bibr" target="#b19">(Liu et al., 2021a)</ref> refers to leverage special templates in the input context to aid the language model prediction with respect to both understanding and generation. Recently, thanks to the success of <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, various prompting strategies including discrete natural language prompt <ref type="bibr" target="#b34">(Shin et al., 2020;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref>, continuous prompts <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b18">Li and Liang, 2021;</ref><ref type="bibr" target="#b16">Lester et al., 2021;</ref><ref type="bibr" target="#b27">Qin and Eisner, 2021;</ref><ref type="bibr" target="#b50">Zhong et al., 2021)</ref>, tuning bias <ref type="bibr" target="#b24">(Logan IV et al., 2021)</ref> and many other prompting strategies have appeared.</p><p>Prompting's advantage and effectiveness in a wide range of NLP applications has been verified in recent literature, including text classification <ref type="bibr" target="#b15">(Hu et al., 2021;</ref><ref type="bibr" target="#b25">Min et al., 2021;</ref><ref type="bibr" target="#b35">Sun et al., 2021;</ref><ref type="bibr" target="#b17">Li et al., 2021;</ref><ref type="bibr" target="#b47">Zhang et al., 2021b)</ref>, entity typing <ref type="bibr" target="#b8">(Ding et al., 2021)</ref>, few-shot learning <ref type="bibr">(Zheng et al., 2021;</ref><ref type="bibr" target="#b12">Xu et al., 2021a;</ref><ref type="bibr" target="#b48">Zhao et al., 2021;</ref><ref type="bibr" target="#b11">Gu et al., 2021;</ref><ref type="bibr" target="#b46">Zhang et al., 2021a)</ref>, relation extraction <ref type="bibr" target="#b3">(Chen et al., 2021a;</ref><ref type="bibr" target="#b13">Han et al., 2021b;</ref><ref type="bibr" target="#b31">Sainz et al., 2021)</ref>, knowledge probing <ref type="bibr" target="#b50">(Zhong et al., 2021)</ref>, named entity recognition <ref type="bibr" target="#b4">(Chen et al., 2021b)</ref>, machine translation <ref type="bibr" target="#b36">(Tan et al., 2021;</ref><ref type="bibr" target="#b41">Wang et al., 2021b)</ref> and dialogue system <ref type="bibr" target="#b40">(Wang et al., 2021a)</ref>.</p><p>In this work, we are especially interested in scaling prompting methods to smaller models and hard sequence NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present P-tuning v2, a prompting method comparable to fine-tuning universally across scales and tasks. P-tuning v2 is not a conceptually new approach but an optimized and adapted prefix-tuning and deep prompt tuning to NLU challenges. Ptuning v2 shows consistent improvements for models ranging from 330M to 10B and outperforms <ref type="bibr" target="#b16">Lester et al. (2021)</ref> &amp; P-tuning on hard sequence tasks such as sequence tagging by a large margin. Ptuning v2 could be a comprehensive alternative for fine-tuning and a strong baseline for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average scores on RTE, BoolQ and CB of SuperGLUE dev. With 0.1% task-specific parameters, P-tuning v2 can be comparable to fine-tuning across different scales of pre-trained models, while Lester et al. (2021) &amp; P-tuning can only do so at the 10B scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; … (b) P-tuning v2 (Frozen, most scales, most tasks)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: From Lester et al. (2021) &amp; P-tuning to P-tuning v2. Orange tokens (include h 0 , h i ) refer to prompt embeddings we add; blue tokens are embeddings stored or computed by frozen pre-trained language models. Compared to<ref type="bibr" target="#b16">Lester et al. (2021)</ref>, P-tuning v2 adds trainable continuous prompts to inputs of every transformer layer independently (as prefix-tuning (Li and Liang, 2021) does). Additionally, P-tuning v2 removes verbalizers with LM head and returns to the traditional class labels with ordinary linear head to allow its task-universality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ablation study on prompt length and reparamerization using RoBERTa-large. The conclusion can be very different given certain NLU task and dataset. (MQA: Multiple-choice QA)</figDesc><graphic url="image-6.png" coords="7,184.09,225.21,99.21,86.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>for evaluation. They are all bidirectional models designed for NLU Results on part of GLUE and SuperGLUE development set (all metrics are Accuracy). P-tuning v2 significantly surpasses P-tuning &amp; Lester et al. (2021) on models smaller than 10B and matches the performance of fine-tuning. (FT: fine-tuning; PT: P-tuning &amp; Lester et al. (2021); PT-2: P-tuning v2).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GLUE dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SuperGLUE dev</cell></row><row><cell></cell><cell>#Size</cell><cell cols="2">SST-2</cell><cell></cell><cell></cell><cell>MNLI-m</cell><cell></cell><cell></cell><cell>RTE</cell><cell></cell><cell>BoolQ</cell><cell>CB</cell></row><row><cell></cell><cell></cell><cell cols="6">FT PT PT-2 FT PT PT-2</cell><cell cols="4">FT PT PT-2 FT PT PT-2 FT PT PT-2</cell></row><row><cell>BERT large</cell><cell cols="4">335M 93.2 92.4 93.6</cell><cell cols="3">86.6 75.6 85.8</cell><cell cols="2">70.4 53.5 78.3</cell><cell cols="2">77.7 67.2 75.8 94.6 80.4 94.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+7.9)</cell><cell></cell><cell>(+0.0)</cell></row><row><cell cols="8">RoBERTa large 355M 96.4 95.3 96.3 90.2 83.8 90.4</cell><cell cols="2">86.6 58.8 88.4</cell><cell cols="2">86.9 62.3 84.8 98.2 71.4 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.2)</cell><cell></cell><cell>(+1.8)</cell><cell></cell><cell>(+1.8)</cell></row><row><cell>GLM xlarge</cell><cell>2B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">90.3 85.6 90.3</cell><cell cols="2">88.3 79.7 87.0 96.4 76.4 96.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.0)</cell><cell></cell><cell>(+0.0)</cell></row><row><cell>GLM xxlarge</cell><cell>10B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">93.1 89.9 93.1</cell><cell cols="2">88.7 88.8 88.8</cell><cell>98.7 98.2 96.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.0)</cell><cell></cell><cell>(+0.1)</cell></row><row><cell></cell><cell>#Size</cell><cell></cell><cell cols="3">CoNLL03</cell><cell></cell><cell></cell><cell cols="2">OntoNotes 5.0</cell><cell></cell><cell>CoNLL04</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell cols="3">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2 MPT-2</cell></row><row><cell>BERT large</cell><cell cols="5">335M 92.8 81.9 90.2</cell><cell>91.0</cell><cell cols="3">89.2 74.6 86.4</cell><cell>86.3</cell><cell>85.6 73.6 84.5</cell><cell>86.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.0)</cell></row><row><cell cols="6">RoBERTa large 355M 92.6 86.1 92.4</cell><cell>92.8</cell><cell cols="3">89.8 80.8 89.4</cell><cell>89.8</cell><cell>88.8 76.2 87.8</cell><cell>90.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.0)</cell><cell>(+0.8)</cell></row><row><cell cols="6">DeBERTa xlarge 750M 93.1 90.2 93.1</cell><cell>93.1</cell><cell cols="3">90.4 85.1 90.4</cell><cell>90.5</cell><cell>89.1 82.4 86.5</cell><cell>90.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.0)</cell><cell>(+0.0)</cell><cell></cell><cell></cell><cell>(+0.0)</cell><cell>(+0.1)</cell><cell>(+1.0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Comparison Methods. We compare our P-tuning</cell></row><row><cell>v2 (PT-2) with vanilla fine-tuning (FT), P-tuning</cell></row><row><cell>&amp; Lester et al. (2021) (PT). Additionally, for hard</cell></row><row><cell>tasks regarding the sequence tagging, we present</cell></row><row><cell>our results on multi-task P-tuning v2 (MPT-2) with</cell></row><row><cell>more details presented in Section 4.3.</cell></row><row><cell>4.2 P-tuning v2: Across Scales</cell></row><row><cell>Table 1 presents P-tuning v2's performances across</cell></row><row><cell>different model scales. For simple NLU tasks such</cell></row><row><cell>as SST-2 (single sentence classification), Lester</cell></row><row><cell>et al. (</cell></row></table><note>Results on Named Entity Recognition (NER) test set (all metrics are micro-f1 score). P-tuning v2 is generally comparable to fine-tuning, and multitask P-tuning v2 can bring in a further improvement. (FT: finetuning; PT: P-tuning &amp;<ref type="bibr" target="#b16">Lester et al. (2021)</ref>; PT-2: P-tuning v2; MPT-2: Multi-task P-tuning v2) purposes, covering a wide range of sizes from about 300M to 10B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>RoBERTa large 355M 88.9 94.6 1.2 12.0 88.1 94.1 88.0 94.1 86.5 89.4 50.2 50.2 82.1 85.5 83.4 86.7 DeBERTa xlarge 750M 90.1 95.5 2.4 19.0 90.4 Results on Question Answering (Extractive QA). Prompt tuning &amp; P-tuning performs extremely poor on question answering, while P-tuning v2's performance is generally reasonable, and can be better than fine-tuning with DeBERTa-xlarge. (FT: fine-tuning; PT: P-tuning &amp; Lester et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SQuAD 1.1 dev</cell><cell></cell><cell></cell><cell>SQuAD 2.0 dev</cell></row><row><cell></cell><cell>#Size</cell><cell>FT</cell><cell></cell><cell>PT</cell><cell cols="2">PT-2</cell><cell cols="2">MPT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>MPT-2</cell></row><row><cell></cell><cell cols="8">EM F1 EM F1 EM F1 EM F1</cell><cell>EM F1 EM F1 EM F1 EM F1</cell></row><row><cell>BERT large</cell><cell cols="9">335M 84.2 91.1 1.0 8.5 77.8 86.0 82.3 89.6 78.7 81.9 50.2 50.2 69.7 73.5 72.7 75.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95.7</cell><cell cols="3">89.6 95.4 88.3 91.1 50.2 50.2 88.4</cell><cell>91.1</cell><cell>88.1 90.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.3)</cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell>(+0.1)</cell><cell>(+0.0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(2021); PT-2: P-tuning v2; MPT-2: Multi-task</cell></row><row><cell>P-tuning v2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>#Size</cell><cell></cell><cell cols="2">CoNLL12</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CoNLL05 WSJ</cell><cell>CoNLL05 Brown</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell></cell><cell>FT</cell><cell>PT</cell><cell>PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2 MPT-2</cell></row><row><cell>BERT large</cell><cell cols="4">335M 84.9 64.5. 83.2</cell><cell>85.1</cell><cell></cell><cell cols="3">88.5 76.0 86.3</cell><cell>88.5</cell><cell>82.7 70.0 80.7</cell><cell>83.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.0)</cell><cell>(+0.4)</cell></row><row><cell cols="5">RoBERTa large 355M 86.5 67.2 84.6</cell><cell>86.2</cell><cell></cell><cell cols="3">90.2 76.8 88.3</cell><cell>90.0</cell><cell>85.6 70.7 84.2</cell><cell>85.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.1)</cell></row><row><cell cols="5">DeBERTa xlarge 750M 86.5 74.1 85.7</cell><cell>87.1</cell><cell></cell><cell cols="3">91.2 82.3 90.6</cell><cell>91.1</cell><cell>86.9 77.7 86.3</cell><cell>87.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.6)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Semantic Role Labeling (SRL). P-tuning v2 shows a consistent improvement overLester et al.   </figDesc><table><row><cell cols="3">(2021) &amp; P-tuning on SRL. (FT: fine-tuning; PT: P-tuning &amp; Lester et al. (2021); PT-2: P-tuning v2; MPT-2:</cell></row><row><cell>Multi-task P-tuning v2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SST-2 RTE BoolQ CB</cell></row><row><cell>CLS &amp; linear head</cell><cell>96.3</cell><cell>88.4 84.8 96.4</cell></row><row><cell>Verbalizer &amp; LM head</cell><cell>95.8</cell><cell>86.6 84.6 94.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison between [CLS] label with linear head and verbalizer with LM head on RoBERTa-large.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Codes will be at https://github.com/THUDM/ P-tuning-v2 † corresponding to: Zhilin Yang (zhiliny@tsinghua.edu.cn) and Jie Tang (jietang@tsinghua.edu.cn) * indicates equal contribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We use "prompt tuning" to refer to a class of methods rather than a particular method.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
				<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
	<note>HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</title>
				<meeting>the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaprompt: Adaptive promptbased finetuning for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07650</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lightner: A lightweight generative framework with prompt-guided attention for low-resource ner</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00720</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Prompt-learning for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ppt: Pre-trained prompt tuning for few-shot learning</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pretrained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<publisher>AI Open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Boseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gichang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyeon Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongpil</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02035</idno>
		<idno>arXiv:2109.04650</idno>
	</analytic>
	<monogr>
		<title level="m">What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Selfsupervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Zhilin Yang, and Jie Tang. 2021b. Gpt understands, too</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04106</idno>
		<title level="m">Noisy channel language model prompting for few-shot text classification</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
				<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06599</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Label verbalization and entailment for effective zeroand few-shot relation extraction</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03659</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><surname>De Meulder</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Nsp-bert: A prompt-based zero-shot learner through an original pre-training task-next sentence prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangping</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03564</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06609</idno>
		<title level="m">Msp: Multi-stage prompting for making pre-trained language models better translators</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Topicrefine: Joint topic prediction and dialogue response generation for multi-turn end-to-end dialogue system</title>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheong</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05187</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13627</idno>
		<title level="m">Language models are good translators</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</title>
				<meeting><address><addrLine>PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fewclue: A chinese fewshot learning evaluation benchmark</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07498</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Better feature integration for named entity recognition</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05316</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13161</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Aspect sentiment quad prediction as paraphrase generation</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00796</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at few-shot crosslingual transfer: The choice of shots matters</title>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5751" to="5767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12742</idno>
		<title level="m">Sebastian Ruder, and Zhilin Yang. 2021. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05240</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
