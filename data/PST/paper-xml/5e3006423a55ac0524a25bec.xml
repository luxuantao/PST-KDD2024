<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
							<email>wang.chi@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansanw@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3366423.3380132</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Taxonomy Expansion; Self-supervised Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Taxonomies have been fundamental to organizing knowledge for centuries <ref type="bibr" target="#b44">[45]</ref>. In today's Web, taxonomies provide valuable knowledge to support many applications such as query understanding <ref type="bibr" target="#b16">[17]</ref>, content browsing <ref type="bibr" target="#b53">[54]</ref>, personalized recommendation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b62">63]</ref>, and web search <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b52">53]</ref>. For example, many online retailers (e.g., eBay and Amazon) organize products into categories of different granularities, so that customers can easily search and navigate this category taxonomy to find the items they want to purchase. In addition, supervision used for learning inputs inputs outputs Figure <ref type="figure">1</ref>: An example of expanding one computer science field-of-study taxonomy to include new concepts such as "Quantum Computing", "Meta Learning", and "TPU ".</p><p>web search engines (e.g., Google and Bing) leverage a taxonomy to better understand user queries and improve the search quality.</p><p>Existing taxonomies are mostly constructed by human experts or in a crowdsourcing manner. Such manual curations are timeconsuming, labor-intensive, and rarely complete. To reduce the human efforts, many automatic taxonomy construction methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60]</ref> are proposed. They first identify "is-A" relations (e.g., "iPad" is an "Electronics") using textual patterns <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref> or distributional similarities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>, and then organize extracted concept pairs into a directed acyclic graph (DAG) as the output taxonomy <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>. As the web contents and human knowledge are constantly growing, people need to expand an existing taxonomy to include new emerging concepts. Most of previous methods, however, construct a taxonomy entirely from scratch and thus when we add new concepts, we have to re-run the entire taxonomy construction process. Although being intuitive, this approach has several limitations. First, many taxonomies have a top-level design provided by domain experts and such design shall be preserved. Second, a newly constructed taxonomy may not be consistent with the old one, which can lead to instabilities of its dependent downstream applications. Finally, as targeting the scenario of building taxonomy from scratch, most previous methods are unsupervised and cannot leverage signals from the existing taxonomy to construct a new one.</p><p>In this paper, we study the taxonomy expansion task: given an existing taxonomy and a set of new emerging concepts, we aim to automatically expand the taxonomy to incorporate these new concepts (without changing the existing relations in the given taxonomy). <ref type="foot" target="#foot_0">1</ref> Figure <ref type="figure">1</ref> shows an example where a taxonomy in computer science domain is expanded to include new subfields (e.g., "Quantum Computing") and new techniques (e.g., "Meta Learning" and "UDA"). Some previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref> attempt this task by using an additional set of labeled concepts with their true insertion positions in the existing taxonomy. However, such labeled data are usually small and thus forbid us from learning a more powerful model that captures the subsumption semantics in the existing taxonomy.</p><p>We propose a novel framework named TaxoExpan to tackle the lack-of-supervision challenge. TaxoExpan formulates a taxonomy as a directed acyclic graph (DAG), automatically generates pseudotraining data from the existing taxonomy, and uses them to learn a matching model for expanding a given taxonomy. Specifically, we view each concept in the existing taxonomy as a query and one of its parent concepts as an anchor. This gives us a set of positive ⟨query concept, anchor concept⟩ pairs. Then, we generate negative pairs by sampling those concepts that are neither the descendants nor the direct parents of the query concept in the existing taxonomy. In Figure <ref type="figure">1</ref>, for example, the ⟨"GPU ", "Integrated Circuit"⟩ is a positive pair and ⟨"GPU ", "Label Propagation"⟩ is a negative pair. We refer to these training pairs as self-supervision data, because they are procedurally generated from the existing taxonomy and no human curation is involved.</p><p>To make the best use of above self-supervision data, we develop two novel techniques in TaxoExpan. The first one is a positionenhanced graph neural network (GNN) which encodes the local structure of an anchor concept using its ego network (egonet) in the existing taxonomy. If we view this anchor concept as the "parent" of the query concept, this ego network includes the potential "siblings" and "grand parents" of the query concept. We apply graph neural networks (GNNs) to model this ego network. However, regular GNNs fail to distinguish nodes with different relative positions to the query (i.e., some nodes are grand parents of the query while the others are siblings of the query). To address this limitation, we present a simple but effective enhancement to inject such position information into GNNs using position embedding. We show that such embedding can be easily integrated with existing GNN architectures (e.g., GCN <ref type="bibr" target="#b22">[23]</ref> and GAT <ref type="bibr" target="#b49">[50]</ref>) and significantly boosts the prediction performance. The second technique is a new noise-robust training scheme based on the InfoNCE loss <ref type="bibr" target="#b46">[47]</ref>. Instead of predicting whether each individual ⟨query concept, anchor concept⟩ pair is positive or not, we first group all pairs sharing the same query concept into a single training instance and learn a model to select the positive pair among other negative ones from the group. We show that such training scheme is robust to the label noise and leads to performance gains.</p><p>We test the effectiveness of TaxoExpan framework on three realworld taxonomies from different domains. Our results show that TaxoExpan can generate high-quality concept taxonomies in scientific domains and achieves state-of-the-art performance on the WordNet taxonomy expansion challenge <ref type="bibr" target="#b21">[22]</ref>.</p><p>Contributions. To summarize, our major contributions include:</p><p>(1) a self-supervised framework that automatically expands existing taxonomies without manually labeled data; (2) an effective method for enhancing graph neural network by incorporating hierarchical positional information; (3) a new training objective that enables the learned model to be robust to label noises in self-supervision data; and (4) extensive experiments that verify both the effectiveness and the efficiency of TaxoExpan framework on three real-world large-scale taxonomies from different domains.</p><p>The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 formalizes our problem. Then, we present our TaxoExpan framework in Section 4 and conduct experiments in Section 5. Finally, we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review two lines of related work: taxonomy construction and graph neural network. Taxonomy Construction and Expansion. Automatic taxonomy construction is a long-standing task in the literature. Most existing approaches focus on building the entire taxonomy by first extracting hypernym-hyponym pairs and then organizing all hypernymy relations into a tree or DAG structure. For the first hypernymy discovery step, methods fall into two categories: (1) pattern-based methods which leverage pre-defined patterns <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref> to extract hypernymy relations from a corpus, and (2) distributional methods which calculate pairwise term similarity metrics based on term embeddings <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52]</ref> and use them to predict whether two terms hold the hypernymy relation. For the second hypernymy organization step, most methods formulate it as a graph optimization problem. They first build a noisy hypernymy graph using hypernymy pairs extracted and then derive the output taxonomy as a particular tree or DAG structure (e.g., maximum spanning tree <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, optimal branching <ref type="bibr" target="#b48">[49]</ref>, and minimum-cost flow <ref type="bibr" target="#b13">[14]</ref>) from the hypernymy graph. Finally, there are some methods that leverage entity set expansion techniques <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b61">62]</ref> to incrementally construct a taxonomy either from scratch or from a tiny seed taxonomy.</p><p>In many real-world applications, some existing taxonomies may have already been laboriously curated by experts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref> or via crowdsourcing <ref type="bibr" target="#b31">[32]</ref>, and are deployed in online systems. Instead of constructing the entire taxonomy from scratch, these applications demand the feature of expanding an existing taxonomy dynamically. There exist some studies on expanding WordNet with named entities from Wikipedia <ref type="bibr" target="#b45">[46]</ref> or domain-specific concepts from different corpora <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>. Task 14 of SemEval 2016 challenge <ref type="bibr" target="#b21">[22]</ref> is specifically setup to enrich WordNet with concepts from domains like health, sport, and finance. One limitation of these approaches is that they depend on the synset structure unique to WordNet and thus cannot be easily generalized to other taxonomies.</p><p>To address the above limitation, more recent works try to develop methodologies for expanding a generic taxonomy. Wang et al. <ref type="bibr" target="#b50">[51]</ref> design a hierarchical Dirichlet model to extend the category taxonomy in search engines using query logs. Plachouras et al. <ref type="bibr" target="#b35">[36]</ref> learn paraphrase models on external paraphrase datasets and apply learned models to directly find paraphrases of concepts in the existing taxonomy. Vedula et al. <ref type="bibr" target="#b47">[48]</ref> combine multiple features, some of which are retrieved from an external Bing Search API, into a ranking model to score candidate positions in terms of their matching scores with the query concept. Aly et al. <ref type="bibr" target="#b1">[2]</ref> first learn term embeddings in a hyperbolic space and then attach each new concept to its most similar node in the existing taxonomy based on the hyperbolic embeddings. Comparing with these methods, our TaxoExpan framework has two advantages. First, it requires no external data resource and makes full use of the existing taxonomy as the self supervision, which leads to a broader application scope. Second, TaxoExpan explicitly models the local structure around each candidate position, which boosts the quality of expanded taxonomy. Graph Neural Network. Our work is also related to Graph Neural Network (GNN) which is a generic method of learning on graphstructure data. Many GNN architectures have been proposed to either learn individual node embeddings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50]</ref> for the node classification and the link prediction tasks or learn an entire graph representation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61]</ref> for the graph classification task. In this work, we tackle the taxonomy expansion task with a fundamentally different formulation from previous tasks. We leverage some existing GNN architectures and enrich them with additional relative position information. Recently, You et al. <ref type="bibr" target="#b57">[58]</ref> propose a method to add position information into GNN. Our methods are different from You et al.. They model the absolute position of a node in a full graph without any particular reference points; while our technique captures the relative position of a node with respect to the query node. Finally, some work on graph generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57]</ref> involves a module to add a new node into a partially generated graph, which shares the similar goal as our model. However, such graph generation model typically requires fully labeled training data to learn from. To the best of our knowledge, this is the first study on how to expand an existing directed acyclic graph (as we model a taxonomy as a DAG) using self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>In this section, we first define a taxonomy, then formulate our problem, and finally discuss the scope of our study. Taxonomy. A taxonomy T = (N, E) is a directed acyclic graph where each node n ∈ N represents a concept (i.e., a word or a phrase) and each directed edge ⟨n p , n c ⟩ ∈ E indicates a relation expressing that concept n p is the most specific concept that is more general than concept n c . In other words, we refer to n p as the "parent" of n c and n c as the "child" of n p . Problem Definition. The input of the taxonomy expansion task includes two parts: (1) an existing taxonomy T 0 = (N 0 , E 0 ), and (2) a set of new concepts C. This new concept set can be either manually specified by users or automatically extracted from text corpora. Our goal is to expand the existing taxonomy T 0 into a larger taxonomy T = (N 0 ∪ C, E 0 ∪ R), where R is a set of newly discovered relations each including one new concept c ∈ C. Example 1. Figure <ref type="figure">1</ref> shows an example of our problem. Given a field-of-study taxonomy T 0 in the computer science domain and a set of new concepts C = {"UDA", "Meta Learning", . . . }, we find each new concept's best position in T 0 (e.g., "UDA" under "Semi-supervised Learning" as well as "GPU" under "Integrated Circuit") and expand T 0 to include those new concepts.</p><p>Simplified Problem. A simplified version of the above problem is that we assume the input set of new concepts contains only one element (i.e., |C| = 1), and we aim to find one single parent node of this new concept (i.e., |R| = 1). We discuss the connection between these two problem settings at the end of Section 4.1. Discussion. In this work, we follow previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> and assume each concept in N 0 ∪ C has an initial embedding vector learned from this concept's surface name, or if available, its definition sentences <ref type="bibr" target="#b38">[39]</ref> and associated web pages <ref type="bibr" target="#b50">[51]</ref>. We also note that our problem formulation assumes those relations in the existing taxonomy are not modified. We acknowledge that such modification is necessary in some cases, but it is much less frequent and requires high cautiousness from human curators. Therefore, we leave it out of the scope of automation in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE TAXOEXPAN FRAMEWORK</head><p>In this section, we first introduce our taxonomy model and expansion goal. Then, we elaborate how to represent a query concept and an insertion position (i.e., an anchor concept), based on which we present our query-concept matching model. Finally, we discuss how to generate self-supervision data from the existing taxonomy and use them to train the TaxoExpan framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Taxonomy Model and Expansion Goal</head><p>A taxonomy T describes a hierarchical organization of concepts. These concepts form the node set N in T . Mathematically, we model each node n ∈ N as a categorical random variable and the entire taxonomy T as a Bayesian network. We define the probability of a taxonomy T as the joint probability of node set N which can be further factorized into a set of conditional probabilities as follows:</p><formula xml:id="formula_0">P(T |Θ) = P(N |T , Θ) = |N | i=1 P(n i |parent T (n i ), Θ),</formula><p>where Θ is the set of model parameters and parent T (n i ) is the set of n i 's parent node(s) in taxonomy T .</p><p>Given learned model parameters Θ, an existing taxonomy T 0 = (N 0 , E 0 ), and a set of new concepts C, we can ideally find the best taxonomy T * by solving the following optimization problem:</p><formula xml:id="formula_1">T * = arg max T P(T |Θ) = arg max T |N 0 ∪C | i=1 log P(n i |parent T (n i ), Θ).</formula><p>This naïve approach has two limitations. First, the search space of all possible taxonomies over the concept set |N 0 ∪ C| is prohibitively large. Second, we cannot guarantee the structure of existing taxonomy T 0 remains unchanged, which can be undesirable from the application point of view.</p><p>We address the above limitations by restricting the search space of our output taxonomy to be the exact expansion of the existing taxonomy T 0 . Specifically, we keep the parents of each existing taxonomy node n ∈ N 0 unchanged and only try to find a single parent node of each new concept in C. As a result, we divide the above computationally intractable problem into the following set of |C| tractable optimization problems:</p><formula xml:id="formula_2">a * i = arg max a i ∈N 0 log P(n i |a i , Θ), ∀i ∈ {1, 2, . . . , | C | },<label>(1)</label></formula><p>where a i is the parent node of a new concept n i ∈ C and we refer to it as the "anchor concept". Discussion. The above equation defines |C| independent optimization problems and each problem aims to find one single parent of a new concept n i . Therefore, we essentially reduce the more generic taxonomy expansion problem into |C | independent simplified problems (c.f. Section 3) and tackle it by inserting new concepts "hospital room" "hospital" "room" "intensive care unit" "low dependency unit"</p><p>"operating room"</p><p>The ego nodes "hospital room" "classroom" "court" "gallery" "area" "room" grandparent parent sibling one-by-one into the existing taxonomy. As a result of the above reduction, possible interactions among new concepts are ignored and we leave it to the future work. In the following sections, we continue to answer two keys questions: (1) how to model the conditional probability P(n i |a i , Θ), and ( <ref type="formula" target="#formula_3">2</ref>) how to learn model parameters Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Query-Anchor Matching</head><p>We model the matching score between a query concept n i and an anchor concept a i by projecting them into a vector space and calculating matching scores using their vectorized representations.</p><p>We show the entire model architecture of TaxoExpan in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Representing Query Concept.</head><p>In this study, we assume each query concept has an initial feature vector learned based on some text associated with this concept. Such text can be as simple as the concept surface name, or in some prior studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref>, the definition sentences and clicked web pages about the concept. We represent each query concept n i using its initial feature vector denoted as n i . We will discuss how to obtain such initial feature vectors using embedding learning methods in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Representing Anchor Concept.</head><p>Each anchor concept corresponds to one node in the existing taxonomy T 0 that could be the "parent" of a query concept. One naïve way to represent an anchor concept is to directly use its initial feature vector. A key limitation of this approach is that it captures only the "parent" node information and loses other surrounding nodes' signals, which could be crucial for determining whether the query concept should be put in this position. We illustrate this limitation below: Example 2. Suppose we are given a query concept " high dependency unit" to predict whether it should be under the " hospital room" node (i.e., an anchor concept) in an existing taxonomy. As these two concepts have dissimilar embeddings based on their surface names, we may believe this query concept shouldn't be placed underneath this anchor concept. However, if we know that this anchor concept has two children nodes, i.e., " intensive care unit" and " low dependency unit", that are closely related to the query concept, we are more likely to put the query concept under this anchor concept, correctly.</p><p>The above example demonstrates the importance of capturing local structure information in the anchor concept representation. We model the anchor concept using its ego network. Specifically, we consider the anchor concept to be the "parent" node of a query concept. The ego network of the anchor concept consists of the "sibling" nodes and "grand parent" nodes of the query concept, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. We represent the anchor concept based on its ego network using a graph neural network. Graph Neural Network Architectures. Given an anchor concept a i with its corresponding ego network G a i and its initial representation a i , we use a graph neural network (GNN) to generate its final representation a i . This GNN contains two components: (1) a graph propagation module that transforms and propagates node features over the graph structure to compute individual node embeddings in G a i , and (2) a graph readout module that combines node embeddings into a vector representing the full ego network G a i . The final graph embedding encodes all local structure information centered around the anchor concept and we use it as the final anchor representation a i .</p><p>A graph propagation module uses a neighborhood aggregation strategy to iteratively update the representation of a node u by aggregating representations of its neighbors N (u) and itself. We denote N (u) ∪ {u} as N (u). After K iterations, a node's representation captures the structural information within its K-hop neighborhood. Formally, we define a GNN with K-layers as follows:</p><formula xml:id="formula_3">h (k ) u = AGG (k ) {h (k −1) v |v ∈ N (u)} , k ∈ {1, . . . , K },<label>(2)</label></formula><p>where h</p><formula xml:id="formula_4">(k ) u is node u's feature in the k-th layer; h<label>(0)</label></formula><p>u is node u's initial feature vector, and AGG (k ) is an aggregation function in the k-th layer. We instantiate AGG (k) using two popular architectures:</p><p>Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">[23]</ref> and Graph Attention Network (GAT) <ref type="bibr" target="#b49">[50]</ref>. GCN defines the AGG function as follows:</p><formula xml:id="formula_5">AGG (k ) {h (k −1) v |v ∈ N (u)} = ρ v ∈ N (u) α (k −1) uv W (k −1) h (k −1) v ,<label>(3)</label></formula><p>where α</p><formula xml:id="formula_6">(k−1) uv = 1/ | N (u)|| N (v)</formula><p>| is a normalization constant (same for all layers); ρ is a non-linear function (e.g., ReLU), and W (k −1) is the learnable weight matrix. If we interpret α (k −1) uv as the importance of node v's feature to node u, GCN calculates it using only the graph structure without leveraging the node features. GAT addresses this limitation by defining α (k −1) uv as follows:</p><formula xml:id="formula_7">α (k −1) uv = exp γ z (k −1) [W (k −1) h (k −1) u ∥W (k −1) h (k −1) v ] v ′ ∈ N (u) exp γ z (k −1) [W (k −1) h (k −1) u ∥W (k −1) h (k −1) v ′ ] ,<label>(4)</label></formula><p>where both z (k−1) and W (k −1) are learnable parameters; γ (•) is another non-linear function (e.g., LeakyReLU), and "∥" represents the concatenation operation. Plugging the above α (k −1) uv into Eq. ( <ref type="formula" target="#formula_5">3</ref>) we obtain the aggregation function in a single-head GAT. Finally, We execute M independent transformations of Eq. ( <ref type="formula" target="#formula_5">3</ref>) and concatenate their output features to compose the final output embedding of node u. This defines the aggregation function in a multi-head GAT (with M heads) as follows:</p><formula xml:id="formula_8">AGG (k ) {h (k −1) v |v ∈ N (u)} = M ∥ m=1 ρ v ∈ N (u) α (k −1) uv W (k −1) m h (k −1) v ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">W (k −1) m</formula><p>is the m-th weight matrix in the m-th attention head. After obtaining each node's final representation h (K ) u , we generate the ego network's representation h G using a graph readout "hospital room" "hospital" "room" "intensive care unit" "low dependency unit"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Propagation Module</head><p>"high dependency unit"  </p><formula xml:id="formula_10">h (0) a h (0) b h (0) c h (0) d h (0)</formula><formula xml:id="formula_11">h G = READOUT({h (K ) u |u ∈ G }),<label>(6)</label></formula><p>where READOUT is a permutation invariant function <ref type="bibr" target="#b58">[59]</ref> such as element-wise mean or sum.</p><p>Position-enhanced Graph Neural Networks. One key limitation of the above GNN model is that they fail to capture each node's position information relative to the query concept. Take Figure <ref type="figure" target="#fig_0">2</ref> as an example, the "hospital room" node in the left ego network is the anchor node itself while in the right ego network it is the child of the anchor node. Such position information will influence how node feature propagates within the ego network and how the final graph embedding is aggregated. An important innovation in TaxoExpan is the design of positionenhanced graph neural networks. The key idea is to learn a set of "position embeddings" and enrich each node feature with its corresponding position embedding. We denote node u's position as p u and its position embedding at k-th layer as p in Eqs. <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref> and adjust the dimensionality of W (k −1) accordingly. Such position embeddings help us to learn better node representations from two aspects. First, we can capture more neighborhood information. Take W (k −1) h (k−1) v in the right hand side of Eq. (3) as an example, we enhance it to the following:</p><formula xml:id="formula_12">W (k −1) ∥O (k −1) h (k −1) v ∥p (k −1) v = W (k −1) h (k −1) v + O (k −1) p (k −1) v ,</formula><p>where O (k−1) is another weight matrix used to transform position embeddings. The above equation shows that a node's new representation is jointly determined by its neighborhoods' contents (i.e., h</p><formula xml:id="formula_13">(k −1) v</formula><p>) and relative positions in the ego network (i.e., p</p><formula xml:id="formula_14">(k −1) v ).</formula><p>Second, for GAT architecture, we can better model neighbor importance as the term α . Furthermore, we propose two schemes to inject position information in the graph readout module. The first one, called weighted mean readout (WMR), is defined as follows:</p><p>READOUT({h where α p u is the parameter indicating the importance of position p u . The second scheme is called concatenation readout (CR) which combines the average embeddings of nodes with the same position as follows:</p><formula xml:id="formula_15">(K ) u |u ∈ G }) = u ∈G log(1 + exp(α pu )) u ′ ∈G log(1 + exp(α p ′ u )) h (K ) u ,<label>(7)</label></formula><p>READOUT({h</p><formula xml:id="formula_16">(K ) u |u ∈ G }) = ∥ p ∈P I(p u = p)h (K ) u u ′ ∈G I(p u ′ = p) , (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where P is the set of all positions we are modeling and I(•) is an indicator function which returns 1 if its internal statement is true and returns 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Matching Query Concept and Anchor Concept.</head><p>Based on the learned query concept representation n i ∈ R D 1 and anchor concept representation a i ∈ R D 2 , we calculate their match score using a matching module f (•) : R D 2 × R D 1 → R. We study two architectures. The first one is a multi-layer perceptron with one hidden layer, defined as follows:</p><formula xml:id="formula_18">f MLP (a i , n i ) = σ (W 2 γ (W 1 (a i ∥n i ) + B 1 ) + B 2 ) ,<label>(9)</label></formula><p>where {W 1 , B 1 , W 2 , B 2 } are parameters; σ (•) is the sigmoid function, and γ (•) is the LeakyReLU activation function. The second architecture is a log-bilinear model defined as follows:</p><formula xml:id="formula_19">f LBM (a i , n i ) = exp a T i Wn i , (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>where W is a learnable interaction matrix. We choose these MLP and LBM as they are representative architecures in linear and bilinear interaction models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Learning and Inference</head><p>The above section discusses how to model query-anchor matching using a parameterized function f (•|Θ). In this section, we first introduce how we learn those parameters Θ using self-supervision from the existing taxonomy. Then, we establish the connection between the matching score with the conditional probability P(n i |a i ), and discuss how to conduct model inference.</p><p>Self-supervision Generation. Figure <ref type="figure" target="#fig_5">4</ref> shows the generation process of self supervision data. Given one edge ⟨n p , n c ⟩ in the existing taxonomy T 0 = (N 0 , E 0 ), we first construct a positive ⟨anchor, query⟩ pair by using child node n c as the "query" and parent node n p as the "anchor". Then, we construct N negative pairs by fixing the query node n c and randomly selecting N nodes {n l r | N l =1 } ⊂ N 0 that are neither parents nor descendants of n c . These N + 1 pairs (one positive and N negatives) collectively consist of one training instance X = {⟨n p , n c ⟩, ⟨n 1 r , n c ⟩, . . . , ⟨n N r , n c ⟩}. By repeating the above process for each edge in T 0 , we obtain the full self-supervision dataset X = {X 1 , . . . , X | E 0 | }. Notice that a node with C parents in T 0 will derive C training instances in X.</p><p>Model Training. We learn our model on X using the InfoNCE loss <ref type="bibr" target="#b46">[47]</ref> as follows:</p><formula xml:id="formula_21">L(Θ) = − 1 |X| X i ∈X log f (n p , n c ) ⟨n j ,nc ⟩∈X i f (n j , n c ) ,<label>(11)</label></formula><p>where the subscript j ∈ [1, 2, . . . , N + 1]. If j = 1, ⟨n j , n c ⟩ is a positive pair, otherwise, ⟨n j , n c ⟩ is a negative pair. The above loss is the cross entropy of classifying the positive pair ⟨n p , n c ⟩ correctly, with f (n p ,n c ) ⟨n j ,nc ⟩∈X i f (n j ,n c ) as the model prediction. Optimizing this loss results in f (a i , n i ) estimating the following probability density (up to a multiplicative constant):</p><formula xml:id="formula_22">f (a i , n i ) ∝ P(a i |n i ) P(a i ) . (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>We prove the above result in Appendix and summarize our selflearning procedure in Algorithm 1. We establish the connection between matching score f (a i , n i ) with the probability P(n i |a i ) in Eq. 1 as follows:</p><formula xml:id="formula_24">P(n i |a i ) = P(a i |n i ) P(a i ) • P(n i ) ∝ f (a i , n i ) • P(n i ).<label>(13)</label></formula><p>We elaborate the implication of this equation below.</p><p>Model Inference. At the inference stage, we are given a new query concept n i and apply the learned model f (•|Θ) to predict its parent node in the existing taxonomy T 0 . Mathematically, we aim to find the anchor position a i that maximizes P(n i |a i ), which is equivalent to maximizing f (a i , n i ) because of Eq. ( <ref type="formula" target="#formula_24">13</ref>) and the fact that P(n i ) is the same across all positions. Therefore, we rank all candidate positions a i based on their matching scores with n i and select the top ranked one as the predicted parent node of this query concept.</p><p>Although we currently select only the top one as query's single parent, we can also choose top-k ones as query's parents, if needed. Summary. Given an existing taxonomy and a set of new concepts, our TaxoExpan first generates a set of self-supervision data and learns its internal model parameters using Algorithm 1. For each new concept, we run the inference procedure and find its best parent node in the existing taxonomy. Finally, we place these new cost per query is expensive, we can significantly reduce it using two strategies. First, most computation efforts of TaxoExpan are matrix multiplications and thus we use GPU for acceleration. Second, as the graph propagation and graph readout modules are queryindependent (c.f. Fig. <ref type="figure" target="#fig_5">4</ref>), we pre-compute all anchor representations and cache them. When a set of queries are given, we only run the matching module. In practice, it takes less than 30 seconds to calculate all matching scores between 2,450 queries with over 24,000 anchor positions on a single K80 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we study the performance of TaxoExpan on three large-scale real-world taxonomies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Expanding MAG Field-of-Study Taxonomy</head><p>5.1.1 Datasets. We evaluate TaxoExpan on the public Field-of-Study (FoS) Taxonomy<ref type="foot" target="#foot_1">2</ref> in Microsoft Academic Graph (MAG) <ref type="bibr" target="#b43">[44]</ref>. This FoS taxonomy contains over 660 thousand scientific concepts and more than 700 thousand taxonomic relations. Although being constructed semi-automatically, this taxonomy is of high quality, as shown in the previous study <ref type="bibr" target="#b41">[42]</ref>. Thus we treat each concept's original parent nodes as its correct anchor positions. We remove all concepts that have no relation in the original FoS taxonomy and then randomly mask 20% of leaf concepts (along with their relations) for validation and testing <ref type="foot" target="#foot_2">3</ref> . The remaining FoS taxonomy is then treated as the input existing taxonomy. We refer to this dataset as MAG-Full. Based on MAG-Full, we construct another dataset focusing on the computer science domain. Specifically, we first select a subgraph consisting of all descendants of "computer science" node and then mask 10% of leaf concepts in this subgraph for validation and another 10% of leaf nodes for testing. We name this dataset as MAG-CS.</p><p>To obtain the initial feature vector, we first construct a corpus that consists of all paper abstracts mentioning at least one concept in the original MAG dataset. Then, we use " " to concatenate all tokens in one concept (e.g., "machine learning" → "ma-chine_learning") and learn 250-dimension word embeddings using skipgram model in word2vec<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b32">[33]</ref>. Finally, we use these learned embeddings as the initial feature vector. Table <ref type="table" target="#tab_0">1</ref> lists the statistics of these two datasets. All datasets and our model implementations are available at: https://github.com/mickeystroller/TaxoExpan. 5.1.2 Evaluation Metrics. As our model returns a rank list of all candidate parents for each input query concept, we evaluate its performance using the following three ranking-based metrics.</p><p>• Mean Rank (MR) measures the average rank position of a query concept's true parent among all candidates. For queries with multiple parents, we first calculate the rank position of each individual parent and then take the average of all rank positions. Smaller MR value indicates better model performance. • Hit@k is the number of query concepts whose parent is ranked in the top k positions, divided by the total number of queries. • Mean Reciprocal Rank (MRR) calculates the reciprocal rank of a query concept's true parent. We follow <ref type="bibr" target="#b54">[55]</ref> and use a scaled version of MRR in the below equation:</p><formula xml:id="formula_25">MRR = 1 |C| c ∈ C 1 |parent(c)| i ∈par ent (c) 1 ⌈R i,c /10⌉</formula><p>, where parent(c) represents the parent node set of the query concept c, and R i,c is the rank position of query concept c's true parent i. We scale the original MRR by a factor 10 in order to amplify the performance gap between different methods. 5.1.3 Compared Methods. We compare the following methods:</p><p>(1) Closest-Parent: A rule-based method which first scores each candidate position in the existing taxonomy based on its cosine distance to the query concept between their initial embedding, and then ranks all positions using this score. The position with the smallest distance is chosen to be query concept's parent. (2) Closest-Neighbor: Another rule-based method that scores each position based on its distance to the query concept plus the average distance between its children nodes and the query. (3) dist-XGBoost: A self-supervised boosting method that works directly on 39 manually-designed features generated using initial node embeddings without any embedding transformation. We input these features into XGBoost <ref type="bibr" target="#b8">[9]</ref>, a tree-based boosting model, to predict the matching score between a query concept and a candidate position. (4) ParentMLP: A self-supervised method that first concatenates the query concept embedding with the candidate position embedding and then feeds them into a Multi-Layer Perceptron (MLP) for prediction. ( <ref type="formula" target="#formula_8">5</ref>) DeepSetMLP: Another self-supervised method that extends ParentMLP by adding information of candidate position's children nodes. Specifically, we first use DeepSet architecture <ref type="bibr" target="#b58">[59]</ref> to generate the representation of the children node set and then concatenate it with query &amp; candidate position representations before the final MLP module. ( <ref type="formula" target="#formula_11">6</ref>) TaxoExpan: Our proposed framework using position-enhanced GAT (PGAT) as graph propagation module, weighted mean readout (WMR) for graph readout, and log-bilinear model (LBM) for query-anchor matching. We learn this model using our proposed InfoNCE loss. 5.1.4 Implementation Details and Parameter Settings. For a fair comparison, we use the same 250-dimension embeddings across all compared methods. We use Google's original word2vec implementation <ref type="foot" target="#foot_4">5</ref> for learning embeddings and employ gensim <ref type="foot" target="#foot_5">6</ref> to load trained embeddings for calculating term distances in Closest-Parent, Closest-Neighbor, and dist-XGBoost methods. For the other three methods, we implement them using PyTorch and DGL framework <ref type="foot" target="#foot_6">7</ref> . We tune hyper-parameters in all self-supervised methods on the masked validation set. For TaxoExpan, we use a two-layer positionenhanced GAT where the first layer has four attention heads (of size 250) and the second layer has one attention head (of size 500). For both layers, we use 50-dimension position embeddings and apply dropout with rate 0.1 on the input feature vectors. We use Adam optimizer with initial learning rate 0.001 and ReduceLROnPlateau scheduler <ref type="foot" target="#foot_7">8</ref> with three patience epochs. We discuss the influence of these hyper-parameters in the next subsection. 5.1.5 Experimental Results. We present the experimental results in the following aspects. 1. Overall Performance. Table <ref type="table" target="#tab_1">2</ref> presents the results of all compared methods. First, we find that Closest-Neighbor method clearly outperforms Closest-Parent method and DeepSetMLP is much better than ParentMLP. This demonstrates the effectiveness of modeling local structure information. Second, we compare dist-XGBoost method with Closest-Neighbor and show that self-supervision indeed helps us to learn an effective way to combine various neighbor distance information. All four self-supervised methods outperform rule-based methods. Finally, our proposed TaxoExpan has the overall best performance across all the metrics and defeats the second best method by a large margin. 2. Ablation Analysis of Model Architectures. TaxoExpan contains three key components: a graph propagation module, a graph readout module, and a matching model. Here, we study how different choices of these components affect the performance of TaxoExpan. Table <ref type="table" target="#tab_2">3</ref> lists the results and the first column contains the index of each model invariant.  First, we analyze graph propagation module by using simple average scheme for graph readout and MLP for matching. By comparing model 1 to model 3 and model 2 to model 4, we can see that graph attention architecture (GAT) is better than graph convolution architecture (GCN). Furthermore, the position-enhanced variants clearly outperform their non-position counterparts (model 3 versus model 1 and model 4 versus model 2). This illustrates the efficacy of the position embeddings in the graph propagation module.</p><p>Second, we study graph readout module by fixing the graph propagation module to be the best two variants among models 1-4. We can see both model 5 &amp; 6 outperform model 3 and model 7 &amp; 8 outperform model 4. This signifies that the position information also helps in the graph readout module. However, the best strategy of incorporating position information depends on the graph propagation module. The concatenation readout scheme works better for PGCN while the weighted mean readout is better for PGAT. One possible explanation is that the concatenation readout leads to more parameters in matching model and as PGAT itself has more parameters than PGCN, further introducing more parameters in PGAT may cause the model to be overfitted.</p><p>Finally, we examine the effectiveness of different matching models. We replace the MLP in models 5-8 with LBM to create model variants 9-12. We can clearly see that LBM works better than MLP. It could be that LBM better captures the interaction between the query representation and the final anchor representation. We compare models trained using Binary Cross Entropy (BCE) loss with those trained using InfoNCE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ablation Analysis of Training Schemes.</head><p>In this subsection, we evaluate the effectiveness of our proposed training scheme. In this study, we first group a set of positive and negative ⟨query, anchor ⟩ pairs into one single training instance (c.f. Sect. 4.3) and learn the model using InfoNCE loss (c.f. Eq. <ref type="bibr" target="#b10">(11)</ref>). An alternative is to treat these pairs as different instances and train the model using standard binary cross entropy (BCE) loss. Under this training scheme, we formulate our problem as a binary classification task. We compare these two training schemes for the top 4 best models in Table <ref type="table" target="#tab_2">3</ref> (i.e., model <ref type="bibr">7, 8, 11, and 12)</ref>. Results are shown in Figure <ref type="figure" target="#fig_7">5</ref>. Our proposed training scheme with InfoNCE loss is overall much better, it beats the BCE loss scheme on 14 out of total 16 cases. One reason is that BCE loss is very sensitive to the noises in the generated self-supervision data while InfoNCE loss is more robust to such label noise. Furthermore, we find that LBM matching can benefit more from our training scheme with InfoNCE loss -with larger margin on all 8 cases, compared with the simple MLP matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hyper-parameter Sensitivity Analysis.</head><p>We analyze how some hyper-parameters in TaxoExpan affect the performance in Figure <ref type="figure" target="#fig_8">6</ref>. First, we find that choosing an approximate position embedding dimension is important. The model performance increases as this dimensionality increases until it reaches about 50. When we further increase position embedding dimension, the model will overfit and the performance decreases. Second, we study the effect of negative sampling ratio N . As shown in Figure <ref type="figure" target="#fig_8">6</ref>, the model performance first increases as N increases until it reaches about 30 and then becomes stable. Finally, we examine two hyper-parameters controlling the model complexity: the number of heads in PGAT and the final graph embedding dimension. We observe that the best model performance is reached when the number of attention heads falls in range 3 to 5 and the graph embedding dimension is set to 500. Too many attention heads or too large graph embedding dimension will lead to overfit and performance degradation.  <ref type="figure" target="#fig_12">8</ref> shows some outputs of TaxoExpan on both MAG-CS and MAG-Full datasets. On MAG-CS dataset, we can see that over 20% of queries have their true parents correctly ranked at the first position and less than 1.5% queries have their "true" parents ranked outside of top 1000 positions. Among these 1.5% significantly wrong queries, we find some of them actually have incorrect existing parents. For example, the concept "boils and carbuncles", which is a disease entity, is mistakenly put under parent node "dataset". Similar cases also happen on MAG-Full dataset where we find the concept "blood staining" is currently under "laryngeal mask airway".</p><p>Besides the above label errors, we also observe two common mistake patterns. The first type of mistakes is caused by term ambiguity. For instance, the term "java" in concept "java apple" refers to an island in Indonesia where fruit apple is produced, rather than a programming language used in Apple company. The second type of mistakes results from term granularity. For example, TaxoExpan outputs the two most likely parent nodes of concept "captcha" are "artificial intelligence" and "computer security". Although these two concepts are certainly relevant to "captcha", they are too general compared to its true parent node "internet privacy". Finally, we observe that TaxoExpan can return very sensible anchor positions of query concepts, even though they are not exactly the current "true" parents. For example, the concept "medline plus" refers to a large online medical library and thus is related to both "world wide web" and "library science". Also, the concept "email hacking" is clearly relevant to both "internet privacy" and "hacker".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>TaxoExpan for Taxonomy Self-Cleaning. From the above case studies, we find another interesting application of TaxoExpan is to use it for cleaning the existing taxonomy. Specifically, we partition all leaf nodes of the existing taxonomy into 5 groups and randomly mask one group of nodes. Then, we train a TaxoExpan model on the remaining nodes and predict on the masked leaf nodes. Next, we select those entities whose true parents appear at the bottom of the rank lists returned by TaxoExpan (i.e., the long-tail part of two histograms in Figure <ref type="figure" target="#fig_12">8</ref>). The parents of those selected entities are highly questionable and calls for further manual inspections. Our preliminary experiments on the MAG-CS taxonomy shows that about 30% of these entities have existing parent nodes which are less appropriate than the parents inferred by TaxoExpan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on SemEval Task Benchmark</head><p>5.2.1 Datasets. We further evaluate TaxoExpan using SemEval Task 14 Benchmark dataset <ref type="foot" target="#foot_8">9</ref>  <ref type="bibr" target="#b21">[22]</ref> which includes WordNet 3.0 as the existing taxonomy and additional 1,000 domain-specific concepts with manual labels, split into 400 training concepts and 600 testing concepts. Each concept is either a verb or a noun and has a textual definition of a few sentences. The original task goal is to enrich the taxonomy by performing two actions for each new concept: (1) attach, where a new concept is treated as a new synset and is attached as a hyponym of one existing synset in WordNet, and (2) merge, where a new concept is merged into an existing synset. However, previous state-of-the-art methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>, including the winning solution, are only performing the attach operation. In this work, we also follow this convention and attach each new concept to the top-ranked synset in the WordNet. Finally, we obtain the initial feature vectors (for both new concepts and existing words in the WordNet) using pre-trained subword-aware fasttext embeddings <ref type="foot" target="#foot_9">10</ref> . For each concept, we generate its definition embedding and name embedding by averaging the embedding of each token in its textual definition and name string, correspondingly. Then, we sum the definition and name embeddings of a concept and use them as the initial embeddings for the TaxoExpan model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation</head><p>Metrics. We use the three official metrics defined in original SemEval Task 14 for evaluation: (1) Accuracy (Wu&amp;P) is the semantic similarity between a predicted parent node x p and the true parent x t , calculated as</p><formula xml:id="formula_26">Wu&amp;P(x p , x t ) = 2•dept h LC A(xp , x t ) dept h xp +dept h x t</formula><p>, where depth x is the depth of node x is the WordNet taxonomy and LCA(x p , x t ) represents the Least Common Ancestor of x p and x t . (2) Recall is the percentage of concepts for which an attached parent is predicted<ref type="foot" target="#foot_10">11</ref> . (3) F1 is the harmonic mean of Wu&amp;P accuracy and recall.  <ref type="formula" target="#formula_15">7</ref>) TaxoExpan-FWFS: Similar to ETF-FWFS, this is the ensemble model of FWFS and TaxoExpan. We treat the FWFS heuristic as a binary feature and add it into the final matching module. For all previous methods, we directly report their best performance in the literature. For the remaining methods, we tune them following the same procedure described in the Section 5.1.4. <ref type="table" target="#tab_4">4</ref> shows the experimental results on SemEval dataset. We can see that both dist-XGBoost and TaxoExpan methods can outperform the previous winning system of this task (i.e., MSejrKU) and the baseline ETF. In addition, we can see the FWFS heuristic is indeed very powerful for this dataset and incorporating it as a strong feature can significantly boost the performance. However, this feature requires human-labeled definition sentences and thus can not be easily generalized to taxonomies other than WordNet. Finally, we show that TaxoExpan-FWFS can achieve the new state-of-the-art performance on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Experimental Results. Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Wu&amp;P Recall F1</p><p>MSejrKU <ref type="bibr" target="#b38">[39]</ref> 0.523 0.973 0.680 FWFS <ref type="bibr" target="#b21">[22]</ref> 0.514 1.000 0.679 ETF <ref type="bibr" target="#b47">[48]</ref> 0.473 1.000 0.642 ETF-FWFS <ref type="bibr" target="#b47">[48]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX Proof of Loss Function</head><p>Here we prove that optimizing the loss function in Eq. ( <ref type="formula" target="#formula_21">11</ref>) will result in f (•) estimating the probability density in Eq. <ref type="bibr" target="#b11">(12)</ref>. By construction, X contains query n c 's one positive anchor (i.e., its true parent n p ) sampled from the true distribution P(a i |n c ) and N negative anchors {n l r | N l =1 } sampled from a uniform distribution P(a i ). If we merge these N + 1 anchors into a small set and consider the task of selecting true anchor n p 's position j * in [1, 2, . . . , N + 1], we can view Eq. ( <ref type="formula" target="#formula_21">11</ref>   .</p><p>From above, we can see that the optimal value for f (a j , n c ) is proportional to P(a j |n c ) P(a j ) .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two egonets correspond to two anchor concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Overview of TaxoExpan framework. д emb is an embedding model that provides query concept's initial feature vector h q and the initial feature vector of each node in the egonet. The graph propagation module transforms initial feature vectors into better node representations based on which the graph readout module outputs the egonet embedding as the final anchor representation. Finally, a matching module inputs both query and anchor representations and outputs their matching score.</figDesc><graphic url="image-3.png" coords="5,306.84,114.38,59.26,74.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(k ) u . We replace each node feature h (k −1) u with its position-enhanced version h (k −1) u ∥p (k −1) u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Self-supervision generation.</figDesc><graphic url="image-2.png" coords="5,202.29,114.38,59.26,74.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 4 X 7 X 8 Update</head><label>1478</label><figDesc>Self-supervised learning of TaxoExpan Input: A taxonomy T 0 ; negative size N , batch size B; model f (• |Θ). Output: Learned model parameters Θ. 1 Randomly initialize Θ; 2 while L(Θ) in Eq. (11) not converge do 3 Enumerate edges in T 0 and sample B edges without replacement; = { } # current batch of training instances; 5 for each sampled edge ⟨n p , n c ⟩ do 6 Generate N negative pairs { ⟨n l r , n c ⟩ | N l =1 }; ← X ∪ { ⟨n p , n c ⟩, ⟨n 1 r , n c ⟩, . . . , ⟨n N r , n c ⟩ }; Θ based on X. 9 Return Θ; concepts underneath their predicted parents one at a time, and output the expanded taxonomy. Computational Complexity Analysis. At the training stage, our model uses |E (0) | training instances every epoch and thus scales linearly to the number of edges in the existing taxonomy. At the inference stage, for each query concept, we calculate |N (0) | matching scores, one for every existing node in T 0 . Although such O(|N (0) |)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation analysis of training schemes on MAG-CS dataset. We compare models trained using Binary Cross Entropy (BCE) loss with those trained using InfoNCE loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hyper-parameter sensitivity analysis on MAG-CS dataset. We use PGAT for graph propagation, WMR for graph readout, and LBM for query-graph matching. Model is trained using InfoNCE loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (Left) Training time of 20 epochs on GPU with respect to % of sampled nodes in the existing taxonomy. (Right) Inference time of all 2450 queries in MAG-CS dataset. Note here y-axis is in logarithm scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>5. 2 . 3</head><label>23</label><figDesc>Baseline Methods. We compare the following methods:(1) FWFS<ref type="bibr" target="#b21">[22]</ref>: The original baseline in Task 14. Given a concept c with its definition d c , this method picks the first word w in d c that has the same part of speech as c and treats this word as the parent node of c. (2) MSejrKU [39]: The winning solution of Task 14. This method leverages distributional and syntactic features to train a SVM classifier which is then used to predict the goodness of fit for a new concept with an existing synset in WordNet. (3) ETF [48]: The current state-of-the-art method that learns a LambdaMART model with 15 manually designed features, including topological features from the taxonomy's graph structure and semantic features from corpus and Bing search results. (4) ETF-FWFS [48]: The ensemble model of FWFS and ETF, which adds the FWFS property as a binary feature into the Lamb-daMART model in ETF. (5) dist-XGBoost: The same tree boosting model described in the previous subsection 5.1.3. (6) TaxoExpan: Our proposed taxonomy expansion framework. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>) as the cross entropy of position distribution P from model prediction relative to the true distribution P * . Specifically, the model predicted position distribution Pj = f (a j ,n c )N +1 k =1 f (a k ,n c ) where one of {a k | N +1k =1 } is the true anchor and all the others are negative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example output of TaxoExpan on MAG-CS and MAG-Full datasets. We draw a histogram of the ranks of query concepts' true parents within the rank list returned by TaxoExpan. In subfigure (a), for example, we have 519 (out of 2450) queries that their parents are exactly ranked in the first position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics. |N | and |E | are the number of nodes and edges in the existing taxonomy. |D| indicates the taxonomy depth and |C| is the number of new concepts.</figDesc><table><row><cell>Dataset</cell><cell>|N |</cell><cell>|E |</cell><cell>|D|</cell><cell>|C|</cell></row><row><cell>MAG-CS</cell><cell>24,754</cell><cell>42,329</cell><cell>6</cell><cell>2,450</cell></row><row><cell cols="3">MAG-Full 355,808 638,674</cell><cell>6</cell><cell>37,804</cell></row><row><cell>SemEval</cell><cell>95,882</cell><cell>89,089</cell><cell>20</cell><cell>600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall results on MAG-CS and MAG-Full datasets. We run all methods three times and report the averaged result with standard deviation. Note that smaller MR indicates better model performance. For all other metrics, larger values indicate better performance. We highlight the best two models in terms of the average performance under each metric.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">MAG-CS</cell><cell></cell><cell></cell><cell cols="2">MAG-Full</cell></row><row><cell></cell><cell>MR</cell><cell>Hit@1</cell><cell>Hit@3</cell><cell>MRR</cell><cell>MR</cell><cell>Hit@1</cell><cell>Hit@3</cell><cell>MRR</cell></row><row><cell>Closest-Parent</cell><cell cols="2">1327.16 (±0.000) 0.0531 (±0.000)</cell><cell>0.0986 (±0.000)</cell><cell cols="3">0.2691 (±0.000) 14355.5 (±0.000) 0.0360 (±0.000)</cell><cell>0.0728 (±0.000)</cell><cell>0.1897 (±0.000)</cell></row><row><cell cols="2">Closest-Neighbor 382.07 (±0.000)</cell><cell>0.1085 (±0.000)</cell><cell>0.2000 (±0.000)</cell><cell>0.3987 (±0.000)</cell><cell>4160.8 (±0.000)</cell><cell>0.0221 (±0.000)</cell><cell>0.0419 (±0.000)</cell><cell>0.1405 (±0.000)</cell></row><row><cell>dist-XGBoost</cell><cell>136.86 (±1.832)</cell><cell>0.1903 (±0.010)</cell><cell>0.3483 (±0.014)</cell><cell cols="5">0.6618 (±0.003) 426.70 (±8.047) 0.1498 (±0.076) 0.3046 (±0.009) 0.5621 (±0.002)</cell></row><row><cell>ParentMLP</cell><cell cols="2">114.79 (±12.25) 0.0729 (±0.088)</cell><cell>0.2656 (±0.037)</cell><cell>0.6454 (±0.009)</cell><cell>457.14 (±39.81)</cell><cell>0.098 (±0.094)</cell><cell>0.1928 (±0.086)</cell><cell>0.4950 (±0.012)</cell></row><row><cell>DeepSetMLP</cell><cell cols="5">115.26 (±9.159) 0.1988 (±0.005) 0.3581 (±0.016) 0.6653 (±0.015) 444.83 (±27.59)</cell><cell>0.1461 (±0.005)</cell><cell cols="2">0.2971 (±0.064) 0.6392 (±0.017)</cell></row><row><cell>TaxoExpan</cell><cell cols="8">80.33 (±5.470) 0.2121 (±0.010) 0.3823 (±0.012) 0.6929 (±0.003) 341.31 (±33.62) 0.1523 (±0.009) 0.3087 (±0.010) 0.6453 (±0.035)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation analysis of model architectures on MAG-CS dataset. We assign an index to each model variant (shown in the first column). All models are run three times with their averaged scores reported.</figDesc><table><row><cell></cell><cell>Graph</cell><cell>Graph</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ind</cell><cell></cell><cell></cell><cell>Matching</cell><cell>MR</cell><cell>Hit@1 Hit@3</cell><cell>MRR</cell></row><row><cell></cell><cell cols="2">Propagate Readout</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>GCN</cell><cell>Mean</cell><cell>MLP</cell><cell cols="3">167.82 0.1581 0.2964 0.6002</cell></row><row><cell>2</cell><cell>GAT</cell><cell>Mean</cell><cell>MLP</cell><cell cols="3">131.46 0.1584 0.3192 0.6409</cell></row><row><cell>3</cell><cell>PGCN</cell><cell>Mean</cell><cell>MLP</cell><cell cols="3">148.54 0.1809 0.3015 0.6255</cell></row><row><cell>4</cell><cell>PGAT</cell><cell>Mean</cell><cell>MLP</cell><cell cols="3">100.80 0.1896 0.3304 0.6525</cell></row><row><cell>5</cell><cell>PGCN</cell><cell>WMR</cell><cell>MLP</cell><cell cols="3">144.81 0.1798 0.3014 0.6309</cell></row><row><cell>6</cell><cell>PGCN</cell><cell>CR</cell><cell>MLP</cell><cell cols="3">135.89 0.1902 0.3118 0.6348</cell></row><row><cell>7</cell><cell>PGAT</cell><cell>WMR</cell><cell>MLP</cell><cell cols="3">92.62 0.1945 0.3584 0.6619</cell></row><row><cell>8</cell><cell>PGAT</cell><cell>CR</cell><cell>MLP</cell><cell>95.84</cell><cell cols="2">0.1897 0.3512 0.6596</cell></row><row><cell>9</cell><cell>PGCN</cell><cell>WMR</cell><cell>LBM</cell><cell cols="3">139.41 0.1829 0.3370 0.6642</cell></row><row><cell>10</cell><cell>PGCN</cell><cell>CR</cell><cell>LBM</cell><cell cols="3">130.12 0.1934 0.3462 0.6776</cell></row><row><cell>11</cell><cell>PGAT</cell><cell>WMR</cell><cell>LBM</cell><cell cols="3">80.33 0.2121 0.3823 0.6929</cell></row><row><cell>12</cell><cell>PGAT</cell><cell>CR</cell><cell>LBM</cell><cell>84.40</cell><cell cols="2">0.2089 0.3813 0.6894</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Model performance on SemEval dataset. TaxoExpan versus all previous state-of-the-art methods. We report the best performance of all existing methods in the literature.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This paper studies taxonomy expansion when no human labeled supervision data are given. We propose a novel TaxoExpan framework which generates self-supervision data from the existing taxonomy and learns a position-enhanced GNN model for expansion. To make the best use of self-supervision data, we design a noiserobust objective for effective model training. Extensive experiments demonstrate the effectiveness and efficiency of TaxoExpan on three taxonomies from different domains. Interesting future work includes modeling inter-dependency among new concepts, leveraging current method to cleaning the input existing taxonomy, and incorporating feedbacks from downstream applications (e.g., search &amp; recommendation) to generate more diverse supervision signals for expanding the taxonomy. IIS 17-04532, and IIS-17-41317, and DTRA HDTRA11810026. Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of any U.S. Government. We thank Yuxiao Dong, Ziniu Hu, Li Ma for insightful discussions on this project and anonymous reviewers for valuable feedbacks.</figDesc><table><row><cell></cell><cell>.562</cell><cell>1.000 0.720</cell></row><row><cell>dist-XGBoost</cell><cell>0.528</cell><cell>1.000 0.691</cell></row><row><cell>TaxoExpan</cell><cell>0.543</cell><cell>1.000 0.704</cell></row><row><cell>TaxoExpan-FWFS</cell><cell>0.566</cell><cell>1.000 0.723</cell></row><row><cell>6 CONCLUSION</cell><cell></cell><cell></cell></row><row><cell cols="2">7 ACKNOWLEDGEMENT</cell><cell></cell></row><row><cell cols="3">Research was sponsored in part by DARPA under Agreements</cell></row><row><cell cols="3">No. W911NF-17-C-0099 and FA8750-19-2-1004, National Science</cell></row><row><cell>Foundation IIS 16-18481,</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We recognize that the modification of an existing taxonomy is necessary in some cases. However, it happens much less frequently and requires high cautiousness from human curator. Therefore, we leave it out of the scope of automation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Here we mask only leaves because if we remove intermediate nodes, we have to remove their descendants from the candidate parent pool, which causes different masked nodes (as testing query concepts) having different candidate pools.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We also test CBOW model, fastText<ref type="bibr" target="#b6">[7]</ref> and BERT embedding<ref type="bibr" target="#b10">[11]</ref> (averaged across all concept mentions), and empirically we find skipgram model in word2vec works best on this dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/tmikolov/word2vec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/RaRe-Technologies/gensim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/dmlc/dgl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">http://alt.qcri.org/semeval2016/task14/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">We use the wiki-news-300d-1M-subword.vec.zip version on fastText official website.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">This metric is used because the original task allows a model to decline to place new concepts in order to avoid making placements with low confidence.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM DL</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised Distributional Hypernym Discovery via Domain Adaptation</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extending WordNet with Fine-Grained Collocational Information via Supervised Distributional Learning</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rodríguez-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured Learning for Taxonomy Induction with Belief Propagation</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">Alejandra</forename><surname>Bocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
		<title level="m">Archi-WordNet: Integrating WordNet with Domain-Specific Knowledge</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching Word Vectors with Subword Information</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Jian Jhen Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparing Constraints for Taxonomic Organization</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Cocos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>WordNet</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards new information resources for public health -From WordNet to MedicalWordNet</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taxonomy Induction Using Hypernym Subsequences</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Acquisition of Hyponyms from Large Text Corpora</title>
		<author>
			<persName><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understand Short Texts by Harvesting and Analyzing Semantic Knowledge</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Taxonomy-Aware Multi-Hop Reasoning Networks for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaole</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MetaPAD: Meta Pattern Discovery from Massive Text Corpora</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reserating the awesometastic: An automatic extension of the WordNet taxonomy for novel terms</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SemEval-2016 Task 14: Semantic Taxonomy Enrichment</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@NAACL-HLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web</title>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph Classification using Structural Attention</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Deep Generative Models of Graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Information-Theoretic Definition of Similarity</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Medical Subject Headings (MeSH)</title>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A User-Centered Concept Mining System for Query and Document Understanding at Tencent</title>
		<author>
			<persName><forename type="first">Bang</forename><surname>Wu Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Zhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunfeng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network</title>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Endto-End Reinforcement Learning for Automatic Taxonomy Induction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CrowdTC: Crowdsourced Taxonomy Construction</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><forename type="middle">Chen</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PATTY: A Taxonomy of Relational Patterns with Semantic Types</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Graph-Based Algorithm for Inducing Lexical Taxonomies from Scratch</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Comparison of Two Paraphrase Models for Taxonomy Augmentation</title>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inclusive yet Selective: Supervised Distributional Hypernymy Detection</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MSejrKu at SemEval-2016 Task 14: Taxonomy Enrichment by Evidence Ranking</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sejr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schlichtkrull</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Héctor</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alonso</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@NAACL-HLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SetExpan: Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">HiExpan: Task-Guided Taxonomy Construction by Hierarchical Tree Expansion</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Web-scale system for scientific knowledge exploration</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An Overview of Microsoft Academic Service (MAS) and Applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Building Enterprise Taxonomies</title>
		<author>
			<persName><forename type="first">Darin</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Named Entity WordNet</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Monachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enriching Taxonomies With Functional Domain Knowledge</title>
		<author>
			<persName><forename type="first">Nikhita</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">K</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ajwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">OntoLearn Reloaded: A Graph-Based Algorithm for Taxonomy Induction</title>
		<author>
			<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet model for taxonomy expansion for search engines</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Characterising Measures of Lexical Distributional Similarity</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Probase: a probabilistic taxonomy for text understanding</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Constructing Task-Specific Taxonomies for Document Collection Browsing</title>
		<author>
			<persName><forename type="first">Grace</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Position-aware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep Sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">TaxoGen: Constructing Topical Concept Taxonomy by Adaptive Term Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Entity Set Expansion via Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Xiangling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Taxonomy discovery for personalized recommendation</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
