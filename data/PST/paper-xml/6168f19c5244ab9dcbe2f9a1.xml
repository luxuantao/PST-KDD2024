<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
							<email>benjamin.muller@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
							<email>lucas@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Istitute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Lind</surname></persName>
							<email>ericlind@amazon.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Amazon Alexa AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-Domain Generative Question Answering has achieved impressive performance in English by combining document-level retrieval with answer generation. These approaches, which we refer to as GENQA, can generate complete sentences, effectively answering both factoid and non-factoid questions. In this paper, we extend GENQA to the multilingual and cross-lingual settings. For this purpose, we first introduce GEN-TYDIQA, an extension of the TyDiQA dataset with well-formed and complete answers for Arabic, Bengali, English, Japanese, and Russian. Based on GEN-TYDIQA, we design a cross-lingual generative model that produces full-sentence answers by exploiting passages written in multiple languages, including languages different from the question. Our cross-lingual generative system outperforms answer sentence selection baselines for all 5 languages and monolingual generative pipelines for three out of five languages studied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Improving coverage of the world's languages is essential for retrieval-based Question Answering (QA) systems to provide a better experience for non-English speaking users. One promising direction for improving coverage is multilingual, multisource, open-domain QA. Multilingual QA systems include diverse viewpoints by leveraging answers from multiple linguistic communities. Further, they can improve accuracy, as all facets necessary to answer a question are often unequally distributed across languages on the Internet <ref type="bibr" target="#b59">(Valentim et al., 2021)</ref>.</p><p>With the advance of large-scale language models, multilingual modeling has made impressive progress at performing complex NLP tasks without requiring explicitly translated data. Building on pre-trained language models <ref type="bibr" target="#b17">(Devlin et al., 2019;</ref><ref type="bibr" target="#b14">Conneau et al., 2020;</ref><ref type="bibr">Xue et al., 2021;</ref><ref type="bibr" target="#b36">Liu et al., 2020)</ref>, it is now possible to train models that accurately process textual data in multiple languages <ref type="bibr" target="#b29">(Kondratyuk and Straka, 2019)</ref> and perform crosslingual transfer <ref type="bibr" target="#b45">(Pires et al., 2019)</ref> using annotated data in one language to process another language.</p><p>At the same time, answer generation-based approaches have been shown to be effective for many English QA tasks, including Machine Reading (MR) <ref type="bibr" target="#b27">(Izacard and Grave, 2021;</ref><ref type="bibr">Lewis et al., 2020c)</ref>, question-based summarization <ref type="bibr" target="#b26">(Iida et al., 2019;</ref><ref type="bibr" target="#b22">Goodwin et al., 2020;</ref><ref type="bibr" target="#b16">Deng et al., 2020)</ref>, and, most relevant to this work, answer generation for retrieval-based QA <ref type="bibr" target="#b25">(Hsu et al., 2021</ref>) -that we refer to as GENQA.</p><p>Compared to generative MR models, GENQA approaches are trained to produce complete and expressive sentences that are easier to understand than extracted snippets <ref type="bibr" target="#b12">(Choi et al., 2021)</ref>. Most importantly, they are trained to generate entire sentences, allowing them to answer both factoid or non-factoid questions, e.g., asking for descriptions, explanation, or procedures.</p><p>In this paper, we study and propose a simple technique for open-domain QA in a cross-lingual setting. Following <ref type="bibr" target="#b25">Hsu et al. (2021)</ref> (and as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>), we work with a pipeline made of 3 main modules. First, a document retriever that retrieves relevant documents given a question; second, an answer sentence selection (AS2) model <ref type="bibr" target="#b21">(Garg et al., 2020;</ref><ref type="bibr" target="#b62">Vu and Moschitti, 2021</ref>) that ranks the sentences from the retrieved documents based on how likely they are to include the answer; and third, a generative model that generates a full sentence to answer the question given the sentence candidates.</p><p>Our contribution focuses on the generative model. We introduce CROSSGENQA. CROSS-GENQA can generate full-sentence answers using sentence candidates written in multiple languages including languages different from the question and English.</p><p>Given the scarcity of annotated corpora for GENQA, especially in languages different from English, we introduce the GEN-TYDIQA dataset. GEN-TYDIQA is an extension of TyDiQA, a dataset for typologically diverse languages in which questions are answered with passages and short spans extracted from Wikipedia <ref type="bibr" target="#b13">(Clark et al., 2020)</ref>. Our GEN-TYDIQA includes humangenerated, fluent, self-contained answers in Arabic, Bengali, English, Russian and Japanese, making it a valuable resource for evaluating multilingual generative QA systems. We found human-generated answers to be essential in evaluating GENQA: compared to the standard approach of providing reference documents, they dramatically speed-up annotations and improve inter-annotator agreement.</p><p>Our evaluation shows that our CROSSGENQA system outperforms AS2 ranking models, and matches or exceeds similar monolingual pipelines.</p><p>In summary, our contribution is three-fold:</p><p>(i) We introduce GEN-TYDIQA  Generating Fluent Answers for QA The Generation of fluent and complete-sentence answers is still in its infancy, as most generative models for QA are used for extractive QA (e.g., <ref type="bibr" target="#b24">(Guu et al., 2020;</ref><ref type="bibr" target="#b32">Lewis et al., 2020b;</ref><ref type="bibr">Asai et al., 2021a,b)</ref>. Approaches to ensure response fluency have been explored in the context of dialogue systems <ref type="bibr" target="#b6">(Baheti et al., 2020;</ref><ref type="bibr" target="#b42">Ni et al., 2021)</ref>, but remain nevertheless understudied in the context of QA. Providing natural sounding answers is a task of particular interest to provide a better experience for users of voice assistants. One resource for this task is the MS-MARCO dataset <ref type="bibr" target="#b41">(Nguyen et al., 2016)</ref>. It includes 182,669 question and answer pairs with human-written well-formed answers. However, it only contains samples in English.</p><p>Our GEN-TYDIQA extends TyDiQA <ref type="bibr" target="#b13">(Clark et al., 2020)</ref> adding natural human-generated answers for Arabic, Bengali, English, Japanese, and Russian. To the best of our knowledge, it is the first work that provides well-formed, natural-sounding answers for non-English languages.</p><p>Multilingual Extractive QA Designing QA models for languages different from English is challenging due to the limited number of resources and the limited size of those datasets. For this reason, many studies leverage transfer learning across languages by designing systems that can make use of annotated data in one language to model another language. For instance, <ref type="bibr" target="#b13">Clark et al. (2020)</ref> showed that concatenating the training data from multiple languages improves the performance of a model on all the target languages for extractive QA. In the Open-Retrieval QA setting, multilingual modeling can be used to answer questions in one language using information retrieved from other languages. <ref type="bibr" target="#b15">Da San Martino et al. (2017)</ref> showed how cross-language tree kernels can be used to rank English answer candidates for Arabic questions. <ref type="bibr" target="#b40">Montero et al. (2020)</ref> designed a cross-lingual question similarity technique to map a question in one language to a question in English for which an answer has already been found. <ref type="bibr" target="#b2">Asai et al. (2021a)</ref> showed that extracting relevant passages from English Wikipedia can deliver better answers than relying only on the Wikipedia corpora of the question language. <ref type="bibr" target="#b62">Vu and Moschitti (2021)</ref> showed how machine translated questionanswer pairs can be used to train a multilingual QA model; in their study, they leveraged English data to train an English and German AS2 model.</p><p>Finally, <ref type="bibr" target="#b4">Asai et al. (2021c)</ref> introduced CORA and reached state-of-the-art performance on openretrieval span-prediction question answering across 26 languages. While related to our endeavor, it is significantly different in several key aspects. First, unlike CROSSGENQA, CORA does not produce full, complete sentences; rather, it predicts spans of text that might contain a factoid answer. Second, it mainly relies on sentence candidates that are written in English and in the question language; by contrast, in our work we choose to translate the questions into a variety of languages, allowing us to use monolingual retrieval pipelines to retrieve candidate sentences in diverse languages. We show that this form of cross-lingual GENQA outperforms monolingual GENQA in a majority of the languages studied.</p><p>Answer Sentence Selection (AS2) The AS2 task originated in the TREC QA Track <ref type="bibr" target="#b61">(Voorhees, 2001)</ref>; more recently, it was revived by <ref type="bibr" target="#b63">Wang et al. (2007)</ref>. Neural AS2 models have also been explored <ref type="bibr" target="#b64">(Wang and Jiang, 2017;</ref><ref type="bibr" target="#b21">Garg et al., 2020)</ref>. AS2 models receive as input a question and a (potentially large) set of candidate answers; they are trained to estimate, for each candidate, its likelihood to be a correct answer for the given question.</p><p>Several approaches for monolingual AS2 have been proposed in recent years. <ref type="bibr" target="#b52">Severyn and Moschitti (2015)</ref> used CNNs to learn and score question and answer representations, while others proposed alignment networks <ref type="bibr" target="#b54">(Shen et al., 2017;</ref><ref type="bibr" target="#b58">Tran et al., 2018;</ref><ref type="bibr" target="#b55">Tay et al., 2018)</ref>. Compare-and-aggregate architectures have also been extensively studied <ref type="bibr" target="#b64">(Wang and Jiang, 2017;</ref><ref type="bibr" target="#b8">Bian et al., 2017;</ref><ref type="bibr" target="#b67">Yoon et al., 2019)</ref>. Tayyar <ref type="bibr" target="#b56">Madabushi et al. (2018)</ref> exploited fine-grained question classification to further improve answer selection. <ref type="bibr" target="#b21">Garg et al. (2020)</ref> achieved state-of-the-art results by finetuning transformer-based models on a large QA dataset first, and then adapting to smaller AS2 dataset. <ref type="bibr" target="#b39">Matsubara et al. (2020)</ref> showed how, similar in spirit to GENQA, multiple heterogeneous systems for AS2 can be be combined to improve a question answer pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The GEN-TYDIQA Dataset</head><p>To more efficiently evaluate our multilingual generative pipeline (lower cost and higher speed), we built GEN-TYDIQA, an evaluation dataset for answer-generation-based QA in Arabic, Bengali, English, Japanese, and Russian. This extends the TyDiQA <ref type="bibr" target="#b13">(Clark et al., 2020)</ref> dataset.</p><p>TyDiQA is a QA dataset that includes questions for 11 typologically diverse languages. Each entry is composed of a human-generated question and a single Wikipedia document providing relevant information. For a large subset of its questions, TyDiQA also contains a human-annotated passage extracted from the Wikipedia document, as well as a short span of text that answers the question. We extend the TyDiQA validation set<ref type="foot" target="#foot_1">2</ref> by collecting human-generated answers based on the provided questions and passages using Amazon Mechanical Turk<ref type="foot" target="#foot_2">3</ref> (cf. Appendix C.1 for hiring criteria and rewards). Collecting human-generated answers is crucial for properly evaluating GENQA models, as we will show in section 5.4. We use a two-stage data collection process:</p><p>(EN) Question: What do pallid sturgeons eat? TyDiQA Span: -GEN-TYDIQA Answer: Pallid sturgeons eat various species of insects and fish depending on the seasons.</p><p>(RU) Question: Когда закончилась Английская революция? When did the English Revolution end? TyDiQA Span: 1645 GEN-TYDIQA Answer: Английская революция, известная также как Английская гражданская вой закончилась в 1645, когда Кромвель создал «Армию нового образца», одержавшую решающую победу в сражении при Нэйcби The English Revolution, also known as the English Civil War; ended in 1645, when Cromwell created the "Army of the new model", which won a decisive victory at the Battle of Naysby.</p><p>(JA) Question: ストーンズリバーの戦いによる戦死者 は何人 How many were the deaths from the Battle of Stones River? TyDiQA Span: 23,515名 23,515 people GEN-TYDIQA Answer: ス ト ー ン ズ リ バ ー の 戦 い で23,515人が川で殺されました。 23,515 people were killed in the river in the Battle of Stones River.</p><p>Table <ref type="table">1</ref>: GEN-TYDIQA question and answer samples.</p><p>(1) Answer Generation We show each turker a question and its corresponding passage, and ask them to write an answer that meets the following three properties: (i) The answer must be factually correct and aligned with the information provided in the passage. If a passage is not sufficient to answer a question, turkers will respond "no answer". (ii) The answer must be a complete and grammatically correct sentence, or at most a few sentences. (iii) The answer should be self-contained; that is, it should be understandable without reading the question or the passage. Based on this condition, "yes" or "no" are not acceptable answers.</p><p>(2) Answer Validation We show each question alongside its corresponding passage and the humangenerated answer from Step (1) to five turkers. We ask them to label if the collected answer meets the three properties listed above: correctness, completeness, and self-containedness. We aggregate labels and keep only answers that received at least 3/5 positive judgements for each property. Table <ref type="table">1</ref> contains some examples of the data collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Statistics</head><p>We report the number of GEN-TYDIQA collected human-generated natural answers in table 2, and our coverage of the TyDiQA dataset. We do not reach 100% coverage due to our highly selective validation stage: we only accept answers that receive 3/5 votes for each property, a process that guarantees a high-quality dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilingual GenQA Systems</head><p>Our goal is to build a QA system that, given a question in a target language, retrieves the top-k most relevant passages from text sources in multiple languages, and generates an answer in the target language from these passages (even if the passages are in a different language from the question).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Definition and System Architecture</head><p>We first describe the AS2 and GENQA tasks in a language-independent monolingual setting, and then generalize to the cross-lingual setting.</p><p>In the monolingual setting for a language L i , an AS2 system takes as input a question q and a possibly large set of candidate answers C L i (e.g. all sentences from Wikipedia in the language L i ), ranks each candidate answer given q, and returns the top-ranking candidate c m ∈ C L i . A GENQA system uses the top k AS2-ranked answers in C L i to synthesize a machine-generated answer g in language L i .</p><p>The cross-lingual GENQA task extends this setup as follows: Consider a set of languages {L 1 , . . . , L r }. Given a question q in language L i , let M = ∪ r j=1 C L j be the set of relevant candidate sentence answers for q in any language. A cross-lingual GENQA system uses the top k ranked answers in M -regardless of language -to generate an answer g in L i .</p><p>Our architecture, illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, consists of the following components: (i) question translation 4 from L i to produce queries q L j in each language L j , (ii) a document retriever for each L j to get C L j , (iii) a monolingual AS2 model for each language, which sorts the candidates in C L j in terms of probability to be correct given q L j , where C L j is created by splitting the retrieved documents into sentences, (iv) an aggregator component, which builds a multilingual candidate set M using the top k candidates for each language, and (v) a cross-lingual answer generation model, which generates g from M .</p><p>We now present in more details each component of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multilingual Passage Retrieval</head><p>To obtain candidates for our multilingual pipeline, we used Wikipedia snapshots collected in May 2021. We processed each snapshot using WikiExtractor <ref type="bibr" target="#b5">(Attardi, 2015)</ref>, and create monolingual indices using PyTerrier <ref type="bibr" target="#b38">(Macdonald and Tonellotto, 2020)</ref>. During retrieval, we first translate queries in each language using AWS Translate. We validate the good quality of this system for all our languages in table 9 in the Appendix. We then use BM25 <ref type="bibr" target="#b50">(Robertson et al., 1995)</ref> to score documents. We choose BM25 because, as shown by <ref type="bibr" target="#b57">Thakur et al. (2021)</ref>, it is competitive with DPR-based models <ref type="bibr" target="#b28">(Karpukhin et al., 2020)</ref> and it outperforms DPR across a great diversity of domains.</p><p>Evaluation We evaluate the different retrievers independently: for each question, we compare the exact match of the title of the retrieved document with the gold document's title provided by TyDiQA. We compute the Hit@N at the document level, i.e., the percentage of questions having a correct document in the top-N predicted documents. In our experiments, we retrieve the top-100 documents from Wikipedia to feed them to the AS2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AS2 models for different languages</head><p>We build AS2 models by fine-tuning the multilingual masked-language model XLM-R (Conneau et al., 2020) into multiple languages, using question/sentence pairs, which we created with the TyDiQA dataset. We followed the procedure by <ref type="bibr" target="#b21">Garg et al. (2020)</ref> performed on the NQ dataset <ref type="bibr" target="#b30">(Kwiatkowski et al., 2019)</ref> to build the ASNQ dataset for English. For each ⟨question, Wikipedia document, span⟩ triplet from the TyDiQA dataset, we use the span to identify positive and negative sentence candidates in the Wikipedia document. We first segment each document at the sentence level using the spacy library 5 . We define positive examples to be the sentences that contain the span provided by the TyDiQA dataset, and negative examples to be all other sentences from the same Wikipedia document. We report statistics on AS2-TyDiQA in the 5 https://spacy.io/ Appendix in table 11. For more details, we refer the reader to <ref type="bibr" target="#b21">Garg et al. (2020)</ref>.</p><p>Model We fine-tune XLM-R extended with a binary classification layer on the AS2-TyDiQA dataset described above. At test time, we rank the candidates using the model output probability. Preliminary experiments confirmed the results of <ref type="bibr" target="#b13">Clark et al. (2020)</ref> regarding machine reading models on TyDiQA : the best performance is obtained when concatenating the datasets from all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multilingual Answer Generation Models</head><p>We extended the work of Hsu et al. ( <ref type="formula">2021</ref>) on monolingual GENQA modeling. For each question, this model takes the top-5 candidates ranked by AS2 as input. For CROSS-LINGUAL GENQA, we build a set of multiligual candidates M with two methods: (i) TOP 2 / LANG., which selects the top 2 candidates for each language and concatenates them (in total 2 × 5 = 10); and (ii) TOP 10, which selects the 10 candidates associated with the highest scores regardless of their language.</p><p>Model We used the pre-trained multilingual T5 language model (MT5) by <ref type="bibr">Xue et al. (2021)</ref>. This is an encoder-decoder transformer-based model <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> pre-trained with a spanmasking objective on a large amount of web-based data from 101 languages (we use the base version). We fine-tuned MT5 following <ref type="bibr" target="#b25">(Hsu et al., 2021)</ref>: for each sample, we give the model the question concatenated with the candidates M as input and a natural answer as the generated output. GENQA models are trained on MS-MARCO <ref type="bibr" target="#b41">(Nguyen et al., 2016)</ref> <ref type="foot" target="#foot_4">6</ref> , which includes 182,669 examples of ⟨question, 10 candidate passages, natural answer⟩ instances in English. When the language of the question (and answer) is not English or when we use candidates in multiple languages, we translate the training samples with Amazon's AWS Translate service and fine-tune the model on the translated data. For instance, to design a GENQA model answering questions in Arabic using input passages in Arabic, English, and Bengali, we fine-tune the model with questions and gold standard answers translated from English to Arabic, and input candidates in English, Arabic, and Bengali, where the latter two are translated from the MS-MARCO English passages.</p><p>Evaluation As pointed out by <ref type="bibr" target="#b11">Chen et al. (2019)</ref>, automatically evaluating generation-based QA systems is challenging. We experimented with BLEU <ref type="bibr" target="#b43">(Papineni et al., 2002)</ref> and ROUGE-L <ref type="bibr" target="#b35">(Lin, 2004)</ref>, two standard metrics traditionally used for evaluating generation-based systems, but found that they do not correlate with human judgment. For completeness, we report them in the Appendix D.2 along with a detailed comparison with human judgment. Thus, we rely on human evaluation through Amazon Mechanical Turk<ref type="foot" target="#foot_5">7</ref> : we ask three turkers to vote on whether the generated answer is correct, and report the P ositiveV otes T otalV otes as system Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Multilinguality and the different components of our system pipeline raise interesting research questions. Our experimental setup is defined by the combinations of our target set of languages with respect to questions, candidates, and answers. We experiment with GENQA in the monolingual (one model per language) and multilingual (one model for several languages) settings, where the question and candidates in the same language are used to generate an answer. Then we experiment with a cross-lingual GENQA model that is fed candidates in multiple languages. Despite being an apparent more complex task, we find that in many cases, the cross-lingual model outperform all other settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We approach multilingual generation-based question answering in three ways:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MONOLINGUAL GENQA (MONOGENQA)</head><p>The candidate language is the same as the question. For each language (Arabic, Bengali, English, Japanese and Russian), we monolingually fine-tune MT5, and report the performance of each GENQA model on the GEN-TYDIQA dataset (Tab. 5).</p><p>Our contribution is to show that this approach, first introduced by Hsu et al. (2021) for English, delivers similar performance for other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MULTILINGUAL GENQA (MULTIGENQA)</head><p>We train one MT5 for all five languages by concatenating their training and validation sets. This single model can answer questions in multiple languages, but it requires that answer candidates be in the same language as the question. We report </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CROSS-LINGUAL GENQA (CROSSGENQA)</head><p>We use candidates in multiple languages (Arabic, Bengali, Russian, English, Arabic) to answer a question in a target language. We retrieve and rerank sentence candidates in each language, aggregate candidates across all the languages, and finally generate answers (in the same language as the question). We report the performance on the GEN-TYDIQA dataset (table <ref type="table">5</ref>).</p><p>These experiments aim to determine whether our generative QA model can make use of information retrieved from multiple languages and outperform the baseline methods.</p><p>Manual Evaluation We stress the fact that all the results derived in the following experiments were manually evaluated with Amazon Mechanical Turk. In total, we run 34 tasks (system evaluations), requiring around 60k Hits, for a total manual evaluation of 20k QA pairs (times 3 turkers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feasibility Study</head><p>To explore whether a model fed with candidates written in languages different from the question can still capture relevant information to answer the question, we conduct a feasibility study using the MS-MARCO dataset with English as our target language and machine translated candidates.</p><p>For each question, we translate the top 5 candidate passages to different languages and provide these translated candidates as input to the model. We experiment with three translation settings: all candidates translated to German (DE); each candidate translated to a random choice of German, Spanish, French or Italian (DE-ES-FR-IT); translated to Arabic, Japanese or Korean (AR-JA-KO). We compare all these CROSS-LINGUAL GENQA models with a Clozed-Book QA Model (Roberts </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report the performance in table <ref type="table" target="#tab_2">3</ref>. All CROSS-LINGUAL GENQA models outperform significantly the Clozed-book approach. This means that even when the candidates are in languages different from the question, the model is able to extract relevant information to answer the question. We observe this even when the candidates are in languages distant from the question language (e.g., Arabic, Japanese, Korean).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GEN-TYDIQA Experiments</head><p>This section reports experiments of the full GENQA pipeline tested on the GEN-TYDIQA dataset with candidates retrieved from Wikipedia.</p><p>For each question, we retrieve documents with a BM25-based retriever, rank relevant candidates using the AS2 model, and feed them to the GENQA models. We note that we cannot compare the model performance across languages: as pointed out in <ref type="bibr" target="#b13">(Clark et al., 2020)</ref> regarding TyDiQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MONOGENQA Performance</head><p>We measure the impact of the retrieval and AS2 errors by computing the ideal GENQA performance, when fed with gold candidates (TyDiQA gold passage). We report the results in table 4. We evaluate the performance of the GENQA models, also comparing it to AS2 models on the GEN-TYDIQA dataset of each language. We report the results in table 5 (cf. MONOGENQA). The first row shows the document retrieval performance in terms of Hit@100 for the different languages considered in our work. We note comparable results among all languages, where Arabic reaches the highest accuracy, 70.7, and Japanese the lowest, 57.0. The latter may be Table <ref type="table">5</ref>: Hit@100 doc. of the retriever and Accuracy of GENQA models on GEN-TYDIQA. All CROSS-GENQA experiments use candidates aggregated from all the languages (AR, BN, EN, JA, RU).</p><p>due to the complexity of indexing ideogram-based languages. However, a more direct explanation is the fact that retrieval accuracy strongly depends on the complexity of queries (questions), which varies across languages for GEN-TYDIQA. Similarly to <ref type="bibr" target="#b13">Clark et al. (2020)</ref>, we find that queries in English and Japanese are more complex to answer compared to other languages.</p><p>Regarding answering generation results, rows 2 and 3 for English confirm Hsu et al. ( <ref type="formula">2021</ref>)'s findings: GENQA outperforms significantly AS2 by 4.6% (43.6 vs. 39.0). We also note a substantial improvement for Bengali (+9.4%, 67.4 to 58.0). In contrast, Arabic and Russian show similar accuracy between GENQA and AS2 models. Finally, AS2 seems rather more accurate than GENQA for Japanese (70.4 vs 64.3). Results reported by <ref type="bibr">Xue et al. (2021)</ref> show MT5 to be relatively worse for Japanese than all other languages we consider in many downstream tasks, so the regression seen here might be rooted in similar issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MULTIGENQA Performance</head><p>We compare the performance of the MONOLINGUAL GENQA models (one model per language) to the performance of the MULTILINGUAL GENQA model fine-tuned after concatenating the training datasets from all the languages. We report the performance in table 5 (cf. MULTIGENQA): multilingual fine-tuning improves the performance over monolingual finetuning for all languages except English. This shows that models benefit from training on samples from different languages. For Bengali, we observe an improvement of around 9% in accuracy. This result has a strong practical consequence: at test time, we do not need one GENQA model per language, we can rely on a single multilingual model trained on the concatenation of datasets from multiple languages (except for English, where we find that the monolingual model is more accurate). This result generalizes what has been shown for extractive QA <ref type="bibr" target="#b13">(Clark et al., 2020)</ref> to the GENQA task. Table <ref type="table">6</ref>: GENQA scores in English on Japanese-culturespecific questions extracted from TyDiQA. CANDI-DATES defines the language set of the input candidates.</p><p>CROSSGENQA Performance Our last and most important contribution is in table 5, which reports the performance of a GENQA model trained and evaluated with candidates in multiple languages.</p><p>This model can answer a user question in one language (e.g., Japanese) by using information retrieved from many languages, e.g., Arabic, Bengali, English, Japanese, and Russian). For Arabic, Japanese, and Russian, we observe that CROSS-LINGUAL GENQA outperforms other approaches by a large margin, e.g., for Russian, 13.8% (74.6-60.8) better than AS2, and an 8% percent improvement over MULTIGENQA.</p><p>For Bengali, the model fails at generate good quality answers (CROSSGENQA models reach at best 25.3% in accuracy compared to the 76.9% reached by the MULTIGENQA model). We hypothesize that this is the consequence of a poor translation quality of the question from Bengali to other languages as English, Arabic, or Japanese, which leads to poor candidate retrieval and selection, ultimately resulting in inaccurate generation.</p><p>Finally, we compare the two candidate aggregation strategies used for CROSS-LINGUAL GENQA: TOP 2 / LANG. and TOP 10 (see section 4.4). We observe that the aggregation strategy impacts moderately the downstream performance. For English, Arabic, Japanese and Russian the gap between the two methods is at most 2 points in accuracy. We leave the refinement of candidate selection in the multilingual setting for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>Examples Table <ref type="table" target="#tab_5">7</ref> shows the output of AS2, MULTILINGUAL GENQA, and CROSS-LINGUAL GENQA models to questions in Russian and Bengali. For Bengali, the GENQA models provide a correct and fluent answer while the AS2 model does not. For Russian, only the CROSS-LINGUAL GENQA model is able to answer correctly the question. This because AS2 does not rank the right information in the top k, while CROSS-LINGUAL GENQA can find the right information in another  language in the multi-language candidate set.</p><p>Error Propagation We observe (table 4) that the GENQA models are highly impacted by the retriever and AS2 quality. For example, English GENQA performance drops of 27.9 (65.3-37.4) points in Accuracy. This suggests that large improvement could be achieved by improving the document retriever and/or AS2 modules.</p><p>Culture-Specific Questions in English One striking result across our experiments is the lower performance of CROSS-LINGUAL GENQA model than GENQA model on English. We hypothesize that English questions from the GEN-TYDIQA dataset are more easily answered using information retrieved from English compared to other languages because those questions are centered on  cultures specific to English-speaking countries.</p><p>To verify our hypothesis, we re-run the same set of experiments, using culture-specific Japanese questions rather than English queries. To do so, we (i) took the Japanese questions set from GEN-TYDIQA, (ii) manually translated it in English, (iii) manually select 116 questions that are centered on Japanese culture, and (iv) run the same GENQA pipeline on those questions. The results reported in table <ref type="table">6</ref> show that CROSSGENQA outperforms MONOGENQA, suggesting that the former improves also the English setting if the question set is culturally not centered on English, i.e., it requires answers that cannot be found in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use of Reference Answer in Model Evaluation</head><p>We found the use of human-generated reference answers to be crucial to ensure a consisted annotation of each model. A comparison between annotation with and without reference answer is provided in table 8. When using a reference, we found annotators to be dramatically more consistent, achieving a Fleiss' Kappa <ref type="bibr" target="#b20">(Fleiss, 1971</ref>) of 0.5017; when providing no reference answer, the inter-annotation agreement dropped to 0.1387. This trend is reflected in the number of questions with strong (4+ annotators agree) and perfect agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limits</head><p>Our system requires translating the questions. We also use the standard BM25 approach. Even though it was shown to be more robust compared to dense retriever <ref type="bibr" target="#b57">(Thakur et al., 2021;</ref><ref type="bibr">Rosa et al., 2022)</ref>, using a cross-lingual retriever <ref type="bibr" target="#b34">(Li et al., 2021)</ref> could improve performance and save the cost of translating the question. This has been explored by <ref type="bibr" target="#b4">Asai et al. (2021c)</ref> but their retriever mainly retrieves passages in English and the question language which may lead to English-centric answers. Another limit is the fact that our system is not designed to handle questions that are not answerable.</p><p>In the future, we may want to integrate a no-answer setting to avoid unwanted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We study retrieval-based Question Answering systems using answer generation in a multilingual context. We proposed (i) GEN-TYDIQA, a new multilingual QA dataset that includes natural and complete answers for Arabic, Bengali, English, Japanese, and Russian; based on this dataset (ii) the first multilingual and cross-lingual GENQA retrieval-based systems. The latter can accurately answer questions in one language using information from multiple languages, outperforming answer sentence selection baseline for all languages and monolingual pipeline for Arabic, Russian, and Japanese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion</head><p>A.1 Machine Translation of the Questions and BM25 Retriever Engines</p><p>Our work introduces CROSS-LINGUAL GENQA, a system that can answer questions -with complete sentence answers -in multiple languages using candidates in multiple languages, possibly distinct from the question. They were many possible design choices to achieve such a goal. We chose to rely on automatically translating the questions before retrieving relevant documents in several languages using multiple (monolingual) BM25 retrievers. We could have chosen to use the recently released multilingual Dense passage Retrieval (mDPR) <ref type="bibr" target="#b3">(Asai et al., 2021b)</ref>. We decided not to for the two following reasons. First, as shown by <ref type="bibr" target="#b57">Thakur et al. (2021)</ref>, BM25 is a very reasonable design choice for a retriever engine, that outperforms other approaches in many settings (including dense retrievers). Second, as seen in <ref type="bibr" target="#b3">(Asai et al., 2021b)</ref>, multilingual dense retrievers usually retrieve passages in the same language as the question or English. This means that mDPR is highly biased toward the English language. In our work, by combining translation and monolingual retrievers, we can control the language set that we use for answer generation. We leave for future work the refinement of mDPR to enable for more diversity in the retrieved passage languages and to integrate it in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Machine Translation Errors</head><p>At test time, our system applies Machine Translation to the question to formulate queries in different languages and retrieve candidates for these languages using the BM25 retrieval engine.  <ref type="bibr" target="#b23">(Goyal et al., 2022)</ref>) on the FLORES devtest dataset <ref type="bibr" target="#b23">(Goyal et al., 2022)</ref>. Cell(i,j) reports the score of AWS/M2M from language i to language j. AWS translate outperforms the M2M model for all language pairs. tems are very powerful tools, trained on millions of data points and, thanks to Transformer model, they take the entire question context into account (other cross-query formulations can be applied but they will be probably less accurate and multilingual DPR is an excellent research line but not as much assessed as BM25 as effective and general approach). Clearly MT errors can impact the quality of our candidates. However, if a question is badly translated the retrieved content will be inconsistent with the candidates retrieved for the question in the original language (and also inconsistent with candidates retrieved using questions translated in other languages). Our joint modeling through large generation-based Transformers can recover from these random errors. For example, for 3 languages out of 5, we show that the Cross-GenQA pipelines that use MT for the question outperform monolingual pipelines <ref type="bibr">(MONOGENQA and MULTIGENQA)</ref>. This shows that translation errors are recovered by our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 AWS-Translation for Machine Translation</head><p>For translating the questions automatically, we use AWS Translate. AWS Translate is a machine translation API that competes and outperforms in some cases other available translation APIs<ref type="foot" target="#foot_6">8</ref> . We compare the performance of a strong baseline on the FLORES dataset in table <ref type="table">9</ref>. We find that AWS translate outperforms the baseline for all the language pairs we work with. We leave for future work the study of the impact of different machine translation systems on our CROSS-LINGUAL GENQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ethics Statement</head><p>B.1 Potential Harms of GENQA All our GENQA are fine-tuned from a large pretrained language model, MT5 <ref type="bibr">(Xue et al., 2021)</ref>. In general, large language models have been shown to have a potential to amplify societal biases <ref type="bibr" target="#b7">(Bender et al., 2021)</ref>, and might leak information about the datasets they were trained on <ref type="bibr">(Carlini et al., 2021)</ref>.</p><p>In particular, the Colossal Cleaned Crawled Corpus (C4) and its multilingual counterpart (MC4) that were used to train MT5 have been shown to disproportionately under-represent content about minority individuals <ref type="bibr" target="#b18">(Dodge et al., 2021)</ref>.</p><p>In its use as a retrieval-based question answering system, GENQA also can also cause harm due to (i) the use of candidate sentences that are extracted from web documents, and (ii) model hallucinations that are produced during decoding. In this work, (i) is mitigated by only relying on content from Wikipedia, which, while not immune to vandalism <ref type="bibr" target="#b0">(Alkharashi and Jose, 2018)</ref>, is of much higher quality of unvetted web data. Regarding the risk of model hallucinations, this work does not attempt to directly mitigate any potential issue through modeling; rather, we always show annotators reference answer so that hallucination that result in factually incorrect answers can be properly caught during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 GEN-TYDIQA Copyright</head><p>Our GEN-TYDIQA dataset is based on the Ty-DiQA dataset questions <ref type="bibr" target="#b13">(Clark et al., 2020)</ref>. Ty-DiQA is released under the Apache 2.0 License which allows modification and redistribution of the derived dataset. Upon acceptance of this paper, we will release GEN-TYDIQA and honor the terms of this license.</p><p>GEN-TYDIQA answers were collected using Amazon Mechanical Turk. No geolocation filters or any personal information were used to hire turkers. Additionally, GEN-TYDIQA questions treat scientific or cultural topics that can be answered objectively using Wikipedia. For these reasons, the collected answers cannot be used to identify their authors. Finally, to ensure the complete anonymity of the turkers, we will not release the turkers id along with the collected answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Energy Consumption of Training</head><p>All our experiments are based on the MT5 base model. We run all our fine-tuning and evaluation runs using 8 Tesla P100 GPUs<ref type="foot" target="#foot_7">9</ref> , which have a peak energy consumption of 300W each. Finetuning our CROSS-LINGUAL GENQA models on MS-MARCO <ref type="bibr" target="#b41">(Nguyen et al., 2016)</ref> takes about 24 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Reproducibility C.1 Mechanical-Turk Settings</head><p>In this paper, we rely on Amazon Mechanical Turk for two distinct uses.</p><p>On the one hand, we use it to build the GEN-TYDIQA dataset. For data collection, we request 1 turker per question to generate an answer. For the GEN-TYDIQA data validation, we request 5 turkers to select only answers that are correct, aligned with the provided passage, self-contained and complete.</p><p>On the other hand, we use Amazon Mechanical Turk to estimate the answer accuracy of our models. To do so, for each question, we provide the GEN-TYDIQA reference and ask 3 turkers to vote on whether the generated answer is correct or not.</p><p>For those two uses, we use the following Amazon Mechanical Turk filters to hire turkers.</p><p>• We hire turkers that received at least a 95%</p><p>HIT<ref type="foot" target="#foot_8">10</ref> approval rate.</p><p>• We request turkers that have performed at least 500 approved HITs.</p><p>• When possible, we use the "master turker" filter<ref type="foot" target="#foot_9">11</ref> provided by Amazon Mechanical Turk. We find that this filter can only be used for English. For other languages, this filter leads to a too-small turker pool making it unusable in practice.</p><p>On Mechanical turk, the reward unit for workers is the HIT. In our case, a HIT is the annotation/validation of a single question. We make sure that each turker is paid at least an average of 15 USD/hour. To estimate the fair HIT reward, we first run each step with 100 samples ourselves in order to estimate the average time required per task. For data collection, we set the HIT reward to 0.50 USD based on an estimation of 0.5 HIT/min. For data validation, we set it to 0.15 USD based on an estimation of 1.6 HIT/min. For model evaluation,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our proposed Cross-Lingual, Retrieval-based GENQA pipeline.</figDesc><graphic url="image-1.png" coords="2,70.87,70.87,456.99,93.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>QA Researchers have introduced several datasets for QA in multiple languages. Unlike our GEN-TYDIQA, to the best of our knowledge, they are designed exclusively for extractive QA. Artetxe et al. (2019) extended the English machine reading SQuAD dataset (Rajpurkar et al., 2016) by translating the test set to 11 languages. Similarly, Lewis et al. (2020a) collected new question and answer pairs for 7 languages following the SQuAD format. Recently, Longpre et al. (2020) released MKQA, which includes question and answer pairs (predominantly Yes/No answers and entities) for 26 languages. Clark et al. (2020) released TyDiQA, a dataset for extractive QA in 11 typologically diverse languages. Riabi et al. (2020) and Shakeri et al. (2021) have explored the use of techniques to synthetically generate data for extractive question answering using cross-lingual transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>BN-EN-JA-RU TOP 10 56.9 CROSSGENQA AR-BN-EN-JA-RU TOP 2 / LANG 63.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Question:When was Justin Drew Bieber born? AS2 Prediction: Matthew Lawrence Hayden, AM (born October 29, 1971) is a former Australian cricketer born in Kingroy, Queensland.MULTIGENQA Prediction:Justin Drew Bieber was born on March 1, 1994.CROSSGENQA PredictionJustin Drew Bieber was born on March 1, 1994. Question: トゥールのグレゴリウスはいつ生まれた？ When was Gregory of Tours born? AS2 Prediction: グ レ ゴ リ ウ ス14世 (Gregorius XIV,1535年2月11日 -1591年10月16日)はローマ教皇 (在位：1590年 -1591年)。 Pope Gregory XIV (February 11, 1535 -October 16, 1591) is the Pope of Rome (reigned: 1590 -1591). MULTIGENQA Prediction:ト ゥ ー ル の グ レ ゴ リ ウ ス は、1535年2月11日に生まれた。 Gregory of Tours was born on February 11, 1535. CROSSGENQA Predictionト ゥ ー ル の グ レ ゴ リ ウ ス は538年頃11月30日に生まれた。 Gregory of Tours was born on November 30, 538.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics on GEN-TYDIQA Answers</figDesc><table><row><cell>Lang. (iso)</cell><cell cols="3">#Answers Avg. Length (utf-8) %TyDiQA</cell></row><row><cell>Arabic (AR)</cell><cell>859</cell><cell>152.5</cell><cell>75.7</cell></row><row><cell>Bengali (BN)</cell><cell>89</cell><cell>177.2</cell><cell>63.6</cell></row><row><cell>English (EN)</cell><cell>593</cell><cell>64.0</cell><cell>79.5</cell></row><row><cell>Japanese (JA)</cell><cell>550</cell><cell>112.0</cell><cell>62.1</cell></row><row><cell>Russian (RU)</cell><cell>595</cell><cell>277.9</cell><cell>52.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Impact of the candidate language set on CROSS-LINGUAL GENQA in English on MS-MARCO. The language set is controlled with machine translation.</figDesc><table><row><cell>Model</cell><cell cols="2">CANDIDATES Accuracy</cell></row><row><cell cols="2">MONOGENQA EN</cell><cell>77.9</cell></row><row><cell cols="2">CROSSGENQA DE</cell><cell>70.5</cell></row><row><cell cols="2">CROSSGENQA DE ES FR IT</cell><cell>68.8</cell></row><row><cell cols="2">CROSSGENQA AR JA KO</cell><cell>31.4</cell></row><row><cell>Clozed-Book</cell><cell>NONE</cell><cell>21.0</cell></row><row><cell cols="3">the performance of this MULTIGENQA model in</cell></row><row><cell>table 5.</cell><cell></cell><cell></cell></row><row><cell cols="3">For this set of experiments, we show that a single</cell></row><row><cell cols="3">multilingual GENQA model can compete with a</cell></row><row><cell cols="2">collection of monolingual models.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CROSSGENQA TOP. 2 / LANG. 73.2 18.5 29.3 71.6 74.7</figDesc><table><row><cell>Model</cell><cell>AR</cell><cell>BN</cell><cell>EN</cell><cell>JA</cell><cell>RU</cell></row><row><cell>RETRIEVER (Hit@100 doc.)</cell><cell cols="5">70.7 66.3 66.9 57.0 67.8</cell></row><row><cell>AS2</cell><cell cols="5">68.0 58.0 39.0 70.4 60.8</cell></row><row><cell>MONOGENQA</cell><cell cols="5">68.4 67.4 43.6 64.3 61.3</cell></row><row><cell>MULTIGENQA</cell><cell cols="5">72.7 76.5 37.4 65.5 66.7</cell></row><row><cell>CROSSGENQA TOP 10</cell><cell cols="5">72.0 25.3 31.0 70.3 74.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Example of predicted answers to questions in Bengali and Japanese. Blue indicates correct predictions while Red incorrect ones. Translations are intended for the reader and are not part of the predictions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison between providing a reference answer and not for evaluating MONOGENQA predictions(EN). Providing a reference increases agreement.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We make GEN-TYDIQA available at the following URL: s3://alexa-wqa-public/datasets/ cross-genqa/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The TyDiQA test set is not publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://requester.mturk.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We used Amazon's AWS Translate service, https:// aws.amazon.com/translate/service. We validate the quality of AWS Translate on the languages we study in the Appendix section A.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Using the train split of the NLGEN(v2.1) version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">We describe in C.1 how we choose and reward turkers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">cf. https://aws.amazon.com/blogs/machinelearning/amazon-translate-ranked-as-1-machine-translationprovider-by-intento/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://www.nvidia.com/en-us/ data-center/tesla-p100/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">A HIT, as defined in Amazon Mechanical Turk, is a Human Intelligent Task. In our case, a HIT consists in generating, validating, or accepting an answer to a single question.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9">  11  As stated on the Amazon Mechanical Turk website, "Amazon Mechanical Turk has built technology which analyzes Worker performance, identifies high performing Workers, and monitors their performance over time. Workers who have demonstrated excellence across a wide range of tasks are awarded the Masters Qualification. Masters must continue to pass our statistical monitoring to retain the Amazon Mechanical Turk Masters Qualification."</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>we set the HIT reward to 0.10 USD based on an estimation of 2.5 HIT/min.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Model Optimization</head><p>All the GENQA experiments we present in this paper are based on fine-tuning MT5 base <ref type="bibr">(Xue et al., 2021)</ref>. Models are implemented in PyTorch <ref type="bibr" target="#b44">(Paszke et al., 2019)</ref>, and leverage transformers <ref type="bibr" target="#b65">(Wolf et al., 2020)</ref> and pytorch-lightning <ref type="bibr" target="#b19">(Falcon and Cho, 2020)</ref>.</p><p>For fine-tuning, we concatenate the question and the candidate sentences, input it to the model and train it to generate the answer. Across all our runs, we use the hyperparameters reported in table <ref type="table">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis D.1 Gold vs. Retrieved Candidates</head><p>We report in table 4 the performance of the MONO-GENQA and MULTIGENQA models when we feed them gold passages (using TyDiQA passage) and compare them with the performance of the same models fed with the retrieved candidates. We discuss those results in section 5.4. BLEU <ref type="bibr" target="#b46">(Post, 2018)</ref>) and the F-score of the ROUGE-L metric <ref type="bibr" target="#b35">(Lin, 2004)</ref> along with the human evaluation accuracy in table 14.</p><p>As seen in previous work discussing the automatic evaluation of QA systems by <ref type="bibr" target="#b10">Chaganty et al. (2018)</ref> and <ref type="bibr" target="#b11">Chen et al. (2019)</ref>, we observe that for many cases, BLEU and ROUGE-L do not correlate with human evaluation. In table 12, we take the predictions of our MULTIGENQA model across all the languages and compute the Spearman rank correlation at the sentence level of the human estimated accuracy with BLEU and ROUGE-L. We find that this correlation is at most 25%. This suggests that those two metrics are not able to discriminate between correct predictions and incorrect ones.</p><p>Additionally, we report the Spearman rank correlation between the Accuracy and BLEU or ROUGE across all our 5 models in table <ref type="table">13</ref>. We find that neither BLEU nor ROUGE-L correlates strongly with human accuracy across all the languages. This means that those metrics are not able to rank the quality of a model in agreement with human judgment. Those results lead us to focus our analysis and to take our conclusions only on human evaluated accuracy. We leave for future work the development of an automatic evaluation method for multilingual GENQA. Table <ref type="table">14</ref>: Performance of GENQA models on GEN-TYDIQA based on retrieved and reranked candidates. QUES-TION indicates the language of the question and the answer while CANDIDATES indicates the language set of the retrieved candidate sentences.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vandalism on collaborative web communities: An exploration of editorial behaviour in wikipedia</title>
		<author>
			<persName><forename type="first">Abdulwhab</forename><surname>Alkharashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Spanish Conference on Information Retrieval</title>
				<meeting>the 5th Spanish Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno>CoRR, abs/1910.11856</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">XOR QA: Cross-lingual open-retrieval question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.46</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="547" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One question answering model for many languages with cross-lingual dense passage retrieval</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7547" to="7560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One question answering model for many languages with cross-lingual dense passage retrieval</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021c</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7547" to="7560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<title level="m">Wikiextractor</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fluent response generation for conversational question answering</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
				<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A compare-aggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3133089</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17, page 1987-1990</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17, page 1987-1990<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
				<imprint>
			<publisher>USENIX Association</publisher>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The price of debiasing automatic metrics in natural language evalaution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating question answering evaluation</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5817</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
				<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decontextualization: Making sentences stand-alone</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00317</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-language question re-ranking</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barroón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Maàrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080743</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint learning of answer selection and answer summary generation in community question answering</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7651" to="7658" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Documenting large webtext corpora: A case study on the colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1286" to="1305" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A framework for contrastive self-supervised learning and designing a new approach</title>
		<author>
			<persName><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00104</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection</title>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7780" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Savery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.289</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3215" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The flores-101 evaluation benchmark for low-resource and multilingual machine translation</title>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjan</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="522" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>ArXiv, abs/2002.08909</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Answer generation for retrievalbased question answering systems</title>
		<author>
			<persName><surname>Chao-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.374</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4276" to="4282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting background knowledge in compact answer generation for why-questions</title>
		<author>
			<persName><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryo</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.3301142</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for opendomain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">75 languages, 1 model: Parsing Universal Dependencies universally</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2779" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="7315" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledgeintensive NLP tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<title level="m">Tim Rocktäschel, et al. 2020c. Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning cross-lingual ir from an english retriever</title>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Arafat Sultan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavani</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.08185</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multilingual denoising pretraining for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mkqa: A linguistically diverse benchmark for multilingual open domain question answering</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Declarative experimentation ininformation retrieval using pyterrier</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICTIR 2020</title>
				<meeting>ICTIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reranking for efficient transformerbased answer selection</title>
		<author>
			<persName><forename type="first">Yoshitomo</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401266</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1577" to="1580" />
		</imprint>
	</monogr>
	<note>and Alessandro Moschitti</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pivot through english: Reliably answering multilingual questions without document retrieval</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<idno>CoRR, abs/2012.14094</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<title level="s">CEUR Workshop Proceedings. CEUR-WS.org</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09">2016. December 9, 2016</date>
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Recent advances in deep learning based dialogue systems: A systematic survey</title>
		<author>
			<persName><forename type="first">Jinjie</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Pandelea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fuzhao Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Ananth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><surname>Cambria</surname></persName>
		</author>
		<idno>ArXiv, abs/2105.04387</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthetic data augmentation for zero-shot crosslingual question answering</title>
		<author>
			<persName><forename type="first">Arij</forename><surname>Riabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<idno>CoRR, abs/2010.12643</idno>
	</analytic>
	<monogr>
		<title level="m">Rachel Keraron, Benoît Sagot, Djamé Seddah, and Jacopo Staiano</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nist Special Publication Sp</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Roberto de Alencar Lotufo, and Rodrigo Nogueira. 2022. No parameter left behind: How distillation and model size affect zero-shot retrieval</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Bonifacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitor</forename><surname>Jeronymo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Queiroz Abonizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767738</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards zero-shot multilingual synthetic question and answer generation for crosslingual reading comprehension</title>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Natural Language Generation</title>
				<meeting>the 14th International Conference on Natural Language Generation<address><addrLine>Aberdeen, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>UK. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-cast attention networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;18</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Integrating question classification and deep learning for improved answer selection</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harish Tayyar Madabushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Barnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3283" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Abhishek Srivastava, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The context-dependent additive recurrent neural net</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><surname>Bui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tracking knowledge propagation across wikipedia languages</title>
		<author>
			<persName><forename type="first">Rodolfo</forename><surname>Vieira Valentim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Comarela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souneil</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Sáez-Trumper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
				<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1046" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The TREC question answering track</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324901002789</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="378" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Multilingual answer sentence reranking via automatically translated data</title>
		<author>
			<persName><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? a quasisynchronous grammar for QA</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A compareaggregate model with latent clustering for answer selection</title>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyomin</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><surname>Jung</surname></persName>
		</author>
		<idno>CoRR, abs/1905.12897</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
