<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep neural network concepts for background subtraction: A systematic review and comparative evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thierry</forename><surname>Bouwmans</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sajid</forename><surname>Javed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maryam</forename><surname>Sultana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soon</forename><forename type="middle">Ki</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neural</forename><surname>Networks</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Thierry Bouwmans Lab. MIA</orgName>
								<orgName type="institution">Univ. La Rochelle</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sajid Javed Dept. of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Maryam Sultana Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Soon Ki Jung Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep neural network concepts for background subtraction: A systematic review and comparative evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">38F1B4305861309D1F5D271A8B777ACA</idno>
					<idno type="DOI">10.1016/j.neunet.2019.04.024</idno>
					<note type="submission">Received date : 12 November 2018 Revised date : 27 February 2019 Accepted date : 30 April 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Background Subtraction</term>
					<term>Restricted Boltzmann Machines</term>
					<term>Auto-encoders Networks</term>
					<term>Convolutional Neural Networks</term>
					<term>Generative Adversarial Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional neural networks have been demonstrated to be a powerful framework for background subtraction in video acquired by static cameras. Indeed, the well-known Self-Organizing Background Subtraction (SOBS) method and its variants based on neural networks have long been the leading methods on the large-scale CDnet 2012 dataset during a long time. Convolutional neural networks, which are used in deep learning, have been recently and excessively employed for background initialization, foreground detection, and deep learned features. The top background subtraction methods currently used in CDnet 2014 are based on deep neural networks, and have demonstrated a large performance improvement in comparison to conventional unsupervised approaches based on multi-feature or multi-cue strategies. Furthermore, since the seminal work of Braham and Van Droogenbroeck in 2016, a large number of studies on convolutional neural networks applied to background subtraction have been published, and a continual gain of performance has been achieved. In this context, we provide the first review of deep neural network concepts in background subtraction for novices and experts in order to analyze this success and to provide further directions. To do so, we first surveyed the background initialization and background subtraction methods based on deep neural networks concepts, and also deep learned features. We then discuss the adequacy of deep neural networks for the task of background subtraction. Finally, experimental results are presented for the CDnet 2014 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last two decades, background subtraction for video taken by static cameras has been one of the most active research topics in computer vision owing to a large number of applications including intelligent surveillance of human activities in public spaces, traffic monitoring, and industrial machine vision <ref type="bibr" target="#b42">[1,</ref><ref type="bibr" target="#b43">2]</ref>. This low-level operation consists of separating the moving objects called "foreground" from the static information called "background" <ref type="bibr" target="#b44">[3,</ref><ref type="bibr" target="#b45">4,</ref><ref type="bibr" target="#b46">5,</ref><ref type="bibr" target="#b47">6,</ref><ref type="bibr" target="#b48">7]</ref>. For example, Figure <ref type="figure">1</ref> shows original frames of a sequence from the BMC 2012 dataset, the extracted background images and the foreground mask obtained by a well-know method. A big variety of models coming from mathematical theories, machine learning and signal processing have been used for background subtraction, including crisp models <ref type="bibr" target="#b49">[8,</ref><ref type="bibr" target="#b50">9,</ref><ref type="bibr" target="#b51">10]</ref>, statistical models <ref type="bibr" target="#b52">[11,</ref><ref type="bibr" target="#b53">12,</ref><ref type="bibr" target="#b54">13,</ref><ref type="bibr" target="#b55">14]</ref>, fuzzy models <ref type="bibr" target="#b56">[15,</ref><ref type="bibr" target="#b57">16,</ref><ref type="bibr" target="#b58">17]</ref>, subspace learning models <ref type="bibr" target="#b59">[18,</ref><ref type="bibr" target="#b60">19,</ref><ref type="bibr" target="#b61">20]</ref>, robust PCA models <ref type="bibr" target="#b62">[21,</ref><ref type="bibr" target="#b63">22,</ref><ref type="bibr" target="#b64">23,</ref><ref type="bibr" target="#b65">24,</ref><ref type="bibr" target="#b48">7]</ref>, neural networks models <ref type="bibr" target="#b66">[25,</ref><ref type="bibr" target="#b67">26,</ref><ref type="bibr" target="#b68">27]</ref> and filter based models <ref type="bibr" target="#b69">[28,</ref><ref type="bibr" target="#b70">29,</ref><ref type="bibr" target="#b71">30,</ref><ref type="bibr" target="#b72">31]</ref>. Similar to PCA models, which have generated renewed interest in this area based on the theoretical advances of robust PCA, created in 2009 by Cand√®s et al. <ref type="bibr" target="#b73">[32]</ref>, after an empty period of development, neural networks have received progressively renewed interest in this field since 2014 <ref type="bibr" target="#b74">[33]</ref> owing to the practical advances in deep 1 neural networks, which are now usable owing to the availability of large-scale datasets <ref type="bibr" target="#b75">[34,</ref><ref type="bibr" target="#b76">35]</ref> for the training, and the progress in computational hardware ability <ref type="foot" target="#foot_0">1</ref> .</p><p>Figure <ref type="figure">1</ref>. Background Subtraction. From top to bottom: Original image <ref type="bibr" target="#b350">(309)</ref>, Background extracted, Foreground mask (Sequences from BMC 2012 dataset <ref type="bibr" target="#b77">[36]</ref>).</p><p>Based from mathematical theories, the simplest way to model a background is to compute the temporal average <ref type="bibr" target="#b49">[8]</ref>, the temporal median <ref type="bibr" target="#b50">[9]</ref> or the histogram over time <ref type="bibr" target="#b51">[10]</ref>. These methods were widely used in traffic surveillance in 1990s owing to their simplicity but are not robust to the challenges faced in video surveillance such as camera jitter, changes in illumination, and dynamic backgrounds. To consider the imprecision, uncertainty and incompleteness in the observed data (i.e video), statistical models began being introduced in 1999 such as single Gaussian <ref type="bibr" target="#b78">[37]</ref>, Mixture of Gaussians (MOG) <ref type="bibr" target="#b53">[12,</ref><ref type="bibr" target="#b54">13]</ref> and Kernel Density Estimation <ref type="bibr" target="#b52">[11,</ref><ref type="bibr" target="#b79">38]</ref>. These methods based on a Gaussian distribution model proved to be more robust to dynamic backgrounds. More sophisticated statistical models were after developed in literature and can be classified into those based on another distribution that alleviate the strict Gaussian constraint (i.e. general Gaussian distribution <ref type="bibr" target="#b80">[39]</ref>, Student's t-distribution <ref type="bibr" target="#b81">[40,</ref><ref type="bibr" target="#b82">41]</ref>, Dirichlet distribution <ref type="bibr" target="#b83">[42,</ref><ref type="bibr" target="#b84">43]</ref>, Poisson distribution <ref type="bibr" target="#b85">[44,</ref><ref type="bibr" target="#b86">45]</ref>), those based on co-occurrence <ref type="bibr" target="#b87">[46,</ref><ref type="bibr" target="#b88">47,</ref><ref type="bibr" target="#b89">48]</ref> and confidence <ref type="bibr" target="#b90">[49,</ref><ref type="bibr" target="#b91">50]</ref>, freedistribution models <ref type="bibr" target="#b92">[51,</ref><ref type="bibr" target="#b93">52,</ref><ref type="bibr" target="#b94">53]</ref>, and regression models <ref type="bibr" target="#b95">[54,</ref><ref type="bibr" target="#b96">55]</ref>. These approaches have improved the robustness to various challenges over time. The most accomplished methods in this statistical category are ViBe <ref type="bibr" target="#b92">[51]</ref>, PAWCS <ref type="bibr" target="#b94">[53]</ref> and SubSENSE <ref type="bibr" target="#b93">[52]</ref>. Another theory that allows the handling of imprecision, uncertainty, and incompleteness is based on the fuzzy concept. In 2006-2008, several authors employed concepts like Type-2 fuzzy sets <ref type="bibr" target="#b57">[16,</ref><ref type="bibr" target="#b97">56,</ref><ref type="bibr" target="#b98">57]</ref>, Sugeno integral <ref type="bibr" target="#b99">[58,</ref><ref type="bibr" target="#b100">59]</ref> and Choquet integral <ref type="bibr" target="#b101">[60,</ref><ref type="bibr" target="#b56">15,</ref><ref type="bibr" target="#b102">61]</ref>. These fuzzy models show robustness in the presence of dynamic backgrounds <ref type="bibr" target="#b58">[17]</ref>. Dempster-Schafer concepts were also be employed in foreground detection <ref type="bibr" target="#b103">[62]</ref>. Based on machine learning, background modeling has been investigated by representation learning (also called subspace learning), support vector machines and neural networks modeling (conventional and deep neural networks). In 1999, reconstructive subspace learning models like Principal Component Analysis (PCA) <ref type="bibr" target="#b61">[20]</ref> were introduced to learn the background in an unsupervised manner. Subspace learning models handle illumination changes more robustly than statistical models <ref type="bibr" target="#b59">[18]</ref>. In further approaches, discriminative <ref type="bibr" target="#b104">[63,</ref><ref type="bibr" target="#b105">64,</ref><ref type="bibr" target="#b106">65]</ref> and mixed <ref type="bibr" target="#b60">[19]</ref> subspace learning models have been used to increase the performance for foreground detection. However, each of these regular subspace methods presents a high sensitivity to noise, outliers, and missing data. To address these limitations, since 2009, a robust PCA through decomposition into low-rank plus sparse matrices <ref type="bibr" target="#b73">[32,</ref><ref type="bibr" target="#b107">66,</ref><ref type="bibr" target="#b108">67,</ref><ref type="bibr" target="#b109">68,</ref><ref type="bibr" target="#b110">69]</ref> has been widely used in the field. These methods are not only robust to changes in illumination but also to dynamic backgrounds <ref type="bibr" target="#b111">[70,</ref><ref type="bibr" target="#b112">71,</ref><ref type="bibr" target="#b113">72,</ref><ref type="bibr" target="#b114">73,</ref><ref type="bibr" target="#b115">74,</ref><ref type="bibr" target="#b116">75]</ref>. However, they require batch algorithms, making them impractical for real-time applications. To address this limitation, dynamic robust PCA as well as robust subspace tracking <ref type="bibr" target="#b117">[76,</ref><ref type="bibr" target="#b118">77]</ref> have been designed to achieve a realtime performance of RPCA-based methods. The most accomplished methods in this subspace learning category are GRASTA <ref type="bibr" target="#b119">[78]</ref>, incPCP <ref type="bibr" target="#b120">[79]</ref>, ReProCS <ref type="bibr" target="#b121">[80]</ref> and MEROP <ref type="bibr" target="#b122">[81]</ref>. However, tensor RPCA based methods <ref type="bibr" target="#b123">[82,</ref><ref type="bibr" target="#b124">83,</ref><ref type="bibr" target="#b125">84,</ref><ref type="bibr" target="#b126">85]</ref> allow to take into account spatial and temporal constraints making them more robust against noise. In 2006, support vector models <ref type="bibr" target="#b127">[86,</ref><ref type="bibr" target="#b128">87,</ref><ref type="bibr" target="#b129">88,</ref><ref type="bibr" target="#b130">89,</ref><ref type="bibr" target="#b131">90,</ref><ref type="bibr" target="#b132">91]</ref> have been introduced for background modeling in order to be more robust to dynamic backgrounds but their main drawback is their sensitivity to the training data. For all these models, the reader can refer to well-known exhaustive and detailed surveys <ref type="bibr" target="#b44">[3,</ref><ref type="bibr" target="#b45">4,</ref><ref type="bibr" target="#b46">5,</ref><ref type="bibr" target="#b47">6,</ref><ref type="bibr" target="#b48">7]</ref>. Below we focus on neural networks models applied to background subtraction.</p><p>Schofield et al. <ref type="bibr" target="#b68">[27]</ref> were the first to use neural networks for background modeling and foreground detection through the application of a Random Access Memory (RAM) neural network. However, a RAM-NN requires the images to represent the background of the scene correctly, and there is no background maintenance stage because once a RAM-NN is trained with a single pass of background images, it is impossible to modify this information. In a further study, Jimenez et al. <ref type="bibr" target="#b133">[92]</ref> classified each zone of a video frame into three classes of background: static, noisy, and impulsive. The classification is conducted using a multilayer perceptron neural network, which requires a training set from specific zones of each training frame. In another study, Tavakkoli <ref type="bibr" target="#b134">[93]</ref> proposed a neural network approach under the concept of novelty detector. During the training step, the background is divided in blocks. Each block is associated to a Radial Basis Function Neural Network (RBF-NN). Thus, each RBF-NN is trained with samples of the background corresponding to its associated block. The decision of using RBF-NN is because it works like a detector and not a discriminant, generating a close boundary for the known class. RBF-NN methods is able to address dynamic object detection as a single class problem, and to learn the dynamic background. However, it requires a huge amount of samples to represent general background scenarios. In Wang et al. <ref type="bibr" target="#b135">[94]</ref>, a hybrid probabilistic and "Winner Take All" (WTA) neural architectures were combined into a single NN model. The algorithm is named Adaptive Background Probabilistic Neural Network (ABPNN) and it is composed of four layers. In the ABPNN model, each pixel is classified as foreground or background according to a conditional probability of being background. This probability is estimated by a Parzen estimation. The foreground regions are further analyzed in order to classify them as a motion or a shadow region. But, ABPNN needs to define specific initial parameter values (specific thresholds values) for each of the analyzed video. In Culibrk et al. <ref type="bibr" target="#b136">[95]</ref>, a feed-forward neural network is used for background modeling based on an adaptive Bayesian model called Background Neural Network (BNN). The architecture corresponds to a General Regression Neural Network (GRNN), that works like a Bayesian classifier. Although the architecture is proposed as supervised, it can be extended as an unsupervised architecture in the background model domain. The network is composed of three sub-networks: classification, activation, and replacement. The classifier sub-network maps the features background/foreground of a pixel to a probabilistic density function using the Parzen estimator. The network has two neurons, one of them estimates the probability of being background, and the other neuron computes the probability of being foreground. But, the main disadvantages are that the model is very complex and that it requires of three networks to define if a pixel belongs to the background. In a remarkable work, Maddalena and Petrosino <ref type="bibr" target="#b137">[96,</ref><ref type="bibr" target="#b138">97,</ref><ref type="bibr" target="#b139">98,</ref><ref type="bibr" target="#b140">99]</ref> proposed a method called Self Organizing Background Subtraction (SOBS) based on a 2D selforganizing neural network architecture preserving pixel spatial relations. The method is considered as nonparametric, multi-modal, recursive and pixel-based. The background is automatically modeled through the neurons weights of the network. Each pixel is represented by a neural map with n √ó n weight vectors. The weights vectors of the neurons are initialized with the corresponding color pixel values using the HSV color space. Once the model is initialized, each new pixel information from a new video frame is compared to its current model to determine if the pixel corresponds to the background or to the foreground. In further works, SOBS was improved in several variants such as Multivalued SOBS <ref type="bibr" target="#b141">[100]</ref>, SOBS-CF <ref type="bibr" target="#b142">[101]</ref>, SC-SOBS <ref type="bibr" target="#b143">[102]</ref>, 3dSOBS+ <ref type="bibr" target="#b144">[103]</ref>, Simplified SOM <ref type="bibr" target="#b145">[104]</ref>, Neural-Fuzzy SOM <ref type="bibr" target="#b146">[105]</ref> and MILSOBS <ref type="bibr" target="#b147">[106]</ref>) which allow this method to be in the leading methods on the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref> during a long time. SOBS shows also interesting performance for stopped object detection <ref type="bibr" target="#b148">[107,</ref><ref type="bibr" target="#b149">108,</ref><ref type="bibr" target="#b150">109]</ref>. But, one of the main disadvantages of SOBS based methods is the need to manual adjust at least four parameters.</p><p>Deep learning methods based on deep neural networks (DNNs) with convolutional neural networks (CNNs), also called ConvNets, have alleviated the disadvantages of the previous approaches based on conventional neural networks <ref type="bibr" target="#b151">[110,</ref><ref type="bibr" target="#b152">111,</ref><ref type="bibr" target="#b153">112]</ref>. Although CNNs have existed for a long time, their success and use in computer vision have long been limited during a long period owing to the size of the available training sets, the size of the considered networks, and the computational capacity. In the area of computer vision the breakthrough was made in the field of image classification in 2012 by Krizhevsky et al. <ref type="bibr" target="#b154">[113]</ref> who first used a supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset <ref type="bibr" target="#b155">[114]</ref> with 1 million training images. With the progress made in storage for Big Data and the GPUs used for deep learning, even larger and deeper networks can be trained, and DNNs are now usable and have been widely applied in several computer vision tasks such object detection <ref type="bibr" target="#b156">[115,</ref><ref type="bibr" target="#b157">116,</ref><ref type="bibr" target="#b158">117,</ref><ref type="bibr" target="#b159">118,</ref><ref type="bibr" target="#b160">119,</ref><ref type="bibr" target="#b161">120]</ref>, semantic segmentation <ref type="bibr" target="#b162">[121,</ref><ref type="bibr" target="#b163">122,</ref><ref type="bibr" target="#b164">123]</ref>, video object segmentation <ref type="bibr" target="#b165">[124,</ref><ref type="bibr" target="#b166">125,</ref><ref type="bibr" target="#b167">126,</ref><ref type="bibr" target="#b168">127,</ref><ref type="bibr" target="#b169">128,</ref><ref type="bibr" target="#b170">129,</ref><ref type="bibr" target="#b171">130,</ref><ref type="bibr" target="#b172">131]</ref>, video anomaly detection <ref type="bibr" target="#b173">[132]</ref>, person detection and tracking <ref type="bibr" target="#b174">[133]</ref>, person re-identification <ref type="bibr" target="#b175">[134,</ref><ref type="bibr" target="#b176">135,</ref><ref type="bibr" target="#b177">136]</ref>, dim small target detection <ref type="bibr" target="#b178">[137]</ref>, face alignment <ref type="bibr" target="#b179">[138,</ref><ref type="bibr" target="#b180">139]</ref>, face recognition <ref type="bibr" target="#b181">[140]</ref>, action recognition <ref type="bibr" target="#b182">[141]</ref>, intelligent transportation system <ref type="bibr" target="#b183">[142,</ref><ref type="bibr" target="#b184">143,</ref><ref type="bibr" target="#b185">144]</ref>, remote sensing <ref type="bibr" target="#b186">[145,</ref><ref type="bibr" target="#b187">146]</ref> to cite a few. More specifically, conventional object detection methods are built on handcrafted features and shallow trainable architectures but performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. In 2014, Girshick et al. <ref type="bibr" target="#b157">[116]</ref> used CNNs for object detection obtaining a gap of more than 30% improvement over the previous best results. For intelligent transportation system, Wang et al. <ref type="bibr" target="#b183">[142]</ref> designed a siamesed fully CNNs method for road detection from the perspective of moving vehicles in the application of autonomous driving. This method also clearly outperforms conventional approaches on the KITTI road detection benchmark. In 2018, Wang et al. <ref type="bibr" target="#b186">[145]</ref> designed an end-to-end Attention Recurrent Convolutional Network (ARCNet) for scene classification of remote sensing. ARCNet gives better performance than handcrafted features and unsupervised learning feature based methods with a gap of 10%-20%. In 2019, Wang et al. <ref type="bibr" target="#b186">[145]</ref> developed an endto-end 2D CNN framework for hyperspectral image change detection in order to provide timely change information about large-scale Earth surface. This method called GETNET outperforms conventional methods based on PCA and SVM. In the field of background subtraction, DNNs have also been successfully applied to background generation <ref type="bibr" target="#b188">[147,</ref><ref type="bibr" target="#b189">148,</ref><ref type="bibr" target="#b190">149,</ref><ref type="bibr" target="#b191">150,</ref><ref type="bibr" target="#b74">33]</ref>, background subtraction <ref type="bibr" target="#b192">[151,</ref><ref type="bibr" target="#b193">152,</ref><ref type="bibr" target="#b194">153,</ref><ref type="bibr" target="#b195">154,</ref><ref type="bibr" target="#b196">155]</ref>, foreground detection enhancement <ref type="bibr" target="#b197">[156]</ref>, ground-truth generation <ref type="bibr" target="#b198">[157]</ref>, and the learning of deep spatial features <ref type="bibr" target="#b199">[158,</ref><ref type="bibr" target="#b200">159,</ref><ref type="bibr" target="#b201">160,</ref><ref type="bibr" target="#b202">161,</ref><ref type="bibr" target="#b203">162]</ref>. More practically, Restricted Boltzman Machines (RBMs) were first employed by Guo and Qi <ref type="bibr" target="#b188">[147]</ref> and Xu et al. <ref type="bibr" target="#b190">[149]</ref> for background generation to further achieve moving object detection through background subtraction. In a similar manner, Xu et al. <ref type="bibr" target="#b191">[150,</ref><ref type="bibr" target="#b74">33]</ref> used deep auto-encoder networks to achieve the same task whereas Qu et al. <ref type="bibr" target="#b189">[148]</ref> used context-encoder for background initialization. As another approach, Convolutional Neural Networks (CNNs) has also been employed to background subtraction by Braham and Droogenbroeck <ref type="bibr" target="#b194">[153]</ref>, Bautista et al. <ref type="bibr" target="#b193">[152]</ref> and Cinelli <ref type="bibr" target="#b195">[154]</ref>. Other authors have employed improved CNNs such as cascaded CNNs <ref type="bibr" target="#b198">[157]</ref>, deep CNNs <ref type="bibr" target="#b192">[151]</ref>, structured CNNs <ref type="bibr" target="#b196">[155]</ref> and two stage CNNs <ref type="bibr" target="#b204">[163]</ref>. Through another approach, Zhang et al. <ref type="bibr" target="#b203">[162]</ref> used a Stacked Denoising Auto-Encoder (SDAE) to learn robust spatial features and modeled the background with density analysis, whereas Shafiee et al. <ref type="bibr" target="#b201">[160]</ref> employed Neural Reponse Mixture (NeREM) to learn deep features used in the Mixture of Gaussians (MOG) model <ref type="bibr" target="#b54">[13]</ref>. In another study, Chan <ref type="bibr" target="#b205">[164]</ref> proposed a deep learning-based scene-awareness approach for change detection in video sequences thus applying the suitable background subtraction algorithm for the corresponding type of challenges. The motivations and contributions of this paper can be summarized as follows:</p><p>‚Ä¢ Numerous studies have been published in the field of background subtraction since the work of <ref type="bibr">Braham and</ref> Van Droogenbroeck in 2016, demonstrating the significant interest in deep neural networks in this field. Furthermore, each new method has been a top algorithm applied to the CDnet 2014 dataset, offering a significant improvement in performance compared to conventional approaches. In addition, DNNs have also been employed in background initialization, foreground detection enhancement, ground-truth generation, and deep learned features, showing its potential application in all fields of background subtraction.</p><p>‚Ä¢ In this context, we provide an exhaustive comparative survey regarding DNN approaches used in the field of background initialization, background subtraction, and foreground detection and their features. To do so, we compare them in terms of the architecture and performance.</p><p>The rest of this paper is organized as follows. First, we provide in Section 2 a short summary of different key points in deep neural networks for novices. In Section 3, we review the different methods based on deep neural networks for the background generation of a video sequence. In Section 4, we describe methods based on deep neural networks for background subtraction with a full comparative overview of the architecture and challenges. In Section 5, deep learned features in this field are surveyed. In addition, we also provide in Section 6 a discussion regarding the adequacy of deep neural networks for background subtraction. Finally, experimental results are presented for background generation in Section 7 and for background subtraction on the CDnet 2014 dataset in Section 8, and some concluding remarks are given in Section 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deep Neural Networks: A Short Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Story Aspects: Birth, Empty Periods and Prosperity</head><p>Artificial Neural Networks (ANNs) have a long history with two periods of inactivity. Since their first development, an increasing number of sophisticated concepts and related architectures have been created for conventional ANNs, and later for deep neural networks. More precisely, ANNs progress from basic networks (less than three layers) to shallow networks (with three layers), and up to deep networks (more than three layers) <ref type="bibr" target="#b206">[165]</ref>. Full surveys can be found in studies by Schmidhuber <ref type="bibr" target="#b151">[110]</ref> in 2015, Yi et al. <ref type="bibr" target="#b207">[166]</ref> in 2016, by Liu et al. <ref type="bibr" target="#b152">[111]</ref> in 2017, and by Gu et al. <ref type="bibr" target="#b153">[112]</ref> in 2018. In addition, a full description of the different ANNs concepts are available at the Neural Network Zoo website 2 . Here, we briefly summarize the main stages of the ANN development. The use of ANNs began in 1943 with the threshold logic unit (TLU) <ref type="bibr" target="#b208">[167]</ref>. In a further study, in 1957 Rosenblatt <ref type="bibr" target="#b209">[168]</ref> designed the first perceptron, whereas in 1962 Widrow <ref type="bibr" target="#b210">[169,</ref><ref type="bibr" target="#b211">170]</ref> developed the Adaptive Linear Neuron (ADALINE). This first generation of neural networks were fundamentally limited in what they could learn to do. During the 1970s (the first empty period), research focused more on the XOR problem. The next period concerned the emergence of more advanced neural networks such as multilayer backpropagation neural networks, CNNs, and long short-term memory (LSTMs) for recurrent neural networks (RNNs) <ref type="bibr" target="#b212">[171]</ref>. This second generation of neural networks mostly used backpropagation of the error signal to obtain derivatives for learning. During the second empty period, research focused more on a support vector machine (SVM), which is an extremely clever type of perceptron developed by Vapnik et al. <ref type="bibr" target="#b213">[172]</ref>. Thus, many researchers abandoned research into neural networks with multiple adaptive hidden layers because an SVM works better with less computational time and training. With the progress of GPUs and the storage of big data, DNNs regained attention, and developments using new deep learning concepts such as deep belief networks <ref type="bibr" target="#b214">[173,</ref><ref type="bibr" target="#b215">174]</ref> in 2006 and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b216">[175,</ref><ref type="bibr" target="#b217">176]</ref> in 2014. In 2017, Liu et al. <ref type="bibr" target="#b152">[111]</ref> classified the deep neural network architectures in the following categories: restricted Boltzmann machines (RBMs), deep belief networks (DBNs), autoencoders (AEs) networks and deep Convolutional Neural Network (CNNs). In addition, deep probabilistic neural networks <ref type="bibr" target="#b218">[177]</ref>, deep fuzzy neural networks <ref type="bibr" target="#b219">[178,</ref><ref type="bibr" target="#b220">179]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b216">[175,</ref><ref type="bibr" target="#b217">176]</ref> can also be considered as other categories. Thus, the main architectures in deep neural networks can be classified into the following categories <ref type="bibr" target="#b151">[110,</ref><ref type="bibr" target="#b152">111]</ref>:</p><p>‚Ä¢ Restricted Boltzmann machines: RBMs have been widely used in deep neural networks owing to their historical importance and relative simplicity <ref type="bibr" target="#b221">[180]</ref>. The RBM was designed by Smolensky under the name "Harmonium" and its use is made popular by Hinton <ref type="bibr" target="#b214">[173]</ref> in 2006. RBMs allow to generate stochastic models of ANNs which can learn the probability distribution according to their inputs. RBMs consist of a variant of Boltzmann machines (BMs) that can be considered as NNs with stochastic processing units connected bidirectionally. RBM is a special type of Markov random fields with stochastic visible units in one layer and stochastic observable units in the other layer. More technically, a RBM is a stochastic neural network meaning that the neuron-like units whose activations have a probabilistic element which depends on the neighbors they are connected to, while a classical neural network meaning these activations have binary activations. Figure <ref type="figure" target="#fig_0">2</ref> shows an a typical RBMs architecture. The neurons are restricted to form a bipartite graph and here is a full connection between the visible units and the hidden ones, while no connection exists between units from the same layer.</p><p>To train an RBM, a Gibbs sampler is commonly used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Deep Belief Networks:</head><p>To study the dependencies between the hidden and visible variables, Hinton <ref type="bibr" target="#b214">[173]</ref> constructed the DBNs by stacking a bank of RBMs. Thus, the DBNs are composed of multiple layers of stochastic and latent variables and can be viewed as a special form of the Bayesian probabilistic generative model. DBNs can be viewed as a composition of simple and unsupervised networks that are RBMs with Sigmoid Belief Networks. Indeed, the main building block of a DBN is a bipartite undirected graphical model (i.e. RBM) in order to learn joint probability distribution of hidden and input variables. By generating new data with given joined probability distribution, DBNs are considered more flexible. For the training, the greatest advantage of DBNs is its ability of learning features, which is achieved by a layer-by-layer learning strategies where the higher level features are learned from the previous layers. Thus, DBNs provide a fast and layerby-layer unsupervised training procedure whereas CNN required a full training procedure. To make learning easier, the network is designed so that no visible unit is connected to any other visible unit and no hidden unit is connected to any other hidden unit. In addition, DBNs are generative neural networks that stack RBMs 5 which act as generative auto-encoders. DBNs are more effective than ANNs in the presence of problems with unlabeled data. Figure <ref type="figure" target="#fig_0">2</ref> shows an a typical DBNs architecture. Every two adjacent layers form an RBM. The visible layer of each RBM is connected to the hidden layer of the previous RBM and the top two layers are non-directional. The directed connection between the above layer and the lower layer is in a top-down manner.</p><p>For training, different layers of RBMs in a DBN are trained sequentially. First, the lower RBMs are trained then the higher ones. After features are extracted by the top RBM, they are propagated back to the lower layers. In comparison with a single RBM, the stacked model increases the upper bound of the log-likelihood guaranteeing stronger learning abilities.</p><p>‚Ä¢ AutoEncoders (AEs) networks: An autoencoder (also called an auto-associator) is another type of ANN, and is an unsupervised learning algorithm used to efficiently code a dataset for the purpose of a reduction in the dimensionality. AEs are also employed to learn generative data models. Figure <ref type="figure" target="#fig_1">3</ref> shows a typical AE architecture. The input data are converted into an abstract representation, which is then converted back into the original format using the decoder function. In practical terms, the AE is trained to encode the input into a representation from which the input can be reconstructed. Thus, the AE attempts to approximate the identity function during this process. The main advantage is that the AE can extract useful features continuously during the propagation and filter out any useless information. Thus, the efficiency of the learning process is improved because the input vector is transformed into a lower dimensional representation during the coding process. Deep autoencoders have demonstrated their effectiveness in discovering non-linear features across many problem domains, but require clean training data. However, in many real applications, data are often corrupted by large outliers or pervasive noise. To address this problem, in 2016, Jiang et al. <ref type="bibr" target="#b222">[181]</ref> designed l 2,1 -norm stacked robust autoencoders, whereas in 2017 Zhou and Paffenroth <ref type="bibr" target="#b223">[182]</ref> proposed the use of robust autoencoders based on the principle of an RPCA developed by Cand√®s et al <ref type="bibr" target="#b73">[32]</ref>. Thus, the input data A are split into two parts A = L + S , where L can be effectively reconstructed by a deep autoencoder and S contains the outliers and noise in the original data A. Because such a split increases the robustness of a conventional deep autoencoder, this model is called Robust Deep Autoencoder (RDA) <ref type="bibr" target="#b223">[182]</ref>. In a similar manner, based on an RPCA, Chalapathy et al. <ref type="bibr" target="#b224">[183]</ref> designed a robust autoencoder that learns a nonlinear subspace capturing the majority of data points, while allowing certain data to have an arbitrary corruption. This method is called Robust Convolutional Autoencoder (RCAE). Both RDA and RCAE inherit the nonlinear representation capability of autoencoder, along with the anomaly discrimination ability of RPCA. But, the input split results in a loss function that is most of time not computationally tractable. To tackle this problem, Teng et al. <ref type="bibr" target="#b225">[184]</ref> proposed DeepSphere which does not have any requirements towards anomaly sparsity. Moreover, DeepSphere is a unified end-toend learning model which does not require any optimization algorithm development. Limited experiments show that the foreground can be captured by DeepSphere. In 2018, Dai et al. <ref type="bibr" target="#b226">[185]</ref> demonstrated that Variational AutoEncoders (VAE) can be viewed as a natural evolution of recent robust PCA models, which are capable of learning nonlinear manifolds of unknown dimension obscured through gross corruptions. In practice, a linear deep autoencoder network (i.e., without the use of nonlinear activation functions at each layer) operates similarly as a dimensionality reduction method such as a PCA. In a similar manner, a robust deep autoencoder can be viewed as an extension of an RPCA in terms of nonlinear dimensions.  ‚Ä¢ Deep Convolutional Neural Networks (CNNs): CNNs are a subtype of the discriminative deep architecture and demonstrate suitable performance in processing 2D data like in images and videos <ref type="bibr" target="#b152">[111]</ref>. The architecture of a CNN is inspired by the visual cortex of animals, and the concept is based on a time-delay neural network (TDNN). In a TDNN, the weights are shared in a temporal dimension, whereas the convolution replaces the general matrix multiplication in a CNN. Thus, the number of weights is decreased with a decrease in the complexity of the network. Furthermore, images can be directly imported into a network, avoiding the feature extraction procedure. CNNs were the first truly successful deep learning architecture owing to the successful training of hierarchical layers. The CNN topology leverages spatial relationships with a decreasing number of parameters in the network, and the performance is improved using standard back-propagation algorithms. In addition, CNNs require minimal pre-processing, allowing an end-to-end solution. Figure <ref type="figure" target="#fig_2">4</ref> shows an a typical CNNs architecture also called ConvNets. However, Cohen and Shashua <ref type="bibr" target="#b227">[186,</ref><ref type="bibr" target="#b228">187]</ref> provided an architecture called SimNets which is a generalization of ConvNets driven by two operators. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets.</p><p>‚Ä¢ Deep probabilistic neural networks: To consider the uncertainty, thereby providing important information regarding the reliability of predictions and the inner workings of a network, in 2018, Gast and Roth <ref type="bibr" target="#b218">[177]</ref> introduced two lightweight deep probabilistic approaches to making supervised learning. Figure <ref type="figure" target="#fig_3">5</ref> shows an illustration of these two approaches. First, Gast and Roth <ref type="bibr" target="#b218">[177]</ref> proposed the use of probabilistic output layers for classification and regression, which require only minimal changes to existing networks. Second, Gast and Roth <ref type="bibr" target="#b218">[177]</ref> used density filtering, demonstrating that activation uncertainties can be propagated through the network. The two probabilistic networks maintain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with empirical errors induced through their predictions. In addition, the robustness to adversarial examples was significantly improved.</p><p>‚Ä¢ Deep fuzzy neural networks: Based on the principle of uncertainty, in 2017, Deng et al. <ref type="bibr" target="#b219">[178]</ref> introduced the concept of fuzzy learning, providing a hierarchical deep neural network that derives information from both fuzzy and neural representations. Thus, the knowledge learned from these two respective views are fused, providing the final data representation to be classified. Figure <ref type="figure" target="#fig_4">6</ref> shows an illustration of the fuzzy DNNs architecture which consists of four parts. In 2018, Feng and Chen <ref type="bibr" target="#b220">[179]</ref> designed a fuzzy RBM by replacing all real-valued parameters with fuzzy numbers. The FRBM then employs the crisp possibilistic mean value of a fuzzy number to defuzzify the fuzzy free energy function. ‚Ä¢ Generative Adversarial Networks (GANs): Generative Adversarial Networks (GAN) GANs represent a breakthrough in machine learning. Introduced in 2014 by Goodfellow et al. <ref type="bibr" target="#b216">[175,</ref><ref type="bibr" target="#b217">176]</ref>, GANs provide a powerful framework for using unlabeled data in the training of machine learning models, and have become one of the most promising paradigms for unsupervised learning. More precisely, GANs allow estimating generative models using an adversarial process in which two models are trained: a generative model that captures the data distribution, and a discriminative model that estimates the probability that a sample was derived from the training data rather than the generative model <ref type="bibr" target="#b216">[175]</ref>. To train the generative model, Goodfellow et al. <ref type="bibr" target="#b216">[175]</ref> maximize the probability of a discriminative model making a mistake. The main advantages of a GAN is as follows: 1) Markov chains are not required, 2) only a backprop is used to obtain the gradients, 3) no inference is required during learning, and 4) a wide variety of functions can be employed. These advantages offer a low computational time. However, GANs also present a statistical advantage over a generator network that is not updated directly with data examples but with gradients circulating through the discriminator. Thus, the components of the input are not copied directly into the generator parameters <ref type="bibr" target="#b216">[175]</ref>. An analysis of GANs can be found in Zhu et al. <ref type="bibr" target="#b229">[188]</ref>.</p><p>The applications of these deep learning architectures are mainly in the areas of speech separation and recognition <ref type="bibr" target="#b230">[189,</ref><ref type="bibr" target="#b231">190,</ref><ref type="bibr" target="#b232">191,</ref><ref type="bibr" target="#b233">192]</ref>, computer vision <ref type="bibr" target="#b152">[111]</ref> and pattern recognition <ref type="bibr" target="#b152">[111]</ref>. In this context, DeepNet architectures for specific applications have emerged, such as the following: AlexNet developed in 2012 by Krizhevsky et al. <ref type="bibr" target="#b154">[113]</ref>for image classification, VGG-Net designed in 2015 by Simonyan and Zisserman <ref type="bibr" target="#b234">[193]</ref> for large-scale image recognition i, U-Net <ref type="bibr" target="#b235">[194]</ref> developed in 2015 by Ronneberger et al. <ref type="bibr" target="#b235">[194]</ref> for biomedical image segmentation, GoogLeNet with inception neural network introduced in 2015 by Szegedy et al. <ref type="bibr" target="#b236">[195]</ref> for computer vision, and Microsoft Residual Network (ResNet) designed in 2016 by He et al. <ref type="bibr" target="#b237">[196]</ref> for image recognition. Thus, all current architectures were designed for a target application such as speech recognition <ref type="bibr" target="#b238">[197]</ref>, computer vision <ref type="bibr" target="#b239">[198]</ref> and pattern recognition <ref type="bibr" target="#b152">[111]</ref>, the specific features of which provide a very impressive performance in comparison with previous state-of-theart methods based on a GMM and graph-cut, as with the problem of foreground detection/segmentation/localization. However, in order to obtain performance gains, the deep neural networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, computation and data (because of the overfitting), thus limiting their usability <ref type="bibr" target="#b240">[199]</ref>. However, many parameters are required with fully-connected layers that employ parameters highly inefficiently. To address this issue, more efficient parameterizations can be designed for fully-connected layers. Such compressed parameter spaces naturally lead to reduced memory and computational costs. Furthermore, high quality parameterizations can extract more meaningful information when relevant data is limited. In this context, several authors proposed deep neural network architecture which replaces matrices by tensors <ref type="bibr" target="#b240">[199,</ref><ref type="bibr" target="#b241">200,</ref><ref type="bibr" target="#b242">201,</ref><ref type="bibr" target="#b243">202,</ref><ref type="bibr" target="#b244">203,</ref><ref type="bibr" target="#b245">204,</ref><ref type="bibr" target="#b246">205]</ref>. For example, Newman et al. <ref type="bibr" target="#b241">[200]</ref> used a tensor neural network (t-NN) whereas Wang et al. <ref type="bibr" target="#b240">[199]</ref> used a tensor ring factorization approach <ref type="bibr" target="#b247">[206]</ref> to compress both the fully connected layers and the convolutional layers of deep neural network obtaining Tensor Ring Networks (TR-Nets). The reader can refer to Cheng et al. <ref type="bibr" target="#b248">[207]</ref> for a survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Features Aspects</head><p>Deep neural networks are parametric models that achieve sequential operations on their input data. Each operation, called a layer, consists of a linear transformation followed by a pointwise linear or nonlinear activation function <ref type="bibr" target="#b249">[208]</ref>. In deep linear neural networks, the function class of a linear multilayer neural network only contains activation functions that are linear with respect to the input <ref type="bibr" target="#b206">[165]</ref>. In contrast, nonlinear activation functions are employed in deep nonlinear neural networks. However, in both cases their loss functions in the weight parameters are nonconvex. As shown in the previous section, DNNs are characterized by their architecture, which becomes increasingly sophisticated over time. In practical terms, an architecture consists of different layers, which are classified as input layers, hidden layers, and output layers. Each layer contains many neurons that are either activated or not following an activation function. An activation function can be viewed as the mapping of the input to the output using a non-linear transform function at each node. Different activation functions can be found in the literature, such as the sigmoid function <ref type="bibr" target="#b250">[209]</ref>, Rectified Linear Unit (ReLU) <ref type="bibr" target="#b251">[210]</ref>, and Probabilistic ReLU (PReLU) <ref type="bibr" target="#b252">[211]</ref>. Once the architecture is determined and the activation functions are chosen for each kind of layer, DNNs need to be trained using a largescale dataset such as the ImageNet dataset <ref type="bibr" target="#b154">[113]</ref>, CIFAR-10 dataset and ILSVRC 2015 dataset for classification tasks. To do so, the architecture is exposed to the training dataset to learn the weights of each neuron in each layer. The parameters are learned using a cost function and are minimized on the desired and predicted outputs. The most common method for training is back-propagation. The gradient of the error function is typically computed on the correct output, and the predicted output is propagated back to the beginning of the network to update its parameters, which requires a gradient descent algorithm. Batch normalization, which normalizes mini-batches, can also be used to accelerate learning because it employs higher learning rates, and regularizes the learning. For the vocabulary, an epoch is a complete pass through a given dataset, and is thus the number times a neural network has been exposed once to every record of the dataset. An epoch is not an iteration, which corresponds to a single update of the neural net model parameters. Many iterations can occur before an epoch is complete. An epoch and an iteration are only identical if the parameters are updated once for each pass through the entire dataset. The reader can refer to the guide of Dumoulin and Visin <ref type="bibr" target="#b253">[212]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Theoretical Aspects</head><p>The empirical success of deep learning presents numerous challenges to theoreticians. In 2018, Vidal et al. <ref type="bibr" target="#b254">[213]</ref> pointed out three main factors, namely, the architectures, regularization techniques, and optimization algorithms, which are critical to the training of well-performing DNNs. Understanding the necessity and interplay of these three factors is essential in an analysis of their success. Thus, the theoretical aspects mainly concern an understanding, provability and the stability of the DNNs <ref type="bibr" target="#b255">[214,</ref><ref type="bibr" target="#b249">208,</ref><ref type="bibr" target="#b254">213,</ref><ref type="bibr" target="#b256">215]</ref>, as well as their properties in the presence of adversarial perturbations <ref type="bibr" target="#b257">[216,</ref><ref type="bibr" target="#b258">217,</ref><ref type="bibr" target="#b259">218,</ref><ref type="bibr" target="#b260">219,</ref><ref type="bibr" target="#b261">220,</ref><ref type="bibr" target="#b262">221,</ref><ref type="bibr" target="#b263">222]</ref>, and their robustness in presence of noisy labels <ref type="bibr" target="#b264">[223,</ref><ref type="bibr" target="#b265">224]</ref>. For this, the principle key features in the design of DNNs need to be mathematically investigated as follows <ref type="bibr" target="#b249">[208,</ref><ref type="bibr" target="#b254">213]</ref>:</p><p>‚Ä¢ Architecture: The number, size, and type of layers are the key characteristics of an architecture and the classes of functions that can be approximated using a feed-forward neural network. The key issue is how the chosen architecture, along with its depth and width, impact the expressiveness, which is its ability to approximate arbitrary functions of the input. Several studies <ref type="bibr" target="#b266">[225,</ref><ref type="bibr" target="#b267">226,</ref><ref type="bibr" target="#b268">227,</ref><ref type="bibr" target="#b269">228]</ref> have shown that neural networks with a single hidden layer with sigmoidal activations are universal function approximators. However, a wide and shallow network has also been obtained using a deep network with significant improvements in performance <ref type="bibr" target="#b249">[208]</ref>. Thus, deep architectures seem to be able to better capture invariant properties of the data as compared to their shallow counterparts. In practice, certain sub-classes of deep neural networks, such as scattering networks <ref type="bibr" target="#b270">[229]</ref> are provably stable and locally invariant signal representations, and reveal the fundamental role of the geometry and stability in that both conditions generalize the performance of a modern deep convolution.</p><p>‚Ä¢ Optimization: This concerns the training of the DNNs and contains two aspects, namely, the datasets used for training, and in most cases, the algorithm used to optimize the network. Indeed, the optimization problem is generally non-convex, and the main issues concern the guarantee of the optimality, the success of the stochastic gradient descent (SGD) following the appearance of the error surface, and whether the local minima are global property holds for deep nonlinear networks. To address the issue of non-convexity, a conventional strategy consists of initializing the network weights at random, and updating the weights using a local descent, checking whether the training error decreases sufficiently fast, and if not, choosing another initialization. In practice, this strategy often leads to different solutions for the network weights while giving approximately the same objective values and classification performance. Empirically, when the size of the network is sufficiently large and ReLU non-linearities are used, all local minima may be global <ref type="bibr" target="#b249">[208]</ref>. SGD have been rigorously analyzed for convex loss functions; however, a loss is a non-convex function of the deep neural network parameters. Thus, the use of an SGD does not provide a guarantee of finding the global optimum. Moreover, critical points are more likely to be saddle points rather than spurious local minima <ref type="bibr" target="#b271">[230]</ref> and the local minima concentrate near the global optimum. However, for certain types of neural networks in which both the loss function and the regularizer are sums of positively homogeneous functions of the same degree, Haeffele and Vidal <ref type="bibr" target="#b272">[231,</ref><ref type="bibr" target="#b273">232]</ref> demonstrated that a local optimum, such as when many of the entries are zero, is also a global optimum. In 2016, Kawaguchi <ref type="bibr" target="#b206">[165]</ref> demonstrated that, for an expected loss function of a deep nonlinear neural network in which the function is non-convex and non-concave, every local minimum is a global minimum, and every critical point that is not a global minimum is a saddle point. The same statements hold for deep linear neural networks with any depth or width and no unrealistic assumptions.</p><p>‚Ä¢ Generalization and regularization properties: The main concerns of this part are how well do DNNs generalize, how should DNNs be regularized, and how should under and over fitting be prevented? Indeed, the main critical issue of a DNN architecture is the ability to generalize from a small number of training examples.</p><p>Based on statistical learning theory, it has been shown that the number of training examples needed to achieve good generalization increases polynomially with the size of the network. In a DNN, the training set contains much fewer data than the number of parameters, preventing an over-fitting using regularization techniques such as a Dropout <ref type="bibr" target="#b274">[233,</ref><ref type="bibr" target="#b275">234,</ref><ref type="bibr" target="#b276">235]</ref> which freezes a random subset of the parameters at each iteration. Then, deep architectures produce an embedding of the input data that approximately keeps the distance between data points 10 in the same class (i.e. the inter-class distance), while increasing the separation between classes (i.e. intra-class distance)</p><p>‚Ä¢ Stability and robustness properties: Output instability of deep neural networks are due to small perturbations in the input that can significantly distort the feature embeddings and output of a neural network <ref type="bibr" target="#b277">[236,</ref><ref type="bibr" target="#b278">237]</ref>.</p><p>In 2015, Giryes et al. <ref type="bibr" target="#b279">[238]</ref> demonstrate the stability of DNNs with random Gaussian weights that perform a distance-preserving embedding of the data. However, stability can be improved by forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) <ref type="bibr" target="#b280">[239,</ref><ref type="bibr" target="#b281">240]</ref>, and an efficient weight normalization technique <ref type="bibr" target="#b282">[241]</ref>.</p><p>Both the architecture and optimization can impact the generalization <ref type="bibr" target="#b255">[214,</ref><ref type="bibr" target="#b249">208,</ref><ref type="bibr" target="#b254">213,</ref><ref type="bibr" target="#b256">215]</ref>. Furthermore, several architectures are easier to optimize than others <ref type="bibr" target="#b249">[208,</ref><ref type="bibr" target="#b254">213]</ref>. The first replies regarding the global optimality were provided in 2016 by Kawaguchi <ref type="bibr" target="#b206">[165]</ref> and in 2018 by Yun et al. <ref type="bibr" target="#b256">[215]</ref>. Their main conclusion is that DNNs are more difficult to train than classical neural networks owing to their non-convexity, but not too difficult owing to the nonexistence of poor local minima and the property of the saddle points. In 2018, Wang et al. <ref type="bibr" target="#b283">[242]</ref> showed that deep neural networks can be better understood by utilizing the knowledge obtained by the visualization of the output images obtained at each layers. Other authors provided either a theoretical analysis or visualizing analysis in a context of an application. For example, Basu et al. <ref type="bibr" target="#b284">[243]</ref> published a theoretical analysis for texture classification whereas Minematsu et al. <ref type="bibr" target="#b285">[244,</ref><ref type="bibr" target="#b286">245]</ref> provided a visualizing analysis for background subtraction. In 2019, Kahng et al. <ref type="bibr" target="#b287">[246]</ref> provided GANLab that allows users to interactively train generative models and visualize the dynamic training processs intermediate results. Despite these first valuable investigations, an understanding of DNNs remains low. Nevertheless, DNNs have been successfully applied in many computer vision applications, with a large increase in performance. This success is intuitively due to the following reasons: 1) features are learned rather than being manual hand-crafted, 2) more layers capture more invariance characteristics, 3) more data allow a deeper training, 4) more computing CPU, 5) better regularization functions (Dropout <ref type="bibr" target="#b274">[233]</ref>) and 6) new non-linearity functions (max-pooling, ReLU <ref type="bibr" target="#b288">[247]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Implementation Aspects</head><p>For software implementation, many libraries for the development of different programming languages are available for the implementation of DNNs. The most known libraries are Caffe <ref type="bibr" target="#b289">[248]</ref>, MatConvNet <ref type="bibr" target="#b290">[249]</ref> from Matlab, Microsoft Cognitive Toolkit (CNTK), TensorFlow <ref type="bibr" target="#b291">[250]</ref>, Theano<ref type="foot" target="#foot_2">3</ref> and Torch <ref type="foot" target="#foot_3">4</ref> . All these software support interfaces of C, C++ and/or Python for quick development. For a full list, the reader are referred to go on the deeplearning.net <ref type="foot" target="#foot_4">5</ref>website. There is also a Deep Learning library for Java (DL4J <ref type="foot" target="#foot_5">6</ref> ). For hardware implementation and optimization, there are several designed GPUs from NVIDIA with dedicated SDKs <ref type="foot" target="#foot_6">7</ref> . For example, the deep learning GPU Training System (DIGITS<ref type="foot" target="#foot_7">8</ref> ) provides fast training of DNNs for computer vision applications like image classification, segmentation and object detection tasks whilst NVIDIA Jetson is designed for embedded systems. For NVIDIA Volta GPUs, TensorRT<ref type="foot" target="#foot_8">9</ref> allows optimizing the deep learning inference and runtime. It also allows the deployment of trained neural networks for inference to hyper-scale data centers, or embedding. A deep neural network accelerator based on FPGA has also been developed <ref type="bibr" target="#b292">[251]</ref>.</p><p>In the following sections, we survey all previous DNN approaches used in background/foreground separation by comparing their advantages and disadvantages, as well as their performance on the CDnet 2014 dataset  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background Generation</head><p>Background generation <ref type="bibr" target="#b293">[252,</ref><ref type="bibr" target="#b294">253,</ref><ref type="bibr" target="#b295">254]</ref> (also called background initialization <ref type="bibr">[255, 256] [257, 258]</ref>, background estimation <ref type="bibr" target="#b300">[259,</ref><ref type="bibr" target="#b301">260]</ref>, and background extraction <ref type="bibr" target="#b302">[261]</ref>) refers the initialization of the background. In general, a model is often initialized using the first frame or a background model over a set of training frames that either contain or do not contain foreground objects. This background model can be the temporal average or temporal median. However, such a state is impossible in several types of environments owing to the required bootstrapping, and a sophisticated model is then needed to construct the first image. The top algorithms applied to the SBMnet dataset are Motion-assisted Spatio-temporal Clustering of Low-rank (MSCL) <ref type="bibr" target="#b303">[262]</ref> and LaBGen <ref type="bibr" target="#b304">[263,</ref><ref type="bibr" target="#b305">264,</ref><ref type="bibr" target="#b306">265]</ref> which are based on robust PCA <ref type="bibr" target="#b46">[5,</ref><ref type="bibr" target="#b47">6]</ref> and the robust estimation of the median, respectively. Figure <ref type="figure" target="#fig_5">7</ref> shows samples of background generation of three videos from the SBMI dataset <ref type="bibr" target="#b307">[266]</ref>. In practical terms, the main challenge is to obtain the first background model when more than half of the training contains foreground objects. This learning process can be achieved off-line and thus a batch-type algorithm can be applied. Deep neural networks are suitable for this type of task and several DNN methods have recently been used in this field. We classified such networks into the following categories described below. Table <ref type="table">1</ref> shows an overview of these methods. In addition, a list of publications dealing with these networks is available at the Background Subtraction Website<ref type="foot" target="#foot_9">10</ref> and is updated regularly .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Restricted Boltzmann Machines (RBMs)</head><p>In 2013, Guo and Qi <ref type="bibr" target="#b188">[147]</ref> were the first authors who applied Restricted Boltzmann Machine (RBM) to background generation by using a Partially-Sparse RBM (PS-RBM) framework in order to detect moving objects by background subtraction. This framework models the image as the integration of RBM weights as shown in Figure <ref type="figure" target="#fig_6">8</ref>. By introducing a sparsity target, the learning process alleviate the tendency of growth in weights. Once the sparse constraints are added to the objective function, the hidden units only keep active in a rather small portion on the specific training data. In this context, Guo and Qi <ref type="bibr" target="#b188">[147]</ref> proposed a controlled redundancy technique, that allow the hidden units to learn the distinctive features as sparse as possible, meanwhile, the redundant part rapidly learns the similar information to reduce the total error. The PS-RBM provides accurate background modeling even in dynamic and noisy environments. Practically, PS-RBM provided similar results than DPGMM <ref type="bibr" target="#b83">[42]</ref>, KDE <ref type="bibr" target="#b52">[11]</ref>, KNN <ref type="bibr" target="#b79">[38]</ref>, and SOBS <ref type="bibr" target="#b137">[96]</ref> methods on the CDnet 2012 dataset.</p><p>In 2015, Xu et al. <ref type="bibr" target="#b190">[149]</ref> proposed a Temporally Adaptive RBM (TARBM) background subtraction to take into account the spatial coherence by exploiting possible hidden correlations among pixels while exploiting the temporal coherence too. Figure <ref type="figure" target="#fig_7">9</ref> illustrates the difference between a conventional RBM and a Temporally Adaptive RBM. For TARBM, the visible layer consists of a pair of components, each with the same number of units, corresponding to a window of two adjacent frames. One single hidden layer provides the sequential components, where b is the corresponding bias vector. W l and W r are the connection weight matrices, respectively. A regularization term R T provides temporal constraints for reconstructed data. As a result, the augmented temporally adaptive model can generate a more stable background given noisy inputs and adapt quickly to changes in the background while maintaining all advantages of PS-RBM including an exact inference and effective learning procedure. Figure <ref type="figure" target="#fig_8">10</ref> shows the pipeline of TARBM background subtraction. TARBM outperforms the standard RBM, and is robust in the presence of dynamic  changes to the background and illumination.</p><p>In 2018, Sheri et al. <ref type="bibr" target="#b308">[267]</ref> employed a Gaussian-Bernoulli restricted Boltzmann machine (GRBM), which differs from an ordinary restricted Boltzmann machine (RBM), using real numbers as inputs. This network results in a constrained mixture of Gaussians, which is one of the most widely used techniques for solving the background subtraction problem. GRBM then easily learns the variance of the pixel values and takes advantage of the generative model paradigm of an RBM. In the case of PTZ cameras, Rafique et al. <ref type="bibr" target="#b309">[268]</ref> modeled a background scene using an RBM. The generative modeling paradigm of an RBM provides an extensive and nonparametric background learning framework. An RBM was then trained using one-step contrastive divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Auto Encoder Networks (DAE)</head><p>In 2014, Xu et al. <ref type="bibr" target="#b74">[33]</ref> designed a background generation method based on two auto-encoder neural networks. First, the approximate background images are computed using an auto-encoder network called a reconstruction network from the current video frames. Second, the background model is learned based on these background images using another auto-encoder network called a background network (BN). In addition, the background model is updated on-line to incorporate more training samples over time. Figure <ref type="figure" target="#fig_9">11</ref> shows the background generation pipeline. Experimental results on the I2R dataset <ref type="bibr" target="#b314">[273]</ref> show that DAN outperforms MOG <ref type="bibr" target="#b54">[13]</ref>, Dynamic Group Sparsity (DGS) <ref type="bibr" target="#b315">[274]</ref>, Robust Dictionary Learning (RDL) <ref type="bibr" target="#b316">[275]</ref> and Online RDL (ORDL) <ref type="bibr" target="#b317">[276]</ref>. In a further work, Xu et al. <ref type="bibr" target="#b191">[150]</ref> improved this method by using an Adaptive Tolerance Measure. Thus, DAN-ATM can handle large variations of dynamic background more efficiently than DAN. Experimental results on the I2R dataset <ref type="bibr" target="#b314">[273]</ref> confirm this increase in performance.</p><p>Qu et al. <ref type="bibr" target="#b189">[148]</ref> employed a context-encoder network for a motion-based background generation method by removing the moving foreground objects and learning the features. After removing the foreground, a context-encoder is also applied to predict the missing pixels of an empty region and to generate a background model of each frame. The architecture is based on AlexNet, which produces a latent feature representation of the input image samples with empty regions. The decoder has five upper convolutional layers and uses the feature representation to fill in the missing regions of the input samples. The encoder and the decoder are connected through a channel-wise fully connected layer. This allows information to be propagated within the activations of each feature map. The experiments conducted by Qu et al. <ref type="bibr" target="#b189">[148]</ref> are limited but convincing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">FC-FlowNet</head><p>Halfaoui et al. <ref type="bibr" target="#b301">[260]</ref> employed a CNN architecture for background estimation, which can provide a background image with only a small set of frames containing foreground objects. The CNN is trained using estimated background patches, followed by a post-processing step to obtain the final background image. More precisely, this architecture is based on FlownNetSimple <ref type="bibr" target="#b318">[277]</ref>, which is a two-stage architecture developed for the prediction of the optical flow motion vectors. The first stage is contraction, whereas the second stage is refinement. The contraction stage is a succession of convolutional layers. This rather generic stage extracts high-level abstractions of the stacked input images, and forwards the gained feature maps to the upper convolutional refinement stage to enhance the coarse-to-fine transformations. Halfaoui et al. <ref type="bibr" target="#b301">[260]</ref> adapted this architecture by providing a Fully-concatenated version called FCFlowNet (See Figure <ref type="figure" target="#fig_10">12</ref>). Experimental results on the SBMC 2016 dataset <ref type="foot" target="#foot_10">11</ref> demonstrate the robustness against very short or long sequences, a dynamic background, changes in illumination, and intermittent object motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">U-Net</head><p>In 2017, Tao et al. <ref type="bibr" target="#b310">[269]</ref> proposed an unsupervised deep learning model for background modeling called BM-Unet. This method is based on the generative U-Net architecture <ref type="bibr" target="#b235">[194]</ref> which for a given frame (input) provides the corresponding background image (output) with a probabilistic heat map of the color values. However, to tackle camera jitter and quick changes in illumination, this method learns parameters automatically and uses the differences in intensity and optical flow features in addition to the color features. Moreover, BM-Unet can be applied to a new video sequence without the need for re-training. More precisely, Tao et al. <ref type="bibr" target="#b310">[269]</ref> proposed two algorithms named Baseline BM-Unet and Augmented BM-Unet that can handle static background and background with illumination changes and camera jitter, respectively. Figure <ref type="figure" target="#fig_11">13</ref> shows an illustration of the baseline BM-Unet and augmented BM-Unet architectures. The Augmented BM-Unet is based on the so called guide features which are used to guide the network to generate the background corresponding to the target frame. Experimental results <ref type="bibr" target="#b310">[269]</ref> on the SBMnet dataset <ref type="foot" target="#foot_11">12</ref>  <ref type="bibr" target="#b294">[253]</ref> demonstrate promising results over neural networks methods (BEWiS <ref type="bibr" target="#b319">[278]</ref>, BE-AAPSA <ref type="bibr" target="#b320">[279]</ref>, and FC-FlowNet <ref type="bibr" target="#b301">[260]</ref>), and state-of-the-art methods (Photomontage <ref type="bibr" target="#b321">[280]</ref>, LabGen-P <ref type="bibr" target="#b304">[263]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Generative Adversarial Networks (GANs)</head><p>Generative Adversarial Networks (GAN) have been a breakthrough in machine learning. Introduced in 2014, a GAN <ref type="bibr" target="#b216">[175,</ref><ref type="bibr" target="#b217">176]</ref> provides a powerful framework for applying unlabeled data to the training of machine learning models, and is one of the most promising paradigms for unsupervised learning. Based on a hybrid GAN, Sultana et al. <ref type="bibr" target="#b311">[270]</ref> designed an unsupervised Deep Context Prediction (DCP) for background initialization in the context of background/foreground separation. Figure <ref type="figure" target="#fig_12">14</ref> shows the pipeline of DCP. More precisely, DCP is an unsupervised visual feature learning hybrid GAN based on context prediction. It is followed by a semantic inpainting network for texture optimization. Sultana et al. <ref type="bibr" target="#b311">[270]</ref> additionally trained a context prediction model using scene-specific data patches with a resolution of 128 √ó 128 for three epochs. The texture optimization is done with VGG-16 network pre-trained on ImageNet <ref type="bibr" target="#b155">[114]</ref> for classification. The frame selection for inpainting the background is then achieved through a summation of the pixel values using a forward frame difference technique. If the sum of the difference pixels is small, the current frame is then selected. Experimental results on the SBMnet dataset <ref type="bibr" target="#b294">[253]</ref> show that DCP achieves an average gray level error of 8.724 which is the lowest among all compared low-rank methods, namely, RFSA <ref type="bibr" target="#b322">[281]</ref>, GRASTA <ref type="bibr" target="#b323">[282]</ref>, GOSUS <ref type="bibr" target="#b324">[283]</ref>, SSGoDec <ref type="bibr" target="#b325">[284]</ref>, and DECOLOR <ref type="bibr" target="#b326">[285]</ref>.</p><p>In a further study, Sultana et al. <ref type="bibr" target="#b312">[271]</ref> used a GAN model for RGB-D video sequences by separately training two GANs (See Figure <ref type="figure" target="#fig_13">15</ref>): one for RGB video and one for depth video to generate background images. Each generated background sample is then subtracted from the given test sample to detect the foreground objects either in terms of the RGB or depth. Finally, the final foreground mask is obtained by combining the two foreground masks using a logical AND. Experiments on the SBM-RGBD <ref type="foot" target="#foot_12">13</ref> dataset <ref type="bibr" target="#b327">[286]</ref> show that ForeGAN-RGBD model outperforms cwisardH+ <ref type="bibr" target="#b328">[287]</ref>, RGB-SOBS <ref type="bibr" target="#b143">[102]</ref>, and SRPCA <ref type="bibr" target="#b111">[70]</ref> with an average F-Measure score of 0.8966.</p><p>In 2019, Sultana and Jung <ref type="bibr" target="#b313">[272]</ref> provided an illumination invariant method using ForeGAN. Thus, this method proposed is inspired from ForeGAN-RGBD model designed by Sultana et al. <ref type="bibr" target="#b312">[271]</ref>, which has been adapted for background generation by introducing scene-specific illumination information into DCGAN model <ref type="bibr" target="#b329">[288]</ref> (See Figure <ref type="figure" target="#fig_14">16</ref>). First, the ForeGAN model is trained on background image samples with various illumination conditions including dynamic changes. For testing, the GAN model generates the same background sample as test sample with similar illumination conditions via back-propagation technique. The generated background sample is then subtracted from the given test sample to segment foreground objects. Experimental results on the Illumination Conditions from Dawn until Dusk (ICDD <ref type="foot" target="#foot_13">14</ref> ) dataset show that Illumination-Invariant ForeGAN outperforms robust subspace learning methods, namely, GRASTA <ref type="bibr" target="#b323">[282]</ref>, DECOLOR <ref type="bibr" target="#b326">[285]</ref>, 3TD <ref type="bibr" target="#b330">[289]</ref> RMAMC <ref type="bibr" target="#b331">[290]</ref> TVRPCA <ref type="bibr" target="#b332">[291]</ref>.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Background Subtraction</head><p>Background subtraction consists of comparing the background image with the current image to label pixels as background or foreground pixels.The top-three algorithms on the large-scale dataset CDnet 2014 for supervised approaches are DNN-based methods, namely, FgSegNet <ref type="bibr" target="#b196">[155]</ref>, BSGAN <ref type="bibr" target="#b367">[326]</ref>, cascaded CNN <ref type="bibr" target="#b198">[157]</ref> followed by three non-supervised multi-feature/multi-cue approaches, namely, SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, PAWCS <ref type="bibr" target="#b94">[53]</ref>, IUTIS <ref type="bibr" target="#b383">[342]</ref>. This is a classification task, which can be successfully achieved using a DNN. Different methods for this have been developed, and we review them in the following sub-sections. Table <ref type="table">2</ref> shows an overview of these methods. In addition, the list of publications is available at the Background Subtraction Website <ref type="foot" target="#foot_14">14</ref> and is regularly updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convolutional Neural Networks</head><p>In 2016, Braham and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> were the first authors to use Convolutional Neural Networks (CNNs) for background subtraction. This model named ConvNet has a similar structure than LeNet-5 <ref type="bibr" target="#b372">[331]</ref> (See Figure <ref type="figure" target="#fig_16">17</ref>). Thus, the background subtraction model involves four stages: background image extraction using a temporal gray-scale median, specific-scene dataset generation, network training, and background subtraction. More precisely, the background model is built for a specific scene. For each frame in a video sequence, image patches that are centered on each pixel are extracted and are then combined with the corresponding patches from the background model. Braham and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> used a patch size of 27 √ó 27. After, these combined patches are fed to the network to predict probability of foreground pixels. For the architecture, Braham and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> employed 5 √ó 5 local receptive fields, and 3 √ó 3 non-overlapping receptive fields for all pooling layers. The first and second convolutional layers have 6 and 16 feature maps, respectively. The first fully connected layer has 120 hidden units and the output layer consists of a single sigmoid unit. The algorithm needs for training the foreground results of a previous segmentation algorithm named IUTIS <ref type="bibr" target="#b383">[342]</ref> or the ground truth information provided in CDnet 2014 <ref type="bibr" target="#b76">[35]</ref>. Half of the training examples are used for training ConvNet and the remaining frames are used for testing. By using the results of the IUTIS method <ref type="bibr" target="#b383">[342]</ref>, the segmentation produced by the ConvNet is very similar to other state-of-theart methods whilst the algorithm outperforms all other methods significantly when using the ground-truth information especially in videos of hard shadows and night videos. Evaluated on the CDnet 2014 dataset (excluding the IOM and PTZ categories), this method with IUTIS and GT achieved an average F-Measure score of 0.7897 and 0.9046, respectively. In 2016, Baustita et al. <ref type="bibr" target="#b193">[152]</ref> also used a simple CNN but for the specific task of vehicle detection. For pedestrian detection, Yan et al. <ref type="bibr" target="#b337">[296]</ref> employed the similar scheme with both visible and thermal images. Then, the inputs of the network have a size of 64 √ó 64 √ó 8 which includes the visible frame (RGB), thermal frame (IR), visible background (RGB) and thermal background (IR). The outputs of the network have a size of 64√ó64√ó2. Experiments on OCTBVS dataset 16 show that this method outperforms T2-FMOG <ref type="bibr" target="#b57">[16]</ref>, SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, and DECOLOR <ref type="bibr" target="#b326">[285]</ref>. For biodiversity detection in terrestrial and marine environments, Weinstein <ref type="bibr" target="#b338">[297]</ref> employed the GoogLeNet architecture integrated in a software called DeepMeerkat 17 . Experiments on humming bird videos show robust performance in challenging outdoor scenes where moving foliages occur. In 2019, Salman et al. <ref type="bibr" target="#b339">[298]</ref> employed a unified approach to detect freely moving fish in unconstrained underwater environments using a Region-Based Convolutional Neural Network (R-CNN). Experiments on underwater scenes show that R-CNN outperforms KDE <ref type="bibr" target="#b52">[11]</ref>, texton-based KDE <ref type="bibr" target="#b384">[343]</ref>, and ViBe <ref type="bibr" target="#b92">[51]</ref>.</p><p>Remarks: ConvNet is the simplest way to learn the differences between the background and foreground when using a CNN. The study by Braham and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> has a significant merit of being the first application of deep learning for background subtraction, and can thus be used as a reference for comparison in terms of the improvement in performance. But, it presents several limitations: 1) It has difficulty learning high-level information through patches <ref type="bibr" target="#b354">[313]</ref>; 2) Because of an over-fitting caused by highly redundant data for training, the network is scenespecific. In practice, it can only process a certain type of scenery, and needs to be retrained for other video scenes <ref type="bibr" target="#b192">[151]</ref>. This fact is usually not a problem because the camera is fixed when filming similar scenes. However, this may not be the case for certain applications, as pointed out by Hu et al. <ref type="bibr" target="#b363">[322]</ref>; 3) Each pixel is processed independently, and the foreground mask may then contain isolated false positives and false negatives ; 4) ) It is computationally expensive owing to a large number of patches extracted from each frame, as stated by Lim and Keles <ref type="bibr" target="#b344">[303]</ref>; 5) It requires a preprocessing or post-processing of the data, and hence is not based on an end-to-end learning framework <ref type="bibr" target="#b363">[322]</ref>; 6) ConvNet uses few frames as input, and thus cannot consider the long-term dependencies of the input video sequences <ref type="bibr" target="#b363">[322]</ref>; and finally 7) ConvNet is a deep encoder-decoder network, namely, a generator network. However, a classical generator network produces blurry foreground regions, and such networks cannot preserve the object edges because they minimize the classical loss functions (e.g., Euclidean distance) between the predicted output and the ground-truth <ref type="bibr" target="#b354">[313]</ref>. Since the introduction of this valuable work, posterior methods developed in the literature have attempted to alleviate these limitations, which are the main challenges to the use of a DNN in background subtraction. Table <ref type="table" target="#tab_1">3</ref> shows a comparative overview with all the posterior methods whereas Table <ref type="table" target="#tab_2">4</ref> show an overview in terms of the challenges. These tables are discussed in detail in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-scale and cascaded CNNs</head><p>In 2016, Wang et al. <ref type="bibr" target="#b198">[157]</ref> proposed a deep learning method for an iterative ground-truth generation process in the context of background modeling algorithms validation. In order to yield the ground truths, this method segments the foreground objects by learning the appearance of foreground samples. Figure <ref type="figure" target="#fig_17">18</ref> illustrates the pipeline. First, Wang et al. <ref type="bibr" target="#b198">[157]</ref> designed basic CNN and the multi-scale CNN which processed each pixel independently based on the information contained in their local patch of size 31 √ó 31 in each channel RGB. The basic CNN model consists of 4 convolutional layers and 2 fully connected layers. The first 2 convolutional layers come with 2 √ó 2 max pooling layer. Each convolutional layer uses a filter size of 7 √ó 7 and Rectified Linear Unit (ReLU) as the activation function. By considering the CNN output as a likelihood probability, a cross entropy loss function is employed for training. Figure <ref type="figure" target="#fig_18">19</ref> shows the corresponding basic CNN architecture. Because, this basic model processes patches of size 31 √ó 31, its performance is limited to distinguish foreground and background objects with the same size or less. This limitation is alleviated using a multi-scale CNN model, which gives three outputs of three different sizes that are further combined in the original size. Figure <ref type="figure" target="#fig_19">20</ref> shows the multi-scale CNN architecture. To model the dependencies among adjacent pixels and thus enforce the spatial coherence, Wang et al. <ref type="bibr" target="#b198">[157]</ref> employed a multi-scale CNN model with a cascaded architecture, called a cascaded CNN. A CNN has the advantage of learning or extracting its own features, which may be better than hand-designed features. To learn the foreground features, a CNN is fed with manually generated foreground objects from some frames of a video sequence. After this step, the CNN employs generalization to segment the remaining frames of the video. Wang et al. <ref type="bibr" target="#b198">[157]</ref> trained scene specific networks using 200 frames by manual selection. cascaded CNN provides an overall F-Measure of 0.9209 in CDnet2014 dataset <ref type="bibr" target="#b76">[35]</ref>. For the cascaded CNN's implementation 18 available online, Wang et al. <ref type="bibr" target="#b198">[157]</ref> used the Caffe library 19  <ref type="bibr" target="#b289">[248]</ref> and MatConvNet 20 . The limitations of cascaded CNN are as follows: 1) it is more dedicated to ground-truth generation than an automated background/foreground separation method, and 2) it is also computationally expensive.     In 2018, Lim and Keles <ref type="bibr" target="#b344">[303]</ref> proposed a method called FgSegNet-M 21 based on a triplet CNN and a transposed convolutional neural network (TCNN) attached to the end of the network in an encoder-decoder structure. Figure <ref type="figure" target="#fig_20">21</ref> illustrates the FgSegNet architecture. Practically, the four blocks of the pre-trained VGG-16 <ref type="bibr" target="#b234">[193]</ref> are employed at the beginning of the proposed CNN under a triplet framework as a multiscale feature encoder. Furthermore, a decoder network is integrated at the end to map the features to a pixel-level foreground probability map. A threshold is then applied to this map to obtain binary segmentation labels. Figure <ref type="figure" target="#fig_21">22</ref> shows the architecture of each CNN in the triplet network. The first four blocks are modified copies of the pre-trained VGG-16 <ref type="bibr" target="#b234">[193]</ref>. In addition, the third and fourth max pooling layers were removed and dropouts between each layer of fourth convolutional block were inserted. Figure <ref type="figure" target="#fig_21">22</ref> illustrates the TCNN architecture. The output of the encoding network is a concatenated form of the feature maps in three different scales. This map is fed to the TCNN to learn the weights for decoding the feature maps. Finally, the output will be a dense probability mask. Practically, Lim and Keles <ref type="bibr" target="#b344">[303]</ref> generated scene specific models using only a few frames (to 50 up to 200) similar to Wang et al. <ref type="bibr" target="#b198">[157]</ref>. Experimental results <ref type="bibr" target="#b344">[303]</ref> show that TCNN outperforms both ConvNet <ref type="bibr" target="#b194">[153]</ref> and cascaded CNN <ref type="bibr" target="#b198">[157]</ref>, and practically outperformed all the reported methods by an overall F-Measure of 0.9770. In a further study, Lim and Keles <ref type="bibr" target="#b345">[304]</ref> designed a variant of FgSegNet-M called FgSegNet-S by adding a Feature Pooling Module (FPM) which operates on top of the final encoder (CNN) layer. In an additional study, Lim et al. <ref type="bibr" target="#b346">[305]</ref> proposed an improved architecture called FgSegNet-V2. Figure <ref type="figure" target="#fig_22">23</ref> illustrates the FgSegNet-V2 architecture. Lim et al. <ref type="bibr" target="#b346">[305]</ref> also provided a modified FPM module with feature fusion. Figure <ref type="figure" target="#fig_23">24</ref> shows both the FPM module of the FgSegNet-S and the modified FPM module of FgSegNet-V2. FgSegNet-V2 22  ranks number one on the CDnet 2014 dataset.</p><p>These previous methods usually require a large amount of densely labeled video training data. To solve this problem, Liao et al. <ref type="bibr" target="#b347">[306]</ref> designed a multi-scale cascaded scene-specific (MCSS) CNN-based background subtraction method with a novel training strategy. The architecture combines ConvNets <ref type="bibr" target="#b194">[153]</ref> and the multiscale-cascaded architecture <ref type="bibr" target="#b198">[157]</ref> using a training that takes advantage of the balance of positive and negative training samples. Figure <ref type="figure" target="#fig_24">25</ref> shows the pipeline of Multi-MCSS. Experimental results show that MCSS outperforms Deep CNN <ref type="bibr" target="#b192">[151]</ref>, TCNN <ref type="bibr" target="#b204">[163]</ref> and SFEN <ref type="bibr" target="#b355">[314]</ref> with a score of 0.904 on the CDnet 2014 dataset when excluding the PTZ category.</p><p>In 2018, Liang et al. <ref type="bibr" target="#b348">[307]</ref> developed a multi-scale CNN based background subtraction method by learning a specific CNN model for each video to ensure accuracy, while avoiding manual labeling by using a guided learning scheme. First, Liang et al. <ref type="bibr" target="#b348">[307]</ref> applied the SubSENSE algorithm <ref type="bibr" target="#b93">[52]</ref> to obtain an initial foreground mask. An adaptive strategy is then applied to select reliable pixels to guide the CNN training because the outputs of SubSENSE cannot be directly used as ground truth owing to a lack of accuracy in the results. A simple strategy was also proposed     to automatically select informative frames for the guided learning. Figure <ref type="figure" target="#fig_25">26</ref> shows the pipeline for the manual labeling and the pipeline for the guided automatic scheme. Experiments on the CDnet 2014 dataset show that Guided Multi-scale CNN achieves a better F-Measure score of 0.7591 than DeepBS <ref type="bibr" target="#b192">[151]</ref> and SuBSENSE <ref type="bibr" target="#b93">[52]</ref>.</p><p>In 2018, Patil et al. <ref type="bibr" target="#b349">[308]</ref> proposed a compact multi-scale CNN for deep saliency map in order to detect moving objects. Figure <ref type="figure" target="#fig_26">27</ref> and 28 show the corresponding pipeline and architecture, respectively. First, the background image is estimated using a temporal histogram based on several input frames in order to generate the saliency map. Second, a compact multi-scale encoder-decoder network is used to learn multi-scale semantic feature of estimated saliency to obtain the foreground masks. Practically, the encoder allows to extract multi-scale features from multi-scale saliency map and the decoder allows to learn the mapping of low resolution multi-scale features into high resolution output frame. Experimental results show that MsEDNet outperforms SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, DeepBS <ref type="bibr" target="#b192">[151]</ref>, SFEN <ref type="bibr" target="#b355">[314]</ref> with VGG16, SFEN+PSL <ref type="bibr" target="#b355">[314]</ref> with VGG16 and SFEN+PSL+CRF <ref type="bibr" target="#b355">[314]</ref> with VGG16 on the CDnet 2014 dataset when excluding the four challenging "LFR", "NVD", "PTZ", and "TBL" categories.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fully CNNs</head><p>Cinelli <ref type="bibr" target="#b195">[154]</ref> proposed a similar method to that of Braham and Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> by exploring the advantages of Fully Convolutional Neural Networks (FCNNs) <ref type="bibr" target="#b375">[334]</ref> to diminish the computational requirements. A FCNN uses a convolutional layer to replace the fully connected layer in traditional convolution networks, which can avoid the disadvantages caused by a fully connection layer. Cinelli tested both the LeNet5 <ref type="bibr" target="#b372">[331]</ref> and ResNet <ref type="bibr" target="#b237">[196]</ref> architectures. Because the ResNet presents a greater degree of hyperparameter setting (namely, the size of the model and even the layer organization) compared to LeNet5, Cinelli also used different features of the ResNet architectures for optimization of the background/foreground separation. To do so, Cinelli used networks designed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC <ref type="foot" target="#foot_20">23</ref> ), which deal with 224 √ó 224 pixel images, and those for the CIFAR-10 and CIFAR-100 datasets <ref type="foot" target="#foot_21">24</ref> , which have 32 √ó 32 pixel-images as input. The FAIR <ref type="foot" target="#foot_22">25</ref> implementation is employed. From this study, the best models on the CDnet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> are the 32-layer CIFAR-derived dilated network and the pre-trained 34-layer ILSVRC-based dilated model adapted through direct substitution. However, Cinelli <ref type="bibr" target="#b195">[154]</ref> only provided visual results without an F-measure score.</p><p>In another study, Yang et al. <ref type="bibr" target="#b350">[309]</ref> also used a FCNN but with a structure of shortcut connected block with multiple branches. Each block provides four different branches. Figure <ref type="figure" target="#fig_28">29</ref> shows the structure of the FCNN for background modeling. The front of three branches is used to calculate different features by applying a different atrous convolution, and the last branch is the shortcut connection. Figure <ref type="figure" target="#fig_29">30</ref> shows the shortcut connected block with multiple branches. For the spatial information, atrous convolution <ref type="bibr" target="#b385">[344]</ref> is employed instead of a common convolution to avoid considerable details by expanding the receptive fields. For the activation layers, PReLU Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b252">[211]</ref> was introduced as a learned parameter to transform values of less than zero. Yang et al. <ref type="bibr" target="#b350">[309]</ref> also employed a refinement method using Conditional Random Fields (CRF). Experimental results show that this method outperforms traditional background subtraction methods (MOG <ref type="bibr" target="#b54">[13]</ref> and Codebook <ref type="bibr" target="#b386">[345]</ref>) as well as recent state-of-art methods (ViBe <ref type="bibr" target="#b92">[51]</ref>, PBAS <ref type="bibr" target="#b387">[346]</ref> and P2M <ref type="bibr" target="#b388">[347]</ref>) on the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref>. But, Yang et al. <ref type="bibr" target="#b350">[309]</ref> evaluated their method on a subset of 6 sequences of CDnet 2012 <ref type="bibr" target="#b75">[34]</ref> instead of all the categories of CDnet 2014 <ref type="bibr" target="#b76">[35]</ref> making a comparison with other DNN methods more difficult to apply.</p><p>In 2018, Akilan <ref type="bibr" target="#b351">[310,</ref><ref type="bibr" target="#b389">348,</ref><ref type="bibr" target="#b359">318]</ref> designed a Multi-View receptive field Fully CNN (MV-FCN) based on fully convolutional structure, inception modules <ref type="bibr" target="#b390">[349]</ref>, and residual networking. MV-FCN is based on inception module <ref type="bibr" target="#b236">[195]</ref> designed by Google that performs convolution of multiple filters with different scales on the same input to simulate human cognitive processes in perceiving multi-scale information, and ResNet <ref type="bibr" target="#b237">[196]</ref> developed by Microsoft that acts as lost feature recovery mechanism. In addition, Akilan <ref type="bibr" target="#b351">[310]</ref> exploits intra-domain transfer learning that boosts the correct foreground region prediction. Figure <ref type="figure" target="#fig_30">31</ref> shows the MV-FCN architecture. MV-FCN consists of two Complementary Feature Flows (CFF) and a Pivotal Feature Flow (PFF). The PFF is essentially an encoder-decoder CNN whereas CFF1 and CFF2 complement its learning ability. The PFF only employs convolution kernels size of 3 √ó 3, whereas CFF1 and CFF2 uses filters size of 5 √ó 5 and 9 √ó 9 respectively in their first conv layers. Practically, MV-FCN employs inception modules at early and late stages with three different sizes of receptive fields to capture invariance at various scales. The features learned in the encoding phase are fused with appropriate feature maps in the decoding phase through residual connections for achieving enhanced spatial representation. These multi-view receptive fields and residual feature connections provide generalized features for a more accurate pixel-wise foreground region identification. The training is made using the CDnet 2014 <ref type="bibr" target="#b76">[35]</ref>. Akilan et al. <ref type="bibr" target="#b351">[310]</ref> evaluated MV-FCN against classical neural networks (Stacked Multi-Layer <ref type="bibr" target="#b391">[350]</ref>, Multi-Layered SOM <ref type="bibr" target="#b147">[106]</ref>), and two deep learning approaches (SDAE <ref type="bibr" target="#b203">[162]</ref>, Deep CNN <ref type="bibr" target="#b192">[151]</ref>) on the CDnet 2014 <ref type="bibr" target="#b76">[35]</ref> but only on selected sequences making the comparison less complete. In 2018, Zeng and Zhu <ref type="bibr" target="#b352">[311]</ref> developed a Multiscale Fully Convolutional Network (MFCN) for moving object detection in infrared videos. MFCN does not need to extract the background images. The input is frames from different sequences, and the output is a probability map. Practically, Zeng and Zhu <ref type="bibr" target="#b352">[311]</ref> used the VGG-16 as architecture and the inputs have a size of 224 √ó 224. The VGG-16 network is split into five blocks with each block containing some convolution and max pooling operations (See Figure <ref type="figure" target="#fig_63">32</ref> and<ref type="figure" target="#fig_32">33</ref>). The lower blocks have a higher spatial resolution and contain more low-level local features, whereas the deeper blocks contain more high-level global features at a lower resolution. A contrast layer is added behind the output feature layer based on the average pooling operation with a kernel size of 3 √ó 3. To exploit multi-scale features from multiple layers, Zeng and Zhu <ref type="bibr" target="#b352">[311]</ref> employed a set of  deconvolution operations to up-sample the features, creating an output probability map the same size as the input. For the loss function, the cross-entropy is used. The layers from VGG-16 are initialized with pre-trained weights, whereas the other weights are randomly initialized with a truncated normal distribution. The adam optimizer method is used for updating the model parameters. Experimental results on the THM category of CDnet 2014 <ref type="bibr" target="#b76">[35]</ref> dataset show that MFCN obtains a score of 0.9870 in this category whereas cascaded CNN <ref type="bibr" target="#b198">[157]</ref> obtains 0.8958 and MFCN achieves a score of 0.96 over all the categories. In a further study, Zeng and Zhu <ref type="bibr" target="#b353">[312]</ref> provided an improved version of MFCN with contrast layers, which obtains an average measure of 0.9830 on CDnet 2014 <ref type="bibr" target="#b76">[35]</ref> dataset. In another study, Zeng and Zhu <ref type="bibr" target="#b197">[156]</ref> fused the results produced by different background subtraction algorithms (SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, FTSG <ref type="bibr" target="#b392">[351]</ref>, and CwisarDH+ <ref type="bibr" target="#b328">[287]</ref>) in order to output a more precise result. This method called CNN-SFC outperforms its direct competitor IUTIS <ref type="bibr" target="#b383">[342]</ref> on the CDnet 2014 dataset.</p><p>In 2018, Lin et al. <ref type="bibr" target="#b354">[313]</ref> designed a deep Fully Convolutional Semantic Network (FCSN) for background subtraction. First, an FCN can learn the global differences between the foreground and the background. Second, SuBSENSE <ref type="bibr" target="#b93">[52]</ref> algorithm is employed to generate robust background image with better performance, which is concatenated into the input of the network together with the video frame. Furthermore, Lin et al. <ref type="bibr" target="#b354">[313]</ref> initialized the weights of FCSN by partially using pre-trained weights of FCN-VGG16, because these weights are applied to semantic segmentation. Then, FCSN can understand semantic information of images and converge faster. In addition, FCSN uses less training data and get better result with the help of pre-trained weights. Figure <ref type="figure" target="#fig_33">34</ref> shows the FCSN architecture. For two input images with a current frame and a background image, corresponding output image with foreground obtained by proposed fully convolutional networks model. FCSN contains 20 convolutional layers and 3 deconvolutional lay- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Deep CNNs</head><p>In 2017, Babaee et al. <ref type="bibr" target="#b192">[151]</ref> proposed a deep CNNs based moving objects detection method that contains the following components: an algorithm for background initialization via an average model in RGB, a CNN model for background subtraction, and a post-processing module of the networks output using a spatial median filter. First, Babaee et al. <ref type="bibr" target="#b192">[151]</ref> proposed distinguishing the foreground and background pixels using the SuBSENSE algorithm <ref type="bibr" target="#b93">[52]</ref>, and then only use the background pixel values to obtain the background averaging model. To achieve an adaptive memory length based on the motion of the camera and objects in the video frames, Babaee et al. <ref type="bibr" target="#b192">[151]</ref> used Flux Tensor with Split Gaussian Models (FTSG <ref type="bibr" target="#b392">[351]</ref>) algorithm. For the network architecture and training, Babaee et al. <ref type="bibr" target="#b192">[151]</ref> trained the CNNs with background images obtained by the SuBSENSE algorithm <ref type="bibr" target="#b93">[52]</ref>. With images of size 240 √ó 320 pixels, the network is trained with pairs of RGB image patches (triplets of size 37 √ó 37) from video, background frames and the respective ground truth segmentation patches (CDnet 2014 <ref type="bibr" target="#b76">[35]</ref> with around 5% of the data). Thus, instead of training a network for a specific scene, Babaee et al. <ref type="bibr" target="#b192">[151]</ref> trained their model all at once by combining training frames from various video sequences including 5% of frames from each video sequence. On the other hand, the same training procedure than ConvNet <ref type="bibr" target="#b194">[153]</ref> is employed. Each image-patches are combined with background-patches then fed to the network. The network contains 3 convolutional layers and a 2-layer Multi-Layer Perceptron (MLP). Rectified Linear Unit (ReLU) <ref type="bibr" target="#b288">[247]</ref> is used as activation function after each convolutional layer and the sigmoid function after the last fully connected layer. In addition, batch normalization layers are used before each activation layer to decrease over-fitting and to also provide higher learning rates for training. Finally, a spatialmedian filtering is applied in the post-processing step. This method provided foreground mask more precise than ConvNet <ref type="bibr" target="#b194">[153]</ref> and not very prone to outliers in presence of dynamic backgrounds. Finally, deep CNN based background subtraction outperforms the existing algorithms when the challenge does not lie in the background modeling maintenance. Deep CNN obtained an F-Measure score of 0.7548 in CDnet2014 dataset <ref type="bibr" target="#b76">[35]</ref>. The limitations of Deep CNN are as follows: 1) It cannot handle the camouflage regions well within foreground objects, 2) It provides a poor performance on PTZ video sequences, and 3) owing to the corruption of the background images, it performs poorly in presence of large changes in the background.</p><p>In a further study, Zhao et al. <ref type="bibr" target="#b204">[163]</ref> proposed an end-to-end two-stage deep CNN (TS-CNN) framework. Figure <ref type="figure" target="#fig_34">35</ref> shows the pipeline of TS-CNN. The current frame is the input of the network to reconstruct the background. The reconstructed background image is then concentrated to the current frame and fed into the following fully convolutional network to obtain the foreground mask. More precisely, a convolutional encoder-decoder sub-network is used to reconstruct the background images and encode rich prior knowledge of the background scenes, whereas the reconstructed background and current frame are the inputs into a multi-channel fully convolutional sub-network for accurate foreground detection in the second stage. In the two-stage CNN, the reconstruction and segmentation losses are jointly optimized. The encoder contains a set of convolutions, and represents the input image as a latent feature vector. The decoder restores the background image from the feature vector. The l 2 loss was employed as  the reconstruction loss. After training, the encoder-decoder network separates the background from the input image and restores a clean background image. The second network can learn semantic knowledge of the foreground and background. Therefore, it could handle various challenges such as the nighttime lighting, shadows and camouflaged foreground objects. Experimental results <ref type="bibr" target="#b204">[163]</ref> show that the TS-CNN outperforms SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, PAWCS <ref type="bibr" target="#b94">[53]</ref>, FTSG <ref type="bibr" target="#b392">[351]</ref> and SharedModel <ref type="bibr" target="#b393">[352]</ref> in the case of night videos, camera jitter, shadows, thermal imagery and bad weather. In CDnet2014 dataset <ref type="bibr" target="#b76">[35]</ref>, TS-CNN and Joint TS-CNN obtained an F-Measure score of 0.7870 and 0.8124, respectively.</p><p>In 2017, Li et al. <ref type="bibr" target="#b343">[302]</ref> designed an adaptive deep CNN (ADCNN) to predict object locations in a surveillance scene. Figure <ref type="figure" target="#fig_35">36</ref> illustrates the pipeline of ADCNN. First, the current image is the input into the transferred CNN, which outputs 256 feature maps. The 256 feature maps are then forward propagated using several context CNNs. Thus, an equal number of object masks at their corresponding scales are generated. Finally, the detection results are obtained by merging the bounding boxes, which are estimated on object masks. More precisely, a generic CNNbased classifier is transferred to the surveillance scene by selecting useful kernels. The context information of the surveillance scene is then learned using the regression model for an accurate location prediction. Although they focus on object detection and thus do not use the principle of background subtraction, ADCNNs have achieved very interesting performance on several surveillance datasets for pedestrian detection and vehicle detection. Furthermore, Li et al. <ref type="bibr" target="#b343">[302]</ref> provided results with the CUHK square dataset <ref type="bibr" target="#b394">[353]</ref>, the MIT traffic dataset <ref type="bibr" target="#b395">[354]</ref> and the PETS 2007 <ref type="foot" target="#foot_23">26</ref> instead of the CDnet2014 dataset <ref type="bibr" target="#b76">[35]</ref>.</p><p>In 2017, Chen et al. <ref type="bibr" target="#b355">[314]</ref> proposed the detection of moving objects using an end-to-end deep sequence learning architecture with the pixel-level Semantic Features (SFEN). Figure <ref type="figure" target="#fig_36">37</ref> shows the pipeline of SFEN. Video sequences are the input into a deep convolutional encoder-decoder network to extract pixel-level Semantic Features (SFEN). Practically, Chen et al. <ref type="bibr" target="#b355">[314]</ref> used the VGG-16 <ref type="bibr" target="#b234">[193]</ref> as encoder-decoder network, although other architectures, such as GoogLeNet <ref type="bibr" target="#b390">[349]</ref>, ResNet50 <ref type="bibr" target="#b237">[196]</ref> can also be used in this framework. An attention long short-term memory model called Attention ConvLSTM is used to integrate pixel-wise changes over time. A Spatial Transformer Network (STN) model and a Conditional Random Fields (CRF) layer are then employed to reduce the sensitivity to camera motion and  to smooth the foreground boundaries, respectively. Experimental results <ref type="bibr" target="#b355">[314]</ref> on the two large-scale dataset CDnet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> and LASIESTA <ref type="bibr" target="#b396">[355]</ref> indicate that the proposed method obtained similar results as Convnet <ref type="bibr" target="#b194">[153]</ref> with a better performance for the category "Night videos", "Camera jitter", "Shadow" and "Turbulence". Attention ConvLSTM obtained an F-Measure score of 0.8292 with VGG-16, 0.7360 with GoogLeNet and 0.8772 with ResNet50 as can be seen in Table <ref type="table">12</ref>.</p><p>In 2018, Patil and Murala <ref type="bibr" target="#b356">[315]</ref> designed a compact end-to-end convolutional neural network architecture called motion saliency foreground network (MSFgNet) in order to estimate the background and to extract the foreground from video frames. Figure <ref type="figure" target="#fig_37">38</ref> shows the pipeline of MSFgNet. First, a long video is divided into a number of small video streams (SVS) that are the input of MSFgNet which estimates the background frame for each SVS. Second, the saliency map is obtained using the estimated background and the current frame. In addition, a compact encoderdecoder network extracts the foreground from the estimated saliency maps. In practice, MSFgNet consists of two main networks: 1) a Motion-saliency network (MSNet) composed of a Background Estimation Network (BENet) and Saliency Estimation Network (SMNet), and 2) a Foreground extraction network (FgNet). Figure <ref type="figure" target="#fig_38">39</ref> shows the MSFgNet architecture. However, MSFgNet handles approximately 168 and 87 times less parameters compared to cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and SFEN <ref type="bibr" target="#b355">[314]</ref>, respectively. MSFgNet also obtains better performance compared to cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and SFEN <ref type="bibr" target="#b355">[314]</ref> in terms of the average F-measure score on the CDnet 2014 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Structured CNNs</head><p>In 2017, Lim et al. <ref type="bibr" target="#b196">[155]</ref> developed an encoder-decoder structured CNN (Struct-CNN) for background subtraction. Thus, the background subtraction model involves the following components: a background image extraction via a temporal median in RGB, network training, background subtraction and foreground extraction based on super-pixel information. Figure <ref type="figure" target="#fig_39">40</ref> illustrates the structure of Struct-CNN. The structure is thus similar to the VGG16 network <ref type="bibr" target="#b234">[193]</ref> after excluding the fully connected layers. The encoder converts the 3 (RGB) channel input (images of size 336√ó336 pixels) into 512-channel feature vector through convolutional and max-pooling layers yielding a 21√ó21√ó512  feature vector. Then, the decoder converts the feature vector into a 1-channel image of size 336 √ó 336 pixels providing the foreground mask through deconvolutional and unpooling layers. Lim et al. <ref type="bibr" target="#b196">[155]</ref> trained this encoder-decoder structured network in the end-to-end manner using CDnet 2014 <ref type="bibr" target="#b76">[35]</ref>. For the architecture, the decoder consists of six deconvolutional layers and 4 unpooling layers. In all deconvolutional layers, except for the last one, features are batch-normalized and the Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b385">[344]</ref> is employed as an activation function. The last deconvolutional layer which is the prediction layer used the sigmoid activation function to normalize outputs and then to provide the foreground mask. 5 √ó 5 kernels are used in all convolutional while a 3 √ó 3 kernel is employed in the prediction layer. In order to suppress the incorrect boundaries and holes in the foreground mask, Lim et al. <ref type="bibr" target="#b196">[155]</ref> used the superpixel information obtained by an edge detector. Experimental results <ref type="bibr" target="#b196">[155]</ref> show that Struct-CNN outperforms SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, PAWCS <ref type="bibr" target="#b94">[53]</ref>, FTSG <ref type="bibr" target="#b392">[351]</ref> and SharedModel <ref type="bibr" target="#b393">[352]</ref> in the case of bad weather, camera jitter, low frame rate, intermittent object motion and thermal imagery. Struct-CNN obtained an F-Measure score of 0.8645 on the CDnet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> excluding the "PTZ" category. Lim et al. <ref type="bibr" target="#b196">[155]</ref> excluded this category, arguing that they focused only on static cameras.</p><p>Le and Pham <ref type="bibr" target="#b357">[316]</ref> also proposed an encoder-decoder structured CNN for background subtraction. In the encoder, features of both the target frame and background frame are extracted and then subtracted to obtain the foreground mask. Le and Pham <ref type="bibr" target="#b357">[316]</ref> also combined features of target frame passed from the low-lever block CNN through skip connection to enhance the representation of changing description. Next, the decoder part estimates the change map with finest resolution. Experimental results provided only on several challenging videos of the CDnet 2014 dataset, show that EDS-CNN outperforms bothe SubSENSE <ref type="bibr" target="#b93">[52]</ref> and DeepBS <ref type="bibr" target="#b192">[151]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Double Encoding-Slow Decoding CNNs</head><p>In 2018, Akilan and Wu <ref type="bibr" target="#b358">[317]</ref> proposed a strategy called Double Encoding-Slow Decoding (DESD) to improve a basic encoder-decoder CNN. This method has also been called sEnDec by Akilan <ref type="bibr" target="#b359">[318]</ref>, and by Akilan and Wu <ref type="bibr" target="#b397">[356]</ref>. The DESD EnDec CNN consists of two sub-networks, as shown in Figure <ref type="figure" target="#fig_40">41</ref>, namely, encoding and decoding networks. Both networks exploit structured residual feature fusions. Instead of ConvNets <ref type="bibr" target="#b194">[153]</ref>, DeepBS <ref type="bibr" target="#b192">[151]</ref>, FCNN <ref type="bibr" target="#b350">[309]</ref> and Struct-CNN <ref type="bibr" target="#b196">[155]</ref>, this architecture does not use any pooling or hidden FC layers, but subsumes conv, transpose convolution (convT), and cat layers, which are interconnected to capture spatio-temporal contextual cues of moving objects. An input feature map applied at the sub-sampling stage is encoded twice before reaching to the next level of reduced spatial dimension. This process works as a micro auto-encoder. In the up-sampling subnetwork, each spatial dimension of decoded feature maps is improved using two sets of residual feature cat operations interspersed with a BN, thereby fusing two individual encoded feature maps from the sub-sampling stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">3D-CNNs</head><p>In 2017, Sakkos et al. <ref type="bibr" target="#b360">[319]</ref> designed an end-to-end 3D-CNNs to track temporal changes in video sequences avoiding the use of a background model for the training. Here, 3D-CNNs can handle multiple scenes without further fine-tuning on each scene individually. Figure <ref type="figure" target="#fig_41">42</ref> illustrates the 3D-CNNs architecture. More precisely, Sakkos et al. <ref type="bibr" target="#b360">[319]</ref> used C3D branch <ref type="bibr" target="#b381">[340]</ref>. The input employs a video of ten frames connected to the first group of layers (CRP-1) in groups of four frames with stride 2. CRP-1 is then connected to CRP-2 in the same manner and CRP-3 has access to the features of all frames. CRP-4 is performing 2D operations only, whereas CR has no pooling layer. The upsampling layers (US-1, US-2, US-3 and US-4) are connected to CRP-2, CRP-3, CRP-4 and CR, respectively. Then, they are concatenated before applying the final convolution. Experimental results <ref type="bibr" target="#b360">[319]</ref> reveal that 3D-CNN provides a better performance than ConvNet <ref type="bibr" target="#b194">[153]</ref> and deep CNN <ref type="bibr" target="#b192">[151]</ref>. Furthermore, experiments on the ESI dataset <ref type="bibr" target="#b398">[357]</ref>, which presents extreme and sudden changes in illumination, show that 3D-CNN outperforms two designed illumination invariant background subtraction methods that are Universal Multimode Background Subtraction (UMBS) <ref type="bibr" target="#b399">[358]</ref> and ESI <ref type="bibr" target="#b398">[357]</ref>. 3D-CNNs obtained an average F-Measure score of 0.9507 in CDnet 2014 dataset. In 2018, Gao et al. <ref type="bibr" target="#b361">[320]</ref> also employed 3D-CNNs for background subtraction. Figure <ref type="figure" target="#fig_42">43</ref> shows the comparison between a 2D convolution operation and a 3D convolution operation demonstrating the advantage of a 3D convolution for the background subtraction task. Figure <ref type="figure" target="#fig_43">44</ref> illustrates the 3D CNNs architecture. Practically, Gao et al. <ref type="bibr" target="#b361">[320]</ref> only provided experimental results on several sequences of the CDnet 2012 dataset, making it more difficult to compare their algorithm than had the results been provided on the CDnet 2014 dataset.</p><p>In 2018, Yu et al. <ref type="bibr" target="#b362">[321]</ref> employed a spatial-temporal attention-based 3D ConvNets to jointly model the appearance and motion of objects-of-interest in a video for a Relevant Motion Event detection Network (ReMotENet). Figure <ref type="figure" target="#fig_44">45</ref> shows the ReMotENet architecture. The input is a 4D representation of a video and the outputs are binary predictions of relevant motion involving different moving objects. The architecture is based on the C3D branch <ref type="bibr" target="#b381">[340]</ref>. However, instead of using max pooling both spatially and temporally, Yu et al. <ref type="bibr" target="#b362">[321]</ref> separated the spatial and temporal max pooling to capture fine-grained temporal information, and deepen the network to learn better representations. Experimental results demonstrate that ReMotENet achieves a comparable or even better performance, and is three-to four-orders of magnitude faster than the object detection based method. It can detect relevant motion in a 15s video in 4 -8 milliseconds on a GPU and a fraction of second on a CPU with model size of less than 1 MB.</p><p>In another study, Hu et al. <ref type="bibr" target="#b363">[322]</ref> developed a 3D atrous CNN model to learn deep spatial-temporal features without losing resolution information. Figure <ref type="figure" target="#fig_45">46</ref> shows the architecture of the 3D atrous CNN model, whereas Figure      47 shows how the 3D atrous ConvLSTM network at time steps t -1, t and t + 1. Figure <ref type="figure" target="#fig_47">48</ref> illustrates of 3D atrous convolution demonstrating its interest for the background subtraction task. More precisely, this model is combined with two convolutional long short-term memory (ConvLSTM) networks in order to capture both short-and long-term spatiotemporal information of the input video data. Furthermore, 3D Atrous ConvLSTM is a completely end-to-end framework that does not require any pre-or post-processing of the data. Experiments on CDnet 204 dataset show that 3D atrous CNN outperforms SuBSENSE <ref type="bibr" target="#b94">[53]</ref>, cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and DeepBS <ref type="bibr" target="#b192">[151]</ref>.</p><p>In 2018, Wang et al. <ref type="bibr" target="#b364">[323]</ref> proposed a multi-scale 3D Fully CNN (MFC3D) architecture in order to learn multiscale features in both spatial and temporal domains. The MFC3D uses an encoder-decoder structure. Figure <ref type="figure" target="#fig_48">49</ref> shows the architecture of MFC3D. The input of the network is a video with 16 consecutive frames, including the current frame and 15 previous frames. The encoder extracts multiscale spatial-temporal features, namely, two spatial scale and two temporal scale features from the input sequences, whereas the decoder merges the features to reconstruct the pixel-wise detection result, which is the probability of each pixel belong to the foreground. The probability is then thresholded to obtain the foreground mask. Therefore, the network establishes a mapping from a video sequence to the pixel-wise classification results. Experiments on CDnet 204 dataset show that MFC3D obtains better a F-Measure score than cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and DeepBS <ref type="bibr" target="#b192">[151]</ref> over all categories. MFC3D reaches an average F-measure score 0.9619 whereas FC3D (MFC3D without multi-scale process) obtains a score of 0.9524.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Retrospective Convolutions</head><p>Chen et al. <ref type="bibr" target="#b365">[324]</ref> proposed the use of retrospective convolutions to avoid the temporal limitation of 3D CNNs. Retrospective convolution directly links the current frame to any previous frame and detects instantaneous changes. Figure <ref type="figure" target="#fig_49">50</ref> illustrates the comparison between 3D convolution, retrospective convolution and atrous retrospective convolution. The 3D convolution kernel of works on three consecutive frames, and a frame can not be linked directly to another one with more than 2-frame interval. A retrospective convolution kernel of spatial size relate the current frame to each of all preceding frames. An atrous retrospective convolution kernel with dilation expands the FoV from 3 √ó 3 to 5 √ó 5. An Atrous Retrospective Pyramid Pooling (ARPP) module is further employed to enhance retrospective convolution with multi-scale field-of-views. Figure <ref type="figure" target="#fig_50">51</ref> shows the architecture based on ResNet-18, ARPP and multilevel encoder-decoder modules. To address the problem of foreground-specific overfitting in learning-based methods, Chen et al. <ref type="bibr" target="#b365">[324]</ref> employed a data augmentation method called static sample synthesis which guides the network to focus on learning change-cued information rather than specific spatial features of foreground. Finally, an end-to-end framework allows to fuse change features of different scales and realizes pixel-wise prediction. Experimental results provided on several challenging videos of the CDnet 2014 dataset show that ResNet-18 + ARPP outperforms MOG <ref type="bibr" target="#b54">[13]</ref>, ViBe <ref type="bibr" target="#b92">[51]</ref> and SuBSENSE <ref type="bibr" target="#b94">[53]</ref>.   For the input of the CNNs, Zhao et al. <ref type="bibr" target="#b340">[299]</ref> employed Random Permutation of Temporal Pixels (RPoTP) features instead of using the intensity values, as in the previous methods. Figure <ref type="figure" target="#fig_52">52</ref> illustrates the RPoTP features used to represent the distribution of past observations for a particular pixel, in which the temporal correlation between observations is deliberately no ordered over time. The RPoTP features from all pixels are fed into the convolutional neural network to learn a classifier to achieve background subtraction. A convolutional neural network (CNN) is then used to learn the distribution and thereby determine whether the current observation is foreground or background. The random permutation allows the framework to focus primarily on the distribution of observations, rather than be disturbed by spurious temporal correlations. For a large number of RPoTP features, the pixel representation is captured even with a small number of ground-truth frames. Figure <ref type="figure" target="#fig_53">53</ref> shows the architecture of DPDL. Experiments on the CDnet 2014 dataset show that DPDL is effective even with only a single ground-truth frame giving similar performance than the MOG model in this case. With 20 GTs, DPDL obtains similar scores as SubSENSE <ref type="bibr" target="#b94">[53]</ref>. Finally, DPDL <ref type="foot" target="#foot_24">27</ref> with 40 GTs achieves an average F-Measure score of 0.8106, outperforming DeepBS <ref type="bibr" target="#b192">[151]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.2.">Depth feature</head><p>Wang et al. <ref type="bibr" target="#b341">[300]</ref> proposed the use of a BackGround Subtraction neural Networks for Depth videos (BGSNet-D) to detect moving objects in scenes in which the color information cannot be obtained. Thus, BGSNet-D is suitable for dark scenes, where the color information is difficult to obtain. CNNs can extract features in color images, but cannot be applied to depth images directly because edge noises occur and there is an absence of pixels in the captured data. To address this problem, Wang et al. <ref type="bibr" target="#b341">[300]</ref> designed an extended min-max normalization method to pre-process the depth images. After pre-processing, the two inputs of the CNNs are the average background image in depth and the current image. The architecture is therefore similar to that of ConvNets with three convolutional layers. In each convolutional layer, a filter with 3 √ó 3 local receptive fields and a 1 √ó 1 stride is used. ReLU follows as the activation  function in hidden layers. The batch normalization layer and pooling layer are both applied after each ReLU layer. Finally, all feature maps are employed as inputs of an MLP, which contains three fully connected layers. A sigmoid is used as an activation function, and the output only consists of a single unit. Experiments on the SBM-RGBD <ref type="foot" target="#foot_25">28</ref>dataset <ref type="bibr" target="#b327">[286]</ref>show that BGSNet-D outperforms existing methods that use only the depth data, and even reaches a level of performance similar to those methods that use RGB-D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Generative Adversarial Networks</head><p>In 2018, Bakkay et al. <ref type="bibr" target="#b366">[325]</ref> proposed a background subtraction method based on conditional Generative Adversarial Network (cGAN). Figure <ref type="figure" target="#fig_3">54</ref> shows the pipeline of this model, called BScGAN, which consists of two successive networks: generator and discriminator networks. Figure <ref type="figure" target="#fig_54">55</ref> shows the cGAN architecture. The generator learns the mapping from the background and the current image for the foreground mask. The discriminator then learns a loss function to train this mapping by comparing the ground truth and predicted output by observing the input image and background. For the architecture, the generator network follows the encoder-decoder architecture of Unet network with skip connections <ref type="bibr" target="#b382">[341]</ref>. The encoder part includes down-sampling layers that decrease the size of the feature maps followed by convolutional filters. It consists of eight convolutional layers. The first layer uses a 7 √ó 7 convolution to provide 64 feature maps. The 8th layer generates 512 feature maps with a 1 √ó 1 size. Their weights are randomly initialized. In addition, the six middle convolutional layers are ResNet blocks. In all encoder layers, leaky-ReLU non-linearities are used. The decoder part uses up-sampling layers followed by deconvolutional filters to construct an output image with the same resolution as the input image. Its architecture is similar to that of the encoder, including eight deconvolutional layers, but with reverse layer ordering and down-sampling layers being replaced by up-sampling layers. For the discriminator network, the architecture is composed of four convolutional and downsampling layers. The first layer generates 64 feature maps. Moreover, the fourth layer generates 512 feature maps Figure <ref type="figure" target="#fig_3">54</ref>. Pipeline of BScGAN. For training, G provides a fake foreground mask ≈∑ from each input x. D learns to discriminate between a fake example ≈∑ and a real example y under condition of x. Backpropagating D and G leads to provide better masks. For testing, G outputs a foreground mask y 0 from each input x 0 (Image from Bakkay et al. <ref type="bibr" target="#b366">[325]</ref>). with a 30 √ó 30 size. The convolutions are 3 √ó 3 spatial filters and their corresponding weights are randomly initialized. Leaky ReLU functions are employed as activation functions. Experimental results on CDnet 2014 datasets shows that BScGAN outperforms ConvNets <ref type="bibr" target="#b194">[153]</ref>, cascaded CNN <ref type="bibr" target="#b198">[157]</ref>, and Deep CNN <ref type="bibr" target="#b192">[151]</ref> with an average F-Measure score of 0.9763 when excluding the "PTZ" category.</p><p>In 2018, Zheng et al. <ref type="bibr" target="#b367">[326]</ref> employed a Bayesian GAN (BGAN) approach. First, a median filter algorithm is used to extract the background, and a network based on a BGAN is then trained to classify each pixel, thereby dealing with the challenges of sudden and slow illumination changes, a non-stationary background, and ghosting. Deep CNNs are adopted to construct the generator and discriminator of a BGAN. In a further study, Zheng et al. <ref type="bibr" target="#b368">[327]</ref> proposed a parallel version of the BGAN algorithm called (BPVGAN).</p><p>In 2018, Bahri et al. <ref type="bibr" target="#b369">[328]</ref> designed an end-to-end framework called Neural Unsupervised Moving Object Detection (NUMOD), which is based on a batch method named ILISD <ref type="bibr" target="#b400">[359]</ref>. NUMOD can work in either online or batch mode thanks to the parametrization through a generative neural network. NUMOD decomposes each frame into three parts: changes in the background, foreground, and illumination. It uses a fully connected generative neural network to generate a background model by finding a low-dimensional manifold for the background of the image sequence. For the architecture, NUMOD uses two generative fully connected networks (GFCNs). Net1 estimates the background image from the input image, whereas Net2 generates a background image from an illumination-invariant image. These two networks have the exact same architecture. Thus, the input to the GFCN is an optimizable lowdimensional latent vector. Then, two fully connected hidden layers are followed by ReLU non-linearity. The second hidden layer is fully connected to the output layer, which is followed by the sigmoid function. A loss term is employed to impose the output of the GFCN to be similar to the current input frame. A GFCN is similar to the decoder part of an auto-encoder. In an auto-encoder, the low-dimensional latent code is learned by the encoder, whereas in a GFCN, it is a free parameter that can be optimized and input into the network. During training, this latent vector learns a low-dimensional manifold of the input distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11.">Encoder-Decoder Networks</head><p>CNNs can difficulty deal with temporal events in video sequences that have long-term dependencies. In particular, a dense pixel-wise prediction is a hard problem for CNNs owing to the huge memory and large numbers of parameters needed to learn the temporal correlation. To address this problem, Choo et al. <ref type="bibr" target="#b333">[292]</ref> designed in 2018 a Multi-Scale Recurrent encoder-decoder Neural Network (MSRNN), which compresses the spatio-temporal features at the encoder and restores them to the original sized results at the decoder. Figure <ref type="figure" target="#fig_55">56</ref> shows the architecture which has recurrent layers both in the encoder and decoder at each scale level. The recurrent layers are convolutional LSTM, which maintain the shapes of features. These multi-scale LSTM layers stacked with the convolutional layers enable the network to learn the temporal information from the consecutive frames and produce the dense predictions. More precisely, Choo et al. <ref type="bibr" target="#b333">[292]</ref> employed a convolutional long short-term memory (LSTM) into the encoder-decoder architecture. MSRNN successfully learns the spatio-temporal relation with a small number of parameters compared to CNNs. MSRN is trained with limited duration of video frames, and shows robustness against different challenges under different time duration. MSRNN outperforms IUTIS-5 <ref type="bibr" target="#b383">[342]</ref> and STSOM <ref type="bibr" target="#b401">[360]</ref> on CDnet 2014 dataset. In addition, Choo et al. <ref type="bibr" target="#b333">[292]</ref> studied the influence of recurrent layers through ablation showing that the performance of the architecture is then reduced as can be seen in Table <ref type="table">12</ref>. In a further study, Choo et al. <ref type="bibr" target="#b334">[293]</ref> proposed an unsupervised version of MSRNN. Figure <ref type="figure" target="#fig_56">57</ref> shows the corresponding structure which is divided into two branches. The recurrent branch learns the spatiotemporal information by stacking the convolutional LSTM in the form of multiscale encoder-decoder. The semantic branch extracts visual information from each frames. The tensors of the two branches are piled with the original resolution of the image. Then, pixels are classified as background or foreground according to the softmax value. Binary labels are then created through the augmentation. Because it is not possible to synthesize semantic and optical flow labels with unlabeled training phase video, the semantic branch is also trained for background subtraction.</p><p>In 2019, Farnoosh et al. <ref type="bibr" target="#b335">[294]</ref> designed a Deep Probabilistic Background Model (DeepPBM) based on Variational autoencoders (VAEs) <ref type="bibr" target="#b402">[361,</ref><ref type="bibr" target="#b403">362]</ref>. DeepPBM is a generative modeling of the background allowing to compute backgrounds of a specific scene in presence of illumination changes and variations in the background. However, DeepPBM is based on two main hypotheses. First, the background lies on a low-dimensional subspace represented by a series of latent variables. Second, the latent subspace of the background embedded by a non-linear mapping of the video frames fit a Gaussian distribution model. Figure <ref type="figure" target="#fig_57">58</ref> illustrated that the encoder learns an efficient representation of the input video and projects that into a stochastic lower dimensional space determined by latent variables. The decoder attempts to recover the original data, given the probabilistic latent variables from the encoder. The entire network is trained by comparing the original input data with its reconstructed output. For long-term videos, experimental results show that DeepPBM outperforms RPCA <ref type="bibr" target="#b73">[32]</ref> on the BMC 2012 dataset <ref type="bibr" target="#b77">[36]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep Learned Features</head><p>The features used play an important role in the robustness against the challenge met in a video sequence <ref type="bibr" target="#b404">[363]</ref>. Historically, low-level and hand-craft features such as color <ref type="bibr" target="#b405">[364,</ref><ref type="bibr" target="#b406">365]</ref>, edge <ref type="bibr" target="#b407">[366,</ref><ref type="bibr" target="#b408">367]</ref>, texture <ref type="bibr" target="#b409">[368,</ref><ref type="bibr" target="#b410">369]</ref>, motion <ref type="bibr" target="#b411">[370,</ref><ref type="bibr" target="#b412">371]</ref>, and depth <ref type="bibr" target="#b413">[372,</ref><ref type="bibr" target="#b414">373,</ref><ref type="bibr" target="#b415">374,</ref><ref type="bibr" target="#b416">375,</ref><ref type="bibr" target="#b63">22,</ref><ref type="bibr" target="#b417">376]</ref> features have often been employed to deal with illumination changes, dynamic background, and camouflage. However, an operator needs to be chosen <ref type="bibr" target="#b101">[60,</ref><ref type="bibr" target="#b56">15,</ref><ref type="bibr" target="#b102">61]</ref> to fuse the results derived from the different features or a feature selection scheme <ref type="bibr" target="#b418">[377,</ref><ref type="bibr" target="#b419">378]</ref>. Nevertheless, none of these approaches can finally compete with approaches based on deep learned features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Convolutional Neural Networks</head><p>Dou et al. <ref type="bibr" target="#b420">[379]</ref> proposed employing CNN features to deal with challenges met in video surveillance. First, given a cleaned background image without moving objects, Dou et al. <ref type="bibr" target="#b420">[379]</ref> constructed adjustable neighborhood of each pixel in the background image to form windows. The CNN features are then extracted with a pre-trained CNN model for each window to obtain a features based background model. Second, Dou et al. <ref type="bibr" target="#b420">[379]</ref> extracted features for the current frame with the same operation as the background model. After, a distance map between the background image and the current frame is constructed by using the Euclidean distance. Third, the distance map is fed into graph cut algorithm to obtain the foreground mask. The background model is also updated with a learning rate. Figure <ref type="figure" target="#fig_58">59</ref> illustrates the architecture with 8 layers conv-net model. A 224 by 224 crop of an image in RGB is the input which is convolved with 96 different 1st layer filters (red), each of size 7 √ó 7 employing a stride of 2 in both x and y. The resulting feature maps are then passed through a ReLu, pooled, and contrast normalized across feature maps to give 96 different 55 √ó 55 element feature maps. Similar operations are repeated in layers 2-5. The last two layers are fully connected. The final layer is a c-way soft-max function with c being the number of classes. Experimental results on the Wallflower dataset <ref type="bibr" target="#b72">[31]</ref> show that the proposed method outperforms MOG <ref type="bibr" target="#b54">[13]</ref> and LBP <ref type="bibr" target="#b409">[368]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Stacked Denoising AutoEncoders</head><p>Zhang et al. <ref type="bibr" target="#b203">[162]</ref> designed a deep learned features based block-wise method with a binary spatio-temporal background model. Figure <ref type="figure" target="#fig_59">60</ref> shows the corresponding pipeline that consists of two parts: Stacked Denoising Au-toEncoder (SDAE) learning binary background modeling. Based on SDAE, the deep learning module learns a deep image representation encoding the intrinsic scene information. This leads to the robustness of feature description. Figure <ref type="figure" target="#fig_60">61</ref> illustrates the SDAE network. The binary background model captures the spatio-temporal scene distribution information in the Hamming space to perform foreground detection. Experimental results <ref type="bibr" target="#b203">[162]</ref> on the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref> demonstrate that SDAE provides a better performance than traditional methods, namely, MOG <ref type="bibr" target="#b54">[13]</ref>, KDE <ref type="bibr" target="#b52">[11]</ref>, and LBP <ref type="bibr" target="#b409">[368]</ref>, and therecent state-of-art model PBAS <ref type="bibr" target="#b387">[346]</ref>. To address the robustness against stationary noise, Garcia-Gonzalez et al. <ref type="bibr" target="#b421">[380]</ref> also used a stacked denoising autoencoders to generate a set of robust features for each patch of the image. This set is then considered as the input of a probabilistic model to determine whether that region is part of the background or foreground.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Neural Reponse Mixture</head><p>Shafiee et al. <ref type="bibr" target="#b201">[160,</ref><ref type="bibr" target="#b202">161]</ref> proposed a Neural Reponse Mixture (NeRM) framework to extract rich deep learned features with which to build a reliable MOG background model. Figure <ref type="figure" target="#fig_61">62</ref> shows the motion detection based on the NeRM framework. The first synaptic layer of StochasticNet <ref type="bibr" target="#b422">[381]</ref> is trained on the ImageNet dataset <ref type="bibr" target="#b155">[114]</ref> as a primitive, low-level, feature representation. Thus, the neural responses of the first synaptic layer at all pixels in the frame is then used as a feature to distinguish motion caused by objects moving in the scene. It is worth noting that the formation of StochasticNets used in the NeRM framework is a one-time and off-line procedure which is not implemented on an embedded system. The final formed StochasticNet is transferred to the embedded system. Then, MOG model is employed using the deep learned features. Experimental results <ref type="bibr" target="#b201">[160]</ref> on the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref> show that MOG-NeRM globally outperforms both the MOG model with RGB features and Color based Histogram model called CHist <ref type="bibr" target="#b423">[382]</ref>, but does not achieve the best scores for the "intermittentObjectMotion"', "Low frame rate", "Night video", and "Thermal" categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Motion Feature Networks</head><p>Nguyen et al. <ref type="bibr" target="#b200">[159]</ref> combined a sample-based background model with a feature extractor obtained by training a triplet network (See Figure <ref type="figure" target="#fig_62">63</ref>). This network is constructed by three identical CNNs, each of which is called a Motion Feature Network (MF-Net). Thus, each motion patterns is learned from small image patches and each input images of any size is transformed into feature embeddings for high-level representations. A sample based background model is then used with the color feature and the extracted deep motion features. To classify whether a pixel is background or foreground, Nguyen et al. <ref type="bibr" target="#b200">[159]</ref> employed the l 1 distance. Furthermore, an adaptive feedback scheme is also employed. The training is made with the CDNet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> and the offline trained network is then used on the fly without re-training on any video sequence before each execution. Experimental results <ref type="bibr" target="#b200">[159]</ref> on BMC 2012 dataset and CDNet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> show that MF-Net outperforms SOBS, LOBSTER and SuBSENSE in the case of dynamic backgrounds. Lee and Kim <ref type="bibr" target="#b199">[158]</ref> proposed a method for learning the pattern of the motions using the Factored 3-Way Restricted Boltzmann Machines (RBM) <ref type="bibr" target="#b424">[383]</ref> and obtaining the global motion from the sequential images. Once this global motion is identified between frames, background subtraction is achieved by selecting the regions that do not respect the global motion. These regions are thus considered as the foreground region</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Adequacy for the background subtraction task</head><p>All the previous works demonstrated the performance of DNN for background subtraction but not discuss the reason why DNN works well. A first way to analyze these performance is to compare these different methods. For this, we have grouped in Table <ref type="table" target="#tab_1">3</ref> a comparative overview of the architectures while we show an overview in terms of the challenges in Table <ref type="table" target="#tab_2">4</ref>. From Table <ref type="table" target="#tab_1">3</ref>, we can see that it is possible to have two types of input: current image only, background and current images. In the first case, the authors works either with the current images without computing a background image or with a end-to-end solution that first generates a background image. In the second case, the authors have to compute the background image by using the temporal median or another model like SuBSENSE. The output is always the foreground mask except for NUMOD which provide the background and the foreground mask but also an illumination change mask. For the architecture, most of the authors employed a well-know architecture (LeNet-5, VGG-16 and U-Net) that they slighly adapted to the task of background subtraction. Only few authors proposed a full designed architecture for background subtraction. Table <ref type="table" target="#tab_2">4</ref> groups the solutions of the different methods for the limitations of ConvNets <ref type="bibr" target="#b194">[153]</ref>. To learn the process at different level, the most common solutions are multiscale and cascaded strategies alleviating the drawback to work with patches. For the training, over-fitting is often the case producing scene-specific methods. For the dataset used for the training, most of the authors employed the CDnet 2014 dataset with a part devoted to the training phase and another part for the testing phase. End-to-end solutions are well proposed as well as spatial and temporal strategies. Most of the time, the architecture is a generative one even if a combination of generative and discriminative would be better suitable for background subtraction. Indeed, the background modeling is more a reconstructive task whereas the foreground detection is more a discriminative task.</p><p>To analyze how and why the DNN works well for this application, Minematsu et al. <ref type="bibr" target="#b285">[244,</ref><ref type="bibr" target="#b286">245]</ref> provided a valuable analysis by testing a quasi-similar method than ConvNet <ref type="bibr" target="#b194">[153]</ref> and found that the first layer performs the role of background subtraction using several filters whilst the last layer categorizes some background changes into a group without supervised signals. Thus, DNN automatically discovers background features through feature extraction by background subtraction and the integration of the features <ref type="bibr" target="#b285">[244]</ref> showing its potential for background/foreground separation. This first analysis is very valuable but the adequacy of a DNN method for the application of background/foreground separation should also be investigated in other key issues, that are the challenges and requirements met in background subtraction, and the adequacy of the architecture for background subtraction. More experimentally, Karadag and Erdas <ref type="bibr" target="#b425">[384]</ref> observed that deep learning approaches detect changes in presence of static backgrounds successfully but they are more sensitive in the case of dynamic backgrounds and camera jitter although they provide better performance than conventional approaches. In 2018, Akilan et al. <ref type="bibr" target="#b426">[385]</ref> studied the gap of performance between traditional models (i.e. statistical models and conventional ANNs) and two deep neural networks models that achieve about 9% and 7% improvements in terms of F-Measure.</p><p>To be effective, a background/foreground separation method should address the following challenges and requirements met in this application: (1) its robustness to noise, (2) its spatial and temporal coherence, (3) the existence of an incremental version, (4) the existence of a real-time implementation, and (5) the ability to deal with the challenges met in video sequences. Issue (1) is ensured for deep learning methods because a DNN learns the deep features of the background and foreground during the training phase. For issue (2), spatial and temporal processing need to be added to pixel-wise DNN methods because, as explained in Akilan <ref type="bibr" target="#b351">[310]</ref>, one of the main challenges in DNN methods is dealing with objects of very different scales and the dithering effect at bordering pixels of foreground objects. In literature, several authors have added spatial and temporal constraints using several spatial and/or temporal strategies. These strategies can be either incorporated in an end-to-end solution or can be done via a post-processing applied to the foreground mask. For example, cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and MV-FCN <ref type="bibr" target="#b351">[310]</ref> employed a multi-scale strategy whereas DeepBS <ref type="bibr" target="#b192">[151]</ref> used a spatial median filter. Struct-CNN <ref type="bibr" target="#b196">[155]</ref> is based on a superpixel strategy whilst Attention ConvLSTM+CRF <ref type="bibr" target="#b355">[314]</ref> employed Conditional Random Field (CRF). In another manner, Sakkos et al. <ref type="bibr" target="#b360">[319]</ref> used directly 3D-CNN for temporal coherence whereas Chen et al. <ref type="bibr" target="#b355">[314]</ref> used a spatial and temporal processing in Attention ConvLSTM. For issue <ref type="bibr" target="#b44">(3)</ref>, there is no need to update the background model in the DNN method if the training is sufficiently large to learn all appearances of the model in terms of changes in illumination and dynamics (waving trees, water rippling, waves, etc.), but is required otherwise. In this last case, several authors employed an end-to-end solution in which a DNN method is used for background generation to determine the background image over time. The output of this DNN-based background generation is then the input of the DNN-based background subtraction with the current image to determine the foreground mask. For issue (4), DNNs are time consuming when not applying a specific GPU and optimizer. Thus, the key point in achieving a suitable DNN method for background subtraction is to have a large training dataset and additional spatial/temporal strategies, and to apply them using a specific graphics card if possible. For issue <ref type="bibr" target="#b46">(5)</ref>, which regards the challenges met in video sequences, such as changes in illumination and dynamic backgrounds, a DNN alone may be sufficient if the architecture allows learning these changes, as applied in several studies, or if additional networks can be added.</p><p>For the adequacy of the architecture, it is necessary to check the features of the DNNs, namely, (1) type of architecture, and (2) parameters such as number of neurons, number of layers, etc. In the literature, we can only find two works comparing different architectures for background/foreground separation: Cinelli <ref type="bibr" target="#b194">[153]</ref> tested both LeNet5 <ref type="bibr" target="#b372">[331]</ref> and ResNet <ref type="bibr" target="#b237">[196]</ref> architectures whereas Chen et al. <ref type="bibr" target="#b355">[314]</ref> compared the VGG-16 <ref type="bibr" target="#b234">[193]</ref>, the GoogLeNet <ref type="bibr" target="#b390">[349]</ref>, and the ResNet50 <ref type="bibr" target="#b237">[196]</ref>. In these two works, ResNet <ref type="bibr" target="#b237">[196]</ref> provided the best results. However, these architectures were first designed for different classification tasks using the ImageNet dataset <ref type="bibr" target="#b154">[113]</ref>, CIFAR-10 dataset or ILSVRC 2015 dataset, but not for background/foreground separation using a corresponding dataset such as the CDnet 2014 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results for Background Generation</head><p>For comparison, we analyzed the results obtained by different algorithms on the well-known publicly available SBMnet dataset <ref type="bibr" target="#b294">[253]</ref> in a quantitative manner. Practically, only FCFlowNet <ref type="bibr" target="#b301">[260]</ref> was fully evaluated on this dataset. Looking at SBMnet dataset, the top algorithm is MSCL <ref type="bibr" target="#b303">[262]</ref> based on RPCA decomposition followed by a superpixel approach <ref type="bibr" target="#b427">[386]</ref> and the LabGen's group algorithms <ref type="bibr" target="#b304">[263,</ref><ref type="bibr" target="#b305">264,</ref><ref type="bibr" target="#b306">265]</ref>. The rank of FCFlowNet is only 19. However, FCFlowNet is also outperformed by conventional neural networks approaches like BEWiS <ref type="bibr" target="#b319">[278]</ref>, SC-SOBS-C4 <ref type="bibr" target="#b428">[387]</ref>, and BE-AAPSA <ref type="bibr" target="#b320">[279]</ref>. This counter performance can be explained by the fact that deep learning is difficult in presence of several challenges like very short sequences, and thus can not outperform methods with specific designed strategies using optical flow for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Results for Background Subtraction</head><p>For comparison, we present the results obtained on the well-known publicly available CDnet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> both in a qualitative and quantitative manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">CDnet 2014 dataset and Challenges</head><p>CDnet 2014 dataset <ref type="bibr" target="#b76">[35]</ref> was developed as part of Change Detection Workshop challenge (CDW 2014). This dataset includes all the videos from the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref> plus 22 additional camera-captured videos providing 5 different categories that incorporate challenges that were not addressed in the 2012 dataset. The categories are as follows: baseline, dynamic backgrounds, camera jitter, shadows, intermittent object motion, thermal, challenging Weather, low frame-rate, night videos, PTZ and turbulence. In addition, whereas ground truths for all frames were made publicly available for the CDnet 2012 dataset for testing and evaluation, in the CDnet 2014, ground truths of only the first half of every video in the 5 new categories is made publicly available for testing. The evaluation will, however, be across all frames for all the videos (both new and old) as in CDnet 2012. All challenges of these different categories have different spatial and temporal properties. It is important to determine both the solved and unsolved challenges. Both the CDnet 2012 and CDnet 2014 datasets allow highlighting under which situations it is difficult to provide robust foreground detection for existing background subtraction methods. The following remarks can be made regarding the development described in <ref type="bibr" target="#b429">[388]</ref>:</p><p>‚Ä¢ Conventional background subtraction methods can efficiently deal with challenges met in "baseline" and "bad weather" sequences.</p><p>‚Ä¢ The "Dynamic backgrounds", "thermal video" and "camera jitter" categories are a reachable challenge for top-performing background subtraction.</p><p>‚Ä¢ The "Night videos", "low frame-rate", and "PTZ" video sequences represent significant challenges. ‚Ä¢ Two statistical models, namely, the well-known MOG <ref type="bibr" target="#b54">[13]</ref> and RMOG <ref type="bibr" target="#b55">[14]</ref>. The Mixture of K Gaussians (MOG) was introduced in 1999 by Stauffer and Grimson <ref type="bibr" target="#b54">[13]</ref> to model dynamic backgrounds. Each pixel is thus characterized by a mixture of K Gaussians. Once the background model is defined, the different parameters of the mixture of Gaussians must be initialized. The parameters of the MOG's model are the number of Gaussians K, the weight œâ i,t associated to the i th Gaussian at time t, the mean ¬µ i,t and the covariance matrix Œ£ i,t . K determines the multi-modality of the background and by the available memory and computational power and it is commonly set from 3 to 7 <ref type="bibr" target="#b54">[13]</ref>. This model can handle better dynamic backgrounds than the mean, median, or single Gaussian model owing to its multi-modality. In 2013, Varadarajan et al. <ref type="bibr" target="#b55">[14]</ref> improved the MOG by taking into account the spatial relationship between pixels. Thus, regions are modeled as mixture distributions rather than as individual pixels.</p><p>‚Ä¢ One multi-cues model called Self-Balanced SENsitivity SEgmenter (SubSENSE) <ref type="bibr" target="#b93">[52]</ref> proposed in 2014 by St-Charles et al. <ref type="bibr" target="#b93">[52]</ref>. SubSENSE is a sample-based method that allows building a background model rather than building a model based on a specific distribution. SubSENSE is also non-parametric. Its primary goal is to address the issue of dynamic background modeling while increasing the foreground detection sensitivity through awareness of spatio-temporal variations, and decreasing the sensitivity to illumination variations. SuBSENSE offers a very effective feedback scheme that is able to identify static and dynamic background regions, adjust the model parameters to promote sample matching, and increase the overall foreground detection accuracy. It works at the pixel level, leading to better segmentation results in complex heterogeneous scenes. Because it is based on a sample consensus modeling approach, it still holds a significant memory footprint, while offering a fast processing speed. However, it does not handle intermittently moving foreground objects particularly well owing to the memoryless nature of its model, and to the random nature of its updating rules.</p><p>‚Ä¢ Two conventional neural networks, namely, SC-SOBS <ref type="bibr" target="#b143">[102]</ref> and AAPSA <ref type="bibr" target="#b320">[279]</ref>. SC-SOBS <ref type="bibr" target="#b143">[102]</ref> is an extension of SOBS that uses the spatial coherence and takes into account uncertainty in the background model. The SC-SOBS algorithm outperforms the crisp SOBS for moving object detection <ref type="bibr" target="#b142">[101]</ref> and parked vehicles detection <ref type="bibr" target="#b149">[108]</ref>. In the auto-adaptive parallel SOM architecture (AAPSA), a suspicious foreground analysis is conducted by continuously monitoring the segmentation results and thereby obtaining a reduction of the false positive rates.</p><p>Deep learning models include the following: five CNNs based methods (cascaded CNN <ref type="bibr" target="#b198">[157]</ref>, DeepBS <ref type="bibr" target="#b192">[151]</ref>, FgSeg-Net <ref type="bibr" target="#b344">[303]</ref>, FgSegNet-SFPM <ref type="bibr" target="#b345">[304]</ref>, FgSegNet-V2 <ref type="bibr" target="#b346">[305]</ref>) and two GANs based methods (BSPVGAN <ref type="bibr" target="#b368">[327]</ref>, DCP <ref type="bibr" target="#b311">[270]</ref>). All visual results come from the CDnet 2014 website except for DCP, for which the authors kindly provided the results. We also let in the four figures the number ID as well as the name as it is provided in the CDnet 2014 website.</p><p>B) Qualitative Analysis Table <ref type="table">6</ref> shows the visual results obtained using MOG, RMOG, and SuBSENSE. We can see that SuBSENSE clearly improves the foreground mask by reducing false positives and negative detections. From Table <ref type="table">7</ref>, we can remark that cascaded CNN outperforms the classical neural networks SC-SOBS and AAPSA except in the "Low-frame Rate" and "Night Videos" categories. In Table <ref type="table">8</ref>, FgSegNet and FgSegNet-SFPM (that are top methods in CDnet 2014 dataset) visually outperforms DeepBS in the "Baseline" and "Thermal"' Categories. In Table <ref type="table">9</ref>, we can remark that Semantic BGS <ref type="bibr" target="#b430">[389]</ref> obtains similar visual results than semi-supervised MSRNN <ref type="bibr" target="#b333">[292]</ref> and worse than unsupervised MSRNN <ref type="bibr" target="#b333">[292]</ref>. In Table <ref type="table">10</ref>, FgSegNet-V2 which is the top method in CDnet 2014 dataset is compared with GAN based methods that give similar visual results. Finally, we can state that the foreground mask was progressively improved over time when using statistical models, multi-cue models, conventional neural networks, and deep learning models in order of quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2.">Quantitative Evaluation</head><p>A) Comparison setup We compared the F-measures obtained using the different algorithms with the F-measures of other representative background subtraction algorithms over a complete evaluation dataset, namely, (A) two conventional statistical models (MOG <ref type="bibr" target="#b54">[13]</ref>, RMOG <ref type="bibr" target="#b55">[14]</ref>, (B) three advanced non-parametric models (SubSENSE <ref type="bibr" target="#b93">[52]</ref>, PAWCS <ref type="bibr" target="#b94">[53]</ref>, and Spectral-360 <ref type="bibr" target="#b431">[390]</ref>), and (C) two conventional neural networks models (SOBS-CF <ref type="bibr" target="#b142">[101]</ref>, SC-SOBS <ref type="bibr" target="#b143">[102]</ref>). Deep learning models for background separation are classified based on their applied architecture:</p><p>‚Ä¢ Convolutional Neural Networks: We grouped the scores of 22 algorithms based on a CNN, namely, two basic CNN algorithms (two variants of ConvNet <ref type="bibr" target="#b194">[153]</ref>), seven multi-scale or/and cascaded CNN algorithms (cascaded CNN <ref type="bibr" target="#b198">[157]</ref>, FgSegNet-M <ref type="bibr" target="#b344">[303]</ref>, FgSegNet-S <ref type="bibr" target="#b345">[304]</ref>, FgSegNet-V2 <ref type="bibr" target="#b346">[305]</ref>, MCSS <ref type="bibr" target="#b347">[306]</ref>, Guided Multiscale CNN <ref type="bibr" target="#b348">[307]</ref>, and MsEDNet <ref type="bibr" target="#b349">[308]</ref>), 1 fully CNN algorithms (MFCN <ref type="bibr" target="#b352">[311]</ref>), seven deep CNN algorithms (DeepBS <ref type="bibr" target="#b192">[151]</ref>, TS-CNN <ref type="bibr" target="#b204">[163]</ref>, Joint TS-CNN <ref type="bibr" target="#b204">[163]</ref>, five variants of Attention ConvLSTM <ref type="bibr" target="#b355">[314]</ref>), one structured CNN algorithm (Struct-CNN <ref type="bibr" target="#b196">[155]</ref>), and four 3D CNN algorithms (3D CNN <ref type="bibr" target="#b360">[319]</ref>, 3D Atrous CNN <ref type="bibr" target="#b363">[322]</ref>, FC3D <ref type="bibr" target="#b364">[323]</ref>, MFC3D <ref type="bibr" target="#b364">[323]</ref>).</p><p>‚Ä¢ Generative Adversarial Networks: We grouped scores of four GAN algorithms, namely, DCP <ref type="bibr" target="#b311">[270]</ref>, BSc-GAN <ref type="bibr" target="#b366">[325]</ref>, BGAN <ref type="bibr" target="#b367">[326]</ref>, and BPVGAN <ref type="bibr" target="#b368">[327]</ref>.</p><p>Furthermore, these algorithms can be labeled as pixel-wise, spatial-wise, temporal wise, and spatio-temporal-wise algorithms. For pixel-wise algorithms, they were directly applied by the authors to background/foreground separation without specific processing by considering the spatial and temporal constraints. With these algorithms, each pixel is processed independently based or not on the information contained in their local patch, such as in ConvNet <ref type="bibr" target="#b194">[153]</ref>. Thus, they may produce isolated false positives or false negatives. For spatial-wise algorithms, these algorithms model the dependencies among adjacent spatial pixels and thus enforce spatial coherence, as in cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and MFCN <ref type="bibr" target="#b352">[311]</ref> with a multi-scale strategy, Deep CNN (DeepBS) <ref type="bibr" target="#b192">[151]</ref> with spatial median filtering, Struct-CNN <ref type="bibr" target="#b196">[155]</ref> with super-pixel filtering, and Attention ConvLSTM+CRF <ref type="bibr" target="#b355">[314]</ref> with Conditional Random Field. The temporal-wise algorithms model the dependencies among adjacent temporal pixels, and thus enforce temporal coherence, such as Joint TS-CNN <ref type="bibr" target="#b204">[163]</ref> with background reconstruction feedback and 3D-CNN <ref type="bibr" target="#b360">[319]</ref>. The spatio-temporal-wise algorithms model both the dependencies among adjacent spatial and temporal pixels, and thus enforce both spatial and temporal coherence, such as Attention ConvLSTM+PSL+CRF <ref type="bibr" target="#b355">[314]</ref> with different architectures. Table <ref type="table">12</ref> groups the different F-measures which come either from the corresponding papers or directly from ChangeDetection.net website. Barnich and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref> did not test ConvNet on the "Intermittent Motion Object" and "PTZ" categories because they claimed that their method is not designed for it. Similarly, Lim et al. <ref type="bibr" target="#b196">[155]</ref> did not evaluate Struct-CNN on the "PTZ" category, nor were MCSS and BScGAN. Zeng and Zhu <ref type="bibr" target="#b352">[311]</ref> only evaluated MFCN on the "THM" category because this method was designed for infrared video. For these methods, the average F-measure is achieved by indicating the missing category or number of missing categories. For FgSegNet-M <ref type="bibr" target="#b344">[303]</ref>, FgSegNet-S <ref type="bibr" target="#b345">[304]</ref>, FgSegNet-V2 <ref type="bibr" target="#b346">[305]</ref>, we noticed that the F-measure reported by the authors are different than those available on the CDnet website. We report one of the official CDnet, and the overall score provided by the authors are given in parentheses.</p><p>B) Quantitative Analysis Table <ref type="table">12</ref> groups the different F-measures that come either from the corresponding papers or directly from changedetection.net website. We highlighted in bold the best algorithm score in each category. The top-ten methods are indicated along with their rank. Figure <ref type="figure" target="#fig_66">66</ref> and Figure <ref type="figure" target="#fig_67">67</ref> show graphics of the F-measures for the key methods, from MOG to the current leading method, FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref>. In these figures, the more the curve of the method reaches closer to a circle with a radius of 1, the more the method is robust over the eleven categories of CDnet 2014 dataset. By analyzing Table <ref type="table">12</ref> and looking at Figure <ref type="figure" target="#fig_64">64</ref> and Figure <ref type="figure" target="#fig_66">66</ref>.a, we can first see that the representative conventional neural networks, namely, Coherence-based and Fuzzy SOBS (SOBS-CF) <ref type="bibr" target="#b142">[101]</ref> and SOBS with Spatial Coherence (SC-SOBS) <ref type="bibr" target="#b143">[102]</ref> slightly outperform the basic statistical models such as MOG <ref type="bibr" target="#b54">[13]</ref> designed in 1999 even with improvements (i.e. RMOG <ref type="bibr" target="#b55">[14]</ref> developed in 2013). However, SOBS and its variants were the leading methods for the CDnet 2012 dataset <ref type="bibr" target="#b75">[34]</ref> for a long time (approximately two years), demonstrating the interest in neural networks for background subtraction. However, the F-measure did not exceed 0./9 on average, which is relatively low. The F-measure exceeded only 0.9 for the baseline category making these methods only usable and reliable in applications where the environments were not overly complex.</p><p>Second, we can also see in Table <ref type="table">12</ref>, Figure <ref type="figure" target="#fig_64">64</ref> and Figure <ref type="figure" target="#fig_66">66</ref>.b that advanced non-parametric models such as SuBSENSE <ref type="bibr" target="#b93">[52]</ref> and PAWCS <ref type="bibr" target="#b94">[53]</ref> developed in 2014 and 2015, respectively, achieve a chronologically better performance than SOBS-CF and SC-SOBS because of multi-features and multi-cues strategies. The gain in F-measure score was approximately 25%. The average F-measure was approximately 0.75, which becomes more acceptable in terms of reliable use under real conditions. In particular, the F-measure was approximately 0.9 for several challenges (baseline, dynamic backgrounds, camera jitter, and shadow). Thus, these methods are more applicable in more complex environments.</p><p>Third, we can observe that CNN-based methods can achieve a maximum increase in average F-measure of approximately 30% compared to SuBSENSE <ref type="bibr" target="#b93">[52]</ref> and PAWCS <ref type="bibr" target="#b94">[53]</ref>, demonstrating their superiority on this task. Figure <ref type="figure" target="#fig_65">65</ref> compares the performance of PAWCS <ref type="bibr" target="#b94">[53]</ref>, SuBSENSE <ref type="bibr" target="#b93">[52]</ref>, Cascaded CNN <ref type="bibr" target="#b198">[157]</ref> and FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref> and Figure <ref type="figure" target="#fig_66">66</ref>.c also compares SuBSENSE <ref type="bibr" target="#b93">[52]</ref> with several CNNs based methods. The first CNN-based method provides a better performance than SuBSENSE in all categories. In addition, we can see in Figure <ref type="figure" target="#fig_67">67</ref>.a that the top DNNs based methods clearly outperforms SuBSENSE. In Figure <ref type="figure" target="#fig_66">66</ref>.(d), we can also see an increase in performance between the first cascaded CNNs method published in 2016 and one of the top method FgSegNet-M <ref type="bibr" target="#b346">[305]</ref> which was designed in 2018, thereby showing the progress made during a two year period. Such an increase in performance required approximately 5 years before the use of deep neural networks. However, CNNs significantly increase the F-measure under dynamic backgrounds, camera jitter, intermittent object motion, and turbulence categories. For the "PTZ" category, the performance is mitigated as can be seen in works of several authors who did not provide results on this category, arguing that they did not design their method for this type of challenge, although their scores obtained using GANs are extremely interesting. These methods appear to be usable and reliable in an extremely large spectrum of environments, but are mostly scene-specific with supervised mode. We can also see that the training has a significant influence on the performance. Indeed, the results obtained by ConvNet using manual foreground masks (GT) obtained a F-Measure around 0.9 whereas this value falls to approximately 0.79 using the foreground masks from IUTIS, demonstrating a slight increase in performance in comparison with SuBSENSE <ref type="bibr" target="#b93">[52]</ref> and PAWCS <ref type="bibr" target="#b94">[53]</ref>. This fact also highlights that the increase in performance obtained by DNN-based methods is essentially due to their supervised aspects. In addition, their current computation times, as shown in Table <ref type="table" target="#tab_2">4</ref>, are too slow to be currently employed in real applications.</p><p>The top-ten DNN-based methods can be decomposed into three main groups. The first group consists of FgSegNet methods developed by Lim and Keles <ref type="bibr" target="#b344">[303,</ref><ref type="bibr" target="#b345">304,</ref><ref type="bibr" target="#b346">305]</ref>. Indeed, FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref>, FgSegNet-S <ref type="bibr" target="#b345">[304]</ref> and FgSegNet-M <ref type="bibr" target="#b346">[305]</ref> take the top-three places. Their success seems to be due to the architecture of FgSegNet, which is particularly designed for background subtraction, and by their spatial-wise aspects. The second group consists of 3D-CNNs based methods (MCF3D <ref type="bibr" target="#b364">[323]</ref>, 3D Atrous CNN <ref type="bibr" target="#b363">[322]</ref>, FC3D <ref type="bibr" target="#b364">[323]</ref>, and 3D-CNN <ref type="bibr" target="#b360">[319]</ref>). This good performance of 3D-CNN based methods is due to their ability to take into account both spatial and temporal constraints, which are extremely important in this field. Figure <ref type="figure" target="#fig_67">67</ref>.(d) compare the different 3D-CNNs based methods. We can state that MCF3D <ref type="bibr" target="#b364">[323]</ref> offers the closest curve to a circle with a radius of 1 but present a weakness for the IOM category, as compared to the other 3D-CNN based methods. Finally, the third group consists of unsupervised GAN-based methods (BPVGAN <ref type="bibr" target="#b368">[327]</ref>, BVGAN <ref type="bibr" target="#b367">[326]</ref> and BScGAN <ref type="bibr" target="#b366">[325]</ref>). However, their performance can be improved because these methods are pixel-wise without taking into account either the spatial or temporal constraints. Figure <ref type="figure" target="#fig_67">67</ref>.b compare three top DNNs that belongs each to one of the three top groups. We can note that FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref> outperforms both MFC3D <ref type="bibr" target="#b364">[323]</ref> and BPVGAN <ref type="bibr" target="#b368">[327]</ref>. Moreover, FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref> presents no main weaknesses in a single category. Figure <ref type="figure" target="#fig_67">67</ref>.c highlights the increase in performance over 20 years of research between MOG developed in 1999 to FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref> designed in 2018. We can state that the curve of the compared methods progressively increases from the first method, MOG, to FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref>, highlighting our quantitative analysis. Furthermore, the curve of FgSegNet-V2 <ref type="bibr" target="#b344">[303]</ref> is close to a circle with a radius of 1, indicating that deep learning methods are able 55  to reach a quasi-ideal performance.     Table <ref type="table">12</ref>. F-measure metric over the 6 categories of the CDnet2014, namely Baseline (BSL), Dynamic background (DBG), Camera jitter (CJT, Intermittent Motion Object (IOM), Shadows (SHD), Thermal (THM), Bad Weather (BDW), Low Frame Rate (LFR), Night Videos (NVD), PTZ, Turbulence (TBL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">How far are DNNs from the ideal method?</head><p>To evaluate the progress of background subtraction methods since MOG was developed in 1999 until the advent of DNN-based methods in 2018, we computed different key increases in the F-measure in terms of percentage. To do so, we considered a) the gap between MOG and the best conventional neural network (SC-SOBS), b) the gap between SC-SOBS and the best non-parametric multi-cues methods (SubSENSE), c) the gap between SuBSENSE and Cascaded CNNs, d) the gap between SuBSENSE and the best DNNs based method (FgSegNet-V2), and e) the gap between FgSegNet-V2 and the ideal method (F-Measure= 1 in each category). From Table <ref type="table" target="#tab_6">11</ref>, we can see than the big gap was obtained by DNNs methods against SuBSENSE with 24.31 and 32.92 for Cascaded CNN and FgSegNet-V2, respectively. We can also note that the gap of 1.55% that remains between FgSegNet-V2 and the ideal method is less than the gap of 6.93% between Cascaded CNN and FgSegNet-V2. This gap can be partially filled by three main directions: robust deep auto-encoders <ref type="bibr" target="#b226">[185,</ref><ref type="bibr" target="#b222">181,</ref><ref type="bibr" target="#b224">183,</ref><ref type="bibr" target="#b432">391,</ref><ref type="bibr" target="#b223">182]</ref> probabilistic <ref type="bibr" target="#b218">[177]</ref> and fuzzy <ref type="bibr" target="#b219">[178,</ref><ref type="bibr" target="#b220">179]</ref> DNNs, and GANs architecture specifically designed for background subtraction. Nevertheless, it is important to note that the large gap provided by cascaded CNN and FgSegNet-V2 is mainly due to their supervised aspect, and a required drawback of training using labeling data. However, when labeling data are unavailable, efforts should be concentrated on unsupervised GANs as well as unsupervised methods based on semantic background subtraction <ref type="bibr" target="#b430">[389,</ref><ref type="bibr" target="#b433">392]</ref>, and robust subspace tracking <ref type="bibr" target="#b122">[81,</ref><ref type="bibr" target="#b434">393,</ref><ref type="bibr" target="#b435">394,</ref><ref type="bibr" target="#b120">79,</ref><ref type="bibr" target="#b117">76,</ref><ref type="bibr" target="#b118">77]</ref> that are still of interest in the field of background subtraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>In this paper, we first presented a full review of recent advances in deep neural networks as applied to background generation, background subtraction, and deep learned features for the detection of moving objects in video taken by a static camera. Experiment results on the large-scale CDnet 2014 dataset show the increase in performance obtained using supervised deep neural network methods in this field. Although deep neural networks have recently received significant attention for their use in background subtraction during the last two years since the seminal study by Braham and Van Droogenbroeck <ref type="bibr" target="#b194">[153]</ref>, there remain many important and unresolved issues:</p><p>‚Ä¢ The main question remains what is the most suitable type of deep neural network and its corresponding architecture for background initialization, background subtraction, and deep learned features in the presence of complex backgrounds?</p><p>‚Ä¢ Looking at the various experiments conducted, it can be observed that deep learning approaches detect the changes in images with static backgrounds successfully but are more sensitive in the case of dynamic backgrounds and camera jitter, although they do provide a better performance than conventional approaches <ref type="bibr" target="#b425">[384]</ref>.</p><p>In addition, several authors avoid experiments on the "IOM" and the "PTZ" categories. In addition, when the F-Measure is provided for these categories, the score is not very high. Thus, it seems that the current deep neural networks tested face problems in theses cases perhaps because they have difficulties in how to learn the duration of sleeping moving objects and how to handle changes from moving cameras.</p><p>‚Ä¢ For the inputs, all of the authors employed either gray or color images in RGB, with the exception of Zhao et al. <ref type="bibr" target="#b340">[299]</ref> who used a distribution learning feature to improve the performance of a basic CNNs. However, it would be interesting to employ RGB-D images because depth information is extremely helpful in several challenges such as in camouflage images, as developed by Maddalena and Petrosino <ref type="bibr" target="#b436">[395]</ref>. In addition, the conventional neural networks SOBS <ref type="bibr" target="#b437">[396]</ref> is the top algorithm on the SBM-RGBD dataset <ref type="bibr" target="#b327">[286]</ref>. Thus, we can expect that CNNs with RGB-D features as inputs will also achieve a significant performance as a ForeGAN-RGBD <ref type="bibr" target="#b312">[271]</ref> model. However, multi-spectral data would also be interesting to test. Furthermore, a study on the influence of the input feature type would be an area of interest.</p><p>‚Ä¢ Rather than working in the pixel domain, DNNs may also be applied to the measurement domain for use in conjunction with compressive sensing data like in RPCA models <ref type="bibr" target="#b438">[397,</ref><ref type="bibr" target="#b434">393]</ref>.</p><p>Currently, mainly CNNs and basic GANs have been employed for background subtraction. Thus, a future direction may be to investigate the adequacy and use of pyramidal deep CNNs <ref type="bibr" target="#b439">[398]</ref>, deep belief neural networks, deep restricted kernel neural networks <ref type="bibr" target="#b440">[399]</ref>, probabilistic neural networks <ref type="bibr" target="#b218">[177]</ref>, deep fuzzy neural networks <ref type="bibr" target="#b219">[178,</ref><ref type="bibr" target="#b220">179]</ref> and fully memristive neural networks <ref type="bibr" target="#b441">[400,</ref><ref type="bibr" target="#b442">401,</ref><ref type="bibr" target="#b443">402,</ref><ref type="bibr" target="#b444">403,</ref><ref type="bibr" target="#b445">404,</ref><ref type="bibr" target="#b446">405]</ref> for both static and moving cameras <ref type="bibr" target="#b447">[406]</ref>. 59</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. From left to right: Schematic Illustrations of Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs) (Image from Liu et al. [111]).</figDesc><graphic coords="7,183.07,112.67,113.01,99.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Schematic Illustration of AutoEncoders (AEs) networks (Image from Liu et al. [111]).</figDesc><graphic coords="8,241.00,134.02,113.08,98.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Schematic Illustration of Convolutional Neural Networks (CNNs) (Image from Liu et al. [111]).</figDesc><graphic coords="8,141.80,284.66,311.42,98.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Probabilistic Convolutional Neural Networks: a) Conventional CNNs with both activations and outputs as deterministic point estimates, b) Probabilistic CNNs with probabilistic output layers, and c) Probabilistic CNNs replacing all intermediate activations by distributions (Image from Gast and Roth [177]).</figDesc><graphic coords="9,156.00,112.84,282.88,112.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Fuzzy Deep Neural Networks: Fuzzy logic representation part in black, Neural representation part in blue, Fuzzy-and-deep representation fusion part in green and the task driven learning part in red (Image from Deng et al. [178]).</figDesc><graphic coords="9,141.82,381.27,311.24,169.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Background Generation: The first row shows an original image of three videos from the SBMI dataset [266] and the second row shows the corresponding ground truth in the following order from left to right: CaVignal, Foliage and "Hall and Monitor".</figDesc><graphic coords="13,345.53,181.77,90.45,67.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. PS-RBM Architecture (Image from Guo and Qi [147]).</figDesc><graphic coords="14,141.73,112.50,311.82,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparison between conventional RBM and TARBM. The two architectures are similar but the TARBM has an extra term R T in the corresponding objective function. (Image from Xu et al. [149]).</figDesc><graphic coords="14,198.43,327.08,198.43,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. TARBM Pipeline: The model is initially trained on the first several adjacent video frames. After that, the model provides the background for the subsequent frames. h denotes the unbiased sample of the hidden variable h. (Image from Xu et al. [149]).</figDesc><graphic coords="15,155.93,112.59,283.35,99.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Deep Auto Encoder Networks Pipeline (Image from Xu et al. [33]).</figDesc><graphic coords="15,141.80,266.60,311.42,98.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. FC-FlowNet Architecture (Image from Halfaoui et al. [260]).</figDesc><graphic coords="16,141.79,112.75,311.46,113.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. From top to bottom: Baseline BM-Unet and Augmented BM-Unet (Image from Tao et al. [269]).</figDesc><graphic coords="17,205.58,227.30,183.87,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Unsupervised GAN Deep Context Prediction (DCP) Pipeline (Image from Sultana et al. [270]).</figDesc><graphic coords="18,141.73,380.95,311.82,106.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. ForeGAN-RGBD model for RGB-D videos (Image from Sultana et al. [271]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Illumination Invariant ForeGAN Pipeline (Image from Sultana and Jung<ref type="bibr" target="#b313">[272]</ref>).</figDesc><graphic coords="18,189.52,560.70,216.33,128.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. ConvNet's Architecture: The network is trained with two small patches extracted from the input and background images in gray-scale. The network is inspired by LeNet-5 network (Image from Braham and Van Droogenbroeck [153]).</figDesc><graphic coords="23,141.80,112.77,311.43,141.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Pipeline for Ground-truth Generation Process via Multi-scale and Cascaded CNNs (Image from Wang et al. [157]).</figDesc><graphic coords="24,155.96,129.42,283.11,169.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Basic CNN Architecture: 4 convolutional layers, 2 fully connected layer. The first 2 convolutional layers come with a 2 √ó 2 max pooling layer (Image from Wang et al. [157]).</figDesc><graphic coords="24,155.96,373.46,283.13,84.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Multi-scale CNN Architecture (Image from Wang et al. [157]).</figDesc><graphic coords="24,141.79,541.64,311.43,141.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. FgSegNet Architecture (Image from Lim and Keles [303]).</figDesc><graphic coords="25,155.93,112.63,283.30,84.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. From left to right: The first image shows the architecture of each CNN in the triplet network. The second image shows the TCNN architecture (Images from Lim et al.<ref type="bibr" target="#b344">[303]</ref> </figDesc><graphic coords="25,298.90,242.10,127.46,70.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 23 .</head><label>23</label><figDesc>Figure 23. FgSegNet-V2 Architecture (Image from Lim et al.[305]).</figDesc><graphic coords="26,155.94,141.83,283.33,84.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. From left to right: The first image shows the Feature Pooling Module (FPM) with BN (BatchNormalization) and SD (Spatial-Dropout) for FgSegNet-M (Image from Lim and Keles [304]). The second image shows the Modified FPM module (M-FPM) with IN (InstanceNormalization) and SD (SpatialDropout). All convolution layers have 64 features (Image from Lim and Keles [305]).</figDesc><graphic coords="26,298.90,325.82,155.83,84.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 25 .</head><label>25</label><figDesc>Figure 25. Pipeline of the Multi-Scale Cascaded Scene-Specific (MCSS) (Image from Liao et al. [306]).</figDesc><graphic coords="26,127.63,529.19,339.75,141.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 26 .</head><label>26</label><figDesc>Figure 26. Top: Pipeline learning for manual labeling in Wang et al.<ref type="bibr" target="#b198">[157]</ref>. Bottom: Pipeline for guided automatic learning method in Liang et al.<ref type="bibr" target="#b348">[307]</ref> (Image from Liang et al.<ref type="bibr" target="#b348">[307]</ref>).</figDesc><graphic coords="27,127.63,112.84,339.74,141.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 27 .</head><label>27</label><figDesc>Figure 27. Pipeline of MsEDnet Network (Image from Patil et al. [308]).</figDesc><graphic coords="27,141.78,308.45,311.51,113.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 28 .</head><label>28</label><figDesc>Figure 28. From left to right: Encoder architecture, decoder architecture for MsEDnet Network (Image from Patil et al. [308]).</figDesc><graphic coords="27,298.89,637.61,127.55,59.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 29 .</head><label>29</label><figDesc>Figure 29. Fully convolutional network (Image from Yang et al. [309]).</figDesc><graphic coords="28,141.79,112.76,311.46,113.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 30 .</head><label>30</label><figDesc>Figure 30. Structure of shortcut connected block with multiple branches. This block contains four different branches with the same data flow into each branch but different features flows out from each branch because each branch has different layers. From left to right: the front of three branches computes different features by using different atrous convolution whilst the last branch is the shortcut connection. (Image from Yang et al. [309]).</figDesc><graphic coords="29,170.18,112.83,254.55,113.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 31 .</head><label>31</label><figDesc>Figure 31. MV-FCN Architecture: Convk, Si, CTransk, Concat, and BN stand for convolution using kernel size of k and stride of i, transpose convolution with filter size of k, activation maps concatenation, and batch normalization operations, respectively (Image from Akilan [310]).</figDesc><graphic coords="29,141.77,470.05,311.59,113.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 32 .</head><label>32</label><figDesc>Figure 32. MFCN Architecture for IR videos: A FCN architecture covering multi-scale convolution and deconvolution operations. As CNN features are learned from multiple scales, the feature representation contains both category-level semantics and fine-grain details. (Image from Zeng and Zhu [312]).</figDesc><graphic coords="30,141.73,112.51,311.83,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 33 .</head><label>33</label><figDesc>Figure 33. MFCN Architecture for color videos: Based on VGG, MFCN is divided into five stages by max pooling operations. To effectively use multiscale features, a set of convolution and deconvolution operations with the stepwise upsampling strategy aggregate multiscale features, making a feature representation that contains more category-level information and fine-grain details (Image from Zeng and Zhu<ref type="bibr" target="#b353">[312]</ref>).</figDesc><graphic coords="30,155.90,303.49,283.48,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 34 .</head><label>34</label><figDesc>Figure 34. FCSN Architecture: The Pool3 and Pool4 are the result of max pooling layer and the parameter s1 and the parameter s2 are the scale parameters (Image from Lin et al. [313]).</figDesc><graphic coords="31,141.79,112.70,311.46,113.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 35 .</head><label>35</label><figDesc>Figure 35. Pipeline of TS-CNN (Image from Zhao et al. [163]).</figDesc><graphic coords="32,141.78,112.81,311.49,113.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 36 .</head><label>36</label><figDesc>Figure 36. Pipeline of ADCNN (Image from Li et al. [302]).</figDesc><graphic coords="32,141.78,270.03,311.52,84.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 37 .</head><label>37</label><figDesc>Figure 37. Pipeline of SFEN (Image from Chen et al. [314]).</figDesc><graphic coords="33,141.78,112.78,311.53,113.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 38 .</head><label>38</label><figDesc>Figure 38. Pipeline of MSFgNet (Image from Patil and Murala [315]).</figDesc><graphic coords="33,141.78,270.93,311.52,112.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 39 .</head><label>39</label><figDesc>Figure 39. MSFgNet Architecture: a) MSNet: Motion-saliency network (BENet: Background Estimation Network, SMNet: Saliency Estimation Network), b) FgNet: Foreground extraction network (Image from Patil and Murala [315]).</figDesc><graphic coords="34,141.73,112.50,311.82,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 40 .</head><label>40</label><figDesc>Figure 40. Struct-CNN Architecture: Three grayscale images are used as inputs. The encoder is based on the VGG16. The decoder extracts a foreground mask using the features from the encoder (Image from Lim et al. [155]).</figDesc><graphic coords="34,141.78,280.13,311.51,84.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 41 .</head><label>41</label><figDesc>Figure 41. DESD's Architecture: Layer diagram of Double Encoding-Slow Decoding EnDec CNN (Image from Akilan and Wu [317]).</figDesc><graphic coords="35,141.80,112.89,311.38,141.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 42 .</head><label>42</label><figDesc>Figure 42. 3D-CNNs Architecture. Cubes indicate 3D operations across the temporal dimension. Rectangles indicate 2D (spatial only) operations. The plus sign indicates concatenation (Image from Sakkos et al. [319].</figDesc><graphic coords="36,141.77,114.83,311.60,113.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 43 .</head><label>43</label><figDesc>Figure 43. 3D CNNs Architecture: Two convolution layers, two pooling layers, one full connection layer and one output layer (Image from Gao et al. [320].</figDesc><graphic coords="36,141.79,282.61,311.44,113.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 44 .</head><label>44</label><figDesc>Figure 44. Comparison between a 2D convolution operation and a 3D convolution operation (Image from Gao et al. [320].</figDesc><graphic coords="36,198.48,449.76,198.10,84.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 45 .</head><label>45</label><figDesc>Figure 45. 3D CNNs Architecture: The low-level 3D ConvNets only keeps spatial features with spatial-wise max pooling. The high-level 3D ConvNets keeps temporal features using temporal-wise max pooling. Spatial-temporal mask is multiplied with the extracted features from Conv5 before it is fed as the input to Conv6 (Image from Yu et al. [321].</figDesc><graphic coords="36,141.82,579.69,311.27,98.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 46 .</head><label>46</label><figDesc>Figure 46. 3D Atrous CNN Architecture (10 layers): Layer 1 is the input layer. Two parallel structures in layers 2, 3, 4 to gain different temporal information. Their outputs are concatenated in 3DC31 in layer 5. 2D atrous convolution is used to the remaining layers 6, 7, 8, 9 to suppress the time dimension and perform foreground detection. Layer 10 is the output layer (Image from Hu et al. [322].</figDesc><graphic coords="37,141.78,112.69,311.52,113.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 47 . 2 -</head><label>472</label><figDesc>Figure 47. 2-level 3D atrous ConvLSTM network at time steps t -1, t and t + 1. The input of ConvLSTM1 at time step t consists of the output of the feature extractor CNN and the output of ConvLSTM2 for time step t -1. The input of ConvLSTM2 at time step t consists of the output of our feature extractor CNN and the output of ConvLSTM1. The input consists of 12 frames (Image from Hu et al. [322].</figDesc><graphic coords="37,141.78,289.49,311.49,169.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 48 .</head><label>48</label><figDesc>Figure 48. 3D atrous convolution with kernel size (3,3,3) and rate (2,2).(Image from Hu et al. [322].</figDesc><graphic coords="38,198.54,113.11,197.75,84.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 49 .</head><label>49</label><figDesc>Figure 49. MFC3D Architecture: The downsampling rate or the upsampling rate are indicated for each layer. The dimensions of the tensors are shown beside corresponding arrows (Image from Wang et al. [323].</figDesc><graphic coords="38,141.79,242.30,311.43,212.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 50 .</head><label>50</label><figDesc>Figure 50. Comparison between 3D convolution, retrospective convolution and atrous retrospective convolution. (Image from Chen et al. [324].</figDesc><graphic coords="39,155.98,112.84,283.04,84.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 51 .</head><label>51</label><figDesc>Figure 51. Atrous Retrospective Architecture based on ResNet-18, ARPP and multi-level encoder-decoder modules (Image from Chen et al. [324].</figDesc><graphic coords="39,141.80,242.45,311.38,98.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>4. 9 .</head><label>9</label><figDesc>CNNs with Different Input Features 4.9.1. Random Permutation of Temporal Pixels (RPoTP) feature Zhao et al. [299] designed a Deep Pixel Distribution Learning (DPDL) model for background subtraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>Figure 52 .</head><label>52</label><figDesc>Figure 52. RPoTP features encode the distributions of pixel observations that belong to dynamical background R1, moving objects R2 and static background R3 respectively.(Image from Zhao et al. [299].</figDesc><graphic coords="40,212.68,112.93,169.57,141.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 53 .</head><label>53</label><figDesc>Figure 53. Deep Pixel Distribution Learning (DPDL) Architecture (Image from Zhao et al. [299].</figDesc><graphic coords="40,155.98,307.79,283.02,84.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 55 .</head><label>55</label><figDesc>Figure 55. cGAN Architecture (Image from Bakkay et al. [325].</figDesc><graphic coords="42,127.55,112.51,340.17,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 56 .</head><label>56</label><figDesc>Figure 56. Multi-Scale Recurrent encoder-decoder Neural Network Architecture (Image from Choo et al.[292]).</figDesc><graphic coords="43,141.80,142.79,311.39,113.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Figure 57 .</head><label>57</label><figDesc>Figure 57. Pipeline of the unsupervised version of MSRNN (Image from Choo et al. [293]).</figDesc><graphic coords="43,155.99,356.93,282.95,127.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 58 .</head><label>58</label><figDesc>Figure 58. DeepPBM Pipeline based on Variational autoencoders (VAEs) (Image from Farnoosh et al. [294]).</figDesc><graphic coords="43,141.81,585.05,311.31,84.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 59 .</head><label>59</label><figDesc>Figure 59. Deep CNN's features: Architecture of 8 layer conv-net model. A 224 by 224 crop of an image is presented as the input. This is convolved with 96 different 1st layer filters, each of size 7 by 7, using a stride of 2 in both x and y. (Image from Dou et al. [379]).</figDesc><graphic coords="44,141.73,563.99,311.82,99.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure 60 .</head><label>60</label><figDesc>Figure 60. Deep Feature Learning and Binary Background Modeling (Image from Zhang et al. [162]).</figDesc><graphic coords="45,155.96,280.87,283.16,127.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Figure 61 .</head><label>61</label><figDesc>Figure 61. SDAE Architecture: (a) Denoising Autoencoder. (b) Four Stacked Denoising Autoencoder with the input patch of size 16 √ó 16 (Image from Zhang et al. [162]).</figDesc><graphic coords="45,155.96,464.67,283.14,127.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Figure 62 .</head><label>62</label><figDesc>Figure 62. (NeRM Architecture: The neural responses from a highly efficient StochasticNet are used as rich deep features that are used in the MOG model (Image from Shafiee et al. [160]).</figDesc><graphic coords="46,155.97,112.90,283.11,112.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>Figure 63 .</head><label>63</label><figDesc>Figure 63. (Block diagram of MF-Net. The triplet network are trained with a dataset. The trained CNN is then split and modified to work as a feature extractor (Image from Nguyen et al. [159]).</figDesc><graphic coords="47,170.17,112.88,254.58,141.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>8. 2 .</head><label>2</label><figDesc>Performance Evaluation 8.2.1. Qualitative Evaluation A) Comparison setup We compared the visual results obtained on the CDnet 2014 dataset by the different deep learning algorithms with visual results of other representative background subtraction algorithms that are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>Figure 64 .</head><label>64</label><figDesc>Figure<ref type="bibr" target="#b105">64</ref>. Comparison of F-Measure between MOG<ref type="bibr" target="#b54">[13]</ref>, RMOG<ref type="bibr" target="#b55">[14]</ref>, SOBS-CF, SC-SOBS and SuBSENSE<ref type="bibr" target="#b93">[52]</ref>. It can be noted that SOBS-CF and SC-SOBS outperform MOG except on the "BDW" and "PTZ" categories. SuBSENSE provides the best performance.</figDesc><graphic coords="57,141.81,112.85,311.34,141.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 65 .</head><label>65</label><figDesc>Figure 65. Comparison of F-Measure between PAWCS<ref type="bibr" target="#b94">[53]</ref>, SuBSENSE<ref type="bibr" target="#b93">[52]</ref>, Cascaded CNN<ref type="bibr" target="#b198">[157]</ref> and FgSegNet-V2<ref type="bibr" target="#b344">[303]</ref>. It can be noted that Cascaded CNN and FgSegNet-V2 outperform PAWCS and SuBSENSE on all the categories. FgSegNet-V2 provides the best performance.</figDesc><graphic coords="57,141.78,308.42,311.52,141.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure 66 .</head><label>66</label><figDesc>Figure 66. First row: a) Gain in performance between MOG<ref type="bibr" target="#b54">[13]</ref> and conventional neural networks (SOBS-CF, SC-SOBS). b) Gap between conventional NNs, and PAWCS<ref type="bibr" target="#b94">[53]</ref>/SuBSENSE<ref type="bibr" target="#b93">[52]</ref>. Second row: c) Gain in performance between SuBSENSE and CNNs, d) Gain in performance between the first cascaded CNNs and one of the best DNN method (FgSegNet-M<ref type="bibr" target="#b346">[305]</ref>).</figDesc><graphic coords="58,113.38,117.12,368.53,255.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure 67 .</head><label>67</label><figDesc>Figure 67. First row: a) Gain in performance between SuBSENSE [52] and three top DNNs based methods. b) Comparison of three top DNNs. c) Second row: Gain in performance between from MOG (1999) to FgSegNet-V2 [303] (2018) that represents 20 years of research. d) Gain in performance between the different 3D-CNNs based methods.</figDesc><graphic coords="58,113.38,430.96,368.52,255.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="41,155.90,112.51,283.47,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>Output</cell><cell>Architecture</cell><cell>Additional</cell><cell>Activation</cell><cell>Conv.</cell><cell>Fully Conv.</cell><cell>Implementation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Encoder/Decoder</cell><cell>Architecture</cell><cell>Function</cell><cell>Layers</cell><cell></cell><cell>Framework</cell></row><row><cell>Basic CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ConvNets [153]</cell><cell>Backg. (Median)</cell><cell>Foreground</cell><cell>LeNet-5 [331]</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>2</cell><cell>1</cell><cell>-</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Basic CNNs [302]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>CNN-1</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>2</cell><cell>Caffe [248]/MatConvNet [249]</cell></row><row><cell>Basic CNNs [296]</cell><cell>Backg. Visible (Median)</cell><cell>GT</cell><cell>CNN</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Backg. Thermal (Median)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Current Image (Visible)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Current Image (Thermal)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Basic CNNs [297]</cell><cell>Backg. (Median)</cell><cell>Foreground</cell><cell>GoogLeNet [195]</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>Tensorflow [250]</cell></row><row><cell></cell><cell>Current Image</cell><cell>(Bound. Box)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Basic CNNs [299]</cell><cell>Current Image (RPoTP)</cell><cell>Foreground</cell><cell>CNN</cell><cell>-</cell><cell>ReLU</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell>Basic CNNs [300]</cell><cell>Background Image (Average) (Depth)</cell><cell>Foreground</cell><cell>CNN</cell><cell>(MLP)</cell><cell>ReLU/Sigmoid</cell><cell>3</cell><cell>3</cell><cell>-</cell></row><row><cell></cell><cell>Current Image (Depth)</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Multi-scale and cascaded CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-scale CNNs [302]</cell><cell>Current Image</cell><cell>GT</cell><cell>CNN-1</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>Caffe [248]/MatConvNet [249]</cell></row><row><cell>cascaded CNNs [302]</cell><cell>Current Image</cell><cell>GT</cell><cell>CNN-1</cell><cell>CNN-2</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>Caffe [248]/MatConvNet [249]</cell></row><row><cell>FgSegNet-M [303]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>VGG-16 [193]</cell><cell>TCNN</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>Keras [332]/TensorFlow [250]</cell></row><row><cell>FgSegNet-S [304]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>VGG-16 [193]</cell><cell>TCNN/FPM</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>Keras [332]/TensorFlow [250]</cell></row><row><cell>FgSegNet-V2 [305]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>VGG-16 [193]</cell><cell>TCNN/FPM</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>Keras [332]/TensorFlow [250]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feat. Fusions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MCSS [306]</cell><cell>Backg.</cell><cell>Foreground</cell><cell>ConvNets [153]</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>2</cell><cell>2</cell><cell>-</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Guided Multi-scale CNN [307]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>ConvNets [153]</cell><cell>Guided Learning</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>-</cell></row><row><cell>MsEDNet [308]</cell><cell>Back. (Temp. Histogram)</cell><cell>Foreground</cell><cell>Compact CNN</cell><cell>Saliency Map</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Fully CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully CNNs [154]</cell><cell>Backg. (Median)</cell><cell>Foreground</cell><cell>LeNet-5 [331]</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>4</cell><cell>-</cell><cell>Torch7</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully CNNs [154]</cell><cell>Backg. (Median)</cell><cell>Foreground</cell><cell>ResNet [333]</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>Torch7</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deep FCNNs [309]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>Multi. Branches (4)</cell><cell>CRF</cell><cell>PReLU [211]</cell><cell>5 (Atrous)</cell><cell>1</cell><cell>-</cell></row><row><cell>MV-FCN [310]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>U-Net [194]</cell><cell>2CFFs/PFF</cell><cell>ReLU/Sigm.</cell><cell>(2D Conv.)</cell><cell>1</cell><cell>Keras/Python</cell></row><row><cell>MFCN [311]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>VGG-16 [193]</cell><cell></cell><cell>ReLU/Sigm.</cell><cell>5</cell><cell>-</cell><cell>TensorFlow [250]</cell></row><row><cell>CNN-SFC [156]</cell><cell>3 For. Masks</cell><cell>Foreground</cell><cell>VGG-16 [193]</cell><cell></cell><cell>ReLU/Sigm.</cell><cell>13</cell><cell>None</cell><cell>TensorFlow [250]</cell></row><row><cell>FCSN [313]</cell><cell>Backg. (SuBSENSE)</cell><cell>Foreground</cell><cell>FCN/VGG-16 [334]</cell><cell></cell><cell>ReLU/Sigm.</cell><cell>20</cell><cell>3</cell><cell>TensorFlow [250]</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deep CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deep CNN [151]</cell><cell>Backg. (SuBSENSE</cell><cell>Foreground</cell><cell>CNN</cell><cell>Multi-Layer</cell><cell>ReLU/Sigm.</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>/FTSG)</cell><cell></cell><cell></cell><cell>Perceptron</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell>(MLP)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TCNN/Joint TCNN [163]</cell><cell>Backg.</cell><cell>Foreground</cell><cell>MCFC</cell><cell>DCGAN [335]/</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>Caffe [248]/DeepLab [336]</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell>(VGG-16)</cell><cell>Context Enc. [337]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ADCNN [302]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>T-CNN</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>7</cell><cell>None</cell><cell>Caffe [248]</cell></row><row><cell></cell><cell></cell><cell>(Bound. Box)</cell><cell>S-CNN, C-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SFEN [314]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>VGG-16</cell><cell>Attention</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GoogLeNet [195]</cell><cell>ConvLSTM/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet</cell><cell>STN/CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSFgNet [315]</cell><cell>Background (BENet [315])</cell><cell>Foreground</cell><cell></cell><cell>SMNet [315]</cell><cell>BiReLU [338, 339]</cell><cell>2</cell><cell>1</cell><cell>-</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Structured CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Struct CNN [155]</cell><cell>Back. (Median)</cell><cell>Foreground</cell><cell>VGG-16</cell><cell>-</cell><cell>PReLU [211]</cell><cell>13</cell><cell>-</cell><cell>Caffe [248]</cell></row><row><cell></cell><cell>Current Image t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Image t-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D ConvNet [319]</cell><cell>10 Frames</cell><cell>Foreground</cell><cell>C3D Branch [340]</cell><cell>-</cell><cell>-</cell><cell>6 (3D Conv.)</cell><cell>-</cell><cell>Caffe [248]</cell></row><row><cell>3D CNNs [320]</cell><cell>5 Frames</cell><cell>Foreground</cell><cell></cell><cell>-</cell><cell>tanh</cell><cell>4 (3D Conv.)</cell><cell>2</cell><cell>-</cell></row><row><cell>STA-3D ConvNets (ReMoteNet) [321]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>Modified C3D</cell><cell>ST Attention</cell><cell>ReLU</cell><cell>(3D Conv.)</cell><cell>-</cell><cell>TensorFlow [250]</cell></row><row><cell></cell><cell></cell><cell>(Bound. Box)</cell><cell>Branch [321]</cell><cell>ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D Atrous CNN [310]</cell><cell>Current Image</cell><cell>Foreground</cell><cell>3D Atrous</cell><cell>-</cell><cell>ReLU</cell><cell>5 (3D Conv.)</cell><cell>-</cell><cell>TensorFlow [250]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC3D [323]</cell><cell>16 frames</cell><cell>Foreground</cell><cell>3D-CNN</cell><cell>-</cell><cell>ReLU</cell><cell>3 (3D Conv.)</cell><cell>-</cell><cell>TensorFlow [250]</cell></row><row><cell>MFC3D [323]</cell><cell>16 frames</cell><cell>Foreground</cell><cell>3D-CNN</cell><cell>-</cell><cell>ReLU</cell><cell>3 (3D Conv.)</cell><cell>-</cell><cell>TensorFlow [250]</cell></row><row><cell>Generative Adversarial Networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BScGAN [325]</cell><cell>Back. (Median)</cell><cell>Foreground</cell><cell>cGAN [341]</cell><cell>-</cell><cell>Leaky ReLU/Tanh</cell><cell>8</cell><cell>-</cell><cell>Pytorch</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell>Discrim. net</cell><cell></cell><cell>Leaky ReLU/Sigm</cell><cell>4</cell><cell>-</cell><cell>Pytorch</cell></row><row><cell>BGAN [326]</cell><cell>Back. (Median)</cell><cell>Foreground</cell><cell>Bayesian GAN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BPVGAN [326]</cell><cell>Back. (Median)</cell><cell>Foreground</cell><cell>Paralell</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Current Image</cell><cell></cell><cell>Bayesian GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NUMOD [328]</cell><cell>Current Image</cell><cell>Back.</cell><cell>GFCN</cell><cell>-</cell><cell>ReLU/Sigm.</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Illum. Image</cell><cell></cell><cell>Bayesian GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Foreground</cell><cell></cell><cell>Bayesian GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>Deep  </p>Neural Networks Architecture in Background Subtraction: A Comparative Overview. "-" stands for "not indicated" by the authors.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Deep Neural  Networks in Background Subtraction: A Comparative Overview for Challenges. "-" stands for "not indicated" by the authors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Deep Neural Networks for Deep Learned Features: An Overview</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>Gain in terms of F-measure score in percentage over the eleven categories of the CDnet2014, namely, Baseline (BSL), Dynamic background (DBG), Camera jitter (CJT), Intermittent Motion Object (IOM), Shadows (SHD), Thermal (THM), Bad Weather (BDW), Low Frame Rate (LFR), Night Videos (NVD), PTZ, Turbulence (TBL). In bold, maximum gain.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.nvidia.fr/deep-learning-ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.asimovinstitute.org/neural-network-zoo/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://deeplearning.net/software/theano/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://torch.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://deeplearning.net/software-links/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://deeplearning4j.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://developer.nvidia.com/deep-learning-software</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://developer.nvidia.com/digits</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://developer.nvidia.com/tensorrt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://sites.google.com/site/backgroundsubtraction/background-initialization/neural-networks</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>http://pione.dinf.usherbrooke.ca/sbmc2016/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>http://scenebackgroundmodeling.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>https://sites.google.com/view/icdddataset/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_14"><p>https://sites.google.com/site/backgroundsubtraction/recent-background-modeling/deep-learning<ref type="bibr" target="#b57">16</ref> http://vcipl-okstate.org/pbvs/bench/ 17 http://benweinstein.weebly.com/deepmeerkat.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_15"><p>https://github.com/zhimingluo/MovingObjectSegmentation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_16"><p>http://caffe.berkeleyvision.org/tutorial/solver.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_17"><p>http://www.vlfeat.org/matconvnet/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_18"><p>https://github.com/lim-anggun/FgSegNet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_19"><p>https://github.com/lim-anggun/FgSegNet-v2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_20"><p>http://www.image-net.org/challenges/LSVRC/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_21"><p>https://www.cs.toronto.edu/ kriz/cifar.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_22"><p>https://github.com/facebook/fb.resnet.torch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_23"><p>http://www.cvg.reading.ac.uk/pets2007/data.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_24"><p>hhttps://github.com/zhaochenqiu/DPDL</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_25"><p>http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_26"><p>* indicated that the measures come from the corresponding papers otherwise the measures comes from the ChangeDetection.net website. In bold, the best score in each algorithm's category. The top 10 methods are indicated with their rank. There are three groups of leading methods: FgSegNet's group, 3D-CNNs group and GANs group.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories</head><p>Original Ground Truth 4-MOGStauffer <ref type="bibr" target="#b57">16</ref>-MOGMiller , DCP <ref type="bibr" target="#b311">[270]</ref>, BPVGAN <ref type="bibr" target="#b368">[327]</ref>. For DCP, the authors did not tested their algorithm on four categories.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Choo</surname></persName>
		</author>
		<title level="m">Methods Authors -Dates Deep Encoder-Decoder Networks Multi-scale Recurrent ED (MSRNN)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">292</biblScope>
		</imprint>
	</monogr>
	<note>Semi-supervised</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational autoencoders (DeepPBM)) [Unsupervised] Farnoosh et al</title>
		<author>
			<persName><forename type="first">Msrnn ;</forename><surname>Modified</surname></persName>
		</author>
		<author>
			<persName><surname>Choo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="volume">293</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Restricted</forename><surname>Boltzmann Machine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Gracewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">295</biblScope>
		</imprint>
	</monogr>
	<note>RBM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Robust Deep Encoder-Decoder Networks Robust Convolutional Autoencoders (RCAE)</title>
		<author>
			<persName><surname>Chalapathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">183</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dynamic Deep Autoencoders (DeepSphere)</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional Neural Networks [Supervised] CNN (ConvNets) Braham and Van Droogenbroeck</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">)</forename><surname>Cnn (convnets</surname></persName>
		</author>
		<author>
			<persName><surname>Bautista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="volume">152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-scale and cascaded CNN [Supervised] cascaded CNN</title>
		<imprint/>
	</monogr>
	<note>Ground-Truth</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">302</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fgsegnet-M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keles</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fgsegnet-S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keles</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">304</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fgsegnet-V2</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">306</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Guided</forename><surname>Multi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Scale Cnn</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">307</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Msednet</forename><surname>Patil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">308</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully CNNs [Supervised] Basic Fully CNN Cinelli</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Basic</forename><surname>Fully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cnn</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Multiview recep. field FCN (MV-FCN)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">309</biblScope>
		</imprint>
	</monogr>
	<note>Akilan et al.[310</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Multiscale</forename><surname>Fully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cnn (mfcn)</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
		<title level="m">MFCN with Contrast Layers (MFCN-CL) Zeng and Zhu</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cnn-Sfc (foreground</forename><surname>Masks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">156</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><surname>Fcsn) Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fully Conv. Semantic Net</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep CNNs [Supervised] Deep CNNs Babaee et al</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tcnn/Joint Tcnn</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adaptive</forename><surname>Deep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cnn ; Adcnn)</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">302</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sfen Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">314</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Structured</forename><surname>Cnns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Struct</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cnns</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Double Encoding CNNs [Supervised] Double Encoding/Slow Decoding CNNs (DESD) Akilan and Wu</title>
		<author>
			<persName><surname>Encoder-Decoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structured CNNs Le and Pham</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">316</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cnns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Supervised] 3D-CNNs Sakkos et al. [319</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D-Cnns</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">320</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sta-3d</forename><surname>Convnets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ReMoteNet) Yu et al. [321</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cnn</forename><surname>Atrous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Convlstm</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m">Multi-scale FC3D (MFC3D) Wang et al</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m">CNN with LSTM Akilan</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">318</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Retrospective Convolutions [Supervised] Atrous retrospective convolution</title>
		<author>
			<persName><surname>Arconv) Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><surname>Arpp) Chen</surname></persName>
		</author>
		<title level="m">Atrous Retrospective Pyramid Pooling</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Bscgan</forename><surname>Bakkay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gan</forename><surname>Bayesian</surname></persName>
		</author>
		<author>
			<persName><surname>Bgan) Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bayesian</forename><surname>Parallel Vision</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Bpvgan) Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Numod) Bahri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Neural Unsupervised Moving Object Detection</publisher>
			<biblScope unit="volume">328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Multi-Task Gan (mt-Gan)</forename><surname>Sakkos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Foreground GAN (FgGAN) Patil and Murala</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Table 2. Deep Neural Networks in Background Subtraction: An Overview References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Background subtraction in real applications: Challenges, current models and future directions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance analysis of moving object detection using bgs techniques in visual surveillance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Spatio-Temporal Data Science, Inderscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="53" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Traditional and recent approaches in background modeling for foreground detection: An overview</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="31" to="66" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Traditional Approaches in Background Modeling for Video Surveillance, Handbook Background Modeling and Foreground Detection for Video Surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<editor>Bouwmans, B. Hoferlin, F. Porikli, A. Vacavant</editor>
		<imprint>
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
		<respStmt>
			<orgName>Taylor and Francis Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Decomposition into low-rank plus additive matrices for background/foreground separation: A review for a comparative evaluation with a large-scale dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="71" />
			<date type="published" when="2017-02">2017. February 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust PCA via Principal Component Pursuit: A Review for a Comparative Evaluation in Video Surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue on Background Models Challenge, Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2014-05">2014. May 2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="22" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">OR-PCA with MRF for Robust Foreground Detection in Highly Dynamic Backgrounds</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision, ACCV 2014</title>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Background estimation for video surveillance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing New Zealand</title>
		<imprint>
			<biblScope unit="page" from="315" to="320" />
			<date type="published" when="2002">2002. 2002. 2002</date>
		</imprint>
	</monogr>
	<note>IVCNZ</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Median mixture model for background-foreground segmentation in video sequences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Graszka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Graphics, Visualization and Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<title level="m">Real-time adaptive histogram min-max bucket (hmmb) model for background subtraction, IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV 2000</title>
		<imprint>
			<date type="published" when="2000-06">2000. June 2000</date>
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Pulgarin-Giraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alvarez-Meza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Insuasti-Ceballos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellanos-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GMM Background Modeling using Divergence-based Weight Updating, Conference Ibero-american Congress on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 1999</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatial mixture of Gaussians for dynamic background modelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="63" to="68" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fuzzy integral for moving object detection</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Baf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vachon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2008</title>
		<imprint>
			<date type="published" when="2008-06">2008. June 2008</date>
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Type-2 fuzzy mixture of Gaussians model: Application to background modeling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Baf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vachon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing, ISVC 2008</title>
		<imprint>
			<date type="published" when="2008-12">2008. December 2008</date>
			<biblScope unit="page" from="772" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Background Subtraction For Visual Surveillance: A Fuzzy Approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Group</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-03">2012. March 2012</date>
			<biblScope unit="page" from="103" to="139" />
		</imprint>
	</monogr>
	<note>Handbook on Soft Computing for Video Surveillance</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive learning of multi-subspace for foreground detection under illumination changes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desouza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Background modeling and foreground detection via a reconstructive and discriminative subspace learning approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Marghes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasiu</surname></persName>
		</author>
		<idno>IPCV 2012</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing, Computer Vision, and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A Bayesian Computer Vision System for Modeling Human Interactions, ICVS 1999</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-01">January 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Combining ARF and OR-PCA background subtraction of noisy videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Image Analysis and Applications</title>
		<imprint>
			<date type="published" when="2015-09">2015. September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Depth Extended Online RPCA with Spatiotemporal Constraints for Robust Background Subtraction, Korea-Japan Workshop on Frontiers of Computer Vision, FCV 2015</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-01">January 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust background subtraction to global illumination changes via multiple features based OR-PCA with MRF</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">OR-PCA with dynamic feature selection for robust background subtraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Symposium On Applied Computing</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>SAC</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-adaptive SOM-CNN neural system for dynamic object detection in normal and complex scenarios</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramirez-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chacon-Murguia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-organizing retinotopic maps applied to background modeling for dynamic object segmentation in video sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez-Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chacon-Murguia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IJCNN 2013</title>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A system for counting people in video images using neural networks to identify the background scene</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stonham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1421" to="1428" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Vision modules for a multi sensory bridge monitoring approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems, ITSC 2004</title>
		<imprint>
			<date type="published" when="2004-10">2004. October 2004</date>
			<biblScope unit="page" from="971" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adaptive background estimation using an information theoretic cost for hidden state estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IJCNN 2011</title>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A Kalman filter based background updating algorithm robust to sharp illumination changes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Messelodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Modena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2005-09">2005. 2005. September 2005</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Wallflower: Principles and practice of background maintenance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumiit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV 1999</title>
		<imprint>
			<date type="published" when="1999-09">1999. September 1999</date>
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cand√®s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Motion detection via a couple of auto-encoder networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Changedetection.net: A new change detection benchmark dataset, IEEE Workshop on Change Detection, CDW 2012 in conjunction with CVPR 2012</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">CDnet 2014: an expanded change detection benchmark dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CDW 2014 in conjunction with CVPR 2014</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A benchmark dataset for foreground/background extraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lequievre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Background Models Challenge, ACCV 2012</title>
		<imprint>
			<date type="published" when="2012-11">November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><surname>Pfinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="780" to="785" />
			<date type="published" when="1997-07">1997. July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Efficient adaptive density estimation per image pixel for the task of background subtraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zivkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="773" to="780" />
			<date type="published" when="2006-01">2006. January 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Finite asymmetric generalized gaussian mixture models learning for infrared object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elguebaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Real-time video segmentation using Student&apos;s t mixture model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Ambient Systems, Networks and Technologies</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Student&apos;s t-distribution mixture background model for efficient object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Signal Processing, Communication and Computing</title>
		<imprint>
			<date type="published" when="2012-08">2012. August 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Background subtraction with Dirichlet processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV 2012</title>
		<imprint>
			<date type="published" when="2012-10">October 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Online variational learning of finite Dirichlet mixture models, Evolving Systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Adaptive background modeling integrated with luminosity sensors and occlusion processing for reliable vehicle detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1398" to="1412" />
			<date type="published" when="2011-12">2011. December 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A new background subtraction method using bivariate Poisson process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Information Hiding and Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2014-08">2014. August 2014</date>
			<biblScope unit="page" from="419" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Co-occurrence Probability based Pixel Pairs Background Model for Robust Object Detection in Dynamic Scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1374" to="1390" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Co-occurrence-based adaptive background model for robust object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal-Based Surveillance</title>
		<imprint>
			<date type="published" when="2013-09">2013. September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Robust object detection in severe imaging conditions using co-occurrence background model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Optomechatronics</title>
		<imprint>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2014-04">2014. April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Rosell-Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andreu-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodas-Jorda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atienza-Vanacloig</surname></persName>
		</author>
		<title level="m">Background Modelling in Demanding Situations with Confidence Measure, IAPR International Conference on Pattern Recognition, ICPR 2008</title>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Background modeling with motion criterion and multi-modal support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosell-Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atienza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications, VISAPP 2010</title>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">ViBe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06">2011. June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Flexible background subtraction with self-balanced local sensitivity</title>
		<author>
			<persName><forename type="first">P</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Change Detection Workshop</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
	<note>CDW</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A self-adjusting approach to change detection based on background word consensus</title>
		<author>
			<persName><forename type="first">P</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Non-linear Parametric Bayesian Regression for Robust Background Subtraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-12">2009. December 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Accurate and efficient background subtraction by monotonic second-degree polynomial fitting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Modeling of Dynamic Backgrounds by Type-2 Fuzzy Gaussians Mixture Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Baf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MASAUM Journal of Basic and Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="277" />
			<date type="published" when="2009-11">2009. November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<title level="m">A Fuzzy Background Modeling Approach for Motion Detection in Dynamic Backgrounds, International Conference on Multimedia and Signal Processing</title>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Fusing color and gradient features for background model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fusing color and texture features for background model, International Conference on Fuzzy Systems and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2006-09">2006. 2006. September 2006</date>
			<biblScope unit="page" from="887" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Foreground detection using the Choquet integral</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Baf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vachon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Image Analysis for Multimedia Interactive Integral</title>
		<imprint>
			<date type="published" when="2008-05">2008. 2008. May 2008</date>
			<biblScope unit="page" from="187" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Interval-valued model level fuzzy aggregation-based background subtraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chiranjeevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The detection of moving objects in video by background subtraction using Dempster-Shafer theory</title>
		<author>
			<persName><forename type="first">O</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Electronics and Communications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015-03">March 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Background modeling via a supervised subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image, Video Processing and Computer Vision, IVPCV 2010</title>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Background subtraction via incremental maximum margin criterion: A discriminative approach, Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marghes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-10">2012. October 2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1083" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Marghes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<title level="m">Background modeling via incremental maximum margin criterion, International Workshop on Subspace Methods, ACCV 2010 Workshop Subspace 2010</title>
		<imprint>
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Foreground detection based on low-rank and block-sparse matrix decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2012-09">2012. September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Foreground detection by robust PCA solved via a linearized alternating direction method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
		<idno>ICIAR 2012</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Moving object detection by robust PCA solved via a linearized symmetric alternating direction method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing, ISVC 2012</title>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Robust principal component analysis for background subtraction: Systematic evaluation and comparative analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Principal Component Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="238" />
			<date type="published" when="2012-03">2012. March 2012</date>
			<publisher>INTECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Moving Object Detection on RGB-D Videos using Graph Regularized Spatiotemporal RPCA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sultana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Superpixels based Manifold Structured Sparse RPCA for Moving Object Detectio, International Workshop on Activity Monitoring by Multiple Distributed Sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Background subtraction via superpixel-based online matrix decomposition with structured foreground constraints, Workshop on Robust Subspace Learning and Computer Vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12">2015. December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Double-constrained RPCA based on saliency maps for foreground detection in automated maritime surveillance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISBC 2015 Workshop conjunction with AVSS 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Background Subtraction via Fast Robust Matrix Completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on RSL-CV in conjunction with ICCV 2017</title>
		<imprint>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Moving Object Detection through Robust Matrix Completion Augmented with Objectness</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanamurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robust PCA and Robust Subspace Tracking: A Comparative Evaluation, Statistical Signal Processing Workshop</title>
		<imprint>
			<date type="published" when="2018-06">2018. June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Robust Subspace Learning: Robust PCA, Robust Subspace Tracking and Robust Subspace Recovery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanamurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="32" to="55" />
			<date type="published" when="2018-07">2018. July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Incremental gradient on the grassmannian for online foreground and background separation in subsampled video</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International on Conference on Computer Vision and Pattern Recognition, CVPR 2012</title>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Incremental principal component pursuit for video background modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Practical ReProCS for separating sparse and low-dimensional signal sequences from their sum</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A Fast and Memory-efficient Algorithm for Robust PCA (MEROP</title>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal, ICASSP 2018</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Stochastic decomposition into low rank and sparse tensor for robust background subtraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDP</title>
		<imprint>
			<date type="published" when="2015-07">2015. July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Online stochastic tensor decomposition for background subtraction in multispectral video sequences, Workshop on Robust Subspace Learning and Computer Vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Tensor robust principal component analysis with a new tensor nuclear norm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Tensor robust principal component analysis: Better recovery with atomic norm regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Driggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graberz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-01">January 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A probabilistic SVM approach for background scene initialization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2002-09">2002. 2002. September 2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="893" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A genetic approach to training support vector data descriptors for background modeling in video data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ambardekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing, ISVC 2007</title>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Novelty detection approach for foreground region detection in videos with quasi-stationary backgrounds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing, ISVC 2006</title>
		<imprint>
			<date type="published" when="2006-11">2006. November 2006</date>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Incremental svdd training: Improving efficiency of background modeling in videos, International Conference on Signal and Image Processing</title>
		<imprint>
			<date type="published" when="2008-08">2008. August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Robust video-based surveillance by integrating target detection with tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Object Tracking and Classification Beyond the Visible Spectrum in conjunction with CVPR 2006</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Improving target detection by coupling it with tracking, Machine Vision and Application</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Background pixel classification for motion detection in video image sequences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gil-Pita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Work Conference on Artificial and Natural Neural Network</title>
		<imprint>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="718" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Foreground-background segmentation in video sequences using neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems: Neural Networks and Applications</title>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">PNN based motion detection with adaptive learning rate</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence and Security</title>
		<meeting><address><addrLine>CIS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12">2009. 2009. December 2009</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A neural network approach to Bayesian background modeling for video object segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2006-02">2006. February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A self-organizing approach to detection of moving patterns for real-time applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Brain, Vision, and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4729</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<title level="m">A self-organizing neural system for background and foreground modeling, International Conference on Artificial Neural Networks, ICANN 2008</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="652" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Neural model-based segmentation of image motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KES</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A self organizing approach to background subtraction for visual surveillance applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1168" to="1177" />
			<date type="published" when="2008-07">2008. July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Multivalued background/foreground separation for moving object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Fuzzy Logic and Applications</title>
		<imprint>
			<date type="published" when="2009-06">2009. 2009. June 2009</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A fuzzy spatial coherence-based approach to background/foreground separation for moving object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications, NCA 2010</title>
		<imprint>
			<date type="published" when="2010-03">2010. March 2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">The SOBS algorithm: What are the limits?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Change Detection, CVPR 2012</title>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">The 3dSOBS+ algorithm for moving object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2014-05">2014. May 2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Simplified SOM-neural model for video segmentation of moving objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chacon-Muguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gonzalez-Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="474" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Improvement of a neural-fuzzy motion detection vision model for complex scenario conditions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chacon-Murguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramirez-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gonzalez-Duarte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IJCNN 2013</title>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A novel background subtraction approach based on multi-layered self organizing maps</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gemignani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">3D neural model-based stopped object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Self organizing and fuzzy modelling for parked vehicles detection, Advanced Concepts for Intelligent Vision Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACVIS</title>
		<imprint>
			<biblScope unit="page" from="422" to="433" />
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Stopped object detection by learning foreground model in videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="723" to="735" />
			<date type="published" when="2013-05">2013. May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01">2015. January 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alsaadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2017-04">2017. April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">ImageNet: Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Object Detection with Deep Learning: A Review</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page">580587</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">14401448</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Object Detection via Region-based Fully Convolutional Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Fcn</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Evaluating deep semantic segmentation networks for object detection in maritime surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Review on Deep Learning Techniques Applied to Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">A survey on deep learning techniques for image and video semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">A review of semantic segmentation using deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Instance Level Video Object Segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><surname>Maskrnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Unsupervised Video Object Segmentation for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Deep Motion Exploitation for Video Object Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmen-tation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Investigating the Application of Deep Convolutional Neural Networks in Semi-supervised Video Object Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sasikumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Technological University Dublin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master Science Thesis</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Deep video foreground target extraction with complex scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Sensor Networks and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="440" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Deep Learning with a Spatiotemporal Descriptor of Appearance and Motion Estimation for Video Anomaly Detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gunale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mukherji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MDPI Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Computer vision and deep learning techniques for pedestrian detection and tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brunettia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buongiorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bevilacqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page">1733</biblScope>
			<date type="published" when="2018-07">2018. July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Deep learning-based methods for person re-identification: A comprehensive review</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Survey on Deep Learning Techniques for Person Re-Identification Task</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Attention-based Few-Shot Person Re-identification using Meta Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">The Generalized Detection Method for the Dim Small Targets by Faster R-CNN Integrated with GAN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Communication and Information Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Landmark-Free FAME: Face Alignment, Modeling, and Expression Estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">FacePoseNet: Making a Case for Landmark-Free Face Alignment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICCVW</note>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">Deep Face Recognition: A Survey</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A review of Convolutional-Neural-Network-based action recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Embedding structured contour and location prior in siamesed fully convolutional networks for road detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="241" />
			<date type="published" when="2018-01">2018. January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Robust Hierarchical Deep Learning for Vehicular Management</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q W Z</forename><surname>Xiong</surname></persName>
		</author>
		<title level="m">ACM: Adaptive Cross-Modal Graph Convolutional Neural Networks for RGB-D Scene Recognition, AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Scene classification with recurrent attention of vhr remote sensing images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">GETNET: A General End-to-End 2-D CNN Framework for Hyperspectral Image Change Detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2019-01">2019. January 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Partially-sparse restricted Boltzmann machine for background modeling and subtraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2013-12">2013. December 2013</date>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="209" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Motion background modeling based on context-encoder</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Artificial Intelligence and Pattern Recognition, ICAIPR 2016</title>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Temporally adaptive restricted Boltzmann machine for background modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2015-01">2015. January 2015</date>
			<publisher>American Association for Artificial Intelligence</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Dynamic background learning through deep auto-encoder networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network for background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Convolutional neural network for vehicle detection in low resolution traffic videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manalac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Orbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TENCON</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Deep background subtraction with scene-specific convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Signals and Image Processing</title>
		<imprint>
			<date type="published" when="2016-05">2016. May 2016</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Anomaly detection in surveillance videos using deep residual networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cinelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02">February 2017</date>
		</imprint>
		<respStmt>
			<orgName>Universidade de Rio de Janeiro</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master Thesis</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Background subtraction using encoder-decoder structured convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal based Surveillance</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Combining background subtraction algorithms with convolutional neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Interactive deep learning method for segmenting moving objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Background subtraction using the factored 3-way restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Change detection by training a triplet network for motion feature extraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Embedded motion detection via neural response mixture background modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Real-time embedded motion detection via neural response mixture modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Deep learning driven blockwise moving object detection with binary scene modeling, Neurocomputing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Joint background reconstruction and foreground segmentation via a two-stage convolutional neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Deep learning-based scene-awareness approach for intelligent change detection in videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13038</biblScope>
			<date type="published" when="2019-02">2019. February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">A study on deep neural networks framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiusheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhigang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMCEC</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1519" to="1522" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Culloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943. 1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">The perceptron-a perceiving and recognizing automaton</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<idno>85-460-1</idno>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Aeronautical Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Generalization and information storage in networks of ADALINE</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Self Organizing Systems</title>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">30 years of adaptive neural networks: perceptron, madaline, and backpropagation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1415" to="1442" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Deep belief nets, NIPS Tutorial</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Improved techniques for training GANs</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<title level="m" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">A hierarchical fused fuzzy deep neural network for data classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1006" to="1012" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A Fuzzy Restricted Boltzmann Machine: Novel Learning Algorithms Based on the Crisp Possibilistic Mean Value of Fuzzy Numbers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">An Introduction to Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-09">2012. September 2012</date>
			<biblScope unit="volume">7441</biblScope>
			<biblScope unit="page" from="14" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">The l 2,1 -Norm Stacked Robust Autoencoders for Domain Adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Anomaly Detection with Robust Deep Autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robust</forename></persName>
		</author>
		<title level="m">Deep and Inductive Anomaly Detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Deep into hypersphere: Robust and unsupervised anomaly discovery in dynamic networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ertugrul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="2724" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">SimNets: A Generalization of Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<title level="m">Deep SimNets, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Deconstructing Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Combining spectral and spatial features for deep learning based blind speaker separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="457" to="468" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Gated residual networks with dilated convolutions for monaural speech enhancement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Robust speaker localization guided by deep learning-based time-frequency masking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="178" to="188" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation, ICLR 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T B P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06">2016. June 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Computer vision approaches based on deep learning and neural networks: Deep neural networks for video analysis of human pose estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nishani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mediterranean Conference on Embedded Computing</title>
		<meeting><address><addrLine>MECO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wide Compression: Tensor Ring Net, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="9329" to="9338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Horesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kilmer</surname></persName>
		</author>
		<title level="m">Stable Tensor Neural Networks for Rapid Deep Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tamari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<title level="m">Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions, International Conference on Learning Representations, ICLR 2018</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional Rectifier Networks as Generalized Tensor Decompositions, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">On the Expressive Power of Deep Learning: A Tensor Analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Compressing Deep Neural Networks via Adaptive Dimension Adjustment Tucker Decomposition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada-Tucker</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="104" to="115" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">FSNet: Compression of deep convolutional neural networks by filter summary</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b247">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<title level="m">Tensor ring decomposition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">A Survey of Model Compression and Accelerationfor Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">Mathematics of deep learning, Seminar, Univ</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>La Rochelle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018-11">2018. November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Optimal approximation of piecewise smooth functions using deep relu neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Voigtlaender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="296" to="330" />
			<date type="published" when="2018-12">2018. December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">10261034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b254">
	<monogr>
		<title level="m" type="main">Mathematics of deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Learning deep models: Critical points and local openness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nouiehed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">A critical view of global optimality in deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Robust Learning of Fixed-Structure Bayesian Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07">2017. July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title level="m" type="main">Analysis of universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Fast feature fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">NAG: Network for Adversary Generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Intriguing properties of neural networks, International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Robustness of conditional GANs to noisy labels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Denoising Adversarial Autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="968" to="984" />
			<date type="published" when="2019-04">2019. April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">303314</biblScope>
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Whit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforwardnetworks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Approximation and estimation bounds for artificial neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">8721886</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Global optimality in neural network training, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<monogr>
		<title level="m" type="main">Global optimality in tensor factorization, deep learning, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06">2014. June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Mianjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On the implicit bias of dropout, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Ising-dropout: a regularization method for training and compression of deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<monogr>
		<title level="m" type="main">How Robust are Deep Neural Networks?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Improving the Robustness of Deep Neural Networks via Stability Training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">On the stability of deep networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Stable Architectures for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reversible Architectures for Arbitrarily Deep Residual Neural Networks, AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="2811" to="2818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Improving Numerical Stability of Deep Network Training with Efficient Normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sharapov</surname></persName>
		</author>
		<author>
			<persName><surname>Fastnorm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Visualizing deep neural network by alternately image blurring and deblurring</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="162" to="172" />
			<date type="published" when="2018-01">2018. January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Deep neural networks for texture classification: A theoretical analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manoharkarki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName><surname>Gayaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="2018-01">2018. January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Analytics of deep neural network in change detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Analytics of deep neural network-based background subtraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MDPI Journal of Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Understanding Complex Deep Generative Models using Interactive Visual Experimentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Gan Lab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>VAST</note>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caffe</forename></persName>
		</author>
		<title level="m">Convolutional Architecture for Fast Feature Embedding, ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><surname>Matconvnet</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf" />
		<title level="m">Convolutional Neural Networks for MATLAB</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep neural network accelerator based on FPGA</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="254" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Scene Background Initialization: a Taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Extensive Benchmark and Survey of Modeling Methods for Scene Background Initialization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5244" to="5256" />
			<date type="published" when="2017-11">2017. November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<monogr>
		<title level="m" type="main">Background Model Initialization for Static Cameras, Handbook on Background Modeling and Foreground Detection for Video Surveillance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-07">July 2014</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">SBMI-LTD: Stationary Background Model Initialization based on Low-rank Tensor Decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Symposium on Applied Computing</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>SAC</note>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Motion-Aware Graph Regularized RPCA for Background Modeling of Complex Scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<title level="m">Spatiotemporal Low-rank Modeling for Complex Scene Background Initialization, IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Comparison of Matrix Completion Algorithms for Background Initialization in Videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahzah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Background Estimation as a Labeling Problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV 2005</title>
		<imprint>
			<date type="published" when="2005-10">2005. October 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">CNN-Based Initial Background Estimation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Halfaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bouzaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Urfalioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scene Background Modeling Contest in conjunction with ICPR 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<monogr>
		<title level="m" type="main">Background Extraction Based on Joint Gaussian Conditional Random Fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Transactions on Circuits and Systems for Video Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Background-Foreground Modeling Based on Spatio-temporal Sparse Subspace Clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5840" to="5854" />
			<date type="published" when="2017-12">2017. December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">LaBGen-P: A pixel-level stationary background generation method based on LaBGen</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laugraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pierard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scene Background Modeling Contest in conjunction with ICPR 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">A method based on motion detection for generating the background of a scene</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laugraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pierard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">LaBGen-P-Semantic: A First Step for Leveraging Semantic Segmentation in Background Generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laugraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pierard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MDPI Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<title level="m">Towards benchmarking scene background initialization, New Trends in Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2015-09">2015. September 2015</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
	<note>SBMI 2015 with ICIAP 2015</note>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Background subtraction using Gaussian-Bernoulli restricted Boltzmann machine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rafique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Background scene modeling for PTZ cameras using RBM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rafique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Control, Automation and Information Sciences</title>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014</date>
			<biblScope unit="page" from="165" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Background modelling based on generative Unet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palasek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<monogr>
		<title level="m" type="main">Unsupervised deep context prediction for background estimation and foreground segmentation, Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sultana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sultana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<title level="m">Unsupervised RGBD Video Object Segmentation using GANs, ACCV-Workshops</title>
		<imprint>
			<date type="published" when="2018-12">2018. December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<monogr>
		<title level="m" type="main">Illumination Invariant Foreground Object Segmentation using ForeGAN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sultana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Statistical modeling of complex background for foreground object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1459" to="1472" />
			<date type="published" when="2004-11">2004. November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Learning with dynamic group sparsity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV 2009</title>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Background subtraction via robust dictionary learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
	<note>IVP</note>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Online robust dictionary learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
	<note>IVP</note>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Background modeling by weightless neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SBMI 2015 Workshop in conjunction with ICIAP 2015</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Temporal weighted learning model for background estimation with an automatic re-initialization stage and adaptive parameters update</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramirez-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez-Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chacon-Murguia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Interactive digital photomontage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="302" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<monogr>
		<title level="m" type="main">Robust foreground detection using smoothness and arbitrariness constraints</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">2014. September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<monogr>
		<title level="m" type="main">Online robust subspace tracking from partial information</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luiz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosus</forename></persName>
		</author>
		<title level="m">Grassmannian Online Subspace Updates with Structured-sparsity, International Conference on Computer Vision, ICCV 2013</title>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">GoDec: randomized low-rank and sparse matrix decomposition in noisy case</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Moving object detection by detecting contiguous outliers in the low-rank representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">RGB-D dataset: Background learning for detection and tracking from RGBD videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Alcover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">CwisarDH+: Background detection in RGBD videos by learning of weightless neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="242" to="253" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Simultaneous video stabilization and moving object detection in turbulence</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>PAMI</note>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Background recovery from video sequences via online motion-assisted rpca, Visual Communications and Image Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VCIP</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Total variation regularized RPCA for irregularly moving object detection under dynamic background</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1014" to="1027" />
			<date type="published" when="2016-04">2016. April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Multi-scale recurrent encoder-decoder network for dense temporal classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Learning background subtraction by video synthesis and multi-scale recurrent networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision, ACCV 2018</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<monogr>
		<title level="m" type="main">DeepPBM: deep probabilistic background model estimation from video sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farnoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint (February</note>
</biblStruct>

<biblStruct xml:id="b336">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gracewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<title level="m">Dynamic background modeling using deep learning autoencoder network, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2019-03">2019. March 2019</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Deep background subtraction of thermal and visible imagery for pedestrian detection in videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Brain Inspired Cognitive Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Scene-specific convolutional neural networks for video-based biodiversity detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Automatic fish detection in underwater videos by a deep neural network-based hybrid motion learning system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shortis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khurshid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schwanecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICES Journal of Marine Science</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main">Background subtraction based on deep pixel distribution learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Background subtraction on depth videos with convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">Moving-camera video surveillance in cluttered environments using deep features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="2296" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main">Adaptive deep convolutional neural networks for scene-specific object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b344">
	<monogr>
		<title level="m" type="main">Foreground segmentation using a triplet convolutional neural network for multiscale feature encoding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Keles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Foreground segmentation using convolutional neural networks for multiscale feature encoding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="256" to="262" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<monogr>
		<title level="m" type="main">Learning multi-scale features for foreground segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><surname>Keles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">Multiscale cascaded scene-specific convolutional neural networks for background subtraction, Pacific Rim Conference on Multimedia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PCM</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="524" to="533" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Deep background subtraction with guided learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo, ICME 2018</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">MsEDNet: Multi-Scale Deep Saliency Learning for Moving Object Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1670" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Deep background modeling using fully convolutional network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">254262</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<monogr>
		<title level="m" type="main">A foreground inference network for video surveillance using multi-view receptive field</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Multiscale fully convolutional network for foreground object detection in infrared videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Background subtraction using multiscale fully convolutional network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="16010" to="16021" />
			<date type="published" when="2018-03">2018. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Foreground detection in surveillance video with fully convolutional semantic network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="4118" to="4122" />
			<date type="published" when="2018-10">2018. October 2018</date>
		</imprint>
	</monogr>
	<note>ICIP</note>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Pixel-wise deep sequence learning for moving object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">MSFgNet: A Novel Compact End-to-End Deep Network for Moving Object Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Encoder-decoder convolutional neural network for change detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CITA</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Double Encoding -Slow Decoding Image to Image CNN for Foreground Identification with Application Towards Intelligent Transportation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Internet of Things, Green Computing and Communications</title>
		<imprint>
			<publisher>Cyber, Physical and Social Computing</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="395" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Video foreground localization from traditional methods to deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Windsor</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b360">
	<monogr>
		<title level="m" type="main">End-to-end video background subtraction with 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sakkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12">2017. December 2017</date>
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Background Subtraction via 3D Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1271" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<monogr>
		<title level="m" type="main">ReMotENet: efficient relevant motion event detection for large-scale home surveillance videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">3D Atrous Convolutional Long Short-Term Memory Network for Background Subtraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<monogr>
		<title level="m" type="main">Foreground detection with deeply learned multi-scale spatial-temporal features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MDPI Sensors</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<monogr>
		<title level="m" type="main">Learning to detect instantaneous changes with retrospective convolution and static sample synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b366">
	<analytic>
		<title level="a" type="main">BSCGAN: deep background subtraction with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakkay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salmane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ruichek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018-10">2018. October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Background Subtraction Algorithm based on Bayesian Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b368">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang ; Bayesian</surname></persName>
		</author>
		<author>
			<persName><surname>Gans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b369">
	<monogr>
		<title level="m" type="main">Online illumination invariant moving object detection by generative neural network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b370">
	<analytic>
		<title level="a" type="main">Illumination-aware multi-task gans for foreground segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sakkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Fggan: A cascaded unpaired learning for background estimation and foreground segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1770" to="1778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">1998. November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b373">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b374">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b375">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b376">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b377">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b378">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b379">
	<analytic>
		<title level="a" type="main">DehazeNet: An end-to-end system for single image haze removal</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b380">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Adjustable bounded rectifiers: Towards deep binary representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">D: generic features for video analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<monogr>
		<title level="m" type="main">Image to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b383">
	<monogr>
		<title level="m" type="main">How far can you get by combining change detection algorithms?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<idno>CoRR abs/1505.02921</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b384">
	<analytic>
		<title level="a" type="main">A texton-based kernel density estimation approach for background modeling under extreme conditions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kavasidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Image Understanding</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b385">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">10261034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b386">
	<analytic>
		<title level="a" type="main">Spatio-temporal context for codebook-based dynamic background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEU-Int. J. Electron. Commun</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="739" to="747" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b387">
	<monogr>
		<title level="m" type="main">Background segmentation with feedback: The pixel-based adaptive segmenter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiefenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-06">2012. June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b388">
	<analytic>
		<title level="a" type="main">Pixel-to-model distance for robust background reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits Systems and Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="903" to="916" />
			<date type="published" when="2016-05">2016. May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">An Improved Video foreground Extraction Strategy using Multi-view Receptive Field and EnDec CNN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b390">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">Stacked multi-layer self-organizing map for background modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<analytic>
		<title level="a" type="main">Static and moving object detection using flux tensor with split Gaussian models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bunyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palaniappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b393">
	<analytic>
		<title level="a" type="main">Learning sharable models for robust background subtraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo, ICME 2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<analytic>
		<title level="a" type="main">Transferring a generic pedestrian detector towards specific scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="3274" to="3281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b395">
	<analytic>
		<title level="a" type="main">Unsupervised activity perception in crowded and complicated scenes using hierarchical Bayesian models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">539555</biblScope>
			<date type="published" when="2009-03">2009. March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b396">
	<analytic>
		<title level="a" type="main">Labeled dataset for integral evaluation of moving object detection algorithms: LASIESTA</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yaoez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b397">
	<analytic>
		<title level="a" type="main">sEnDec: An improved image to image CNN for foreground localization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Transactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b398">
	<monogr>
		<title level="m" type="main">Real-time robust background subtraction under rapidly changing illumination conditions, Image Vision and Computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vosters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gritti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">10041015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b399">
	<analytic>
		<title level="a" type="main">Universal multimode background subtraction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3249" to="3260" />
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b400">
	<analytic>
		<title level="a" type="main">Moving object detection in time-lapse or motion trigger image sequences using low-rank and invariant sparse decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5133" to="5141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b401">
	<analytic>
		<title level="a" type="main">Spatio-temporal self-organizing map deep network for dynamic object detection from videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vison and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-06">2017. June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b402">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b403">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b404">
	<analytic>
		<title level="a" type="main">On the role and the importance of features for background modeling and foreground detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marghes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frelicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="26" to="91" />
			<date type="published" when="2018-05">2018. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b405">
	<analytic>
		<title level="a" type="main">Color space selection for self-organizing map based foreground detection in video sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lopez-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luque-Baena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dominguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2014-07">2014. July 2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="3347" to="3354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b406">
	<analytic>
		<title level="a" type="main">Optimal color space based probabilistic foreground detector for video surveillance systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1637" to="1641" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>ISIE</note>
</biblStruct>

<biblStruct xml:id="b407">
	<analytic>
		<title level="a" type="main">Tracking-based non-parametric background-foreground classification in a chromaticity-gradient space</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2010-09">2010. September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b408">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Background modeling using adaptive properties of hybrid features, International Conference on Advanced Video and Signal-Based Surveillance</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b409">
	<analytic>
		<title level="a" type="main">A texture-based method for modeling the background and detecting moving objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="662" />
			<date type="published" when="2006">2006. 2006. 2006</date>
		</imprint>
	</monogr>
	<note>PAMI</note>
</biblStruct>

<biblStruct xml:id="b410">
	<analytic>
		<title level="a" type="main">An eXtended center-symmetric local binary pattern for background modeling and subtraction in videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frelicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<imprint>
			<date type="published" when="2015-03">2015. March 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b411">
	<analytic>
		<title level="a" type="main">Incorporating estimated motion in real-time background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing, ICIP 2011</title>
		<imprint>
			<date type="published" when="2011-09">2011. September 2011</date>
			<biblScope unit="page" from="3265" to="3268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b412">
	<analytic>
		<title level="a" type="main">Motion-based background subtraction using adaptive kernel density estimation</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
	<note>CVPR 2004</note>
</biblStruct>

<biblStruct xml:id="b413">
	<analytic>
		<title level="a" type="main">Exploiting Color and Depth for Background Subtraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="254" to="265" />
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b414">
	<monogr>
		<title level="m" type="main">Advanced background modeling with RGB-D sensors through classifiers combination and inter-frame foreground prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jaureguizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Machine Vision and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">A Benchmarking Framework for Background Subtraction in RGBD Videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Alcover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="219" to="229" />
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<monogr>
		<title level="m" type="main">Background subtraction model based on color and depth cues</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandez-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Machine Vision and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b417">
	<analytic>
		<title level="a" type="main">Modelling depth for nonparametric foreground segmentation using RGBD devices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moya-Alcover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Capo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Varona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Online weighted one-class ensemble for feature selection in background/foreground separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frelicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition, ICPR 2016</title>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Superpixel-based online wagging one-class ensemble for feature selection in background/foreground separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frelicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<title level="m">Background subtraction based on deep convolutional neural networks features, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<analytic>
		<title level="a" type="main">Background modeling for video sequences by stacked denoising autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>De Lazcano-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luque-Baena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molina-Cabello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conference of the Spanish Association for Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2018-09">2018. September 2018</date>
		</imprint>
	</monogr>
	<note>CAEPIA</note>
</biblStruct>

<biblStruct xml:id="b422">
	<monogr>
		<title level="m" type="main">Stochasticnet: Forming deep neural networks via stochastic connectivity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<analytic>
		<title level="a" type="main">Efficient hierarchical method for background subtraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b424">
	<analytic>
		<title level="a" type="main">Factored 3-way restricted Boltzmann machines for modeling natural images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b425">
	<analytic>
		<title level="a" type="main">Evaluation of the robustness of deep features on the change detection problem</title>
		<author>
			<persName><forename type="first">O</forename><surname>Karadag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Erdas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing and Communications Applications Conference</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b426">
	<analytic>
		<title level="a" type="main">New trend in video foreground detection using deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Midwest Symposium on Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="889" to="892" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>MWSCAS</note>
</biblStruct>

<biblStruct xml:id="b427">
	<analytic>
		<title level="a" type="main">A Robust Background Initialization Algorithm with Superpixel Motion Detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019-02">2019. February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b428">
	<analytic>
		<title level="a" type="main">Extracting a Background Image by a Multi-modal Scene Background Model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scene Background Modeling workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">Motion detection: Unsolved issues and [potential] solutions, Invited Talk</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SBMI 2015 with ICIAP 2015</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<analytic>
		<title level="a" type="main">Semantic Background Subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pierard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Spectral-360: A Physics-Based Technique for Change Detection, IEEE Change Detection Workshop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sedky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moniri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chibelushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CDW</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<monogr>
		<title level="m" type="main">Robust Auto-encoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Worcester Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b433">
	<monogr>
		<title level="m" type="main">Background Subtraction with Real-time Semantic Segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b434">
	<analytic>
		<title level="a" type="main">Compressive online video backgroundforeground separation using multiple prior information and optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prativadibhayankaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MDPI Journal of Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b435">
	<analytic>
		<title level="a" type="main">Translational and rotational jitter invariant incremental principalcomponent pursuit for video background modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b436">
	<analytic>
		<title level="a" type="main">Background subtraction for moving object detection in rgb-d data: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MDPI Journal of Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b437">
	<analytic>
		<title level="a" type="main">Self-organizing background subtraction using color and depth data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b438">
	<analytic>
		<title level="a" type="main">The effect of recovery algorithms on compressive sensing background subtraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Workshop Sensor Data Fusion: Trends, Solutions, and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b439">
	<monogr>
		<title level="m" type="main">About pyramid structure in convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b440">
	<analytic>
		<title level="a" type="main">Deep restricted kernel machines using conjugate feature duality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2123" to="2163" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b441">
	<analytic>
		<title level="a" type="main">Time: A training-in-memory architecture for memristor-based deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/EDAC/IEEE Design Automation Conference</title>
		<imprint>
			<date type="published" when="2017-06">2017. June 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b442">
	<analytic>
		<title level="a" type="main">Fully memristive neural networks for pattern classification with unsupervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Electronics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b443">
	<analytic>
		<title level="a" type="main">On-chip training of memristor based deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017-05">2017. May 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3527" to="3534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b444">
	<analytic>
		<title level="a" type="main">Analog back propagation learning circuits for memristive crossbar neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Krestinskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ISCAS</note>
</biblStruct>

<biblStruct xml:id="b445">
	<monogr>
		<title level="m" type="main">Learning in memristive neural network architectures using analog backpropagation circuits</title>
		<author>
			<persName><forename type="first">O</forename><surname>Krestinskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b446">
	<analytic>
		<title level="a" type="main">Memristor-based circuit design for multilayer neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="677" to="686" />
			<date type="published" when="2018-02">2018. February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b447">
	<analytic>
		<title level="a" type="main">New trends on moving object detection in video images captured by a moving camera: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1257" to="1117" />
			<date type="published" when="2018-05">2018. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
