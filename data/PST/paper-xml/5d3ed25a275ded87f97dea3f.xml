<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Unified Embedding for Visual Search at Pinterest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
							<email>andrew@pinterest.com</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Discovery</orgName>
								<address>
									<country>Pinterest</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Visual Discovery</orgName>
								<address>
									<country>Pinterest</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
							<email>etzeng@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
							<email>dhukpark@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
							<email>crosenberg@pinterest.com</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Discovery</orgName>
								<address>
									<country>Pinterest</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Unified Embedding for Visual Search at Pinterest</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330739</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-task learning</term>
					<term>embedding</term>
					<term>visual search</term>
					<term>recommendation systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>At Pinterest, we utilize image embeddings throughout our search and recommendation systems to help our users navigate through visual content by powering experiences like browsing of related content and searching for exact products for shopping. In this work we describe a multi-task deep metric learning system to learn a single unified image embedding which can be used to power our multiple visual search products. The solution we present not only allows us to train for multiple application objectives in a single deep neural network architecture, but takes advantage of correlated information in the combination of all training data from each application to generate a unified embedding that outperforms all specialized embeddings previously deployed for each product. We discuss the challenges of handling images from different domains such as camera photos, high quality web images, and clean product catalog images. We also detail how to jointly train for multiple product objectives and how to leverage both engagement data and human labeled data. In addition, our trained embeddings can also be binarized for efficient storage and retrieval without compromising</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Following the explosive growth in engagement with online photography and videos, visual embeddings have become increasingly critical in search and recommendation systems. Content based image retrieval systems (visual search) is one prominent application that heavily relies on visual embeddings for both ranking and retrieval as users search by providing an image. In recent years, visual search has proliferated across a portfolio of companies including Alibaba's Pailitao <ref type="bibr" target="#b33">[34]</ref>, Pinterest Flashlight and Lens <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b10">[11]</ref>. Google Lens, Microsoft's Visual Search <ref type="bibr" target="#b8">[9]</ref>, and Ebay's Visual Shopping <ref type="bibr" target="#b29">[30]</ref>. These applications support a wide spectrum of use cases from Shopping where a user is searching for the exact item to Discovery <ref type="bibr" target="#b31">[32]</ref> where a user is browsing for inspirational and related content. These interactions span both real world (phone camera) and online (web) scenarios.</p><p>Over 250M users come to Pinterest monthly to discover ideas for recipes, fashion, travel, home decor, and more from our content corpus of billions of Pins. To facilitate discovery, Pinterest offers a variety of products including text-to-pin search, pin-to-pin recommendations <ref type="bibr" target="#b13">[14]</ref>, and user-to-pin recommendations. Throughout the years, we've built a variety of visual search products (Figure <ref type="figure">1</ref>) including Flashlight (2016), Lens (2017), and automated Shop-the-Look (2018) to further empower our users to use images (web or camera) as queries for general browsing or shopping <ref type="bibr" target="#b31">[32]</ref>. With over 600 million visual searches per month and growing, visual search is one of the fastest growing products at Pinterest and of increasing importance.</p><p>We have faced many challenges training and deploying generations of visual embeddings over the years throughout the search and recommendation stack at Pinterest. The difficulties can be summarized to the following four aspects:</p><p>Different Applications have Different Objectives: Pinterest uses image embeddings for a variety of tasks including retrieval (pin and image), ranking (text, pin, user, image queries), classification or regression (e.g. neardup classification, click-through-rate prediction, image type), and upstream multi-modal embedding models (PinSAGE <ref type="bibr" target="#b18">[19]</ref>). One observation we made with these multiple applications is that optimization objectives are not the same. Take our visual search products in Figure <ref type="figure">1</ref> as examples: Flashlight optimizes for browsing relevance within Pinterest catalog images. Lens optimizes for browsing Pinterest catalog images from camera photos; hence overcoming the domain shift of camera to Pinterest images is necessary. Finally Shop-the-Look optimizes for searching for the exact product from objects in a scene for shopping.</p><p>Embedding Maintenance/Cost/Deprecation: Specialized visual embeddings per application are the clearest solution to optimizing for multiple consumers and is the paradigm operated at Pinterest prior to 2018. This however has significant drawbacks. For our visual search products alone, we developed three specialized embeddings. As image recognition architectures are evolving quickly <ref type="bibr" target="#b12">[13]</ref> [21] [7] <ref type="bibr" target="#b27">[28]</ref> [10], we want to iterate our three specialized embeddings with modern architectures to improve our three visual search products. Each improvement in embedding requires a full back-fill for deployment, which can be prohibitively expensive. In practice, this situation is further exacerbated by downstream dependencies (e.g. usage in pin-to-pin ranking <ref type="bibr" target="#b13">[14]</ref>) on various specific versions of our embeddings, leading us to incrementally continue to extract multiple generations of the same specialized embeddings. All these considerations make the unification of specialized embeddings into one general embedding very attractive, allowing us clear tracking of external dependency in one lineage along with scalability to support future optimization targets.</p><p>Effective Usage of Datasets: At Pinterest, there are various image data sources including engagement feedback (e.g. pin-topin click data <ref type="bibr" target="#b13">[14]</ref>, Pin-to-Board graph when users save Pins into collections called Boards <ref type="bibr" target="#b1">[2]</ref>) and human curation. When training specialized embeddings for a specific task, deciding what datasets to use or collect is a non-trivial issue, and the choice of data source is often based on human judgement and heuristics which could be suboptimal and not scalable. Multi-task learning simplifies this by allowing the model to learn what data is important for which task through end-to-end training. Through multi-task learning, we want to minimize the amount of costly human curation while leveraging as much engagement data as possible.</p><p>Scalable and Efficient Representation: With billions of images and over 250M+ monthly active users, Pinterest has a requirement for an image representation that is cheap to store and also computationally efficient for common operations such as distance for image similarity. To leverage the large amount of training data that we receive from our user feedback cycles, we also need to build efficient model training procedures. As such, scalablity and efficiency are required both for inference and training.</p><p>In our paper, we describe our implementation, experimentation, and productionization of a unified visual embedding, replacing the specialized visual embeddings at Pinterest. The main contributions of this paper are (1) we present a scalable multi-task metric learning framework (2) we present insights into creating efficient multi-task embeddings that leverage a combination of human curated and engagement datasets (3) we present lessons learned when scaling training of our metric learning method and (4) comprehensive user studies and AB experiments on how our unified embeddings compare against the existing specialized embeddings across all visual search products in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Visual Search Systems</head><p>Visual search has been adopted throughout industry with Ebay <ref type="bibr" target="#b29">[30]</ref>, Microsoft <ref type="bibr" target="#b8">[9]</ref>, Alibaba <ref type="bibr" target="#b33">[34]</ref>, Google (Lens), and Amazon launching their own products. There has also been an increasing amount of research on domain-specific image retrieval systems such as fashion <ref type="bibr" target="#b28">[29]</ref> and product <ref type="bibr" target="#b0">[1]</ref> recommendations. Compared to others, Pinterest has not just one but a variety of visual search products (Figure <ref type="figure">1</ref>), each with different objectives. We focus on addressing the challenges of unifying visual embeddings across our visual search products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metric Learning</head><p>Standard metric learning approaches aim to learn image representations through the relationships between images in the form of pairs <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b0">[1]</ref> or triplets <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b19">[20]</ref>. Similarity style supervision are used to train the representation such that similar images are close in embedding space and dissimilar images apart. Sampling informative negatives is an important challenge of these pair or triplet based approaches, a focus of recent methods such as <ref type="bibr" target="#b22">[23]</ref> [22] <ref type="bibr" target="#b25">[26]</ref>.</p><p>An alternative approach to metric learning are classification based methods <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b32">[33]</ref> which alleviate the need of negative sampling. These methods have recently have been shown to achieve SOTA results across a suite of retrieval tasks <ref type="bibr" target="#b32">[33]</ref> compared with pair or triplet based methods. Given the simplicity and effectiveness of formulating metric learning as classification, we build off the architecture proposed in <ref type="bibr" target="#b32">[33]</ref> and extend it to multi-task for our unified embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Task Learning</head><p>Multi-task learning aims to learn one model that provides multiple outputs from one input <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b17">[18]</ref>. By consolidating multiple singletask models into one multi-task model, previous work have seen both efficiency <ref type="bibr" target="#b24">[25]</ref> and performance <ref type="bibr" target="#b11">[12]</ref> [15] <ref type="bibr" target="#b17">[18]</ref> improvements on each task due to inherent complementary structure that exists in separate visual tasks <ref type="bibr" target="#b30">[31]</ref>. Prior work also investigate how to learn to balance multiple loss objectives to optimize performance <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b2">[3]</ref>. In the context of metric learning, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b34">[35]</ref> explore the idea of learning a conditional mask for each task to modulate either the learned embedding or the internal activations. In our paper, we experiment with multi-task metric learning and evaluate its effects on our web scale visual search products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Problem Setup</head><p>Pinterest is a visual discovery platform in which the contents are predominantly images. To empower the users on Pinterest to browse visually inspired contents and search for an exact item in the image for shopping, we have built three services shown in Figure <ref type="figure">1</ref>: Flashlight, Lens, and Shop-The-Look (STL). Flashlight enables the users to start from the images on Pinterest (or web), and recommends relevant Pins inspired by the input images for the users to browse. Similarly, Lens aims to recommend visually relevant Pins based on the photos our users take with their cameras. STL, on the other hand, searches for products which are best match to the input images for the users to shop. The three services either serve images from different domains (web images v.s. camera photos), or with different objectives (browsing v.s. searching).</p><p>With both cost and engineering resource constraints and the interest of improved performance, we aim to learn one unified image embedding that can perform well for all three tasks. In essence, we would like to learn high quality embeddings of images that can be used for both browsing and searching recommendations. The relevance or similarity of a pair of images is represented as the distance between the respective embeddings. In order to train such embeddings, we collected a dataset for each task addressing its specific objective (Figure <ref type="figure" target="#fig_0">2</ref>), and frame the problem as multitask metric learning that jointly optimizes the relevance for both browsing and searching. We will describe how we collect the dataset for each task, the detailed model architecture, and how we set up multi-task training in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>We describe our training datasets and show some examples in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Flashlight Dataset.</head><p>The Flashlight dataset is collected to define browse relevance for Pinterest (web) images. As a surrogate for relevance, we rely on engagement through feedback from our users in a similar manner to Memboost described in <ref type="bibr" target="#b13">[14]</ref> to generate the dataset. Image sets are collected where a given query image has a set of related images ranked via engagement, and we assign each image set a label (unique identifier) that is conceptually the same as semantic class label. We apply a set of strict heuristics (e.g. number of impressions and interactions for each image in the set) to reduce the label noise of the dataset, resulting in around 800K images in 15K semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Lens Dataset.</head><p>The Lens dataset is collected to define browse relevance between camera photo images and Pinterest images. When collecting the training dataset for prototyping Lens, we found that camera photo engagement on Pinterest is very sparse and as such any dataset collected via user engagement would be too noisy for training. The main obstacle we need to overcome is the domain shift between camera photos and Pinterest images, so for the training set, we collected a human labeled dataset containing 540K images with 2K semantic classes. These semantic classes range from broad categories (e.g. tofu) to fine-grained classes (e.g. the same denim jacket in camera and product shots). Most importantly, this dataset contains a mix of product, camera, and Pinterest images under the same semantic label so that the embeddings can learn to overcome the domain shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Shop-</head><p>The-Look Dataset. The Shop-The-Look dataset is collected to define search relevance for an object in a home decor scene to its product match. To bootstrap the product, we collected a human labeled dataset containing 340K images with 189 product class label (e.g. Bar Stools) and 50K instance labels. Images with the same instance label are either exact matches or are very similar visually as defined by an internal training guide for criteria such as design, color, and material. The overall architecture for multi-task metric learning network. The proposed classification network as proxy-based metric learning is simple and flexible for multi-task learning. Our proposed method also has the binarization module to make learned embedding memory efficient, and the subsampling module is scalable to support large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>Figure <ref type="figure" target="#fig_1">3</ref> illustrates our overall setup for multi-task metric learning architecture. We extend the classification-based metric learning approach of Zhai et al. <ref type="bibr" target="#b32">[33]</ref> to multi-task. All the tasks share a common base network until the embedding is generated, where each task then splits into their own respective branches. Each task branch is simply a fully connected layer (where the weights are the proxies) followed by a softmax (no bias) cross entropy loss. There are four softmax tasks, Flashlight class, Shop-the-Look (STL) product class, STL instance class, and Lens category class. For the Flashlight class and STL instance tasks, the proxies are subsampled using our subsampling module before input to the fully connected layer for efficient training.</p><p>There are two essential modules for web-scale applications: a subsampling module to make our method scalable to hundreds of thousands of classes, and binarization module to make learned embedding storage and operation efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Subsampling Module.</head><p>Given N images in a batch and M proxies to target each with an embedding dimension of D, to compute the similarity (dot product) of embeddings to proxies, we do a NxD by DxM matrix multiplication in the fully connected layer. As such, computation increases with the number of proxy classes (M), an undesirable property as we scale M. Furthermore, we may not even be able to fit the proxy bank (MxD matrix) in GPU memory as we scale M to millions of classes (user engagement training data can easily generate this many classes). To address these issues, we store the proxy bank in CPU RAM (more available than GPU memory and disk can be used later if necessary) along with also implementing class subsampling. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, for each training batch, the subsampling module samples a subset of all classes for optimization. The sampled subset is guaranteed to have all the ground truth class labels of the images in the training batch (the label index slot will change however to ensure the index is within bounds of the number of classes sampled). The pseudocode for the forward pass is provided in Algorithm 1. During the training forward pass for efficiency, the proxies of the sampled classes are moved to GPU for computation asynchronously while the embedding is computed from the base network. The softmax loss only considers the sampled classes. For example, if we subsample only 2048 classes for each iteration, the maximum loss from random guessing is ln(2048) ≈ 7.62.  bandwidth for downstream consumers (e.g. I/O costs to fully ingest the embeddings into map reduce jobs) (3) improve the latency of real-time scoring (e.g. computing the similarity of two embeddings for ranking). In the prior work <ref type="bibr" target="#b32">[33]</ref>, we see that embeddings learned from the classification based metric learning approach can be binarized by thresholding at zero with little drop in performance.</p><p>We consider everything after the global pooling layer but before the task specific branches as the "binarization" module. In this work, instead of LayerNorm as proposed in <ref type="bibr" target="#b32">[33]</ref>, we propose to use GroupNorm <ref type="bibr" target="#b26">[27]</ref> that is better suited for multi-task applications. The empirical results are provided in Section 4.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>Shown in Figure <ref type="figure" target="#fig_1">3</ref>, we train our model in a multi-tower distributed setup. We share parameters throughout the network with the exception of the sparse proxy parameters. Each node (out of eight) has its own full copy of the CPU Embedding bank as sparse parameters cannot be distributed at this moment by the PyTorch framework. Empirically, not distributing the sparse parameters led to no performance impact.</p><p>3.4.1 Mini-Batch and Loss for Multi-task. For every mini-batch, we balance a uniform mix of each of the datasets with an epoch defined by the iterations to iterate through the largest dataset. Each dataset has its own indepedent tasks so we ignore the gradient contributions of images on tasks that it does not have data for. The losses from all the tasks are assigned equal weights and are summed for backward propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Sparse</head><p>Tensor Optimization. We represent the proxy banks that are sparsely subsampled as sparse tensors. This avoids expensive dense gradient calculation for all the proxies during the backward propagation.</p><p>An additional optimization is the handling of momentum. By enabling momentum update on the sparse tensor, the sparse gradient tensors will be aggregated and become expensive dense gradient updates. Since momentum is crucial for deep neural network optimization, we approximate the momentum update for sparse tensor by increasing the learning rate. Assuming we choose momentum = 0.9, the gradients of the current iteration Ĝ will roughly have net update effect of 10x learning rate over the coarse of training:</p><formula xml:id="formula_0">∞ n=1 lr × Ĝ × 0.9 n = 10.0 × lr × Ĝ</formula><p>Although increasing learning rate will affect the optimization trajectory on the loss function surface, thus affect the subsequent gradients, we find this approximation decreases our training time by 40% while retaining comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Deployment</head><p>We train our models using the PyTorch framework and deploy models through PyTorch to ONNX to Caffe2 conversion. For operations where ONNX does not have a compatible representation for, we directly use the ATen operator, a shared backend between PyTorch and Caffe2, to bypass and deploy our Caffe2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we measure the performance of our method on a suite of evaluations including offline measurement, human judgements, and A/B experiments. We demonstrate the efficacy and the impact of our unified embeddings at Pinterest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Our model is trained using PyTorch on one p3.16xlarge Amazon EC2 instances with eight Tesla V100 graphic cards. We use the DistributedDataParallel implementation provided by the PyTorch framework for distributed training. We train our models with largely the same hyperparameters as <ref type="bibr" target="#b32">[33]</ref> with SE-ResNeXt101 <ref type="bibr" target="#b9">[10]</ref> as the base model pre-trained on ImageNet ILSVRC-2012 <ref type="bibr" target="#b4">[5]</ref>. We use SGD with momentum of 0.9, weight decay of 1e-4, and gamma of 0.1. We start with a base learning rate of 0.08 (0.01 x 8 from the linear scaling heuristic of <ref type="bibr" target="#b5">[6]</ref>) and train our model for 1 epoch by updating only new parameters for better initialization with a batch size of 128 per GPU. We then train end-to-end with a batch size of 64 per GPU and apply the gamma to reduce learning rate every 3 epochs for a total of 9 epochs of training (not counting initialization). During training, we apply horizontal mirroring, random crops, and color jitter from resized 256x256 images while during testing we center crop to a 224x224 image from the resized image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offline Evaluation</head><p>Offline measurements are the first consideration when iterating on new models. For each product of interest (Shop-the-Look, Flashlight, and Lens), we have a retrieval evaluation dataset. Some are derived based on the training data while others are sampled according to usage in product. The difference in approach is due to either boostrapping a new product versus improving an existing one.</p><p>The evaluation dataset for Shop-the-Look is generated through human curation as we looked to build this new product in 2018. We sampled home decor scenes according to popularity (# of closeups) on Pinterest and used human curation to label bounding boxes of objects in scene along with ground truth whole image product matches to the objects (criteria determined by our developed internal training guide). This resulted in 600 objects with 1421 ground truth product matches. We measure Precision@1 <ref type="bibr" target="#b16">[17]</ref> for evaluation where for each of the objects, we extract its embedding and generate the top nearest neighbor result in a corpus of 51421 product images (50K sampled from our product corpus + the ground Model STL Flashlight Lens P@1 Avg P@20 Avg P@20 <ref type="bibr" target="#b32">[33]</ref>  truth whole product images). Precison@1 is then the percent of objects that have a ground truth whole product image retrieved. The evaluation datasets for Flashlight and Lens are generated through user engagement. For Flashlight, we sampled a random class subset of the Flashlight training data (Section 3.2), and randomly divided it into 807 images for queries and 42881 images for the corpus across classes. For Lens, we generated the evaluation dataset using user engagement in the same manner as the Flashlight training dataset (Section 3.2) but filtering the query images to be camera images with human judgement. This resulted in 1K query images and 49K images for the corpus across classes. As these datasets are generated from noisy user engagement, we use the evaluation metric of Average Precision@20 where we take the average of Precision@1 to Precision@20 (Precision@K as defined in <ref type="bibr" target="#b31">[32]</ref>). Empirically we have found that though these evaluations are noisy, significant improvements in this Average P@20 have correlated with improvements in our online A/B experiment metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Binarization.</head><p>We experiment with model architectures in Table <ref type="table">1</ref>. We are primarily interested in binarized embedding performance as described in Section 3.3.2. An alternative to binary features for scalability is to learn low dimensional float embeddings. Based on our prior work <ref type="bibr" target="#b32">[33]</ref> however, we found that for the same storage cost, learning binary embedding led to better performance than learning float embeddings.</p><p>Our baseline approach in Table <ref type="table">1</ref> was to apply the LayerNorm (ln) with temperature of 0.05 and NormSoftmax approach of <ref type="bibr" target="#b32">[33]</ref> with a SE-ResNeXt101 featurizer and multi-task heads (Section 3.3). We noticed a significant drop in performance between raw float and binary features. We experimented with variations of the architecture including: Softmax (sm) to remove L2 normalized embedding constraint, GroupNorm <ref type="bibr" target="#b26">[27]</ref> (group=256) for more granularity in normalization, ReLU (r) to ignore negative magnitudes, and Dropout (p=0.5) for learning redundant representations. As shown, our final binarized multi-task embeddings performs favorably to the raw float features baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>STL Flashlight Lens P@1 Avg P@20 Avg P@20 Shop-the-Look (S) 49. Table <ref type="table">3</ref>: Ablation study on datasets. We train specialized embeddings for each training dataset and compare with our unified embedding trained on all training datasets in multitask. We compare against binary feature performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Multi-Task Architecture Ablations. We show our multi-task experiment results in Table <ref type="table" target="#tab_1">2</ref>. Instead of uniform sampling of each dataset in a mini-batch (Section 3.4.1), we experiment with sampling based on dataset size. Instead of assigning equal weights to all task losses, we experiment with GradNorm <ref type="bibr" target="#b2">[3]</ref> to learn the weighting. We see our simple approach achieved the best balance of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.3</head><p>Multi-Task Dataset Ablations. We look at how training with multiple datasets affects our unified embeddings. In Table <ref type="table">3</ref>, we compare our multi-task embedding trained with all three datasets against embeddings (using the same architecture) trained with each dataset independently. When training our embedding with one dataset, we ensure that the total iterations of images over the training procedure is the same as when training with all three datasets. We see that multi-task improves all three retrieval metrics compared with the performance of embeddings trained on a single dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.4</head><p>Unified vs Specialized embeddings. We compare our unified embedding against the previous specialized embeddings <ref type="bibr" target="#b31">[32]</ref> deployed in Flashlight, Lens, and Shop-the-Look in Table <ref type="table">4</ref>. We also include a SENet <ref type="bibr" target="#b9">[10]</ref> pretrained on ImageNet baseline for comparison. We see our unified embedding outperforms both the baseline and all specialized embeddings for their respective tasks. Although the unified embeddings compare favorably to the specialized embeddings, the model architectures of these specialized embeddings are fragmented. Flashlight embedding are generated from a VGG16 <ref type="bibr" target="#b20">[21]</ref> FC6 layer of 4096 dimensions. Lens embedding are generated from a ResNeXt50 <ref type="bibr" target="#b27">[28]</ref> final pooling layer of 2048 dimensions. Shop-the-Look embedding are generated from a ResNet101 <ref type="bibr" target="#b6">[7]</ref> final pooling layer of 2048 dimensions. This fragmentation is undesirable as each specialized embedding can benefit from updating the model architecture to our latest version as seen in the dataset ablation studies in Section 4.2.3. However in practice, this fragmentation is the direct result of challenges in embedding maintenance from focusing on different objectives at different times in the past. Beyond the improvements in offline metrics from multitask as seen in the Ablation study, the engineering simplification of iterating only one model architecture is an additional win.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Judgements</head><p>Offline evaluations allow us to measure, on a small corpus, the improvements of our embeddings alone. Practical information retrieval systems however are complex, with retrieval, lightweight Table <ref type="table">4</ref>: Our binary unified embedding against the existing specialized binary embeddings for each application. We include an ImageNet baseline using a pre-trained SENet <ref type="bibr" target="#b9">[10]</ref> score, and ranking components <ref type="bibr" target="#b10">[11]</ref> using a plethora of features. As such it is important for us to measure the impact of our embeddings end-to-end in our retrieval systems. To compare our unified embeddings with the productionized specialized embeddings for human judgement and A/B experiments, we built separate clusters for each visual search product where the difference between the new and production clusters are the unified vs specialized embeddings.</p><p>At Pinterest, we rely on human judgement to measure the relevance of our visual search products and use A/B experiments to measure engagement. For each visual search product, we built relevance templates (Figure <ref type="figure" target="#fig_4">5</ref>) tuned with an internal training guide for a group of internal workers (similar to Amazon Mechanical Turk) where we describe what relevance means in the product at hand with a series of expected (question, answer) pairs. To control quality, for a given relevance template we ensure that workers can achieve 80%+ precision in their responses against a golden set of (question, answer) pairs. We further replicate each question 5 times, showing 5 different workers the same question and aggregating results with the majority response as the final answer. We also record worker consistency (WC) across different sets of jobs measuring given the same question multiple times, what percent of the questions were answered the same across jobs.</p><p>Questions for judgement are generated from a traffic weighted sample of queries for each product. Given each query (Pin image + crop for Flashlight, Camera image for Lens, and Pin image + crop for Shop-the-Look), we send a request to each product's visual search system to generate 5 results per query. Each (query, result) pair forms a question allowing us to measure Precision@5. We generate two sets of 10K (question, answer) tasks per visual search product, one on the existing production system with the specialized embedding and another on the new cluster with our unified embedding. Our human judgement results are shown in Table <ref type="table">5</ref> for Flashlight and Lens and Table <ref type="table">6</ref> for Shop-the-Look. As we can see, our new unified embeddings significantly improve the relevance of all our visual search products.</p><p>One hypothesis for these significant gains beyond better model architecture is that combining the three datasets covered the weaknesses of each one independently. Flashlight allows crops as input while the engagement generated training data are whole images. Leveraging the Shop-the-Look dataset with crops helps bridge this domain shift gap of crop to whole images. Similarly for Lens, though the query is a camera image and we need to address the camera to pin image domain shift, the content in the corpus are all Pinterest content. As such additionally using Pinterest corpus training data like Flashlight's can allow the embedding to not only handle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A/B Experiments</head><p>A/B experiments at Pinterest are the most important criteria for deploying changes to production systems.</p><p>Flashlight A/B experiment results of our unified embedding vs the old specialized embedding are shown in Figure <ref type="figure" target="#fig_5">6</ref>. We present results on two treatment groups: (1) A/B experiment results on Flashlight with ranking disabled and (2) A/B experiment results on Flashlight with ranking enabled. Flashlight candidate generation is solely dependent on the embedding and as such, when disabling Application</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Win</head><p>Lose Draw P@5 Delta WC Flash. (old vs new) 41.3% 13.9% 44.8% 98.0% Lens (old vs new) 54.0% 7.9% 38.0% +110.1% 92.9%</p><p>Table <ref type="table">5</ref>: Human Judgements for Flashlight and Lens measuring Precision@5 delta comparing unified embedding vs existing specialized embedding along with the percent of queries that are better (Win), worse (Lose), or have the same (Draw) Precision@5. We see that our new unified embedding significantly improves the relevance of the two products.  <ref type="table">6</ref>: Human Judgements for Shop-the-Look measuring Precision@5 comparing unified embedding vs existing specialized embedding. We see that our new unified embedding significantly improves the relevance overall with wins in all categories except one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>ranking we can see impact of our unified embedding without the dilution of the end-to-end system. For deployment, we look at the treatment group with ranking enabled. In both cases, our unified embedding significantly improves upon the existing embedding. We see improvement in top-line volume metrics of impressions, closeups, repins (action of saving a Pin to a Board), clickthroughs, and long clickthoughs (when users remain off-site for an extended period of time <ref type="bibr" target="#b13">[14]</ref>) along with improvement in top-line propensity metrics (percent of Flashlight users who do a specific action daily) of closeuppers, repinners, clickthroughers, and long clickthroughers. Lens A/B experiment results of our unified embedding vs the old specialized embedding are shown in Table <ref type="table" target="#tab_4">7</ref>. As a newer product, Lens A/B experiment results are generated via a custom analysis script hence the difference in reporting between Flashlight and Lens. Similar to the Flashlight A/B results, we see significant improvement to both our engagement and volume metrics when replacing the existing embedding with our unified embedding for Lens.  Shop-the-Look had not launched to users when experimenting with our unified embedding and as such, no A/B experiments could be run. As a prototype, we focused on relevance and as such the human judgement results were used as the launch criteria.</p><p>Given the significantly positive relevance human judgements and A/B experiments results, we deployed our unified embedding to all the visual search products, replacing the specialized embedding with one representation that outperformed on all tasks. Qualitative results of our unified embedding compared to the old specialized embeddings can be seen in Figure <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Improving and maintaining different visual embeddings for multiple customers is a challenge. At Pinterest, we took one step to simplifying this process by proposing a multi-task metric learning architecture capable of jointly optimizing multiple similarity metrics, such as browsing and searching relevance, within a single unified embedding. To measure the efficacy of the approach, we experimented on three visual search systems at Pinterest, each with its own product usage. The resulting unified embedding outperformed all specialized embedding trained with individual task in comprehensive evaluations, such as offline metrics, human judgements and A/B experiments. The unified embeddings are deployed at Pinterest after observing substantial improvement in recommendation performance reflected by better user engagement across all three visual search products. Now with only one embedding to maintain and iterate, we have been able to substantially reduce experimentation, storage, and serving costs as our visual search products rely on a unified retrieval system. These benefits enable us to move faster towards our most important objective -to build and improve products for our users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of our training datasets</figDesc><graphic url="image-2.png" coords="3,317.96,85.60,240.25,246.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The overall architecture for multi-task metric learning network. The proposed classification network as proxy-based metric learning is simple and flexible for multi-task learning. Our proposed method also has the binarization module to make learned embedding memory efficient, and the subsampling module is scalable to support large number of classes.</figDesc><graphic url="image-3.png" coords="4,53.80,85.60,504.41,179.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the subsampling module.</figDesc><graphic url="image-4.png" coords="5,53.80,85.60,240.25,110.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Human Judgement Task templates for Flashlight (top), Shop-The-Look (middle), and Lens (bottom).</figDesc><graphic url="image-5.png" coords="7,329.97,85.61,216.22,401.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: A/B experiment results on Flashlight showing changes in metrics (Blue highlights statistically significant changes) for users across days in the experiment (to diagnose novelty effects if any). We see significant lifts in engagement propensity and volume with our unified embedding compared to the existing specialized embedding.</figDesc><graphic url="image-6.png" coords="8,317.96,85.60,240.25,111.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative results comparing the old embeddings vs our new multi-task embeddings in Flashlight. For each query on the left, the results from old embeddings are shown on the top row, and the new embeddings are shown on bottom row.</figDesc><graphic url="image-7.png" coords="9,53.80,85.60,240.25,179.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,53.80,207.67,504.43,134.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Binarization Module. At Pinterest, we have a growing corpus of billions of images and as such we need efficient representations to (1) decrease cold storage costs (e.g. cost of AWS S3) (2) reduce</figDesc><table><row><cell cols="2">Algorithm 1 Subsampling Proxy Indices</cell></row><row><cell cols="2">Input: targets, num_samples</cell></row><row><cell cols="2">Output: sampled_proxy_idx, remapped_targets</cell></row><row><cell cols="2">Require: len(tarдets) ≤ num_samples</cell></row><row><cell>4:</cell><cell>if s sampled_proxy_idx then</cell></row><row><cell>5:</cell><cell>sampled_proxy_idx .add(s)</cell></row><row><cell>6:</cell><cell>end if</cell></row><row><cell cols="2">7: end while</cell></row><row><cell cols="2">8: sampled_proxy_idx ← list(sampled_proxy_idx)</cell></row><row><cell cols="2">9: remapped_tarдets ← list([])</cell></row><row><cell cols="2">10: for all t ∈ tarдets do</cell></row><row><cell>11:</cell><cell>for all index, label ∈ enumerate(sampled_proxy_idx) do</cell></row><row><cell>12:</cell><cell>if t = label then</cell></row><row><cell>13:</cell><cell>remapped_tarдets.add(index)</cell></row><row><cell>14:</cell><cell>end if</cell></row><row><cell>15:</cell><cell>end for</cell></row><row><cell cols="2">16: end for</cell></row><row><cell cols="2">17: return sampled_proxy_idx, remapped_tarдets</cell></row><row><cell>3.3.2</cell><cell></cell></row></table><note>1: sampled_proxy_idx ← set(tarдets) 2: while len(sampled_proxy_idx) ≤ num_samples do 3: s ← sample(all_labels)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Multi-Task experiments on offline evaluations.</figDesc><table><row><cell>Baseline (f)</cell><cell>47.5</cell><cell>60.1</cell><cell></cell></row><row><cell>[33] Baseline (b)</cell><cell>41.9</cell><cell>55.6</cell><cell>17.7</cell></row><row><cell>+sm + gn (b)</cell><cell>48.4</cell><cell>59.3</cell><cell>17.8</cell></row><row><cell>+sm + gn + r (b)</cell><cell>49.7</cell><cell>61.1</cell><cell>17.6</cell></row><row><cell cols="2">+sm + gn + r + dp (b) (Ours) 52.8</cell><cell>60.2</cell><cell>18.4</cell></row><row><cell cols="4">Table 1: Model architecture experiments on offline evalua-</cell></row><row><cell cols="4">tions (f = float, b = binary). We compare binary embeddings</cell></row><row><cell>for deployment.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">STL Flashlight</cell><cell>Lens</cell></row><row><cell></cell><cell cols="3">P@1 Avg P@20 Avg P@20</cell></row><row><cell>Ours</cell><cell>52.8</cell><cell>60.2</cell><cell>18.4</cell></row><row><cell cols="2">+ No Dataset Balancing 44.4</cell><cell>57.8</cell><cell>18.6</cell></row><row><cell>+ GradNorm</cell><cell>47.2</cell><cell>57.8</cell><cell>17.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>A/B experiment results on Lens. We see significant lifts in engagement propensity and volume with our unified embedding compared to the existing specialized embedding.</figDesc><table><row><cell cols="3">Closeuppers Repinner Clickthrougher</cell></row><row><cell>+16.3%</cell><cell>+26.7%</cell><cell>+24.3%</cell></row><row><cell>Closeup</cell><cell>Repin</cell><cell>Clickthrough</cell></row><row><cell>+32.7%</cell><cell>+46.7%</cell><cell>+35.0%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Visual Similarity for Product Design with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web</title>
				<editor>
			<persName><forename type="first">Jerry</forename><surname>Zitao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Liu Yuchen</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Charles</forename><surname>Sugnet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Ulrich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pranav</forename><surname>Jindal</surname></persName>
		</editor>
		<meeting>the International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Zhao Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.02257" />
	</analytic>
	<monogr>
		<title level="m">CoRR abs/1711</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">2257</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><surname>Tulloch</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR abs/1706</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2677</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep metric learning using Triplet network</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<idno>CoRR abs/1412.6622</idno>
		<ptr target="http://arxiv.org/abs/1412.6622" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Web-Scale Responsive Visual Search at Bing</title>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Komlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapei</forename><surname>Stephen) Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meenaz</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><surname>Sacheti</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219843</idno>
		<ptr target="http://dx.doi.org/10.1145/3219819.3219843" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-19">2018. 2018. August 19-23. 2018</date>
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual Search at Pinterest</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<meeting>the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>CoRR abs/1705.07115</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Related Pins at Pinterest: The Evolution of a Real-World Recommender System</title>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
		<idno>CoRR abs/1702.07969</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-stitch Networks for Multi-task Learning</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR abs/1604</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">3539</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">No Fuss Distance Metric Learning using Proxies</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<idno>CoRR abs/1703.07464</idno>
		<ptr target="http://arxiv.org/abs/1703.07464" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance Evaluation in Content-based Image Retrieval: Overview and Proposals</title>
		<author>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Mcg</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-8655(00)00118-5</idno>
		<idno>S0167-8655(00)00118-5</idno>
		<ptr target="http://dx.doi.org/10.1016/" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="593" to="601" />
			<date type="published" when="2001-04">2001. April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-Domain Self-supervised Multitask Feature Learning using Synthetic Imagery</title>
		<author>
			<persName><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno>CoRR abs/1711.09082</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<editor>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pong Eksombatchai</forename><surname>William</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Hamilton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</editor>
		<meeting>the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Ruining He</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<title level="m">Conditional Similarity Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1781" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sampling Matters in Deep Embedding Learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krähenbühl</surname></persName>
		</author>
		<idno>CoRR abs/1706.07567</idno>
		<ptr target="http://arxiv.org/abs/1706.07567" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Group Normalization. CoRR abs/1803</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">8494</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.437</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2013.437" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="3519" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual Search at eBay</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Bubnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098162</idno>
		<ptr target="http://dx.doi.org/10.1145/3097983.3098162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-13">2017. August 13 -17, 2017. 2101-2110</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Taskonomy: Disentangling Task Transfer Learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
		<idno>CoRR abs/1804.08328</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04680</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Visual Discovery at Pinterest. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Making Classification Competitive for Deep Metric Learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR abs/1811.12649</idno>
		<ptr target="http://arxiv.org/abs/1811.12649" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual Search at Alibaba</title>
		<author>
			<persName><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219820</idno>
		<ptr target="http://dx.doi.org/10.1145/3219819.3219820" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-19">2018. 2018. August 19-23. 2018</date>
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Modulation Module for Multi-task Learning with Applications in Image Retrieval</title>
		<author>
			<persName><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
