<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating scheme inspired by reinforcement learning, which regards the decoders of transformers as environments, and the downstream performance metrics as the rewards to evaluate the hidden selection actions. Our empirical results on real-world long-text summarization and reading comprehension tasks demonstrate effective improvements compared to prior longsequence processing baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref> have become a fundamental model structure for sequential data modeling <ref type="bibr">(Sun et al., 2019a;</ref><ref type="bibr" target="#b9">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b63">Wysocki et al., 2023)</ref>, especially in Natural Language Processing (NLP) <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref>, where texts are regarded as sequences of tokens. Built with transformer architectures, pre-trained language models (PLMs) have recently shown astonishing empirical performance in various NLP tasks such as question answering <ref type="bibr">(Yang et al., 2019a)</ref>, dialogue generation <ref type="bibr" target="#b4">(Byrne et al., 2021)</ref>, summarization <ref type="bibr" target="#b51">(Rush et al., 2015;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016)</ref> and machine translation <ref type="bibr">(Yang et al., 2019b;</ref><ref type="bibr" target="#b41">Pan et al., 2021)</ref>. However, one fatal weakness, which has hindered transformer-based models from being applied in broader application scenarios, is that the computational consumption of self-attention operations increases quadratically with the input sequence length. Therefore, standard transformers have always been challenged by long text processing tasks, such as machine reading comprehension <ref type="bibr" target="#b29">(Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b15">Gong et al., 2020;</ref><ref type="bibr" target="#b42">Pang et al., 2022)</ref> and long-text summarization <ref type="bibr" target="#b20">(Huang et al., 2021;</ref><ref type="bibr" target="#b34">Ma et al., 2022)</ref>.</p><p>To enhance transformers' capability in longsequence processing, prior works pay their research attention to two major perspectives, the efficient attention operation and the sub-sequence processing. The efficient attention <ref type="bibr" target="#b2">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b69">Zaheer et al., 2020;</ref><ref type="bibr" target="#b5">Choromanski et al., 2020)</ref> aims to reduce the memory and calculation cost of selfattentions while still preserving transformers' empirical performance. Unfortunately, most efficient attention methods require customized self-attention implementations, which are not compatible with existing pre-trained language models. Hence, introducing efficient attention leads to re-training the whole transformer model, which is incompatible with the fine-tuning mechanism. Moreover, some empirical results show that sparse-attention methods inevitably sacrifice the performance on short-sequence processing tasks compared with full-attention models <ref type="bibr" target="#b47">(Phang et al., 2022)</ref>.</p><p>Another perspective to long-sequence processing is to decompose the long-input texts into multiple sub-sequences, then process them individually <ref type="bibr" target="#b38">(Moro et al., 2022;</ref><ref type="bibr" target="#b72">Zhang et al., 2022;</ref><ref type="bibr" target="#b36">Mao et al., 2022;</ref><ref type="bibr" target="#b33">Liu et al., 2022)</ref>. Although utilizing the effectiveness of full-attention blocks, sub-sequence processing methods are not good at capturing the global semantic information among different chunks. Moreover, for better fusing information across chunks in the decoder, <ref type="bibr" target="#b21">Ivgi et al. (2023)</ref> put the same fragment in different chunks, which significantly increases the computational cost.</p><p>To gather the advantages of the long-sequence processing methods above, in this paper, we introduce a Simple learning framework with three typical operations: Chunk, Align, and Select, which we call as SimCAS. In more detail, SimCAS first chunks the input long text into a sub-sequence batch then feeds the batch through specially designed encoding blocks with an inter-chunk aligning mechanism, and finally selects semantically representative hidden representation via a contextual selection module. To align the global semantic information among chunks, we introduce a sequential batch alignment (SBA) operation to calibrate the start and end token embeddings with each batch in the encoder layers. For learning the selector, inspired by the recent success of reinforcement learning in NLP <ref type="bibr" target="#b40">(Ouyang et al., 2022)</ref>, we adopt the Proximal Policy Optimization (PPO) <ref type="bibr" target="#b54">(Schulman et al., 2017)</ref> algorithm to train the selector, where the transformer decoder is treated as the environment and the task evaluation metric is regarded as the reward for actions. To evaluate the effectiveness of our SimCAS, we conduct experiments on three long-document datasets (arXiv, GovReport, and PubMed), two multi-document datasets (Multi-News and WCEP), and one reading comprehension dataset (NarrativeQA). Empirical results show that SimCAS can outperform other long-sequence processing baselines and is highly scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Language Modeling The training objective for sequence generation consists of a sequence of token decisions made in an auto-regressive manner. This is formulated as a product of decision probabilities corresponding to specific tokens. Given an input sequence X = (x 1 , x 2 , ? ? ? , x N ) and its corresponding output Y = (y 1 , y 2 , ? ? ? , y M ), we model the following conditional probability:</p><formula xml:id="formula_0">p ? (Y |X) = M m=1 p ? (y m |Y &lt;m , X),<label>(1)</label></formula><p>where Y &lt;m = (y 1 , y 2 , . . . , y m-1 ), and ? represents the model parameters.</p><p>Proximal Policy Optimization In the domain of reinforcement learning (RL), Proximal Policy Optimization (PPO) <ref type="bibr" target="#b54">(Schulman et al., 2017</ref>) is a widely used policy gradient method <ref type="bibr" target="#b22">(Kakade, 2001)</ref> for its remarkable performance and efficiency in solving complex control and decisionmaking tasks <ref type="bibr" target="#b59">(Vinyals et al., 2019;</ref><ref type="bibr" target="#b1">Akkaya et al., 2019)</ref>. The vanilla policy gradient estimator has the form:</p><formula xml:id="formula_1">? ? E ? ? (at|st) [A ? t (a t , s t )] ? ?t [? ? log ? ? (a t |s t ) ?t ]</formula><p>, where s t ? S is the state at t-step, ? ? (a t |s t ) is a stochastic policy acting a t at s t , ?t is the estimated value of the advantage function A ? t (a t , s t ), and ?t denotes the empirical average over a sample batch. The PPO algorithm improves the training stability of the policy gradient, by optimizing the following objective:</p><formula xml:id="formula_2">?t [min(r t (?) ?t , clip(r t (?), 1 -?, 1 + ?) ?t ], (2)</formula><p>where r t (?) = ? ? (at|st) ? ? old (at|st) is the probability ratio between new and old policies, and ? &gt; 0 is a hyperparameter for clipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a long-input text X = (x 1 , x 2 , . . . , x N ) with a fairly large input length N , we aim to design a model p ? (Y |X) to predict a corresponding label sequence Y = (y 1 , y 2 , . . . , y M ), where Y can either be classification labels or output sequence tokens. The major difficulty of the task is that the input length N is so large that the original selfattention operations become infeasible with the quadratic complexity O(N 2 ).</p><p>To address the challenge, we propose a novel method that intuitively splits the long inputs into chunks with feasible lengths, then selects the most representative tokens for decoding steps. To guarantee inter-chunk semantic information extracted during encoding, we design an aligning scheme in the encoder blocks. In the following, we will describe the chunking scheme, the aligning strategy, and the selector design in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Chunking Scheme</head><p>Assume the maximum input sequence length of a pre-trained transformer model is S. We first split the long input sequence into B = ? N S ? chunks with each chunk length equal to S:</p><formula xml:id="formula_3">(c k 1 , c k 2 , . . . , c k S ) B k=1 ,<label>(3)</label></formula><p>where c k i = x (k-1)S+i , and ??? is the ceiling function. Since the last chunk might not have S tokens from X, we append special padding tokens at the  </p><formula xml:id="formula_4">1 2 3 4 5 6 7 8 9 10 12 N-2 N-1 N ? ? ? Encoder Input (N tokens) 11 N-3 [S] [E] [S] [E] 1 2 3 4 ? ? ? 5 6 7 8 [S] [E] N-1 N [P] N-2</formula><formula xml:id="formula_5">? ? [S]1 [E]1 3 4 1 2 5 6 7 8 N-2 N-1 N [P] [S]2 [E]2 [S]k [E]k EOS Alignment BOS Alignment [S] [E] [S] [E] [S] [E]</formula><p>Token Selector Token Selector</p><formula xml:id="formula_6">[S] 0 1 2 3 4 [E]1 5 6 7 8 [E]2 N [P] [E]k N-1 N-2 2 3 [S] 1</formula><p>Token Selector Token Selector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">[S] k</head><p>Token Selector Token Selector   </p><formula xml:id="formula_7">2 3 5 6 [E]2 N [S] 0 ? ? ? 1 2 4 3 M M-1 ? ? ? Selected Tokens Output Feedback Attention Feedback Decoder Input (M tokens) Add &amp; Norm Add &amp; Norm Add &amp; Norm N1? 5 6 7 8 3 4 1 2 N-2 N-1 N [P] N-2 N-2 1 2 3 4 5 6 7 8 9 10 12 n-2 n-1 n ? ? ? Encoder Input (n tokens) 11 n-3 [S] [E] [S] [E] 1 2 3 4 ? ? ? 5 6 7 8 [S] [E] n-1 n PAD n-2</formula><formula xml:id="formula_8">? ? [S]1 [E]1 3 4 1 2 5 6 7 8 n-2 n-1 n PAD [S]2 [E]2 [S]k [E]k EOS Alignment BOS Alignment [S] [E] [S] [E] [S] [E]</formula><p>Token Selector Token Selector</p><formula xml:id="formula_9">[S] 0 1 2 3 4 [E]1 5 6 7 8 [E]2 n PAD [E]k n-1 n-2 2 3 [S] 1</formula><p>Token Selector Token Selector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">[S] k</head><p>Token Selector Token Selector    end, i.e., c k i = [PAD] if (k -1)S + i &gt; N (as the [P] token in Fig. <ref type="figure">1</ref>). After chunking, we add additional beginning of sentence ([BOS]) and end of sentence ([EOS]) tokens to each chunk, and treat the chunks as a normal transformer input batch C with batch size B:</p><formula xml:id="formula_10">C = ([BOS], c k 1 , c k 2 . . . , c k S , [EOS]) B k=1 . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequential Batch Alignment</head><p>After chunking the input text into a standard token batch, we can encode it with the transformer encoder layers. As in Fig. <ref type="figure">1</ref>, we assume the encoder has N 1 layers. Denote the hidden representations of k-th chunk in C (in equation 4) at l-th encoder layer as</p><formula xml:id="formula_11">H k,l = (h k,l 0 , h k,l 1 , . . . , h k,l S , h k,l S+1 )</formula><p>, where h k,l 0 and h k,l S+1 are hidden representation of [BOS] and [EOS] tokens respectively, and h k,l i is the embedding for c k i with 1 ? i ? S. As mentioned in Section 1, chunk-level methods are difficult to capture the inter-chunk global semantic information. Hence, we design a sequential batch alignment, which aligns the information of [BOS] and [EOS] tokens at each encoding block, for [BOS] and [EOS] have been well-recognized representative of sentence semantics <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. More specifically, at l-th layer, our batch alignment average the hidden states of [BOS] and</p><p>[EOS] of all chunks:</p><formula xml:id="formula_12">hl BOS = 1 B B k=1 h k,l 0 , hl EOS = 1 B B k=1 h k,l S+1 .<label>(5)</label></formula><p>Then we replace h k,l 0 and h k,l S+1 with the aligned hl BOS and hl EOS into the hidden states for the nextlayer encoding block, as shown in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Token Selector</head><p>After being encoded into the last hidden space with our sequential batch aligning scheme, the chunks should be reformed back to a sequence for the next decoding steps. Directly tiling the chunks' representations back into a sequence is still infeasible, because of the overlong sequence length. Therefore, we propose a token selector to filter the most representative hidden representations for decoding steps. Inspired by <ref type="bibr" target="#b49">Ramamurthy et al. (2023)</ref>, we design the selector from the perspective of reinforcement learning (RL). Selection Module Design Formally, the token selector takes the last hidden states</p><formula xml:id="formula_13">H L = {h k,L 0 , h k,L 1 , . . . , h k,L S , h k,L S+1 } B k=1</formula><p>and selection actions A t = (a 1 , a 2 , . . . , a t-1 ) as inputs and predicts the next selection action a t , where each action a t has two values "select" and "skip" for operating the t-th token x t in X. We set the state of RL as s t = (H L , A t ), then the selector is a policy ? ? (a t |s t ) to predict next action a t .</p><p>We implement the selector with the actor-critic style <ref type="bibr" target="#b27">(Konda and Tsitsiklis, 1999)</ref>. Both actor and critic are simple three-layer feed-forward networks, but the actor outputs a probability distribution over the action space and the critic outputs a single scalar value. At state s t , to consider the sequential effect of previous action A t , we first take the average of all selected hidden states as</p><formula xml:id="formula_14">ht = k,i I {a j ="select",j&lt;t} h k,L i k,i I {a j ="select",j&lt;t} ,<label>(6)</label></formula><p>where j = (k -1)S + i maps chunk indices back to the input sequence, and I {?} is the indicator function. Then we concatenate the current selector state ht and token information h k,L i , t = (k -1)S + i, to predict next action a t via the actor:</p><formula xml:id="formula_15">? ? (a t |s t ) = actor( ht , h k,L i ). (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>Reward Design To train the selector within an RL scheme, we treat the transformer as an environment and design action rewards based on the decoder outputs. Inspired by <ref type="bibr" target="#b68">Yuan et al. (2021)</ref>, we can directly utilize the language modeling likelihood as the generation quality reward for selection actions:</p><formula xml:id="formula_17">R LM = ? exp{ 1 M log p ? (Y |X)}, (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>where ? is a coefficient that magnifies the value of the reward for easier optimization of the selector. However, R LM is only a scalar value, which cannot provide fine-grained guidance to the selector. Therefore, we use the input-output cross-attention scores to calibrate R LM . More specifically, we denote the cross-attention matrix of the q-th attention head in the l-th layer of the decoder is denoted as A l q ? R M ?N , and the overall cross-attention</p><formula xml:id="formula_19">? = 1 N 2 ?Q N 2 l=1 Q q=1 A l q ,<label>(9)</label></formula><p>where N 2 is the decoder layer number, and Q is the cross-attention head number. With the overall cross-attention ? = (? ij ) M ?N , we adjust the reward to each selected tokens:</p><formula xml:id="formula_20">R + j = ?j 1-? 0 R LM , ?j = 1 M M i=1 ?ij .<label>(10)</label></formula><p>For "skip" action, we intend to limit the selected sequence length. Assume the number of overall input tokens and selected token representations is L all and L select respectively. We set a hyper-parameter L hyper to control the size of L select with the following skipping reward:</p><formula xml:id="formula_21">R -= R LM L all if L select &lt; L hyper R LM L select otherwise. (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>With the selector and rewards designed above, we can optimize the selector with the PPO algorithm described in Section 2. Note that in our setups, the environment (the transformer decoder) is changing during the training steps. Therefore, we alternatively update the selector and the decoder: in each interaction, we first fix the transformer and use the reward R LM to update the selector, then fix the selector and update the transformer with language modeling loss.</p><p>We call our method a Simple long-sequence processing method via Chunking, Aligning, and Selecting (SimCAS). The whole framework is demonstrated in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Efficient Transformers The attention mechanism in transformers requires quadratically increased computational complexity with respect to the input sequence length, which limited the application scenarios, especially for long-text processing. To address this issue, various previous works have been proposed for designing more efficient attention operations <ref type="bibr" target="#b58">(Tay et al., 2023;</ref><ref type="bibr" target="#b13">Fournier et al., 2023)</ref>. Longformer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, BIG-BIRD <ref type="bibr" target="#b69">(Zaheer et al., 2020)</ref>, GMAT <ref type="bibr" target="#b16">(Gupta and Berant, 2020)</ref>, and ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> reduce the memory consumption of dense attentions by elaborate combinations of global attention and local attention mechanisms. Reformer <ref type="bibr" target="#b25">(Kitaev et al., 2020)</ref> leverages a locality-sensitive hashing to the attention mechanism, changing its complexity from O(n 2 ) to O(n log n), where n is the input text sequence length. Routing Transformer <ref type="bibr" target="#b50">(Roy et al., 2021)</ref> applies a sparse routing module based on online k-means to self-attention while reducing the overall complexity of attention. Approximationbased methods, such as Performers <ref type="bibr" target="#b5">(Choromanski et al., 2020)</ref> and RFA <ref type="bibr" target="#b46">(Peng et al., 2021)</ref>, use linear space and time complexity to estimate the attention matrix based on random features. Luna <ref type="bibr" target="#b35">(Ma et al., 2021)</ref> attends only to a fixed number of hidden vectors. Linformer <ref type="bibr" target="#b60">(Wang et al., 2020)</ref> calculates selfattention by a low-rank matrix. However, the vast majority of these methods are difficult to apply to existing PLMs. Moreover, <ref type="bibr" target="#b65">Xiong et al. (2022)</ref> proposes that many efficient-attention transformers do not even perform as well as simple local-attention models on downstream language tasks. Chunking Methods for Long Sequence Another effective solution for long-sequence processing is to chunk the long text into sub-sequence and then process them respectively. Among chunking methods, SLED <ref type="bibr" target="#b21">(Ivgi et al., 2023)</ref> splits the long sequence into overlapping chunks and processes each chunk with a full-attention encoder, then fuse cross-chunk information with a transformer decoder. PageSum <ref type="bibr" target="#b33">(Liu et al., 2022)</ref> separates the long sequence into non-overlapping chunks and effectively tackles them by the principle of locality <ref type="bibr" target="#b7">(Denning, 2005)</ref>. Unlimiformer <ref type="bibr" target="#b3">(Bertsch et al., 2023)</ref> encodes long inputs in chunks and utilizes only the top-k input tokens for every attention head. Sequence Length Reduction Reducing the length of hidden states is the method of model compression from the width perspective, which is promising since some analytical studies showed that there is redundant encoded information in token representations <ref type="bibr" target="#b10">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b26">Klafka and Ettinger, 2020)</ref>. Among the redundancy, some tokens carry more task-specific information than others, suggesting that these tokens are more salient and imperative to be selected to feed into subsequent layers. Compared with model compression via layer-wise pruning, token-level pruning does not come at the expense of model performance in complex reasoning <ref type="bibr" target="#b52">(Sanh et al., 2019;</ref><ref type="bibr">Sun et al., 2019b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate the performance of our proposed Sim-CAS, we primely conduct experiments on two NLP tasks: long-text summarization and machine reading comprehension. In the following, we introduce detailed information about the datasets, baselines, model implementations, and evaluation results of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conduct experiments on two NLP tasks: longtext summarization and machine reading comprehension. For long-text summarization, we use three single-document summarization datasets: arXiv, PubMed <ref type="bibr" target="#b6">(Cohan et al., 2018)</ref>, GovReport <ref type="bibr" target="#b20">(Huang et al., 2021)</ref>, and two multi-document summarization datasets: Multi-News <ref type="bibr">(Fabbri et al., 2019b)</ref>, WCEP <ref type="bibr">(Gholipour Ghalandari et al., 2020)</ref>. For the reading comprehension task, we test on the Narra-tiveQA <ref type="bibr" target="#b28">(Ko?isk? et al., 2018)</ref> dataset. arXiv &amp; PubMed<ref type="foot" target="#foot_0">1</ref> are two long-document summarization datasets in the scientific research domain. Each document is a scientific paper whose summary is the corresponding abstract. GovReport<ref type="foot" target="#foot_1">2</ref> is a long-document summarization dataset based on reports published by the U.S. Government Accountability Office and Congressional Research Service. Multi-News<ref type="foot" target="#foot_2">3</ref> is a large-scale multi-document summarization dataset. It consists of news articles and human-written summaries of these articles. Each summary is professionally written by editors and with links to the original articles cited. WCEP<ref type="foot" target="#foot_3">4</ref> is a dataset for multi-document summarization (MDS). It contains short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP). NarrativeQA<ref type="foot" target="#foot_4">5</ref> is a reading comprehension dataset over entire books from Project Gutenberg and movie scripts from different websites.</p><p>More detailed information about the datasets is provided in Appx. ?A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>There are several competitive baselines for comparison: HiMAP <ref type="bibr">(Fabbri et al., 2019a)</ref>, BERTREG <ref type="bibr">(Gholipour Ghalandari et al., 2020)</ref>, <ref type="bibr">Submod-ular+Abs (Gholipour Ghalandari et al., 2020)</ref>, BART <ref type="bibr" target="#b30">(Lewis et al., 2020)</ref>, PEGASUS <ref type="bibr">(Zhang et al., 2020)</ref>, DynE <ref type="bibr" target="#b18">(Hokamp et al., 2020)</ref>, Graph-Sum <ref type="bibr" target="#b31">(Li et al., 2020)</ref>, <ref type="bibr">BART-Long-Graph (Pasunuru et al., 2021)</ref>, HEPOS <ref type="bibr" target="#b20">(Huang et al., 2021)</ref>, SLED <ref type="bibr" target="#b21">(Ivgi et al., 2023)</ref>, Unlimiformer <ref type="bibr" target="#b3">(Bertsch et al., 2023)</ref>, LED (Longformer Encoder-Decoder) <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, BIGBIRD <ref type="bibr" target="#b69">(Zaheer et al., 2020)</ref>, PRIMERA <ref type="bibr" target="#b64">(Xiao et al., 2022)</ref>, and LED+RELAX <ref type="bibr" target="#b43">(Parnell et al., 2022)</ref>. More details of baselines can be found in Appx. ?C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>Our implementation is based on PyTorch <ref type="bibr" target="#b45">(Paszke et al., 2019)</ref> and Transformers <ref type="bibr" target="#b62">(Wolf et al., 2020)</ref> libraries. We train our model by using 8 NVIDIA V100 32G GPUs. For a fair comparison, we uniformly use the fullattention PLM BART base 6 with 6 layers each for the encoder and decoder as our backbone on the six public datasets above. In detail, the hidden size of each layer is 768, which is converted into 12 attention heads with a hidden unit size of 64 for multi-head attention. Built on the BART base , our framework introduces an additional parameterized selector to filter out more task-specific token representations. The selector follows the actor-critic style <ref type="bibr" target="#b27">(Konda and Tsitsiklis, 1999)</ref> and contains around 8M parameters. There are two Adam optimizers with ? 1 = 0.9, ? 2 = 0.999 for BART base and selector respectively.</p><formula xml:id="formula_23">Base Model Method arXiv GovReport PubMed R-1 R-2 R-L BS R-1 R-2 R-L BS R-1 R-2 R-</formula><p>Additionally, we use a chunk size of 512 (considering the start token and end token) in all experiments. To maintain chunk-wise alignment, we pad the origin long sequence to ensure that all chunks have the same size. During the forward propagation, the embedding layer embeds position representations for each chunk independently. 7  During training, we set the maximum input length to 16384 if not stated otherwise. For efficient training, we update the parameters of the original backbone and selector alternatively. The reward estimation of each action is computed based on decoder cross-attention and the training objective of the generative model. This estimation process is detached from the computation graph and 6 The checkpoint of BART is "facebook/bart-base" containing around 139 M parameters, whose maximum encoding length is 1024.</p><p>7 Considering the context of each chunk is different, although our design may lead to repeated position representation, we argue that after the encoding stage, the combination of the same token and position information would still produce various representations <ref type="bibr">(Kazemnejad et al., 2023)</ref> does not participate in backpropagation.</p><p>At the inference stage, compared to the original generation process, our framework only adds a chunk-wise selection procedure between the encoder and the decoder, which takes very little time. At the decoding stage, the target sequence is generated with beam search in an auto-regressive manner <ref type="bibr" target="#b61">(Wiseman and Rush, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluations</head><p>Like most previous works, for summarization tasks, we measure the quality of generated summaries using the popular metric ROUGE <ref type="bibr" target="#b32">(Lin, 2004</ref>). On the test set of arXiv, PubMed, GovReport, Multi-News, and WCEP, we report full-length F1-based ROUGE-1, ROUGE-2, and ROUGE-L scores computed with the standard ROUGE Perl package. Furthermore, we also use a popular model-based semantic metric BERTScore<ref type="foot" target="#foot_5">8</ref>  <ref type="bibr">(Zhang* et al., 2020)</ref> to demonstrate the superiority of our approaches comprehensively.</p><p>As for the reading comprehension task, we use the F1 and exact match (EM) metrics defined in SCROLLS <ref type="bibr" target="#b55">(Shaham et al., 2022)</ref> to evaluate the model performance on the NarrativeQA dataset. Both metrics normalize the reference and system output strings via lowercasing, removing punctuation and stopwords, and normalizing whitespace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results on Long-document Dataset</head><p>Tab. 1 reports results over three long-document test sets. We note that casting the backbone BART base into our SimCAS can significantly improve the model performance on abstractive summarization tasks. On the arXiv and GovReport test set, the base-size BART enhanced by our framework Sim-CAS achieves comparable performance to original large-size models. Our approach achieves new state-of-the-art performance using the base-size BART on the PubMed test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on Multi-document Dataset</head><p>The results of multi-document summarization tasks in Tab. 2 and 3 have a similar trend. With the exception of PRIMERA, which tailors a pre-training objective for multi-document summarization, our method substantially surpasses previous results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results on reading comprehension Dataset</head><p>See Fig. <ref type="figure">2</ref>, we are surprised to find that the performance of our model on the NarrativeQA test set is greatly improved compared with prior works.</p><p>Given the fact that this dataset has an extremely long average input length (&gt;100K) and short average output length (&lt;10), we believe that the large improvement in performance is due to our method's ability to filter out a tremendous of task-irrelevant  information, which allows the decoding blocks to fuse information more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Analysis</head><p>We further analyze the properties of our framework to gain more insights. Ablation Study We verify the contributions of various components in SimCAS on the GovReport development set and show the ablation study results in Tab. 4. Specifically, we consider taking out SBA and the reinforcement learning-based selector respectively. On the grounds of the results, we can come to the conclusion that 1) removing SBA and the selector substantially hurt model performance; 2) although using a selector reduces the number of the token representations, it can still improve the performance of the model because it can acquire and fuse the information that is more task-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Conditional Skipping Reward</head><p>In practice, to circumvent the selector selecting exceedingly many token representations, we design a conditional reward mechanism for skipping actions to soft constrain the number of selected representations. Fig. <ref type="figure">3</ref> exhibits the effectiveness of our reward mechanism. We can see that as the training progresses, the number of selected tokens gradually converges to the threshold (2048). dataset. Due to memory limitations, we use a maximum input length of 10240 instead of 16384 in the training phase. The experimental results in Tab. 5 show that with the increase in the number of parameters, the performance of our method can be further improved by a large margin, and even outperform the previous task-specific strong baseline. Increasing Maximum Input Length We explore the performance change of our model on a single 32G V100 GPU with the increase of input sequence length during inference. First, we test the efficiency of our framework in processing long sequences. In our setup, we use BART base as the backbone. Each text is encoded and filled to the maximum input length using padding tokens. Fig. <ref type="figure">4</ref> shows that in the case of small magnitude, with the exponential increase of input length, the time cost still increases approximately linearly. We believe that this phenomenon occurs because our method significantly reduces the computation complexity and short sequences make insufficient use of the computation cores. We also note that processing the sequence containing 350K tokens in the inference phase approaches the limit of memory. Additionally, we notice that even as the number of input tokens increases dramatically, the selected tokens are kept at a reasonable size, indicating the capability of our selector to filter out low-contribution information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Larger Model</head><p>2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19</p><formula xml:id="formula_24">0D[LQSXWVL]HLQWRNHQV 5HODWLYHWLPHSHUH[DPSOH $YHUDJH6HOHFWHG7RNHQV 6LP&amp;$6 %$57baseWUXQFDWHVWR $YHUDJH6HOHFWHG7RNHQ</formula><p>Figure <ref type="figure">4</ref>: Effect of increasing the number of input tokens in the inference phase on the time latency and the number of selected tokens. The area marked in blue on the right represents the limit of the number of tokens that the V100 can handle (350K tokens). We also investigate how the model performs when the maximum effective input length is increased during inference. In Tab. 6, we can observe that although the maximum input length is set to 16384 during the training phase due to the memory limitation, exceeding this maximum length during inference still improves the model performance.</p><formula xml:id="formula_25">Maximum Input Length R-1 R-2 R-L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we introduced a simple method for long-text processing via chunking, aligning, and selecting, called SimCAS. We divided the input long sequence into chunks, and encode them with sequential batch alignment to capture the inter-chunk semantics. To select the important token representations in the encoded output of the encoder, we introduced a reinforcement learning-based token selector with the PPO optimization method. We leverage the transformer decoder as an environment and design a reward scheme for the corresponding actions based on the output logits and decoder cross-attention feedback to optimize the selector. Substantial experiment results and analyses demonstrate the satisfying effectiveness of SimCAS. Besides, our method does not depend on any particular tasks or models, which have good generalization ability for various application scenarios.</p><p>For future work, our method can be naturally adopted into non-language long-sequence processing tasks, such as molecular structure analysis. Also, SimCAS have the potential to enhance the long text pre-training to transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although our method significantly reduces the computational complexity of full-attention Transformers, training on long-text datasets still costs a large amount of GPU memory, even if the batch size per device is set to 1. Although we have achieved competitive results with base-size pre-trained language models, how to improve efficiency for extremely large language models remains unexplored. Besides, similar to most of the language generation models, our method has not paid attention to the generation controllability <ref type="bibr" target="#b19">(Hu et al., 2017;</ref><ref type="bibr" target="#b48">Prabhumoye et al., 2020;</ref><ref type="bibr" target="#b17">He et al., 2022)</ref>. Therefore, we cannot guarantee that the generated content never contains repetitive, useless, or harmful information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Potential to select representation</head><p>Because in the decoder cross-attention module, the encoded output from the encoder is used as "Key" and "Value" to participate in the calculation, its attention score can lead to the contribution of the corresponding token representation from encoded output to the current token decision at the decoding step. Fig. <ref type="figure" target="#fig_0">5</ref> demonstrates how the cross-attention scores change during the decoding of the reference output with one example in GovReport. We can observe that 1) most of the token decisions in the decoding phase focus only on a small set of encoded representations; 2) For each token decision, the contributions of different encoded token representations vary greatly. These phenomena suggest that there is still the feasibility of further filtering out low-contribution encoded representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Introduction for Baselines</head><p>We use the competitive baselines that demonstrate the downstream-task results for comparison. Among them, BART <ref type="bibr" target="#b30">(Lewis et al., 2020</ref>) is a standard full-attention PLM for sequence generation.</p><p>Compared with BART, PEGASUS <ref type="bibr">(Zhang et al., 2020)</ref> has a tailored pre-training objective for abstractive text summarization. LED (Longformer Encoder-Decoder) <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> uses the sparse attention-based encoder and full-attention decoder. Before pre-training, its parameters are initialized from BART. BIGBIRD <ref type="bibr" target="#b69">(Zaheer et al., 2020)</ref>, for an encoder-decoder setup, also introduces their specified sparse attention mechanism only at the encoder side. PRIMERA based on LED introduces a task-specific per-training objective for multi-document summarization. SLED <ref type="bibr" target="#b21">(Ivgi et al., 2023)</ref> processes long sequences via short-context PLMs. The origin long sequence is partitioned into overlapping chunks. HEPOS <ref type="bibr" target="#b20">(Huang et al., 2021)</ref> proposes head-wise positional </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Compatible with short-input tasks</head><p>While the sparse-attention transformers have been proven effective on a wide range of long-sequence datasets, as shown in Fig. <ref type="figure">6</ref>, these methods tend to underperform traditional full-attention transformers on the more common short-sequence tasks. However, in real scenarios, short-sequence inputs and long-sequence inputs are often mixed together, and the former occupies the vast majority. This limits the application scope of sparse-attention transformer architecture. In contrast, applying our framework SimCAS to existing full-attention transformers has strong flexibility. Specifically, given the full-attention model under SimCAS, if the input sequence exceeds the maximum length of a single chunk, we will perform chunking and selecting, otherwise, we can naturally switch to a standard short-text encoding form by skipping chunking, aligning, and selecting procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Efficacy of SBA</head><p>As our framework introduces SBA to align interchunk information during the encoding steps, we explore the change of hidden state in the forward propagation process under the influence of this component. Fig. <ref type="figure">7</ref> demonstrates the visualized similarity matrix between each chunk's start hidden state after the encoding block. It can be seen from an array of cosine similarity matrices in the top half that, through the alternation of SBA and encoding blocks, the cosine similarities between the start hidden states from different chunks are all close to 1, which indicates that their directions in high-dimensional space are almost the same.</p><p>Considering that cosine similarity can only reflect the closeness between vector directions, we design a similarity calculation method to add the measure of Euclidean distance. Given a pair of vectors v 1 , v 2 ? R d , the custom similarity Sim between them is formulated as follows:</p><formula xml:id="formula_26">Sim = (v1, v2) ?v1? * ?v2? * (1 + ?v1 -v2?) ,</formula><p>by which, we scale the cosine similarity to observe its change. As can be seen from the lower part of Fig. <ref type="figure">7</ref>, after each encoder layer, the start hidden states will vary in scale due to different contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Experimental Details</head><p>Input Structure Since we aim to minimize datasetspecific modeling, we unify the processing of the used long-document summarization, multidocument summarization, and machine reading comprehension datasets. Specifically, as for the long-document datasets arXiv, PubMed, and Gov-Report, we simply input the long document and truncate it to the maximum input length, which means that all sections are separated by newlines.</p><p>Following the tokenization form of BART, for the multi-document datasets Multi-News and WCEP, all the documents are concatenated to form a long sequence like the long document example and separated with &lt;/s&gt;. In the NarrativeQA dataset for machine reading comprehension, we concatenate together the question, summary, and source document, each of which prepends a prefix to indicate the type of text. Before the embedding operation, we conduct the chunking procedure in our method.</p><p>Training Details In this paper, for the training of BART base on various datasets we uniformly employ the Adam optimizer with the following dynamic learning rate:</p><formula xml:id="formula_27">lr = 5 ? 10 -3 min(step -0.5 , step ? warmup -1.5 ),</formula><p>where warmup indicates the warmup steps, which is set to 2500, step is the number of updating steps, and lr is the learning rate. In addition, there is another separate Adam optimizer for our reinforcement learning-based parameterized selector, in which the learning rate is fixed to 1 ? 10 -4 . The optimization of the selector and transformer is performed alternately, with one model trained while the other model is fixed.</p><p>The Details of Selector In our setting, the selector consists of an actor component and a critic component with PPO optimization, both of which are a simple three-layer feed-forward network, except for the final layers. In order to enable the selector to choose more diverse token representations instead of becoming homogeneous during the chunk-wise selection process, the state space consists of the current token representation and the selector's hidden state that is affected by previous actions. At the beginning of the selection procedure, the initial selector hidden state is the average of the start hidden state of all chunk representations. At each time step, we input the current selector hidden state and a chunk of token representations. And then the actor of the selector outputs a probability distribution over action space, and the critic of the selector outputs a single estimated scalar value for each token based on the hidden state selector and the corresponding token representation. For the state transition after executing current actions, we update the hidden state of the selector using the selected token representation from the current chunk. Note that in order to avoid the extreme case where the selector skips all tokens, in each chunk decision, if all actions are "skipping", they are all switched to "selecting", and the corresponding action probability and value are obtained. Additionally, to increase the training stability, the advantage in the PPO algorithm is approximated using the Generalized Advantage Estimation (GAE) <ref type="bibr" target="#b53">(Schulman et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Case Study on GovReport Dataset</head><p>In addition to using regular automatic evaluation metrics to measure the effect of model generation, we also present some actual output to support the results. Fig. <ref type="figure" target="#fig_2">8</ref> displays several examples of summaries generated by the fine-tuned base model BART large and our BART base -SimCAS. We can observe that the system output of our model has fewer grammatical errors and higher coherence compared with the base model. Furthermore, since our model is able to perceive longer sequences, our output is more informative and better aligned with the reference text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>in may 2018 , gao reported that the trust fund , which pays disability benefits to certain coal miners , faced financial challenges . the trust fund has borrowed from the u.s. treasury 's general fund almost every year since 1979 to make needed expenditures . gao 's june 2019 testimony included preliminary observations that coal operator bankruptcies were further straining trust fund finances because , in some cases , benefit responsibility was transferred to the trust fund . this testimony is based on gao 's report being released today , and describes ( 1 ) how coal mine operator bankruptcies have affected the trust fund , and ( 2 ) how dol managed coal mine operator insurance to limit financial risk to the trust fund . in producing this report , gao identified coal operators that filed for bankruptcy from 2014 through 2016 . gao analyzed information on commercially -insured and self -insured coal operators , and examined workers ' compensation insurance practices in four of the nation 's top five coal producing states . gao also interviewed dol officials , coal mine operators , and insurance company representatives , among others . coal mine operator bankruptcies have led to the transfer of about $ 865 million in estimated benefit responsibility to the federal government 's black lung disability trust fund ( trust fund ) , according to dol estimates . the trust fund pays benefits when no responsible operator is identified , or when the liable operator does not pay . gao previously testified in june 2019 that it had identified three bankrupt , self -insured operators for which benefit responsibility was transferred to the trust fund . since that time , dol 's estimate of the transferred benefit responsibility has grownfrom a prior range of $ 313 million to $ 325 million to the more recent $ 865 million estimate provided to gao in january 2020 . according to dol , this escalation was due , in part , to recent increases in black lung benefit award rates and higher medical treatment costs , and to an underestimate of one company 's ( patriot coal ) future benefit claims . trust fund , filed from 2014 through 2016 dol 's limited oversight of coal mine operator insurance has exposed the trust fund to financial risk , though recent changes , if implemented effectively , can help address these risks . in overseeing self -insurance in the past , dol did not : estimate future benefit liability when setting the amount of collateral required to self -insure ; regularly review operators to assess whether the required amount of collateral should change ; or always take action to protect the trust fund by revoking an operators ' ability to self -insure as appropriate . in july 2019 , dol began implementing a new self -insurance process that could help address past deficiencies in estimating collateral and regularly reviewing self -insured operators . however , dol 's new process still lacks procedures for its planned annual renewal of self -insured operators and for resolving coal operator appeals should operators dispute dol collateral requirements . this could hinder dol from revoking operators ' ability to self -insure should they not comply with dol requirements . further , for those operators that do not self -insure , dol does not monitor them to ensure they maintain adequate and continuous commercial coverage as appropriate . as a result , the trust fund may in some instances assume responsibility for paying benefits that otherwise would have been paid by insurers . gao made three recommendations to dol to establish procedures for self -insurance renewals and coal operator appeals , and to develop a process to monitor whether commercially -insured operators maintain adequate and continuous coverage . dol agreed with these recommendations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BART (1K)</head><p>of the eight coal mine operator bankruptcies gao identified from 2014 through 2016 , three resulted in a transfer of estimated benefit liability from the coal operator to the trust fund and five did not , according to dol . dol estimates for how these bankruptcies will affect the trust fund have considerably increased from what dol had previously reported . in june 2019 , gao reported that dol estimated that between $ 313 million to $ 325 million in benefit liabilities would transfer to thetrust fund as a result of the bankruptcies . in january 2020 , however , dol provided updated estimates stating that $ 865 million in benefits would be transferred to thetrust fund.gao also reported in june 2019 that the federal government does not have a reliable estimate of the amount of collateral dol required from coal mine operators to self-insure , and that benefit liabilities in excess of the collateral can be transferred from the bankrupt operator.what gao foundthe department of labor 's ( dol ) estimates for the extent of black lung benefit liabilities transferred from operators to the u.s. trust fund are considerably higher than dol 's estimates , and dol officials said that their estimates increased , among other reasons , to account for higher black lung award rates that occurred from fiscal years 2016 through 2019 , and higher medical treatment cost inflation in recent years.dol 's prior estimate for the patriot coal ( patriot ) bankruptcy did not fully cover the estimated benefit liabilities . when this occurs , benefit liabilities that in excess or in excess to the collateral could be transferred into the trust trust fund . for example , the collateral required from alpha natural resources ( alpha ) was about $ 12 million and approximately $ 494 million of estimated black lung liability transferred to a trust fund in 2016.since 2016 , several other self-insured operators have also filed for bankruptcy , including cambrian coal , cloud peak energy , murray energy , and westmoreland coal . dol does not estimate future benefit liability when setting collateral or regularly review operators to monitor their changing financial conditions . in the past , agency procedures require that collateral be obtained from operators in an amount deemed necessary and sufficient to secure the payment of the operators ' liability . to determine collateral amounts under the former process , the agency procedures stated that an operator 's net worth was to be equal to 3 , 5 , or 10 years of a operator 's annual black lung benefit payments made at the time of a coal operator 's self-insurance application depending on its net worth . specifically , if net worth were $ 1 billion or greater , then dol procedures set collateral equal to three years of benefit payments . if net worth is $ 1.1 billion , then net worth would equal to 5 , 10 , or 20 years of the operator 's monthly benefit payments , depending on the net worth of the underlying collateral . the amount of the required collateral from these operators would not have to be transferred in the future , and therefore would not be transferred.the federal government has not developed reliable estimates of future black lung benefits liability for the coal operators gao selected for review because dol has not established a time frame for doing so . in december 2017 , d dol announced that it was developing a new estimate of future benefit liabilities for coal operators , but it has not yet determined the extent to which the new estimate will address the limitations of the current estimate . without reliable estimates , it will be difficult for dol to determine whether future benefits liability will be transferred or whether additional actions may be needed to address these limitations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SimCAS (16K)</head><p>coal mine operator bankruptcies resulted in a transfer of estimated benefit liability from the coal operator to the trust fund and five did not , according to the department of labor ( dol ) . dol estimates that between $ 313 million to $ 325 million in benefit liabilities would transfer to the trust fund as a result of these bankruptcies . gao was asked to review dol 's oversight of coal mine operators ' self -insurance . this report examines ( 1 ) the extent to which dol has taken steps to protect the financial interests of the trust fund and ( 2 ) the actions dol took to mitigate the financial losses of the bankrupt operators . to conduct this work , gao reviewed dol documents and interviewed dol officials . from 2014 through 2016 , three self -insured coal mine operations resulted in the transfer of $ 865 million of estimated black lung benefit liability to the u.s. trust fund ( trust fund ) . of the eight bankruptcies gao identified , three resulted in an transfer of the estimated benefit liabilities from the coal mine operators ( coal operators ) : three ( energy future holdings , peabody energy , and walter energy ) did not affect the trust fund . the amount of collateral dol required from these operators to self -insure did not fully cover their estimated benefits liabilities . when this occurs , benefit liabilities in excess of the collateral can be transferred to thetrust fund . for example , the collateral dl required from energy future holdings was about $ 12 million and approximately $ 494 million of expected benefit liability transferred to the trust fund . dol also did not routinely consider potential future claims for which an operator could be responsible . in reviewing the most recent reauthorization memos for each of the self -insured operators , we found that these operators were not reauthorized since 1988 . in january 2020 , dol provided updated estimates stating that $ 865 million in benefits would be transferred from these two bankruptcies as a consequence of the three other bankruptcies , but dol does not have procedures that specify the duration of an operator 's selfinsurance authority or the conditions under which that authority would not be renewed . in the absence of a process to monitor operator compliance with program requirements , the agency risks not identifying or cancelling operator coverage . in addition , the new procedures do not specify , among other things , how long an operator will be authorized to selfinsure ; when an operator is authorized to do so ; when the operator is not renewing the operator 's authority ; and what conditions are under which this authority would be not renewed . the new requirements for setting collateral and for the more frequent review of selfinsured operators are key elements of internal controls , which call for agency management to implement control activities through policy . however , dl 's new self -insurance procedures do not specify , for example , how much collateral the agency will require an operator to secure and how much time dol appeals decisions should be made . in particular , dls does not specify a goal for the amount of time that it will take for the operator to apply for and receive a renewal application . without such a goal , it is difficult for dol to ensure that the application is valid and that the operator has a valid application for and receives an extension of the application . in commenting on a draft of this report , dod concurred with gao 's findings and recommendations . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the average cross-attention of all attention heads at all layers in the decoder (excluding start position). This example is generated by the BART base +SimCAS trained on GovReport.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Comparison of the performance of fullattention PLM BART and sparse-attention PLM LED on short-text summarization datasets CNNDM (Nallapati et al., 2016) and XSum (Narayan et al., 2018). AVG ROUGE denotes the average of ROUGE-1/2/L F1 scores. /D\HU /D\HU /D\HU /D\HU /D\HU /D\HU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example summary generated by BART and SimCAS trained on GovReport dataset. The maximum input length of standard BART and SimCAS is 1024 and 16384 respectively. The sentence in green is included in the SimCAS summary, while the one in red is discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Chunking Encoder Layers Shared self-attention Shared self-attention Shared self-attention Shared self-attention Shared self-attention Shared self-attention</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>? ? ? Selected Tokens Output Feedback Attention Feedback Decoder Input (m tokens) Add &amp; Norm Add &amp; Norm Add &amp; Norm N1?</head><label></label><figDesc></figDesc><table><row><cell cols="11">Figure 1: The learning framework of SimCAS: The long-text inputs are first divided into a batch of chunks and</cell></row><row><cell cols="11">padded with ([P]), with [BOS] ([S]) and [EOS] ([E]) tokens added. Then the chunk batch is encoded with [BOS]</cell></row><row><cell cols="11">and [EOS] representations aligned in each encoder layer. Next, representative hidden states are selected as a</cell></row><row><cell cols="11">subsequence for decoding steps. The decoder output logits are utilized as rewards for updating the token selector.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n-2</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>n-2 n-1</cell><cell>n</cell><cell>PAD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n-2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>? 19.28 ? 42.17 ? 63.44 ? 59.22 ? 25.97 ? 56.50 ? 68.15 ? 48.01 ? 21.03 ? 43.72 ? 66.18 ? Average results on three long-document test sets. ?: significantly better than BART (p &lt; 0.01). R-1/2/L is the ROUGE-1/2/L F1 score. BS refers to the neural model-based metrics BERTScore. The best results are bolded.</figDesc><table><row><cell>L</cell><cell>BS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2 :</head><label>2</label><figDesc>Average results on Multi-News test set. ?: significantly better than BART (p &lt; 0.01). R-1/2/L is the ROUGE-1/2/L F1 score. BS refers to the modelbased metrics BERTScore. The best results are bolded.</figDesc><table><row><cell>System</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell></row><row><cell>BARTlarge</cell><cell cols="3">42.04 14.88 23.34</cell><cell>-</cell></row><row><cell>PEGASUSlarge</cell><cell cols="3">47.52 18.72 24.91</cell><cell>-</cell></row><row><cell>HiMAP</cell><cell cols="3">44.17 16.05 21.38</cell><cell>-</cell></row><row><cell>GraphSum</cell><cell cols="3">45.87 17.56 23.39</cell><cell>-</cell></row><row><cell cols="4">BART-Long-Graph 49.24 18.99 23.97</cell><cell>-</cell></row><row><cell>LED+RELAX</cell><cell cols="3">47.23 18.86 25.03</cell><cell>-</cell></row><row><cell>PRIMERA</cell><cell cols="3">49.94 21.05 25.85</cell><cell>-</cell></row><row><cell cols="5">BARTbase+SimCAS 48.91  ? 19.92  ? 45.02  ? 65.01  ?</cell></row><row><cell>System</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell></row><row><cell>BERTREG</cell><cell cols="3">35.00 13.50 25.50</cell><cell>-</cell></row><row><cell cols="4">SUBMODULAR+ABS 34.40 13.10 25.00</cell><cell>-</cell></row><row><cell>DynE</cell><cell cols="3">35.40 15.10 25.60</cell><cell>-</cell></row><row><cell>LED</cell><cell cols="3">39.79 18.94 32.10</cell><cell>-</cell></row><row><cell>LED+RELAX</cell><cell cols="3">41.11 19.46 33.13</cell><cell>-</cell></row><row><cell>PRIMERA</cell><cell cols="3">46.08 25.21 37.86</cell><cell>-</cell></row><row><cell>BARTbase+SimCAS</cell><cell>45.20</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>? 22.48 ? 37.30 ? 70.46 ?</p>Table 3: Average results on WCEP test set. ?: significantly better than BART (p &lt; 0.01). R-1/2/L is the ROUGE-1/2/L F1 score. BS refers to the neural modelbased metrics BERTScore. The best results are bolded.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on GovReport development set. Performance changes compared with the full model are reported. Larger decreases in metrics are shaded with a darker orange.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>As the training progresses, the actual number of tokens entered into the model and the number of tokens selected by our selector. The red dash line represents the boundary for the conditional skipping reward. Performance on WCEP test set. R-1/2/L are the ROUGE-1/2/L F1 score. BS refers to the model-based metric BERTScore. The best results are bolded.</figDesc><table><row><cell>We apply SimCAS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Performance of BART base -SimCAS on WCEP with different maximum input lengths during inference.</figDesc><table><row><cell>BS</cell></row></table><note><p>R-1/2/L is the ROUGE-1/2/L F1 score. BS refers to the model-based metric BERTScore. ?: the maximum input length in the training phase. +?: the input sequence to the model is complete. The best results are bolded.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Statistics of used datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Train Valid Test Avg. Input Tokens</cell></row><row><cell>arXiv</cell><cell>203.0K 6.4K 6.4K</cell><cell>6021</cell></row><row><cell>PubMed</cell><cell>119.9K 6.6K 6.7K</cell><cell>3049</cell></row><row><cell cols="2">GovReport 17.5K 1.0K 1.0K</cell><cell>9616</cell></row><row><cell cols="2">Multi-News 44.9K 5.6K 5.6K</cell><cell>1793</cell></row><row><cell>WCEP</cell><cell>8.1K 1.0K 1.0K</cell><cell>3866</cell></row><row><cell cols="2">NarrativeQA 32.7K 3.5K 10.6K</cell><cell>121736</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/armancohan/long-summarization</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/luyang-huang96/LongDocSum</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/Alex-Fabbri/Multi-News</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/allenai/PRIMER</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/deepmind/narrativeqa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>In order to make a fair comparison with the previous work, we also use checkpoint "facebook/bart-large-mnli" for BERTScore.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>Similar to existing generative models, there is no guarantee that the generated content of our proposed method is factually consistent and free from hallucination <ref type="bibr" target="#b37">(Maynez et al., 2020;</ref><ref type="bibr" target="#b23">Kang and Hashimoto, 2020)</ref>. Therefore, caution is required when applying the model to real-world scenarios.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ETC: Encoding long and structured inputs in transformers</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciek</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Ribas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07113</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unlimiformer: Longrange transformers with unlimited length input</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2023. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TicketTalk: Toward humanlevel performance with end-to-end, transaction-based dialog systems</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saravanan</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.55</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The locality principle. Commun</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Denning</surname></persName>
		</author>
		<idno type="DOI">10.1145/1070838.1070856</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00512</idno>
		<title level="m">How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Irene</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01749</idno>
		<title level="m">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A practical survey on faster and lighter transformers</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?tan</forename><surname>Marceau Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aloise</surname></persName>
		</author>
		<idno type="DOI">10.1145/3586074</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. Just Accepted</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large-scale multi-document summarization dataset from the Wikipedia current events portal</title>
		<author>
			<persName><forename type="first">Demian</forename><surname>Gholipour Ghalandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">The</forename><surname>Nghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgiana</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><surname>Ifrim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1302" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent chunking mechanisms for long-text machine reading comprehension</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.603</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6751" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03274</idno>
		<title level="m">Gmat: Global memory augmentation for transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CTRLsum: Towards generic controllable text summarization</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5879" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dyne: Dynamic ensemble decoding for multi-document summarization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demian</forename><surname>Gholipour Ghalandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia The</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Glover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient attentions for long document summarization</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Long-Text Understanding with Short-Text Models</title>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00547</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
		<title level="m">A natural policy gradient. Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved natural language generation via loss truncation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.66</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="718" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The impact of positional encoding on length generalization in transformers</title>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Kazemnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Klafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4801" to="4811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The NarrativeQA Reading Comprehension Challenge</title>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging graph to improve abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6232" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Leveraging locality in abstractive text summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Budhaditya</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6081" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-document summarization via deep learning techniques: A survey</title>
		<author>
			<persName><forename type="first">Congbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Emma</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3529754</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Luna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DYLE: Dynamic latent extraction for abstractive long-input summarization</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Budhaditya</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.118</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1687" to="1698" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative marginalized probabilistic neural method for multi-document summarization of medical literature</title>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ragazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Valgimigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Freddi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?aglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Gul?ehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018. 2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
	<note>Training language models to follow instructions with human feedback</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive learning for many-to-many multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="244" to="258" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">QuALITY: Question answering with long input texts, yes</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.391</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5336" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A multi-document coverage reward for relaxed multi-document summarization</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Parnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Jauregi Unanue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Piccardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficiently summarizing text and graph encodings of multi-document clusters</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4768" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Investigating efficiently extending transformers for long input summarization</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04347</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring controllable text generation techniques</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization</title>
		<author>
			<persName><forename type="first">Rajkumar</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiant?</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafet</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Efficient Content-Based Sparse Attention with Routing Transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Highdimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>CoRR, abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SCROLLS: Standardized CompaRison over long language sequences</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12007" to="12021" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09355</idno>
		<title level="m">Patient knowledge distillation for bert model compression</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scaling laws vs model architectures: How does inductive bias influence scaling?</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2023. 2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>W?nsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Apps</surname></persName>
		</author>
		<author>
			<persName><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sequenceto-sequence learning as beam-search optimization</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Wysocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zili</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D?nal</forename><surname>Wysocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Landers</surname></persName>
		</author>
		<author>
			<persName><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00462</idno>
	</analytic>
	<monogr>
		<title level="m">Transformers and the Representation of Biomedical Background Knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="73" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5245" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple local attentions remain competitive for long-context tasks</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1975" to="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">2019a. End-to-end open-domain question answering with BERTserini</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reducing word omission errors in neural machine translation: A contrastive learning approach</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6191" to="6196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bartscore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27263" to="27277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Summ n : A multi-stage summarization framework for long input dialogues and documents</title>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Budhaditya</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1592" to="1604" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
