<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Going Deeper in Facial Expression Recognition using Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-12">12 Nov 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
							<email>ali.mollahosseini@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Chan</surname></persName>
							<email>davidchan@cs.du.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<settlement>Denver</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
							<email>mmahoor@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<settlement>Denver</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Going Deeper in Facial Expression Recognition using Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-12">12 Nov 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1511.04110v1[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current Human Machine Interaction (HMI) systems have yet to reach the full emotional and social capabilities necessary for rich and robust interaction with human beings. Facial expression, which plays a vital role in social interaction, is one of the most important nonverbal channels through which HMI systems can recognize humans' internal emotions. Ekman et al. identified six facial expressions (viz. anger, disgust, fear, happiness, sadness, and surprise) as basic emotional expressions that are universal among human beings <ref type="bibr" target="#b10">[11]</ref>.</p><p>Due to the importance of facial expression in designing HMI and Human Robot Interaction (HRI) systems <ref type="bibr" target="#b32">[33]</ref>, numerous computer vision and machine learning algorithms have been proposed for automated Facial Expression Recognition (FER). Also, there exist many annotated face databases with either human actors portraying basic expressions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, or faces captured spontaneously in an uncontrolled setting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>. Automated FER approaches attempt to classify faces in a given single image or sequence of images as one of the six basic emotions. Although, traditional machine learning approaches such as support vector machines, and to a lesser extent, Bayesian classifiers, have been successful when classifying posed facial expressions in a controlled environment, recent studies have shown that these solutions do not have the flexibility to classify images captured in a spontaneous uncontrolled manner ("in the wild") or when applied databases for which they were not designed <ref type="bibr" target="#b29">[30]</ref>. This poor generalizability of these methods is primarily due to the fact that many approaches are subject or database dependent and only capable of recognizing exaggerated or limited expressions similar to those in the training database. Many FER databases have tightly controlled illumination and pose conditions. In addition, obtaining accurate training data is particularly difficult, especially for emotions such as sadness or fear which are extremely difficult to accurately replicate and do not occur often real life.</p><p>Recently, due to an increase in the ready availability of computational power and increasingly large training databases to work with, the machine learning technique of neural networks has seen resurgence in popularity. Recent state of the art results have been obtained using neural networks in the fields of visual object recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>, human pose estimation <ref type="bibr" target="#b44">[45]</ref>, face verification <ref type="bibr" target="#b42">[43]</ref>, and many more. Even in the FER field results so far have been promising <ref type="bibr" target="#b16">[17]</ref>. Unlike traditional machine learning approaches where features are defined by hand, we often see improvement in visual processing tasks when using neural networks because of the network's ability to extract undefined features from the training database. It is often the case that neural networks that are trained on large amounts of data are able to extract features generalizing well to scenarios that the network has not been trained on. We explore this idea closely by training our proposed network architecture on a subset of the available training databases, and then performing cross-database experiments which allow us to accurately judge the network's performance in novel scenarios.</p><p>In the FER problem, however, unlike visual object databases such as imageNet <ref type="bibr" target="#b7">[8]</ref>, existing FER databases often have limited numbers of subjects, few sample images or videos per expression, or small variation between sets, making neural networks significantly more difficult to train. For example, the FER2013 database <ref type="bibr" target="#b0">[1]</ref> (one of the largest recently released FER databases) contains 35,887 images of different subjects yet only 547 of the images portray disgust. Similarly, the CMU MultiPIE face database <ref type="bibr" target="#b14">[15]</ref> contains around 750,000 images but it is comprised of only 337 different subjects, where 348,000 images portray only a "neutral" emotion and the remaining images do not portray anger, fear or sadness.</p><p>This paper presents a novel deep neural network architecture for the FER problem, and examines the network's ability to perform cross-database classification while training on databases that have limited scope, and are often specialized for a few expressions (e.g. MultiPIE and FERA). We conducted comprehensive experiments on seven wellknown facial expression databases (viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013) and obtain results which are significantly better than, or comparable to, traditional convolutional neural networks or other state-ofthe-art methods in both accuracy and learning time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Algorithms for automated FER usually involve three main steps, viz. registration, feature extraction, and classification. In the face registration step, faces are first located in the image using some set of landmark points during "face localization" or "face detection". These detected faces are then geometrically normalized to match some template image in a process called "face registration". In the feature extraction step, a numerical feature vector is generated from the resulting registered image. These features can be geometric features such as facial landmarks <ref type="bibr" target="#b18">[19]</ref>, appearance features such as pixel intensities <ref type="bibr" target="#b31">[32]</ref>, Gabor filters <ref type="bibr" target="#b22">[23]</ref>, Local Binary Patterns (LBP) <ref type="bibr" target="#b36">[37]</ref>, Local Phase Quantization (LPQ) <ref type="bibr" target="#b51">[52]</ref>, and Histogram of Oriented Gradients (HoG) <ref type="bibr" target="#b28">[29]</ref>, or motion features such as optical flow <ref type="bibr" target="#b17">[18]</ref>, Motion History Images (MHI) <ref type="bibr" target="#b45">[46]</ref>, and volume LBP <ref type="bibr" target="#b50">[51]</ref>. Current state-of-the-art methods, such as those used in Zhang et al. <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> fuse multiple features using multiple kernel learning algorithms. However by using neural networks, we do not have to worry about the feature selection step -as neural networks have the capacity to learn features that statistically allow the network to make correct classifications of the input data. In the third step, of classification, the algorithm attempts to classify the given face as portraying one of the six basic emotions using machine learning techniques.</p><p>Ekman et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> distinguished two conceptual approaches to studying facial behavior: a "message-based" approach and a "sign-based" approach. Message-based approaches categorize facial behaviors as the the meaning of expressions, whereas sign-based approaches describe facial actions/configuration regardless of the action's meaning. The most well-known and widely used sign-based approach is the Facial Action Coding System (FACS) <ref type="bibr" target="#b11">[12]</ref>. FACS describes human facial movements by their appearance on the face using standard facial substructures called Action Units (AUs). Each AU is based on one or a few facial muscles and AUs may occur individually or in combinations. Similarly, FER algorithms can be categorized into both message-based and sign-based approaches. In message-based approaches FER algorithms are trained on databases labeled with the six basic expressions <ref type="bibr" target="#b6">[7]</ref>, and more recently, embarrassment and contempt <ref type="bibr" target="#b26">[27]</ref>. Unlike message-based algorithms, sign-based algorithms are trained to detect AUs in a given image or sequence of images <ref type="bibr" target="#b6">[7]</ref>. These detected AUs are then converted to emotion-specified expressions using EM-FACS <ref type="bibr" target="#b13">[14]</ref> or similar systems <ref type="bibr" target="#b41">[42]</ref>. In this paper, we develop a message-based neural network solution, FER systems are traditionally evaluated in either a subject independent manner or a cross-database manner. In subject independent evaluation, the classifier is trained on a subset of images in a database (called the training set) and evaluated on faces in the same database that are not elements of the training set often using K-fold cross validation or leave-one-subject-out approaches. The crossdatabase method of evaluating facial expression systems requires training the classifier on all of the images in a single database and evaluating the classifier on a different database which the classifier has never seen images from. As single databases have similar settings (illumination, pose, resolution etc.), subject independent tasks are easier to solve than cross database tasks. Subject independent evaluation is not, however, unimportant. If a researcher can guarantee that the data will align well in pose, illumination and other factors with the training set, subject independent evaluation can give a reasonably good representation of the classification accuracy in an online system. Another technique, subject dependent evaluation (person-specific), is also used in limited cases, e.g. FERA 2011 challenge <ref type="bibr" target="#b46">[47]</ref>; often in these scenarios the recognition accuracy is more important than the generalization.</p><p>Recent approaches to visual object recognition tasks, and the FER problem have used increasingly "deep" neural networks (neural networks with large numbers of hidden layers). The term "deep neural network" refers to a relatively new set of techniques in neural network architecture design that were developed in order to improve the ability of neural networks to tackle big-data problems. With the large amount of available computing power continuing to grow, deep neural network architectures provide a learning architecture based in the development of "brain-like" structures which can learn multiple levels of representation and abstraction which allow algorithms for finding complex patterns in images, sound, and text.</p><p>It seems only logical to extend cutting-edge techniques in the field of "deep learning" to the FER problem. Deep networks have a remarkable ability to perform well in flexible learning tasks, such as the cross-database evaluation situation, where it is unlikely that hand-crafted features will easily generalize to a new scenario. By training neural networks, particularly deep neural networks, for feature recognition and extraction we can drastically reduce the amount of time that is necessary to implement a solution to the FER problem that, even when confronted with a novel data source, will be able to perform at high levels of accuracy. Similarly, we see deep neural networks performing well in the subject independent evaluation scenarios, as the algorithms can learn to recognize subtle features that even field experts can miss. These correlations provide the motivation for this paper, as the strengths of deep learning seem to align perfectly with the techniques required for solving difficult "in the wild" FER problems.</p><p>A subset of deep neural network architectures called "convolutional neural networks" (CNNs) have become the traditional approach for researchers studying vision and deep learning. In the 2014 ImageNet challenge for object recognition, the top three finishers all used a CNN approach, with the GoogLeNet architecture achieving a remarkable 6.66% error rate in classification <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b35">36]</ref>. The GoogLeNet architecture uses a novel multi-scale approach by using multiple classifier structures, combined with multiple sources for back propagation. This architecture defeats a number of problems that occur when back-propagation decays before reaching beginning layers in the architecture. Additional layers that reduce dimension allow GoogLeNet to increase in both width and depth without significant penalties, and take an elegant step towards complicated network-in-network architectures described originally in Lin et al. <ref type="bibr" target="#b21">[22]</ref>. In other word, the architecture is composed of multiple "Inception" layers, each of which acts like a micro-network in the larger network, allowing the architecture to make more complex decisions.</p><p>More traditional CNN architectures have also achieved remarkable results. AlexNet <ref type="bibr" target="#b19">[20]</ref> is an architecture that is based on the traditional CNN layered architecture -stacks of convolutions layers followed by max-pooling layers and rectified linear units (ReLUs), with a number of fully connected layers at the top of the layer stack. Their top=5 error rate of 15.3% on the ILSVRC-2012 competition revolutionized the way that we think about the effectiveness of CNNs. This network was also one of the first networks to introduce the "dropout" method for solving the over fitting problem (Suggested by Hinton et al. <ref type="bibr" target="#b37">[38]</ref>) which proved key in developing large neural networks. One of the large challenges to overcome in the use of traditional CNN architectures is their depth and computational complexity. The full AlexNet network performs on the order of 100M operations for a single iteration, while SVM and shallow neural networks perform far fewer operations in order to create a suitable model. This makes traditional CNNs very hard to apply in time restrictive scenarios.</p><p>In <ref type="bibr" target="#b23">[24]</ref> a new deep neural network architecture, called an "AU-Aware" architecture was proposed in order to investigate the FER problem. In an AU-Aware architecture, the bottom of the layer stack consists of convolution layers and max-pooling layers which are used to generate a complete representation of the face. Next in the layer stack, an "AU-aware receptive field layer" generates a complete representation over all possible spatial regions by convolving the dense-sampling facial patches with special filters in a greedy manner. Then, a multilayer Restricted Boltzmann Machine (RBM) is exploited to learn hierarchical features. Finally, the outputs of the network are concatenated as features which are used to train a linear SVM classifier for recognizing the six basic expressions. Results in <ref type="bibr" target="#b23">[24]</ref> show that the features generated by this "AU-Aware" network are competitive with or superior to handcrafted features such as LBP, SIFT, HoG, and Gabor on the CK+, MMI and databases using a similar SVM. However, AU-aware layers do not necessarily detect FACS defined action units in faces.</p><p>In <ref type="bibr" target="#b16">[17]</ref> multiple deep neural network architectures are combined to solve the FER problem in video analysis. These network architectures included: (1) an architecture similar to the AlexNet CNN run on individual frames of the video, (2) a deep belief network trained on audio information, (3) an autoencoder to model the spatiotemporal properties of human activity, and (4) a shallow network focused on the mouth. The CNN is trained on the private Toronto Face Database <ref type="bibr" target="#b39">[40]</ref> and fine tuned on the AFEW database <ref type="bibr" target="#b8">[9]</ref>, yielded an accuracy of 35.58% when evaluated in a subject independent manner on AFEW. When combined with a single predictor, the five architectures produced an ac-curacy of 41.03% on the test set, the highest accuracy in the EmotiW 2013 <ref type="bibr" target="#b8">[9]</ref> challenge, where challenge winner 2014 <ref type="bibr" target="#b25">[26]</ref> achieved 50.40% on test set using multiple kernel methods on Riemannian manifold.</p><p>A 3D CNN with deformable action parts constraints is introduced in <ref type="bibr" target="#b24">[25]</ref> which can detect specific facial action parts under the structured spatial constraints, and obtain the discriminative part-based representation simultaneously. The results on two posed expression datasets, CK+, MMI, and a spontaneous dataset FERA achieve state-ofthe-art video-based expression recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Often improving neural network architectures has relied on increasing the number of neurons or increasing the number of layers, allowing the network to learn more complex functions; however, increasing the depth and complexity of a topology leads to a number of problems such as increased over-fitting of training data, and increased computational needs. A natural solution to the problem of increasingly dense networks is to create deep sparse networks, which has both biological inspiration, and has firm theoretical foundations discussed in Arora et al. <ref type="bibr" target="#b2">[3]</ref>. Unfortunately, current GPUs and CPUs do not have the capability to efficiently compute actions on sparse networks. The Inception layer presented in <ref type="bibr" target="#b35">[36]</ref> attempts to rectify these concerns by providing an approximation of sparse networks to gain the theoretical benefits proposed by Arora et al., however retains the dense structure required for efficient computation.</p><p>Applying the Inception layer to applications of Deep Neural Network has had remarkable results, as implied by <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b40">[41]</ref>, and it seems only logical to extend state of the art techniques used in object recognition to the FER problem. In addition to merely providing theoretical gains from the sparsity, and thus, relative depth, of the network, the Inception layer also allows for improved recognition of local features, as smaller convolutions are applied locally, while larger convolutions approximate global features. The increased local performance seems to align logically with the way that humans process emotions as well. By looking at local features such as the eyes and mouth, humans can distinguish the majority of the emotions <ref type="bibr" target="#b3">[4]</ref>. Similarly, children with autism often cannot distinguish emotion properly without being told to remember to look at the same local features <ref type="bibr" target="#b3">[4]</ref>. By using the Inception layer structure and applying the network-in-network theory proposed by Lin et al. <ref type="bibr" target="#b21">[22]</ref>, we can expect significant gains on local feature performance, which seems to logically translate to improved FER results.</p><p>Another benefit of the network-in-network method is that along with increased local performance, the global pooling performance is increased and therefore it is less prone to overfitting. This resistance to overfitting allows us to increase the depth of the network significantly without worrying about the small corpus of images that we are working with in the FER problem.</p><p>The work that we present in this paper is inspired by the techniques provided by the GoogLeNet and AlexNet architectures described in Sec. 2. Our network consists of two elements, first our network contains of two traditional CNN modules (a traditional CNN layer consists of a convolution layer by a max pooling layer). Both of these modules use rectified linear units (ReLU) which have an activation function described by:</p><formula xml:id="formula_0">f (x) = max(0, x)</formula><p>where x is the input to the neuron <ref type="bibr" target="#b19">[20]</ref>. Using the ReLU activation function allows us to avoid the vanishing gradient problem caused by some other activation functions (for more details see <ref type="bibr" target="#b19">[20]</ref>). Following these modules, we apply the techniques of the network in network architecture and add two "Inception" style modules, which are made up of a 1 × 1, 3 × 3 and 5 × 5 convolution layers (Using ReLU) in parallel. These layers are then concatenated as output and we use two fully connected layers as the classifying layers (Also using ReLU). Figure <ref type="figure">1</ref> shows the architecture of the network used in this paper.</p><p>In this work we register facial images in each of the databases using research standard techniques. We used bidirectional warping of Active Appearance Model (AAM) <ref type="bibr" target="#b33">[34]</ref> and a Supervised Descent Method (SDM) called IntraFace <ref type="bibr" target="#b47">[48]</ref> to extract facial landmarks, however further work could consider improving the landmark recognition in order to extract more accurate faces. IntraFace uses SIFT features for feature mapping and trains a descent method by a linear regression on training set in order to extract 49 points. We use these points to register faces to an average face in an affine transformation. Finally, a fixed rectangle around the average face is considered as the face region. Figure <ref type="figure">2</ref> demonstrates samples of the face registration with this method. In our research, facial registration increased the accuracy of our FER algorithms by 4-10%, which suggests that registration (like normalization in traditional problems) is a significant portion of any FER algorithm.</p><p>Once the faces have been registered, the images are resized to 48×48 pixels for analysis. Even though many databases are composed of images with a much higher resolution testing suggested that decreasing this resolution does not greatly impact the accuracy, however vastly increases the speed of the network. To augment our data, we extract 5 crops of 40×40 from the four corners and the center of the image and utilize both of them and their horizontal flips for a total of 10 additional images.</p><p>In training the network, the learning rates are decreased in a polynomial fashion as: base lr(1 − iter/max iter) 0.  where base lr = 0.01 is the base learning rate, iter is the current iteration and max iter is the maximum allowed iterations. Testing suggested that other popular learning rate policies such as fixed learning rate, step where learning rate is multiplies by a gamma factor in each step, and exponential approach did not perform as well as the polynomial fashion. Using the polynomial learning rate, the test loss converged faster and allowed us to train the network for many iterations without the need for fine-tuning. We also trained the bias nodes twice as fast as the weights of the network, in order to increase the rate at which unnecessary nodes are removed from evaluation. This decreases the number of iterations that the network must run before the loss converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Face Databases</head><p>We evaluate the proposed method on well-known publicly available facial expression databases: CMU Multi-PIE <ref type="bibr" target="#b14">[15]</ref>, MMI <ref type="bibr" target="#b34">[35]</ref>, Denver Intensity of Spontaneous Facial Actions (DISFA) <ref type="bibr" target="#b28">[29]</ref>, extended CK+ <ref type="bibr" target="#b26">[27]</ref>, GEMEP-FERA database <ref type="bibr" target="#b4">[5]</ref>, SFEW <ref type="bibr" target="#b9">[10]</ref>, and FER2013 <ref type="bibr" target="#b0">[1]</ref>. In this section we briefly review the content of these databases.</p><p>CMU MultiPIE: CMU MultiPIE face database <ref type="bibr" target="#b14">[15]</ref> contains around 750,000 images of 337 people under multiple viewpoints, and different illumination conditions. There are four recording sessions in which subjects were instructed to display different facial expressions (i.e. Angry, Disgust, Happy, Neutral, Surprise, Squint, and Scream). We selected only the five frontal viewpoints (-45 • to +45 • ), giving us a total of around 200,000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMI:</head><p>The MMI <ref type="bibr" target="#b34">[35]</ref> database includes more than 20 subjects of both genders (44% female), ranging in age from 19 to 62, having either a European, Asian, or South American ethnicity. An image sequence is captured that has neutral faces at the beginning and the end for each session and subjects were instructed to display 79 series of facial expressions, six of which are prototypic emotions. We extracted static frames from each sequence, where it resulted in 11,500 images.</p><p>CK+: The Extended Cohn-Kanade database (CK+) <ref type="bibr" target="#b26">[27]</ref> includes 593 video sequences recorded from 123 subjects ranging from 18 to 30 years old. Subjects displayed different expressions starting from the neutral for all sequences, and some sequences are labeled with basic expressions. We selected only the final frame of each sequence with peak expression in our experiment, which results in 309 images.</p><p>DISFA: Denver Intensity of Spontaneous Facial Actions (DISFA) database <ref type="bibr" target="#b28">[29]</ref> is one of a few naturalistic databases that have been FACS coded with AU intensity values. This database consists of 27 subjects, each recorded while watching a four minutes video clip by two cameras. Twelve AUs are coded between 0-5, where 0 denotes the absence of the AU, while 5 represents maximum intensities. As DISFA is not emotion-specified coded, we used EMFACS system <ref type="bibr" target="#b13">[14]</ref> to convert AU FACS codes to expressions, which resulted in around 89,000 images in which the majority have neutral expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FERA:</head><p>The GEMEP-FERA database <ref type="bibr" target="#b4">[5]</ref> is a subset of the GEMEP corpus used as database for the FERA 2011 challenge <ref type="bibr" target="#b46">[47]</ref>. It consists of recordings of 10 actors displaying a range of expressions. There are seven subjects in the training data, and six subjects in the test set. The training set contains 155 image sequences and the testing contains 134 image sequences. There are in total five emotion categories in the database: Anger, Fear, Happiness, Relief and Sadness. We extract static frames from the sequences with six basic expressions, which resulted to in around 7,000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SFEW:</head><p>The Static Facial Expressions in the Wild (SFEW) database <ref type="bibr" target="#b9">[10]</ref> is created by selecting static frames from Acted Facial Expressions in the Wild (AFEW) <ref type="bibr" target="#b8">[9]</ref>. The SFEW database covers unconstrained facial expressions, different head poses, age range, and occlusions and close to real world illuminations. There are a total of 95 subjects in the database. In total there are 663 well-labeled usable images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FER2013:</head><p>The Facial Expression Recognition 2013 (FER-2013) database was introduced in the ICML 2013 Challenges in Representation Learning <ref type="bibr" target="#b0">[1]</ref>. The database was created using the Google image search API and faces have been automatically registered. Faces are labeled as any of the six basic expressions as well as the neutral. The resulting database contains 35,887 images most of them in wild settings.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the number of images for six basic expressions and neutral faces in each database. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We evaluated the accuracy of the proposed deep neural network architecture in two different experiments; viz. subject-independent and cross-database evaluation. In the subject-independent experiment, databases are split into training, validation, and test sets in a strict subject independent manner. We used the K-fold cross validation technique with K=5 to evaluate the results. In FERA and SFEW, the training and test sets are defined in the database release, and the results are evaluated on the database defined test set without performing K-fold cross validation. Since there are different samples per emotion per subject in some databases, the training, validation and test sets have slightly different sample sizes in each fold. On average we used 175K samples for training, 56K samples for validation, and 64K samples for test. The proposed architecture was trained for 200 epochs (i.e. 150K iterations on mini-batches of size 250 samples). Table <ref type="table" target="#tab_3">3</ref> gives the average accuracy when classifying the images into the six basic expressions and the neutral expression. The average confusion matrix for subject-independent experiments can be seen in Table <ref type="table" target="#tab_4">4</ref>.</p><p>Here, we also report the top-2 expression classes. As Table 3 depicts, the accuracy of the top-2 classification is 15% higher than the top-1 accuracy in most cases, especially in the wild datasets (i.e. FERA, SFEW, FER2013). We believe that by assigning a single expression to a image can be ambiguous when there is transition between expressions or the given expression is not at its peak, and therefore the top-2 expression can result in a better classification performance when evaluating image sequences.  The proposed architecture was implemented using the Caffe toolbox <ref type="bibr" target="#b15">[16]</ref> on a Tesla K40 GPU. It takes roughly 20 hours to train 175K samples for 200 epochs. Figure <ref type="figure" target="#fig_2">3</ref> shows the training loss and classification accuracy of the top-1 and top-2 classification labels on the validation set of the subject-independent experiment over 150,000 iterations (about 150 epochs). As the figure illustrates, the proposed architecture converges after about 50 epochs.</p><p>In the cross-database experiment, one database is used for evaluation and the rest of databases are used to train the network. Because every database has a unique fingerprint (lighting, pose, emotions, etc.) the cross database task is much more difficult to extract features from (both for traditional SVM approaches, and for neural networks). The proposed architecture was trained for 100 epochs in each experiment. Table <ref type="table" target="#tab_5">5</ref> gives the average cross-database accuracy when classifying the six basic expressions as well as the neutral expression.</p><p>The experiment presented in <ref type="bibr" target="#b29">[30]</ref> is a cross-database experiment performed by training the model on one of the CK+, MMI or FEEDTUM databases and testing the model on the others. The reported result in Table <ref type="table" target="#tab_5">5</ref> is the average results for the CK+ and MMI databases.</p><p>Different classifiers on several databases are presented in <ref type="bibr" target="#b36">[37]</ref> where the results is still one of the state-of-the-   art methods for person-independent evaluation on the MMI database (See Table <ref type="table" target="#tab_3">3</ref>). The reported result in Table <ref type="table" target="#tab_5">5</ref> is the best result using different SVM kernels trained on the CK+ database and evaluated the model on the MMI database. A supervised kernel mean matching is presented in <ref type="bibr" target="#b30">[31]</ref> which attempts to match the distribution of the training data in a class-to-class manner. Extensive experiments were performed using four classifiers (SVM, Nearest Mean Classifier, Weighted Template Matching, and K-nearest neighbors). The reported result in Table <ref type="table" target="#tab_5">5</ref> are the best results of the four classifier when training the model on the MMI and Jaffe databases and evaluating on the CK+ database as well as when the model was trained on the CK+ database and evaluated on the MMI database.</p><p>In <ref type="bibr" target="#b48">[49]</ref> multiple features are fused via a Multiple Ker-nel Learning algorithm and the cross-database experiment is trained on CK+, evaluated on MMI and vice versa. Comparing the result of our proposed approach with these stateof-the-art methods, it can be concluded that our network can generalized well for FER problem. Unfortunately, there is not any study on cross-database evaluation of more challenging datasets such as FERA, SFEW and FER2013. We believe that this work can be a baseline for cross-database of these challenging datasets.</p><p>In <ref type="bibr" target="#b12">[13]</ref>, a Shared Gaussian Processes for multiview and viewinvariant classification is proposed. The reported result is very promising on MultiPIE database which covers multiple views, however on wild setting of SFEW it is not as efficient as MultiPIE. A new sparse representation is employed in <ref type="bibr" target="#b20">[21]</ref>, aiming to reduce the intra-class variation and by generating an intra-class variation image of each expression by using training images.</p><p>[47] is the FERA 2011 challenge baseline and <ref type="bibr" target="#b1">[2]</ref> is the result of UC Riverside team (winner of the challenge). <ref type="bibr" target="#b41">[42]</ref> detects AUs and uses their composition rules to recognize expressions by means of a dictionary-based approach, which is one of the state-of-the-art "sign-based" approaches. <ref type="bibr" target="#b43">[44]</ref> is the winner of the ICML 2013 Challenges on FER2013 database that employed a convolutional neural network similar to AlexNet <ref type="bibr" target="#b19">[20]</ref> but with linear one-vs-all linear SVM top layer instead of a Softmax function. As a benchmark to our proposed solution, we trained a full AlexNet from scratch (as opposed to fine tuning an already trained network) using the same protocol as used to train our own network. As shown in Table <ref type="table" target="#tab_6">6</ref>, our proposed architecture has better performance on MMI &amp; FER2013 and comparable performance on the rest of the databases. The value of the proposed solution over the AlexNet architecture is its training time -Our version of AlexNet performed more than 100M operations, whereas the proposed network performs about 25M operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>As shown in Tables <ref type="table" target="#tab_5">3 and 5</ref>, the results in the subjectindependent tests were either comparable to or better than the current state of the art. It should be mentioned that we have compared our results with the best methods on each database separately, where the hyper parameters of the presented models are fine-tuned for that specific problem. We perform significantly better than the state of the art on Mul-tiPIE and SFEW (no known state of the art has been reported for the DISFA database). The only exceptions to the improved performance are with the MMI and FERA databases. There are a number of explanations for this phenomenon.</p><p>One of the likely reasons for the performance discrepancies on the subject-independent databases is due to the way that the networks are trained in our experiments. Because we use data from all of the studied databases to train the deep architecture, the input data contains image that do not conform to the database setting such as pose and lighting. It is very difficult to avoid this issue as it is hard or impossible to train such a complex network architecture on so little data without causing significant overfitting. Another reason for the decreased performance is the focus on crossdatabase performance. By training slightly less complicated architectures, or even using traditional methods such as support vector machines, or engineered features, it would likely be possible to improve the performance of the network on subject-independent tasks. In this research however, we present a comprehensive solution that can generalize well to the FER "in the wild" problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work presents a new deep neural network architecture for automated facial expression recognition. The proposed network consists of two convolutional layers each followed by max pooling and then four Inception layers. The Inception layers increase the depth and width of the network while keeping the computational budget constant. The proposed approach is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic expressions or the neutral.</p><p>We evaluated our proposed architecture in both subjectindependent and cross-database manners on seven wellknown publicly available databases. Our results confirm the superiority of our network compared to several stateof-the-art methods in which engineered features and classifier parameters are usually tuned on a very few databases. Our network is first which applies the Inception layer architecture to the FER problem across multiple databases. The clear advantage of the proposed method over conventional CNN methods (i.e. shallower or thinner networks) is gaining increased classification accuracy on both the subject independent and cross-database evaluation scenarios while reducing the number of operations required to train the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure 1. Network Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Training loss and classification accuracy on validation set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>5 ,      </figDesc><table><row><cell></cell><cell cols="2">Output</cell><cell></cell></row><row><cell></cell><cell cols="2">Inner Product</cell><cell></cell></row><row><cell></cell><cell>1024</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Inner Product</cell><cell></cell></row><row><cell></cell><cell>4096</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Max Pool</cell><cell></cell></row><row><cell></cell><cell cols="2">3x3x2</cell><cell></cell></row><row><cell></cell><cell cols="2">Concat</cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell>5x5 Conv</cell><cell>1x1 Conv</cell></row><row><cell></cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>1x1x1</cell></row><row><cell>1x1 Conv</cell><cell>3x3 reduce</cell><cell>5x5 reduce</cell><cell>Pool Proj</cell></row><row><cell>1x1x1</cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>3x3x1</cell></row><row><cell></cell><cell cols="2">Max Pool</cell><cell></cell></row><row><cell></cell><cell cols="2">3x3x2</cell><cell></cell></row><row><cell></cell><cell cols="2">Concat</cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell>5x5 Conv</cell><cell>1x1 Conv</cell></row><row><cell></cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>1x1x1</cell></row><row><cell>1x1 Conv</cell><cell>3x3 reduce</cell><cell>5x5 reduce</cell><cell>Pool Proj</cell></row><row><cell>1x1x1</cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>3x3x1</cell></row><row><cell></cell><cell cols="2">Concat</cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell>5x5 Conv</cell><cell>1x1 Conv</cell></row><row><cell></cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>1x1x1</cell></row><row><cell>1x1 Conv</cell><cell>3x3 reduce</cell><cell>5x5 reduce</cell><cell>Pool Proj</cell></row><row><cell>1x1x1</cell><cell>3x3x1</cell><cell>5x5x1</cell><cell>3x3x1</cell></row><row><cell></cell><cell cols="2">Max Pool</cell><cell></cell></row><row><cell></cell><cell cols="2">3x3x2</cell><cell></cell></row><row><cell></cell><cell cols="2">Conv2</cell><cell></cell></row><row><cell></cell><cell cols="2">3x3x1</cell><cell></cell></row><row><cell></cell><cell cols="2">Max Pool</cell><cell></cell></row><row><cell></cell><cell cols="2">3x3x2</cell><cell></cell></row><row><cell></cell><cell cols="2">Conv1</cell><cell></cell></row><row><cell></cell><cell cols="2">7x7x1</cell><cell></cell></row><row><cell></cell><cell>Data</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Network Configuration</figDesc><table><row><cell>Layer type</cell><cell>Patch Size / Stride</cell><cell>Output</cell><cell>1 x 1</cell><cell>3 x 3</cell><cell>3 x 3 reduce</cell><cell>5 x 5</cell><cell>5 x 5 reduce</cell><cell>Proj Pooling</cell><cell># Operations</cell></row><row><cell>Convolution -1</cell><cell>7 × 7 / 2</cell><cell>24 × 24 × 64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.7M</cell></row><row><cell>Max pool -1</cell><cell>3 × 3 / 2</cell><cell>12 × 12 × 64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.7M</cell></row><row><cell>Convolution -2</cell><cell>3 × 3 / 1</cell><cell>12 × 12 × 192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4M</cell></row><row><cell>Max Pool -2</cell><cell>3 × 3 / 2</cell><cell>6 × 6 × 192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4M</cell></row><row><cell>Inception -3a</cell><cell></cell><cell></cell><cell>64</cell><cell>128</cell><cell>96</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>2.6M</cell></row><row><cell>Inception -3b</cell><cell></cell><cell></cell><cell>128</cell><cell>192</cell><cell>128</cell><cell>96</cell><cell>32</cell><cell>64</cell><cell>4.5M</cell></row><row><cell>Max Pool -4</cell><cell>3 × 3 / 2</cell><cell>3 × 3 × 480</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6M</cell></row><row><cell>Inception -4a</cell><cell></cell><cell></cell><cell>192</cell><cell>208</cell><cell>96</cell><cell>48</cell><cell>16</cell><cell>64</cell><cell>1.3M</cell></row><row><cell>Avg Pooling -6</cell><cell></cell><cell>1 × 1 × 1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25.6K</cell></row><row><cell>Fully Connected -7</cell><cell></cell><cell>1 × 1 × 4096</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2M</cell></row><row><cell>Fully Connected -8</cell><cell></cell><cell>1 × 1 × 1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Number of images per each expression in databases</figDesc><table><row><cell></cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>NE</cell><cell>SA</cell><cell>SU</cell></row><row><cell>MultiPie</cell><cell>0</cell><cell>22696</cell><cell>0</cell><cell>47338</cell><cell>114305</cell><cell>0</cell><cell>19817</cell></row><row><cell>MMI</cell><cell>1959</cell><cell>1517</cell><cell>1313</cell><cell>2785</cell><cell>0</cell><cell>2169</cell><cell>1746</cell></row><row><cell>CK+</cell><cell>45</cell><cell>59</cell><cell>25</cell><cell>69</cell><cell>0</cell><cell>28</cell><cell>83</cell></row><row><cell>DISFA</cell><cell>436</cell><cell>5326</cell><cell>4073</cell><cell>28404</cell><cell>48582</cell><cell>1024</cell><cell>1365</cell></row><row><cell>FERA</cell><cell>1681</cell><cell>0</cell><cell>1467</cell><cell>1882</cell><cell>0</cell><cell>2115</cell><cell>0</cell></row><row><cell>SFEW</cell><cell>104</cell><cell>81</cell><cell>90</cell><cell>112</cell><cell>98</cell><cell>92</cell><cell>86</cell></row><row><cell>FER2013</cell><cell>4953</cell><cell>547</cell><cell>5121</cell><cell>8989</cell><cell>6198</cell><cell>6077</cell><cell>4002</cell></row></table><note>* AN, DI, FE, HA, Ne, SA, SU stand for Anger, Disgust, Fear, Happiness, Neutral, Sadness, Surprised respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Average Accuracy (%) for subject-independent</figDesc><table><row><cell></cell><cell>Top-1</cell><cell>Top-2</cell><cell>State-of-the-arts</cell></row><row><cell>MultiPIE</cell><cell>94.7±0.8</cell><cell>98.7±0.3</cell><cell>70.6 [21], 90.6 [13]</cell></row><row><cell>MMI</cell><cell>77.6±2.9</cell><cell>86.8±6.2</cell><cell>63.4 [25], 74.7 [24], 79.8 [30], 86.9 [37]</cell></row><row><cell>DISFA</cell><cell>55.0±6.8</cell><cell>69.8±8.6</cell><cell>-</cell></row><row><cell>FERA</cell><cell>76.7±3.6</cell><cell>90.5±4.6</cell><cell>56.1 [25], 75.0 [2], 55.6 [47]</cell></row><row><cell>SFEW</cell><cell>47.7±1.7</cell><cell>62.1±1.2</cell><cell>26.1 [24], 24.7 [13]</cell></row><row><cell>CK+</cell><cell>93.2±1.4</cell><cell>97.8±1.3</cell><cell>84.1 [30], 84.4 [21], 88.5 [42], 92.0 [24] 92.4 [25], 93.6 [49]</cell></row><row><cell>FER2013</cell><cell>66.4±0.6</cell><cell>81.7±0.3</cell><cell>69.3[44]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average (%) confusion matrix for subject-independent</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>predicted</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>NE</cell><cell>SA</cell><cell>SU</cell></row><row><cell></cell><cell>AN</cell><cell>55.0</cell><cell>7.0</cell><cell>12.8</cell><cell>3.5</cell><cell>7.6</cell><cell>8.5</cell><cell>5.3</cell></row><row><cell></cell><cell>DI</cell><cell>1.0</cell><cell>80.3</cell><cell>1.8</cell><cell>5.8</cell><cell>8.5</cell><cell>2.2</cell><cell>0.1</cell></row><row><cell>Actual</cell><cell>FE HA</cell><cell>7.4 0.7</cell><cell>4.3 3.2</cell><cell>47.0 2.4</cell><cell>8.1 86.6</cell><cell>18.7 5.5</cell><cell>8.6 0.2</cell><cell>5.5 1.0</cell></row><row><cell></cell><cell>NE</cell><cell>2.3</cell><cell>6.3</cell><cell>7.8</cell><cell>5.5</cell><cell>75.0</cell><cell>1.3</cell><cell>1.4</cell></row><row><cell></cell><cell>SA</cell><cell>6.0</cell><cell>11.3</cell><cell>8.9</cell><cell>2.7</cell><cell>13.7</cell><cell>56.1</cell><cell>0.9</cell></row><row><cell></cell><cell>SU</cell><cell>0.8</cell><cell>0.1</cell><cell>2.8</cell><cell>3.5</cell><cell>2.5</cell><cell>0.6</cell><cell>89.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Average Accuracy (%) on cross database</figDesc><table><row><cell></cell><cell>top-1</cell><cell>top-2</cell><cell>[30]</cell><cell>[37]</cell><cell>[31]</cell><cell>[49]</cell></row><row><cell>MultiPIE</cell><cell>45.7</cell><cell>63.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMI</cell><cell>55.6</cell><cell>68.3</cell><cell>51.4</cell><cell>50.8</cell><cell>36.8</cell><cell>66.9</cell></row><row><cell>DISFA</cell><cell>37.7</cell><cell>53.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FERA</cell><cell>39.4</cell><cell>58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SFEW</cell><cell>39.8</cell><cell>55.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CK+</cell><cell>64.2</cell><cell>83.1</cell><cell>47.1</cell><cell>-</cell><cell>56.0</cell><cell>61.2</cell></row><row><cell>FER2013</cell><cell>34.0</cell><cell>51.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Subject-independent comparison with AlexNet results (% accuracy)</figDesc><table><row><cell></cell><cell>Proposed Architecture</cell><cell>AlexNet</cell></row><row><cell>MultiPie</cell><cell>94.7</cell><cell>94.8</cell></row><row><cell>MMI</cell><cell>77.9</cell><cell>56.0</cell></row><row><cell>DISFA</cell><cell>55.0</cell><cell>56.1</cell></row><row><cell>FERA</cell><cell>76.7</cell><cell>77.4</cell></row><row><cell>SFEW</cell><cell>47.7</cell><cell>48.6</cell></row><row><cell>CK+</cell><cell>93.2</cell><cell>92.2</cell></row><row><cell>FER2013</cell><cell>66.4</cell><cell>61.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> *   <p>This work is partially supported by the NSF grants IIS-1111568 and CNS-1427872.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Challenges in representation learning: Facial expression recognition challenge</title>
		<ptr target="http://www.kaggle.com/c/challenges-in" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>-representation-learning-facial-expression-recognitionchallenge</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Facial expression recognition and analysis challenge</title>
		<ptr target="http://sspnet.eu/fera2011/.7,8" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.6343</idno>
		<title level="m">Provable bounds for learning some deep representations</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition in children with autism spectrum disorders: Relations to eye gaze and autonomic state</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Hecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Porges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Autism and Developmental Disorders</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introducing the geneva multimodal emotion portrayal (gemep) corpus. Blueprint for affective computing: A sourcebook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Observer-based measurement of facial expression with the facial action coding system. The handbook of emotion elicitation and assessment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="203" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual analysis of humans</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="377" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
				<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multi-view and view-invariant facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Emfacs-7: Emotional facial action coding system</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California at San Francisco</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
				<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognition of facial expression from optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kenji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3474" to="3483" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial interaction between animated 3d face robot and human beings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
				<imprint>
			<date type="published" when="1997">1997. 1997. 1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3732" to="3737" />
		</imprint>
	</monogr>
	<note>Systems, Man, and Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2012. 1, 3, 4, 8</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intra-class variation reduction using training expression images for sparse representation based facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition. Image processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2014</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
				<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coding facial expressions with gabor wavelets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Third IEEE International Conference on</title>
				<meeting>Third IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="200" to="205" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database. Affective Computing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cross-database evaluation for facial expression recognition. Pattern recognition and image analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Radig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-domain facial expression recognition using supervised kernel mean matching</title>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications (ICMLA), 2012 11th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="326" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pca-based dictionary building for accurate facial expression recognition via sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1082" to="1092" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Expressionbot: An emotive lifelike robotic face for face-to-face communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Graitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Borts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Conyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Humanoid Robots (Humanoids), 2014 14th IEEE-RAS International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1098" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional warping of active appearance model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="875" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Webbased database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepid3: Face recognition with very deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/1502.00873</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The toronto face database</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>UTML TR 2010- 001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Structure-preserving sparse decomposition for facial expression analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3590</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<title level="m">Deep learning using linear support vector machines</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motion history for facial action detection in video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man and Cybernetics</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="635" to="640" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The first facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on</title>
				<imprint>
			<date type="published" when="2008">2011. 2, 6, 7, 8</date>
			<biblScope unit="page" from="921" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Facial expression recognition using {l} {p}-norm mkl multiclass-svm. Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ebear: An expressive bear-like robot</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kargar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RO-MAN: The 23rd IEEE International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="969" to="974" />
		</imprint>
	</monogr>
	<note>Robot and Human Interactive Communication</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local phase quantization and sparse representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zilu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Computation (ICNC)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
