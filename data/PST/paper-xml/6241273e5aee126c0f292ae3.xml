<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5)</title>
				<funder ref="#_U4SzmUq">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_jPXYuh3 #_zguEYP3">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
							<email>shuchang.syt.liu@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
							<email>zuohui.fu@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
							<email>yingqiang.ge@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
							<email>yongfeng.zhang@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ, US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender Systems</term>
					<term>Natural Language Processing</term>
					<term>Multitask Learning</term>
					<term>Personalized Prompt</term>
					<term>Language Modeling</term>
					<term>Unified Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For a long period, different recommendation tasks typically require designing task-specific architectures and training objectives. As a result, it is hard to transfer the learned knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches, e.g., a sequential recommendation model can hardly be applied or transferred to a review generation method. To deal with such issues, considering that language grounding is a powerful medium to describe and represent various problems or tasks, we present a flexible and unified text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict Paradigm" (P5) for recommendation, which unifies various recommendation tasks in a shared framework. In P5, all data such as user-item interactions, item metadata, and user reviews are converted to a common format -natural language sequences. The rich information from natural language assist P5 to capture deeper semantics for recommendation. P5 learns different tasks with the same language modeling objective during pretraining. Thus, it possesses the potential to serve as the foundation model for downstream recommendation tasks, allows easy integration with other modalities, and enables instruction-based recommendation, which will revolutionize the technical form of recommender system towards unified recommendation engine. With adaptive personalized prompt for different users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity for extensive fine-tuning. On several recommendation benchmarks, we conduct experiments to show the effectiveness of our generative approach. We will release our prompts and pretrained P5 language model to help advance future research on Recommendation as Language Processing (RLP) and Personalized Foundation Models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>For the past decades, recommender systems have witnessed significant advancements and played an essential role in people's daily lives, helping their micro decisions and fulfilling their demands with outstanding accuracy. In retrospect, we can summarize the development trend of modern recommender systems -towards a more comprehensive system that accommodates diverse features and a wide spectrum of application scenarios.</p><p>On the one hand, feature engineering and learning in recommender systems has evolved greatly from simple to complex. In early ages, recommender systems typically adopt logistic regression or collaborative filtering <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49]</ref> which only utilizes useritem interaction records to model the behavioral patterns of users.</p><p>Later on, contextual features such as user profile and item metadata are further integrated into the system through more sophisticated models such as factorization machines <ref type="bibr" target="#b44">[44]</ref> and GBDT <ref type="bibr" target="#b19">[19]</ref>. Recently, deep neural network models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b70">70]</ref> facilitate crossing and combination among even more diverse and sophisticated features. As a result, these models gain better representation ability compared with traditional feature engineering based approaches.</p><p>On the other hand, more real-world tasks related to recommender systems have emerged. Besides classical rating prediction and direct user-item matching-based recommendation for users, recent works are broadening the spectrum to new application scenarios such as sequential recommendation <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b76">76]</ref>, conversational recommendation <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b72">72]</ref>, explainable recommendation <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b73">73]</ref> and so on. While the approaches to the aforementioned recommendation tasks are often proposed separately, there is an evident trend of utilizing multiple recommendation tasks to jointly learn transferable representations <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b68">68]</ref>. Although existing recommender systems achieved great success, there is still a considerable gap between current solutions and the foreseeable intersection of the aforementioned trends -a comprehensive recommender system that can accommodate diverse features and different types of tasks. Since recommendation tasks usually share a common useritem pool and have overlapping contextual features, we believe it is promising to merge even more recommendation tasks into a unified framework so that they can implicitly transfer knowledge to benefit each other and enable generalization to other unseen tasks.</p><p>Inspired by the recent progress in multitask prompt-based training <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b63">63]</ref>, in this work, we propose a unified "Pretrain, Personalized Prompt &amp; Predict Paradigm" (denoted as P5). We show that P5 is possible to learn multiple recommendation related tasks together through a unified sequence-to-sequence framework by formulating these problems as prompt-based natural language tasks, where user-item information and corresponding features are integrated with personalized prompt templates as model inputs. P5 sheds light on a promising technical route for unified and instruction-based recommendation. It has three main advantages: 1) P5 deeply immerses recommendation models into a full language environment, where all recommendation tasks are reformulated to NLP tasks with the help of personalized prompts. Since language grounding is sufficiently flexible and powerful to express various kinds of features in text templates, so there is no need to design feature-specific encoders. As a result, P5 can exploit the abundant semantics and knowledge inside the training corpora;</p><p>2) P5 integrates multiple recommendation tasks into a shared textto-text encoder-decoder architecture and trains them with the same language modeling loss rather than designing task-specific architectures and objective functions. In other words, P5 treats all personalized tasks as a conditional text generation problem;   We trained P5 on a multitask collection of personalized prompts. After multitask prompt-based pretraining on recommendation datasets, P5 achieves the capability of zero-shot generalization to unseen personalized prompts and new items.</p><p>3) Trained with instruction-based prompts, P5 attains sufficient zero-shot performance when generalizing to novel personalized prompts or unseen items in other domains.</p><p>In our experiments, we study how P5 performs compared with task-specific approaches on all five task families as well as evaluating P5's zero-shot generalization ability. We also conduct several ablation studies to justify the design details of P5 framework. Overall, our main contributions can be outlined as follows:</p><p>? To the best of our knowledge, we are the first to propose a unified "Pretrain, Personalized Prompt &amp; Predict Paradigm" to integrate various recommendation related tasks into a shared conditional language generation framework. ? We create a collection of personalized prompts that cover five different recommendation task families. ? According to the experimental results, P5 achieves promising performances on the five task families when taking seen prompt templates as model inputs. ? P5 demonstrates sufficient zero-shot generalization ability for novel personalized prompts and new items in unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Unified Frameworks. Many prior works have pursued to solve various tasks in a unified model. As early pioneers, T5 <ref type="bibr" target="#b43">[43]</ref> and GPT-3 <ref type="bibr" target="#b2">[2]</ref> unifies NLP downstream tasks through text-to-text encoderdecoder framework and autoregressive language modeling, respectively. They both allow effective knowledge sharing among different tasks based on a common pretrained language model. Following this trend, recent advances started to focus on unifying large-scale language tasks <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b63">63]</ref> or cross-modality applications <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b67">67]</ref> through a shared sequence-to-sequence framework, where different types of tasks and modalities are all expressed in the format of natural language. However, aforementioned methods never consider personalization in their sequence-to-sequence models. Recently, a line of work <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b68">68]</ref> attempt to learn universal user representations which are easily transferrable to downstream tasks. One limitation of these methods is that they still require additional finetuning on downstream datasets. In contrast, our P5 first takes personalization into an encoder-decoder Transformer model that can generalize to a wide spectrum of recommendation related application scenariostasks that naturally require personalization. Moreover, with the help of prompt-based pretraining, P5 acquires zero-shot generalization ability when transferring to unseen prompts and items. Prompt Learning. The success of GPT series especially GPT-3 <ref type="bibr" target="#b2">[2]</ref> marked the beginning of prompt's popularization on NLP tasks. Trained with huge language data from the Web, GPT-3 exhibited the capability of solving NLP tasks when provided a number of inputoutput examples as exemplar prompts. Besides exemplar prompts, many prompt design methods have proliferated following the "pretrain, prompt, and predict" paradigm <ref type="bibr" target="#b34">[34]</ref>. One type of the methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b54">54]</ref>    prompt types, instruction-based prompts contain detailed task descriptions and adhere more to the natural language format. Since instruction-based prompts are flexible and close to how humans communicate with each other, several pioneer works <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b64">64]</ref> claim that learning from crowd-sourced NLP datasets is a promising route for general purpose NLP systems. Recent works such as FLAN <ref type="bibr" target="#b63">[63]</ref> and T0 <ref type="bibr" target="#b48">[48]</ref> finetuned pretrained language models on large-scale NLP datasets verbalized via human-readable prompts. As a result, such multitask prompt-based tuning brings powerful models that exhibit strong zero-shot ability on unseen tasks. Inspired by the success of these approaches, we create a collection of personalized prompts and then train a sequence-to-sequence model on a variety of recommendation related tasks verbalized according to the constructed personalized prompts. NLP for Recommendation. Recommendation has been interacting with NLP techniques for a long time. The main work mostly address four lines of research: 1) explainable recommendation <ref type="bibr">[11, 16, 28-30, 71, 73]</ref> where NLP models help generating text explanations for a given recommendation; 2) sequential recommendation as language modeling <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b76">76]</ref> which considers user interaction histories as word token sequences; 3) text feature extraction <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b75">75]</ref> which aims to extract informative text encodings that can improve the performance of recommendation; and 4) conversational recommendation <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b72">72]</ref> that reasons the intent of users and gives recommendation in an interactive dialog format. In our work, we explicitly covers the tasks of sequential recommendation and explanation generation, and additionally offers insights on how to formulate a unified NLP framework for other recommendation problems including rating prediction, top-k recommendation, and review summarization. Furthermore, pretrained with instructionbased prompts that share similarity with conversational recommendation, our P5 benefits from the natural language environment and improves the performance on a series of recommendation tasks.</p><p>Zero-shot and Cold Start Recommendation. Recommender systems' performances heavily rely on the available training data, but there are always zero-shot cases where the history records are limited. The evidences of performing well on such startup cases signal a good generalization ability of recommendation models.</p><p>One widely studied problem under this setting is the cold-start recommendation where users <ref type="bibr" target="#b24">[24]</ref> or items <ref type="bibr" target="#b50">[50]</ref> are new to the system with no previous interaction records. Solutions to this problem either learn to model content features <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">40]</ref> so that inference can be made without interaction records or learn to transfer representations from auxiliary domains <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b78">78]</ref>. Another line of work for zero-shot recommendation discusses the quick adaptation to the new domain instead of providing recommendation for cold-start case only. Solutions typically follow the meta learning framework <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b60">60]</ref> that is often explored in other tasks. In our  For the example prompt input "What star rating do you think briana will give item_7391?", P5 adopts an encoder-decoder framework: first encodes the input with a bidirectional text encoder, and then generates the answer through a text decoder autoregressively. In contrast to task-specific recommendation models, our P5 relies on multitask prompt-based pretraining on a large-scale personalized prompt collection, which makes P5 able to adapt to different task families and even generalize to novel personalized prompts.</p><p>work, we ask P5 model pretrained on an auxiliary domain to solve tasks on target domains, where the users are known to P5 but the items have never been seen by the model before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PERSONALIZED PROMPT COLLECTION</head><p>To facilitate the multitask prompt-based pretraining for recommendation, we create a collection of personalized prompt templates. The collection covers five different task families -rating, sequential recommendation, explanation, review, and direct recommendation. Each of these task families contains multiple personalized prompts to help P5 discover various aspects about users and items. As mentioned in <ref type="bibr" target="#b48">[48]</ref>, a prompt is considered as consisting of an input template and a target template, along with a collection of associated metadata. In this work, we further define a personalized prompt as a prompt that includes personalized fields for different users and items. For example, a user's preference can be indicated through either an ID number or a description of the user such as name, gender, age, etc. Moreover, the expected model output of a given personalized prompt should also vary according to its item field. This implies the change of user's preferences towards different items. Such item fields can be represented by either item ID numbers or item metadata that contains detailed descriptions.</p><p>We carefully designed the P5 personalized prompt collection for each task family. For rating prediction task family, we divide the prompts into three categories: 1) Given the information about a user and an item, directly predict the rating score ranging from 1 to 5; 2) Predict whether a user will rate an item a given score. The expected output is yes or no; 3) Predict if a user likes or dislikes an item. Here we consider a star rating equal to or greater than 4 to be a like preference of the user, whereas lower scores indicate a dislike preference. For sequential recommendation task family, we create three types of prompts: 1) Directly predict the next item based on user interaction history. 2) Given user interaction history, choose the possible next item from a candidate list, where only one item is positive. 3) Based on user interaction history, predict whether a given item will be interacted next by the user. For explanation task family, we ask P5 model to generate a textual explanation to justify a user's preference towards a given item. There are two prompt categories in this task family: 1) Directly generate an explanation sentence with user/item information; 2) Generate explanation based on a feature word as hint. For each category, there could be other auxiliary information included such as the review headline and the star rating. For review related task family, we create two types of prompts: 1) Summarize review comment to a shorter review title; 2) Predict the corresponding rating score based on the given review comment. For direct recommendation, we divide all prompts into two groups: 1) Predict whether to recommend an item to a user, the answer should be yes or no. 2) Select the most suitable item to recommend from a list of candidate items. We provide the complete collection of personalized prompts in Appendix A.</p><p>With the designed prompts, it is easy to build input-target pairs from raw data. As illustrated in Figure <ref type="figure" target="#fig_3">2</ref>, we can simply substitute the fields in braces with the corresponding information in the raw data and thus create training input-target pairs or zero-shot testing personalized prompts. It is worth noting that we divide the raw data into three groups -rating/review/explanation shares the same raw data, while sequential and direct recommendation differ in terms of whether to use interaction history as input information. During pretraining, we mix input-target pairs from different task families together to serve as the training data. To enhance P5's robustness and zero-shot generalization, for each raw datum, we only sample a portion of rather than all of the personalized prompts in each task family. In sequential and direct recommendation task families, we also randomly select a group of negative items for those prompts that require a candidate list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE P5 PARADIGM AND MODEL 4.1 The P5 Architecture</head><p>The collection of personalized prompts introduced in the previous section makes it convenient to create a large amount of available pretraining data that covers a wide range of recommendation related tasks. Thanks to the prompt templates, all pretraining data shares a unified format of input-target token sequences, which breaks the boundaries among different tasks. We claim that pretraining multiple recommendation tasks under a unified framework of conditional generation can facilitate all involving tasks together. By immersing P5 in the full language environment throughout the pretraining stage, we also expect its zero-shot generalization capability of understanding unseen personalized prompts with detailed item descriptions. That is the reason why P5 is called a unified "Pretrain, Personalized Prompt, and Predict Paradigm".</p><p>In terms of the model architecture, our P5 is established upon a basic encoder-decoder framework. We employ Transformer <ref type="bibr" target="#b61">[61]</ref> blocks to build both the encoder and decoder. Suppose the embeddings of an input token sequence is</p><formula xml:id="formula_0">x = [? 1 , ? ? ? , ? ? ].</formula><p>As depicted in Figure <ref type="figure" target="#fig_4">3</ref>, before feeding the embedding sequence into the bidirectional text encoder E (?), we add positional encodings P to the raw embeddings to capture their position information in the sequence. Furthermore, to make P5 aware of the personalized information contained in the input sequence, we also apply whole-word embeddings W to indicate whether consecutive sub-word tokens are from the same original word. For instance, if we directly represent the item with ID number 7391 as "item_7391", then the word will be split into 4 separate tokens (i.e., "item", "_", "73", "91") by Sentence-Piece tokenizer <ref type="bibr" target="#b51">[51]</ref>. With the assistance of the shared whole-word embedding "?w10?" (e.g., in Figure <ref type="figure" target="#fig_4">3</ref>), P5 can better recognize the important field with personalized information. Another alternative is to represent each user/item by an extra independent token (e.g., "?item_7391?"). However, this may incur huge amounts of additional tokens when there is a large pool of users and items. Hence, in this paper, we adopt multiple sub-word units to represent a user or item.</p><p>Afterwards, the text encoder takes the sum of the aforementioned three embeddings e = [? 1 , ? ? ? , ? ? ] and outputs their con-</p><formula xml:id="formula_1">textualized representations t = [? 1 , ? ? ? , ? ? ] = E (e).</formula><p>The decoder D (?) then attends to both the previously generated tokens y &lt; ? and the encoder output t and predicts the probability distribution of future tokens: ? ? y ? | y &lt; ? , x = D (y &lt; ? , t). During the pretraining stage, P5 learns the model parameters ? by minimizing the negative log-likelihood of label tokens y conditioned on input text x in an end-to-end manner:</p><formula xml:id="formula_2">L P5 ? = - |y | ?? ?=1 log ? ? y ? | y &lt; ? , x<label>(1)</label></formula><p>This same objective function is shared by all recommendation tasks under P5. As a result, we unify recommendation tasks with one model, one loss, and one data format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recommendation with Pretrained P5</head><p>After pretraining, P5 can directly perform different tasks with either seen or unseen personalized prompts. For rating, explanation, and review tasks, we simply use greedy decoding to generate answers. In contrast, sequential and direct recommendation tasks usually require an item list as target output. In view of this, for sequential recommendation, we apply beam search to generate a list of potential next items and evaluate it under the all items setting. For direct recommendation, we predict the recommended items from a candidate set S = {? 1 , ? ? ? , ? ? }, where only one of the ? candidates is positive. Here, we also use beam search to decode a list of potential target items with the highest scores and then conduct evaluations. Both of the above decoding processes can be written as</p><formula xml:id="formula_3">C = [? 1 , ? ? ? , ? ? ] = Beam_Search(D, t, ?),<label>(2)</label></formula><p>where ? denotes the beam size and C is the output item list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed P5 approach on real-world data and compare it with various representative methods targeting at different task families. Through the performance comparison and ablation studies, we aim to answer the following research questions regarding our unified "Pretrain, Personalized Prompt, and Predict Pargadigm" (P5):</p><p>? RQ1: How does our unified P5 framework perform compared with task-specific methods on all five task families? ? RQ2: Does P5 have enough zero-shot generalization ability when transferring to unseen personalized prompts for either existing or new items? ? RQ3: How do scaling factors such as model size, number of task families, and number of prompts affect the performance of P5?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Datasets. We conduct experiments over three real-world datasets collected from Amazon.com platform with varying domains and sparsity levels. The Amazon datasets provide user ratings and reviews on products from 29 categories. In this paper, we adopt three of them to evaluate our method, namely Sports &amp; Outdoors, Beauty, as well as Toys &amp; Games. The detailed statistics of these datasets are presented in Table <ref type="table" target="#tab_5">1</ref>. Task splits. For rating, explanation, and review task families, we randomly split each dataset into training (80%), validation (10%) and testing (10%) sets, and ensure that there is at least one instance included in the training set for each user and item. Specifically, we extract feature words and explanations of rating scores from review splits with the help of Sentires toolkit <ref type="bibr" target="#b74">[74]</ref> and create explanation splits that are in fact a subset of original review splits. In terms of sequential recommendation task family, for each user interaction sequence, the last item is used as the test data, the item before the last one is used as the validation data, and the remaining data is used for training. To avoid data leakage during pretraining, we follow the training split of sequential recommendation to build positive/negative items for direct recommendation task family. Implementation Details. Our P5 model utilizes the pretrained T5 checkpoints <ref type="bibr" target="#b43">[43]</ref> as backbone. According to the size of T5 backbone, we create two versions of P5, namely P5-small (P5-S) and P5-base (P5-B). For P5-small, there are 6 layers for both encoder and decoder, the model dimensionality is 512 with 8-headed attention. For P5base, encoder and decoder both have 12 Transformer blocks. The model has an embedding dimensionality of 768 and a 12-headed attention. For tokenization, we directly use the SentencePiece <ref type="bibr" target="#b51">[51]</ref> tokenizer with a vocabulary size of 32,128 for parsing sub-word units. We pretrain P5 for 10 epochs with AdamW optimization <ref type="bibr" target="#b36">[36]</ref> on four NVIDIA RTX A5000 GPUs. The batch size is set to 16 for P5-base and 32 for P5-small. We choose 1 ? 10 -3 as the peak learning rate and set the maximum length of input tokens to 512. The warmup strategy is used to adjust the learning rate during training, the warmup stage is set to be the first 5% of all iterations.</p><p>Our default pretrain-predict combination adopts the last prompt in each task family for zero-shot evaluation while all remaining prompts are utilized for multitask prompted pretraining. For rating prediction, we use Gaussian sampling to convert original integer scores to float numbers round to 1 decimal place. In this way, we can avoid overfitting the limited score types. After this change, we increase the number of score classes from 5 to 41. For sequential recommendation, we set the beam size ? to 20. For direct recommendation, the beam size is also 20 and the candidate pool contains 100 items, which consist of one ground truth item and 99 sampled negative ones that the user has not interacted with. Metrics. For rating prediction, we adopt two standard metrics -Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). For sequential recommendation and direct recommendation task families, we employ top-k Hit Ratio (HR@k) and Normalized Discounted Cumulative Gain (NDCG@k) to evaluate the performance and report HR@1, 5, 10 and NGCG@5, 10. For explanation generation and review summarization, we evaluate different methods with BLEU-4, as well as ROUGE-1, ROUGE-2, and ROUGE-L. Except for RMSE and MAE, all other metrics are "the higher, the better". For all tables, bold numbers refer to the best performance, while underlined numbers indicate the second best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Baselines</head><p>To demonstrate P5's competence on a wide range of recommendation related tasks, we gather a collection of representative approaches for difference task families. Rating Prediction and Direct Recommendation. These tasks take the user-item rating/interaction data, but no content or side information is provided. We aim to justify whether the models are able to provide accurate rating prediction or recommendation lists that align with the user preferences. We use MF <ref type="bibr" target="#b23">[23]</ref> as rating prediction and BPR-MF <ref type="bibr" target="#b45">[45]</ref> as direct recommendation baselines, which are among the most popular and representative collaborative filtering methods that learn latent representations of the users and items. Though simple, they have been shown to be more effective than many existing neural models on rating prediction and direct recommendation tasks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b46">46]</ref>. Sequential Recommendation. We adopt several representative sequential recommendation approaches as our baselines. Caser <ref type="bibr" target="#b59">[59]</ref> treats sequential recommendation as a Markov Chain and employs convolutional neural networks to model user interests. HGN <ref type="bibr" target="#b38">[38]</ref> adopts a hierarchical gating networks to learn user behaviors from the perspectives of both long and short terms. GRU4Rec <ref type="bibr" target="#b20">[20]</ref> is originally proposed for session-based recommendation. It utilizes GRU <ref type="bibr" target="#b6">[6]</ref> to model the user click history sequence. BERT4Rec <ref type="bibr" target="#b56">[56]</ref> mimics the BERT-style masked language modeling and learns a bidirectional representation for sequential recommendation. FDSA <ref type="bibr" target="#b69">[69]</ref> focuses on the feature transition patterns by modeling feature sequence with a self-attention module. Explanation Generation. For performance comparison, we consider several baselines with regard to the task of explanation generation. Attn2Seq <ref type="bibr" target="#b11">[11]</ref> learns to encode attributes into vectors, and then invokes an attention mechanism to generate reviews conditioned on the attribute vector. NRT <ref type="bibr" target="#b30">[30]</ref> utilizes GRU <ref type="bibr" target="#b6">[6]</ref> to generate explanations based on user and item IDs. PETER <ref type="bibr" target="#b29">[29]</ref> is a simple and effective framework that attempts to utilize user and item IDs to generate explanations. It is built upon a modified attention mask of the Transformer architecture. There is also a variant PETER+, which takes a hint feature word to assist the explanation generation. Review Related. For review summarization, we adopt pretrained T0 <ref type="bibr" target="#b48">[48]</ref> and GPT-2 <ref type="bibr" target="#b42">[42]</ref> checkpoints hosted by Hugging Face 1 as baselines. For review preference prediction, we only use T0 to make comparisons because GPT-2 fails to perform this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison on Different</head><p>Task Families (RQ1)</p><p>In this section, we pretrain P5 with prompts from all five task families to verify its multitask learning ability. According to the default pretrain-predict task combination, we leave Prompt 1-10, Prompt 2-12, Prompt 3-13, Prompt 4-4, and Prompt 5-8 for zeroshot evaluation and pretrain P5 with the remaining personalized prompts. The performances of P5 and relevant baselines on the five task families are presented in Table <ref type="table" target="#tab_6">2</ref> to Table <ref type="table">7</ref>. For each task family, we choose one or more seen prompts as supplement to the aforementioned zero-shot unseen prompts to perform evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.1</head><p>Rating Prediction. Prompt 1-6 and Prompt 1-10 are used for evaluating P5's performance on rating prediction. The performance comparison is presented in Table <ref type="table" target="#tab_6">2</ref>. From the table, we can see that when testing with seen Prompt 1-6, P5-B gets better MAE and slightly higher RMSE on all three datasets compared with MF. When testing with unseen Prompt 1-10, P5-B can achieve similar performance as Prompt 1-6. Moreover, P5-S always has better MAE but higher RMSE. It seems that P5 is overfitting these data since the task complexity of rating prediction is relatively smaller than other recommendation tasks. Overall, these results show that it is feasible to perform rating prediction on a conditional text generation framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Sequential Recommendation. As illustrated in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Beauty Toys</head><p>HR@5 NDCG@5 HR@10 NDCG@10 HR@5 NDCG@5 HR@10 NDCG@10 HR@5 NDCG@5 HR@10 NDCG@10 2-13) prompts. On Beauty and Toys, P5-S can get even better performances than P5-B. While on Sports, P5-B keeps the advantage over P5-S. The results show that the P5 architecture is effective in modeling the user interaction history and conducting next item prediction with the help of beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Explanation Generation.</head><p>In Table <ref type="table" target="#tab_9">4</ref>, Prompt 3-9 and Prompt 3-12 are used to evaluate P5's performance on explanation generation under feature-based setup, while Prompt 3-3 is used for direct explanation generation without providing a hint word. For the seen Prompt 3-3, PETER achieves better BLUE4 than P5-B on Sports and Toys. However, P5-B has the best ROUGE scores among all compared methods. For feature-based prompts, P5-B outperforms PETER+ by a large margin on all domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.4</head><p>Review Related. We take Prompt 4-2 and Prompt 4-4 to compare P5's performance with T0 on review preference prediction. The performance comparison is shown in Table <ref type="table" target="#tab_10">5</ref>, where we can see that P5-S achieves much better RMSE and MAE on Toys. While on Sports, P5-B shows better zero-shot generalization ability with Prompt 4-4. Additionally, we take Prompt 4-1 to evaluate P5's ability on review summarization and present the results in Table <ref type="table" target="#tab_11">6</ref>. For this task, P5-B and P5-S clearly outperform T0 and GPT-2 except for BLUE4 score on Sports. It is worth noting that GPT-2 and T0 has 1.5B and 11B parameters, respectively. This shows that P5 is able to achieve better performance than these competitive baselines with a much smaller model size.   <ref type="table">7</ref>. From the table, we can see that P5-B and P5-S has great advantages over BPR on all three datasets. With unseen Prompt 5-8, P5-B can achieve the best performance on Sports, whereas P5-S surpasses other methods on Toys. The success of P5 on direct recommendation shows the competence of sequence-to-sequence generation framework in recommendation domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Zero-shot Generalization to Unseen</head><p>Prompts and Items in New Domain (RQ2)</p><p>5.4.1 Transfer to Unseen Personalized Prompts. In this section, we transfer the pretrained P5 models to the previously held-out prompts during pretraining. These unseen prompts are still from the same task families, and the testing items have been seen by P5 during pretraining at least once. The experimental results are also reported in Table <ref type="table" target="#tab_6">2</ref> to Table <ref type="table">7</ref>. As previously discussed in Section 5.3, P5 achieves surprisingly good performances on various </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Beauty Toys</head><p>Table <ref type="table">7</ref>: Performance comparison on direct recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Beauty Toys</head><p>HR@1 HR@5 NDCG@5 HR@10 NDCG@10 HR@1 HR@5 NDCG@5 HR@10 NDCG@10 HR@1 HR@5 NDCG@5 HR@10 NDCG@   task families when being challenged by unseen prompts. On some specific datasets, the performances of P5 on unseen prompts even surpass seen prompts. For example, on Sports, P5-B gets the best performances under Prompt 2-13, Prompt 4-4, and Prompt-5-8. These results show that multitask prompted pretraining empowers P5 enough robustness to understand unseen prompts with wording variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.2</head><p>Transfer to Items in New Domain. Next, we increase the difficulty level of zero-shot transfer. We collect a group of 741 users that exist in all the three domains with their interaction and review histories in other domains. The detailed statistics of these domain transfer evaluation sets are illustrated in Table <ref type="table" target="#tab_13">8</ref>.</p><p>We then challenge P5-B pretrained on one domain with unseen prompts from the Task Family Z, whose item fields are filled with the information from a new product domain. For example, we ask the P5 model pretrained on the Toys domain about an existing user's preference towards an item in the Beauty domain. The full results on all six directions are reported in Table <ref type="table" target="#tab_14">9</ref>. From the table, we notice P5 still maintains sufficient performances for rating prediction (Prompt Z-2 &amp; Z-3), like/dislike prediction (Prompt Z-1 &amp; Z-4), as well as explanation generation with feature word (Prompt Z-6). In contrast, direct explanation generation without feature word (Prompt Z-5 &amp; Z-7) is very difficult for P5 because it lacks awareness of relevant knowledge in the new domain. In Figure <ref type="figure">5</ref>, we provide some example explanations generated by P5-B under the setup of zero-shot domain transfer (Prompt Z-6). We can see that P5 is able to catch different users' rating preferences and hint feature words, then integrate them with the knowledge learned from previous domain to generate plausible explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation on Model Size (RQ3)</head><p>In this section, we will discuss the influence of model size on the performance of P5 on different recommendation tasks. Here, we train two size variants of P5, namely P5-small and P5-base. The parameter numbers of these two P5 models are 60.75M and 223.28M, respectively. From Table <ref type="table" target="#tab_6">2</ref> to Table <ref type="table">7</ref>, we can see that although P5-S is only 1/4 of the size of P5-B, P5-S can beats P5-B on a series of tasks and datasets. For example, P5-S achieves better sequential recommendation, review preference prediction, and direct recommendation performances than P5-B on Beauty and Toys. In contrast, P5-B shows advantages on these tasks for Sports, as well as on most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation on Task Scaling (RQ3)</head><p>Moreover, we explore whether multitask prompted pretraining is superior than pretraining on each task family alone. We pretrain P5-small on Beauty dataset with prompts from every single task family, resulting in five models -P5-S1, P5-S2, P5-S3, P5-S4, and P5-S5. We then compare P5-S on various recommendation tasks with corresponding single task P5 models. The performance comparison between P5-S and P5-SN (? ? <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5]</ref>) is illustrated in Figure <ref type="figure" target="#fig_6">4</ref>. As shown in the figure, P5-SN achieves slightly better performances than P5-S on sequential recommendation and review preference prediction, as well as clearly better results on generation tasks (i.e., 3-9, 3-12, 4-1). Typically, single models perform better than multitask models because they focus on one task family and thus avoid the interference from less relevant tasks. Even so, P5-S exhibits advantages on rating prediction and direct recommendation. This shows that multitask prompt-based pretraining is generally beneficial for P5 to gain enough capability on different recommendation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Ablation on Prompt Scaling (RQ3)</head><p>As mentioned in implementation details, our default pretrain-predict task combination follows the leave-one-out strategy. However, do we need so many prompts during pretraining to enable P5's zeroshot generalization ability? In this section, we explore to reduce the number of pretraining prompts and then make comparisons with the P5 model pretrained under default setup. To this end, we choose a collection of pretraining prompts that has the minimum number of prompts to cover all important personalized fields. Specifically, this combination contains the following 18 personalized prompts: {1-5, 1-6, 1-8, 1-9, 2-1, 2-3, 2-8, 2-11, 3-2, 3-3, 3-6, 3-9, 4-1, 4-2, 4-3, 5-2, 5-5, 5-7}. Similar to the default pretrain-predict combination, the last prompt in each task family is for zero-shot evaluation. We name this prompt scaling variant of P5-small as P5-PS and then pretrain P5-PS on Beauty dataset. The performance comparison between P5-S and P5-PS is also presented in Figure <ref type="figure" target="#fig_6">4</ref>. From the figure, we can observe that P5-S beats P5-PS on most tasks except for some generation tasks (i.e., 3-3, 3-9, 4-1). Interestingly, P5-S outperforms P5-PS on 3-12 -a zero-shot explanation generation task. In fact, P5-S also shows its superiority on other zero-shot tasks such as 1-10, 2-13, and 5-8. Overall, we can find that larger number of high quality personalized prompts can generally help P5 achieve better performances on various recommendation tasks especially zero-shot tasks with unseen prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we present P5 which unifies different recommendation tasks into a shared language modeling and natural language generation framework. By designing a collection of personalized prompts covering five recommendation task families, we transfer all raw data such as the user-item interactions, item metadata, and user reviews to the same format -input-target text pairs. We then pretrain P5 in a full language environment to help it discover deeper semantics for various recommendation tasks. According to our experiments, P5 can beat or achieve similar performance with several representative approaches on all five task families. Moreover, P5 shows the generalization ability on performing zero-shot transfer to new items, new domains, and new personalized prompts. Owing to the constraint of available computational resources, in this work, we only pretrained P5 with the small and base scale settings. In the future, we will continue exploring to enlarge the model size of P5 and employ more powerful base models such as GPT-3. It is also promising for extending P5 to diverse modalities and conversational recommendation. Besides, we plan to investigate prompt search and/or latent prompt techniques to achieve more and better instruction prompts or leverage retrieval-enhanced generation to further boost P5's performance on downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P5</head><label></label><figDesc>What star rating do you think briana will give item_7391? 5.0 I find the purchase history list of user_15466: 4110 -&gt; 4467 -&gt; 4468 -&gt; 4472 I wonder what is the next item to recommend to the user. Can you help me decide? 1581 Help Hong "Old boy" generate a 5-star explanation about this product: OtterBox Defender Case for iPhone 3G, 3GS (Black) [Retail Packaging] you can protect your prescious iphone more safe Give a short sentence describing the following product review from Mom of 3 yo girl: First it came with the packaging open and then as soon as my son took it out it was so easily broken. Hopefully a little glue will fix it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: P5 pretrains on an encoder-decoder Transformer model that takes in textual inputs and produces target responses.We trained P5 on a multitask collection of personalized prompts. After multitask prompt-based pretraining on recommendation datasets, P5 achieves the capability of zero-shot generalization to unseen personalized prompts and new items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Building input-target pairs from raw data according to our designed personalized prompt templates -simply substituting the fields in the prompts with the corresponding information in raw data. The raw data for the five task families of P5 are from three separate sources. Specifically, rating/review/explanation prompts (a) have shared raw data. Sequential recommendation (b) and direct recommendation (c) uses similar raw data, but the former particularly requires the user interaction history. The complete collection of P5 personalized prompts are provided in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: An illustration of the P5 architecture. For the example prompt input "What star rating do you think briana will give item_7391?", P5 adopts an encoder-decoder framework: first encodes the input with a bidirectional text encoder, and then generates the answer through a text decoder autoregressively. In contrast to task-specific recommendation models, our P5 relies on multitask prompt-based pretraining on a large-scale personalized prompt collection, which makes P5 able to adapt to different task families and even generalize to novel personalized prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5. 3 . 5</head><label>35</label><figDesc>Direct Recommendation. Finally, Prompt 5-5 and Prompt 5-8 are applied to evaluate the direct recommendation task under</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison among P5-S, P5-SN, and P5-PS on Beauty.</figDesc><graphic url="image-73.png" coords="8,318.78,369.29,119.43,79.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Toys -&gt; BeautyInput: 3 Input: 4 Input: 5 Input: 6 Figure 5 :</head><label>34565</label><figDesc>Figure 5: Example cases of zero-shot domain transfer on Z-6 task. We demonstrate three transfer directions: Toys to Beauty, Beauty to Sports, and Sports to Toys. generation tasks. Since Sports contains more users and items but has a lower sparsity, it requires a model with higher capacity to discover latent correlation among different personalized factors. All these findings indicate that larger P5 models may not bring better performances, and we should decide an appropriate model size that matches the scale of the training data.</figDesc><graphic url="image-75.png" coords="9,57.72,83.52,494.28,200.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Explanation raw data for Beauty user_id: 7641 item_id: 2051 user_name: stephanie item_title: SHANY</head><label></label><figDesc>Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration) review: Absolutely great product. I bought this for my fourteen year old niece for Christmas and of course I had to try it out, then I tried another one, and another one and another one. So much fun! I even contemplated keeping a few for myself!</figDesc><table><row><cell>Rating / Review /</cell></row><row><cell>explored prompt search for proper discrete</cell></row><row><cell>prompts. Meanwhile, another line of work [17, 26, 31, 35, 41, 77]</cell></row><row><cell>exploited continuous vector embeddings as prompts to improve</cell></row><row><cell>the performance on NLP tasks. Compared with the aforementioned</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>star_rating: 5 summary: Perfect! explanation: Absolutely great product feature_word: product Sequential Recommendation raw data for Beauty user_id: 7641 user_name: Victor</head><label></label><figDesc></figDesc><table><row><cell>purchase_history: 652 -&gt; 460 -&gt; 447 -&gt; 653 -&gt; 654 -&gt; 655 -&gt; 656 -&gt; 8</cell></row><row><cell>-&gt; 657</cell></row><row><cell>next_item: 552</cell></row><row><cell>candidate_items: 4885 , 4280 , 4886 , 1907 , 870 , 4281 , 4222 ,</cell></row><row><cell>4887 , 2892 , 4888 , 2879 , 3147 , 2195 , 3148 , 3179 , 1951 ,</cell></row><row><cell>?? , 1982 , 552 , 2754 , 2481 , 1916 , 2822 , 1325</cell></row><row><cell>Direct Recommendation raw data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>for Beauty user_id: 250 user_name: moriah rose target_item: 520</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Which star rating will user_{{user_id}} give item_{{item_id}}? (1 being lowest and 5 being highest)</cell><cell>{{star_rating}}</cell></row><row><cell></cell><cell>Based on the feature word {{feature_word}}, generate an</cell><cell></cell></row><row><cell></cell><cell>explanation for user_{{user_id}} about this product:</cell><cell>{{explanation}}</cell></row><row><cell></cell><cell>{{item_title}}</cell><cell></cell></row><row><cell></cell><cell>Give a short sentence describing the following product review from {{user_name}}: {{review}}</cell><cell>{{summary}}</cell></row><row><cell></cell><cell>Here is the purchase history of user_{{user_id}}:</cell><cell></cell></row><row><cell></cell><cell>{{purchase_history}}</cell><cell>{{next_item}}</cell></row><row><cell></cell><cell>What to recommend next for the user?</cell><cell></cell></row><row><cell>candidate_items: 4915 , 1823 , 3112 , 3821 , 3773 , 520 , 7384 , random_negative_item: 9711</cell><cell>Choose the best item from the candidates to recommend for {{user_name}}? \n {{candidate_items}}</cell><cell>{{target_item}}</cell></row><row><cell>7469 , 9318 , 3876 , 1143 , 789 , 595 , 3824 , 3587 , 10396 ,</cell><cell></cell><cell></cell></row><row><cell>?? , 2766 , 7498 , 2490 , 3232 , 9711 , 2975 , 1405 , 8051</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the experimental datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Sports Beauty</cell><cell>Toys</cell></row><row><cell>#Users</cell><cell>35,598</cell><cell>22,363</cell><cell>19,412</cell></row><row><cell>#Items</cell><cell>18,357</cell><cell>12,101</cell><cell>11,924</cell></row><row><cell>#Reviews</cell><cell cols="3">296,337 198,502 167,597</cell></row><row><cell>#Sparsity (%)</cell><cell>0.0453</cell><cell>0.0734</cell><cell>0.0724</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on rating prediction.</figDesc><table><row><cell>Methods</cell><cell>Sports RMSE MAE</cell><cell>Beauty RMSE MAE</cell><cell>Toys RMSE MAE</cell></row><row><cell cols="4">MF P5-S (1-6) P5-B (1-6) 1.0357 0.6813 1.2843 0.8534 1.0866 0.6957 1.0234 0.7935 1.1973 0.9461 1.0123 0.7984 1.0594 0.6639 1.3114 0.8434 1.0605 0.7142</cell></row><row><cell cols="4">P5-S (1-10) 1.0522 0.6698 1.3001 0.8444 1.0805 0.7057</cell></row><row><cell cols="4">P5-B (1-10) 1.0292 0.6864 1.2862 0.8530 1.0843 0.7007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 ,</head><label>3</label><figDesc>Prompt 2-3 and Prompt 2-13 are employed for the evaluation of sequential recommendation under all-item setting. From the table,</figDesc><table /><note><p>we can see that P5-B surpasses all competitive baselines with a relatively large gap on both seen (Prompt 2-3) and unseen (Prompt 1 https://huggingface.co/</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on sequential recommendation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on explanation generation.</figDesc><table><row><cell>Caser</cell><cell>0.0116</cell><cell>0.0072</cell><cell>0.0194</cell><cell>0.0097</cell><cell>0.0205</cell><cell>0.0131</cell><cell>0.0347</cell><cell>0.0176</cell><cell>0.0166</cell><cell>0.0107</cell><cell>0.0270</cell><cell>0.0141</cell></row><row><cell>HGN</cell><cell>0.0189</cell><cell>0.0120</cell><cell>0.0313</cell><cell>0.0159</cell><cell>0.0325</cell><cell>0.0206</cell><cell>0.0512</cell><cell>0.0266</cell><cell>0.0321</cell><cell>0.0221</cell><cell>0.0497</cell><cell>0.0277</cell></row><row><cell>GRU4Rec</cell><cell>0.0129</cell><cell>0.0086</cell><cell>0.0204</cell><cell>0.0110</cell><cell>0.0164</cell><cell>0.0099</cell><cell>0.0283</cell><cell>0.0137</cell><cell>0.0097</cell><cell>0.0059</cell><cell>0.0176</cell><cell>0.0084</cell></row><row><cell cols="2">BERT4Rec 0.0115</cell><cell>0.0075</cell><cell>0.0191</cell><cell>0.0099</cell><cell>0.0203</cell><cell>0.0124</cell><cell>0.0347</cell><cell>0.0170</cell><cell>0.0116</cell><cell>0.0071</cell><cell>0.0203</cell><cell>0.0099</cell></row><row><cell cols="2">FDSA P5-S (2-3) P5-B (2-3) 0.0364 0.0182 0.0272</cell><cell>0.0122 0.0169 0.0296</cell><cell>0.0288 0.0361 0.0431</cell><cell>0.0156 0.0198 0.0318</cell><cell>0.0267 0.0508 0.0515</cell><cell>0.0163 0.0385 0.0381</cell><cell>0.0407 0.0668 0.0664</cell><cell>0.0208 0.0436 0.0429</cell><cell>0.0228 0.0385 0.0363</cell><cell>0.0140 0.0269 0.0257</cell><cell>0.0381 0.0499 0.0457</cell><cell>0.0189 0.0305 0.0287</cell></row><row><cell cols="2">P5-S (2-13) 0.0258 P5-B (2-13) 0.0387</cell><cell>0.0159 0.0312</cell><cell>0.0346 0.0460</cell><cell>0.0188 0.0336</cell><cell>0.0502 0.0499</cell><cell>0.0378 0.0366</cell><cell>0.0656 0.0651</cell><cell>0.0428 0.0415</cell><cell>0.0370 0.0346</cell><cell>0.0260 0.0244</cell><cell>0.0471 0.0444</cell><cell>0.0293 0.0276</cell></row><row><cell>Methods</cell><cell cols="12">Sports BLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL Beauty Toys</cell></row><row><cell>Attn2Seq</cell><cell>0.5305</cell><cell>12.2800</cell><cell>1.2107</cell><cell>9.1312</cell><cell>0.7889</cell><cell>12.6590</cell><cell>1.6820</cell><cell>9.7481</cell><cell>1.6238</cell><cell>13.2245</cell><cell>2.9942</cell><cell>10.7398</cell></row><row><cell cols="3">NRT PETER P5-S (3-3) P5-B (3-3) 0.6213 0.4793 0.7112 12.8944 11.0723 0.5902 60.8892 58.7260</cell><cell cols="8">1.1304 1.3283 17.7514 18.5533 18.4670 3.1474 62.2778 21.9762 27.1758 0.5652 7.6674 0.8295 12.7815 1.8543 9.9477 1.9084 9.8635 1.1541 14.8497 2.1413 11.4143 1.9861 14.2716 13.5231 18.0010 2.6533 61.6557 21.6574 25.6646 0.3787 56.7474 56.4732</cell><cell cols="2">3.6708 3.6718 17.1475 17.7930 18.3364 11.1867 11.7010 16.7914</cell></row><row><cell cols="7">PETER+ P5-S (3-9) 7.2129 67.4004 36.1417 30.8359 5.4136 2.4627 24.1181 5.1937 18.4105 3.2606 P5-B (3-9) 3.5598 64.7683 34.0162 26.3184 6.5551 68.2939 25.5541 67.9526 P5-S (3-12) 5.8446 66.5976 35.5160 29.2766 5.5760 68.1710 P5-B (3-12) 4.6977 65.4562 34.9379 27.7223 7.0183 68.1908</cell><cell>5.9668 36.5097 36.7586 36.7876 36.7262</cell><cell cols="5">19.7168 30.7446 31.8136 9.5411 69.6964 40.3364 34.7272 4.7919 28.3083 9.4520 22.7017 8.2721 69.4591 39.9955 33.6941 30.8561 7.5790 69.2164 39.9065 33.1177 32.2162 8.2461 69.2331 39.9456 34.0081</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance on review preference prediction.</figDesc><table><row><cell>Methods</cell><cell>Sports RMSE MAE</cell><cell>Beauty RMSE MAE</cell><cell>Toys RMSE MAE</cell></row><row><cell>T0 (4-2)</cell><cell cols="3">0.6728 0.3140 0.6925 0.3324 0.8282 0.4201</cell></row><row><cell cols="4">T0 (4-4) P5-S (4-2) 0.6712 0.2983 0.6473 0.3069 0.6582 0.3205 0.6503 0.2984 0.7066 0.3663 0.8148 0.4230</cell></row><row><cell cols="4">P5-B (4-2) 0.5855 0.2537 0.7014 0.3360 0.7149 0.3566</cell></row><row><cell cols="4">P5-S (4-4) 0.6909 0.3077 0.6565 0.3122 0.6686 0.3255 P5-B (4-4) 0.5574 0.2463 0.6996 0.3348 0.7360 0.3652</cell></row><row><cell cols="4">1-out-of-100 setting. The experimental results are presented in</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on review summarization.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Statistics on domain transfer evaluation sets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Sports Beauty Toys</cell></row><row><cell>#Users</cell><cell>290</cell><cell>439</cell><cell>487</cell></row><row><cell>#Items</cell><cell>381</cell><cell>586</cell><cell>886</cell></row><row><cell>#Reviews</cell><cell>478</cell><cell cols="2">1,237 1,183</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Performance on zero-shot domain transfer.</figDesc><table><row><cell>Directions</cell><cell cols="2">Z-1 &amp; Z-4 Z-2 &amp; Z-3 Accuracy MAE</cell><cell cols="4">Z-5 &amp; Z-7 BLUE4 ROUGE1 BLUE4 ROUGE1 Z-6</cell></row><row><cell>Toys -&gt; Beauty</cell><cell>0.7922</cell><cell>0.8244</cell><cell>1.8869</cell><cell>61.1919</cell><cell>5.4609</cell><cell>66.4931</cell></row><row><cell>Toys -&gt; Sports</cell><cell>0.8682</cell><cell>0.6644</cell><cell>0.7405</cell><cell>60.9575</cell><cell>2.2601</cell><cell>62.0353</cell></row><row><cell>Beauty -&gt; Toys</cell><cell>0.8073</cell><cell>0.7792</cell><cell>0.0929</cell><cell cols="3">41.3061 11.8046 64.8701</cell></row><row><cell>Beauty -&gt; Sports</cell><cell>0.8676</cell><cell>0.6838</cell><cell>0.0346</cell><cell>39.7191</cell><cell>6.6409</cell><cell>66.9222</cell></row><row><cell>Sports -&gt; Toys</cell><cell>0.8230</cell><cell>0.7443</cell><cell>0.0687</cell><cell cols="3">42.9310 13.3408 69.7910</cell></row><row><cell>Sports -&gt; Beauty</cell><cell>0.8057</cell><cell>0.8102</cell><cell>0.0790</cell><cell cols="3">41.0659 13.1690 66.7687</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">IIS 1910154</rs>, <rs type="grantNumber">2007907</rs>, and <rs type="grantNumber">2046457</rs>. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_U4SzmUq">
					<idno type="grant-number">IIS 1910154</idno>
				</org>
				<org type="funding" xml:id="_jPXYuh3">
					<idno type="grant-number">2007907</idno>
				</org>
				<org type="funding" xml:id="_zguEYP3">
					<idno type="grant-number">2046457</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Input template: For the new product {{item_title}}, we would like to know whether {{user_desc}} will love it. If you think the user will love it, please help explain why.</p><p>Target template: {{explanation}}</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">BLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL T0</title>
		<imprint>
			<biblScope unit="page" from="4" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning</title>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural collaborative reasoning</title>
		<author>
			<persName><forename type="first">Hanxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1516" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying Vision-and-Language Tasks via Text Generation</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards conversational recommender systems</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A troubling analysis of reproducibility and progress in recommender systems research</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Boglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM conference on recommender systems</title>
		<meeting>the 13th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and Sequential/Session-Based Recommendation</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Gabriel De Souza Pereira Moreira</surname></persName>
		</author>
		<author>
			<persName><surname>Rabhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth ACM Conference on Recommender Systems</title>
		<imprint>
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to generate product reviews from attributes</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11982</idno>
		<title level="m">The Turking Test: Can Language Models Understand Instructions? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Popcorn: Human-in-the-loop Popularity Debiasing in Conversational Recommender Systems</title>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="494" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning attribute-to-feature mappings for cold-start recommendations</title>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Personalized Explanation Generation through Visualization</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<title level="m">PPT: Pre-trained Prompt Tuning for Few-shot Learning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on conversational recommender systems</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahtsham</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Addressing cold-start problem in recommendation systems</title>
		<author>
			<persName><forename type="first">Xuan Nhat</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuc</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trong</forename><surname>Duc Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Duc</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd international conference on Ubiquitous information management and communication</title>
		<meeting>the 2nd international conference on Ubiquitous information management and communication</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Melu: Meta-learned user preference estimator for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Hoyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbae</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongwon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsouk</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehee</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1073" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From zero-shot learning to cold-start recommendation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4189" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generate neural template explanations for recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="755" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalized Transformer for Explainable Recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4947" to="4957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural rating regression with abstractive tips generation for recommendation</title>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Amazon. com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What Makes Good In-Context Examples for GPT-3?</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT Understands, Too</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical gating networks for sequential recommendation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="825" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-domain recommendation: An embedding and mapping approach</title>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Tong Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2464" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Content-based recommendation systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Montreal, Quebec, Canada; Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>UAI &apos;09)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering vs. matrix factorization revisited</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM conference on recommender systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grouplens: An open architecture for collaborative filtering of netnews</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neophytos</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM conference on Computer supported cooperative work</title>
		<meeting>the 1994 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Trishala</forename><surname>Neeraj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abheesht</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</editor>
		<editor>
			<persName><surname>Scao</surname></persName>
		</editor>
		<meeting><address><addrLine>Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference World Wide Web</title>
		<meeting>the 10th international conference World Wide Web</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Methods and metrics for cold-start recommendations</title>
		<author>
			<persName><forename type="first">Alexandrin</forename><surname>Andrew I Schein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Kyuyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjae</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00573</idno>
		<title level="m">One4all User Representation for Recommender Systems in E-commerce</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Kyuyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><forename type="middle">Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">Nihlen</forename><surname>Ramstrom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11294</idno>
		<title level="m">Scaling Law for Recommendation Models: Towards Generalpurpose User Representations</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Conversational recommender system</title>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st international acm sigir conference on research &amp; development in information retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Counterfactual explainable recommendation</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1784" to="1793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A meta-learning perspective on cold-start recommendations for items</title>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conrado</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeshua</forename><surname>Bratman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<title level="m">Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Finetuned Language Models are Zero-Shot Learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning from Task Descriptions</title>
		<author>
			<persName><forename type="first">Orion</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1361" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural news recommendation with multi-head self-attention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6389" to="6394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Reinforcement knowledge graph reasoning for explainable recommendation</title>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 42nd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12085</idno>
		<title level="m">Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">One person, one model, one world: Learning continual user representation without forgetting</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="696" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Feature-level Deeper Self-Attention Network for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4320" to="4326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Joint representation learning for top-n recommendation with heterogeneous information sources</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1449" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Explainable recommendation: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="101" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Towards conversational search and recommendation: System ask, user respond</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th acm international conference on information and knowledge management</title>
		<meeting>the 27th acm international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Do users rate or review? Boost phrase-level sentiment labeling with review-level sentiment classification</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Joint deep modeling of users and items using reviews for recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM international conference on web search and data mining</title>
		<meeting>the tenth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<title level="m">Learning to Prompt for Vision-Language Models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cross-Domain Recommendation: Challenges, Progress, and Prospects</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/639</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/639" />
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
