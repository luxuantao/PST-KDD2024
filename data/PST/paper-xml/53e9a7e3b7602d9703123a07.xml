<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-06-06">6 June 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Zhao</surname></persName>
							<email>zhaoji84@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinwen</forename><surname>Tian</surname></persName>
							<email>jwtian@mail.hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Pattern Recognition and Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Lab of Neuro Imaging</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-06-06">6 June 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">C7BCD2942DCB30F59D7801AFBA182E0B</idno>
					<idno type="DOI">10.1016/j.patcog.2013.05.017</idno>
					<note type="submission">Received 16 May 2012 Received in revised form 28 February 2013 Accepted 21 May 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Vector field learning Sparse approximation Regularization Reproducing kernel Hilbert space Outlier Mismatch removal</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In vector field learning, regularized kernel methods such as regularized least-squares require the number of basis functions to be equivalent to the training sample size, N. The learning process thus has OðN 3 Þ and OðN 2 Þ in the time and space complexity, respectively. This poses significant burden on the vector learning problem for large datasets. In this paper, we propose a sparse approximation to a robust vector field learning method, sparse vector field consensus (SparseVFC), and derive a statistical learning bound on the speed of the convergence. We apply SparseVFC to the mismatch removal problem. The quantitative results on benchmark datasets demonstrate the significant speed advantage of SparseVFC over the original VFC algorithm (two orders of magnitude faster) without much performance degradation; we also demonstrate the large improvement by SparseVFC over traditional methods like RANSAC. Moreover, the proposed method is general and it can be applied to other applications in vector field learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For a given set of data S ¼ fðx n ; y n Þ∈X Â Yg N n ¼ 1 , one can learn a function f : X -Y so that it approximates a mapping to output y n for an input x n . In this process one can fit a function to the given training data with a smoothness constraint, which typically achieves some form of capacity control of the function space. The learning process aims to balance the data fitting and model complexity, and thus, produces a robust algorithm that generalizes well to the unseen data <ref type="bibr" target="#b0">[1]</ref>. This can one way be formulated into an optimization problem with a certain choice of regularization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, which typically operates in the Reproducing Kernel Hilbert Space (RKHS) <ref type="bibr" target="#b3">[4]</ref> (associated with a particular kernel). Two wellknown methods along the line of learning with regularization are regularized least-squares (RLS) and support vector machines (SVM) <ref type="bibr" target="#b1">[2]</ref>. These regularized kernel methods have drawn much attention due to their computational simplicity and strong generalization power in traditional machine learning problems such as regression and classification.</p><p>Regularized kernel methods over RKHS often lead to solving a convex optimization problem. For example, RLS directly solves a linear system. However, the computational complexity associated with applying kernel matrices is relatively high. Given a set of N training samples, the kernel matrix is of size N Â N. This suggests a space complexity OðN 2 Þ and a time complexity at least OðN 2 Þ; in fact most regularized kernel methods have core operations such as matrix inversion in RLS which is of OðN 3 Þ. It is therefore computationally demanding if N is large. This situation is more troublesome in vector-valued cases where we focus on the vector-valued regularized least-squares (vector-valued RLS). There are several different names for the RLS model <ref type="bibr" target="#b4">[5]</ref> such as regularization networks (RN) <ref type="bibr" target="#b2">[3]</ref>, kernel ridge regression (KRR) <ref type="bibr" target="#b5">[6]</ref>, least squares support vector machines (LS-SVM) <ref type="bibr" target="#b6">[7]</ref>, etc. We use a widely adopted one in the literature, i.e. RLS in this paper.</p><p>A vector field is a map that assigns each position x∈R P with a vector y∈R D , defined by a vector-valued function. Examples of vector field range from the velocity fields of fluid particles to the optical flow fields associated with visual motion. The problem of vector field learning is tied with functional estimation and supervised learning. We may learn a vector field by using regularized kernel methods and, in particular, vector-valued RLS <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. According to the representer theorem in RKHS, the optimal solution is a linear combination of a number (the size of training data points N) of basis functions <ref type="bibr" target="#b9">[10]</ref>. The corresponding kernel matrix is of size DN Â DN. Compared to the scalar case, the Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr vectored values pose more challenges on large datasets. One possible solution is to learn sparse basis functions; i.e. we could pick a subset of size M basis functions, making the kernel matrix of size DM Â DM. Thus, the computational complexity could be greatly reduced when M 5 N. Moreover, the solution based on sparse bases also enables faster prediction. Therefore, when we learn a vector field via vector-valued regularized kernel methods, using a sparse representation may be of particular advantage.</p><p>Motivated by the recent sparse representation literature <ref type="bibr" target="#b10">[11]</ref>, here we investigate a suboptimal solution to the vector-valued RLS problem. We derive an upper bound for the approximation accuracy based on the representer theorem. The upper bound has a rate of convergence in Oð ffiffiffiffiffiffiffiffiffiffi 1=M p Þ, indicating a fast convergence rate. We develop a new algorithm, SparseVFC, by using the sparse approximation in a robust vector field learning problem, vector field consensus (VFC) <ref type="bibr" target="#b8">[9]</ref>. As in <ref type="bibr" target="#b8">[9]</ref>, the SparseVFC algorithm is applied to the mismatch removal problem. The experimental results on various 2D and 3D datasets show the clear advantages of SparseVFC over the competing methods in efficiency and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Sparse representations <ref type="bibr" target="#b10">[11]</ref> have recently gained a considerable amount of interest for learning the kernel machines <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. To scale up kernel methods to deal with large data, a wealthy body of efficient sparse approximations have been suggested in the scalar case, including the Nyström approximation <ref type="bibr" target="#b15">[16]</ref>, sparse greedy approximations <ref type="bibr" target="#b16">[17]</ref>, reduced support vector machines <ref type="bibr" target="#b17">[18]</ref>, and incomplete Cholesky decomposition <ref type="bibr" target="#b18">[19]</ref>. Common to all these approximation schemes is that only a subset of the latent variables are treated exactly, with the remaining variables being approximated. These methods differ in the mechanism for choosing the subset, and in the matrix form used to represent the hypothesis space. In this paper, we generalize this idea to the vector-valued setting.</p><p>Another approach to achieve the sparse representation is to add regularization term penalizing the ℓ 1 norm of the expansion coefficients <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. However, this does not alleviate the problem, since it has to compute (and invert) the kernel matrix of size N Â N, and then it is not suitable for large datasets. To overcome this problem, Kim et al. <ref type="bibr" target="#b21">[22]</ref> described a specialized interior-point method for solving large scale ℓ 1 -regularized that uses the preconditioned conjugate gradients algorithm to compute the search direction.</p><p>The large scale kernel learning problem can also be addressed by iterative optimization such as conjugate gradient <ref type="bibr" target="#b22">[23]</ref> and more efficient decomposition-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Specially, for the RLS model with the ℓ 2 loss, fast optimization algorithms such as accelerated gradient descent in conjunction with stochastic methods <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> can perform very fast as well. However, a main drawback of these methods is that they have to store a kernel matrix of size N Â N which is not realistic for large datasets.</p><p>The main contributions of our work include (i) we present a sparse approximation algorithm for vector-valued RLS which has linear time and space complexity w.r.t. the training data si ze;</p><p>(ii) we derive an upper bound for the approximation accuracy of the proposed sparse approximation algorithm in terms of regularized risk functional; (iii) based on the sparse approximation and our the vector field learning method VFC, we give a new algorithm SparseVFC which significantly speeds up the original VFC algorithm without scarifies in accuracy; (iv) we apply SparseVFC to mismatch removal which is a fundamental problem in computer vision and the results demonstrate its superiority over the original VFC algorithm and many other state-of-the-art methods.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly review the vector-valued Tikhonov regularization and lay out a sparse approximation algorithm for solving vector-valued RLS. In Section 3, we apply the sparse approximation to robust vector field learning and mismatch removal problem. The datasets and evaluation criteria used in the experiments are presented in Section 4. In Section 5, we evaluate the performance of our algorithm on both synthetic data and real-world images. Finally, we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sparse approximation algorithm</head><p>We start by defining the vector-valued RKHS and recalling the vector-valued Tikhonov regularization, and then lay out a sparse approximation algorithm for solving the vector-valued RLS problem. At last, we derive a statistical learning bound for the sparse approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vector-valued reproducing kernel Hilbert spaces</head><p>We are interested in a class of vector-valued kernel methods, where the hypotheses space is chosen to be a reproducing kernel Hilbert space (RKHS). This motivates reviewing the basic theory of vector-valued RKHS. The development of the theory in the vector case is essentially the same as in the scalar case. We refer to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> for further details and references.</p><p>Let Y be a real Hilbert space with inner product (norm) 〈Á; Á〉 Y , (∥ Á ∥ Y ), for example, Y DR D , X a set, for example, X DR P , and H a Hilbert space with inner product (norm) 〈Á; Á〉 H , (∥ Á ∥ H ). A norm can be defined via an inner product, for example, ∥f∥ H ¼ 〈f; f〉 1=2 H . Next, we recall the definition of RKHS as well as some properties of it which we will use in this paper. Definition 1. A Hilbert space H is an RKHS if the evaluation maps ev x : H-Y are bounded, i.e. if ∀x∈X there exists a positive constant C x such that</p><formula xml:id="formula_0">∥ev x ðfÞ∥ Y ¼ ∥fðxÞ∥ Y ≤C x ∥f∥ H ; ∀f∈H:<label>ð1Þ</label></formula><p>A reproducing kernel Γ : X Â X -BðYÞ is then defined as</p><formula xml:id="formula_1">Γðx; x′Þ≔ev x ev n x′ ;<label>ð2Þ</label></formula><p>where BðYÞ is the space of bounded operators on Y, for example, BðYÞ DR DÂD , and ev n x is the adjoint of ev x . From the definition of the RKHS, we can derive the following properties:</p><p>(i) For each x∈X and y∈Y, the kernel Γ has the following reproducing property</p><formula xml:id="formula_2">〈fðxÞ; y〉 Y ¼ 〈f; ΓðÁ; xÞy〉 H ; ∀f∈H:<label>ð3Þ</label></formula><p>(ii) For every x∈X and f∈H, we have that</p><formula xml:id="formula_3">∥fðxÞ∥ Y ≤∥Γðx; xÞ∥ 1=2 Y;Y ∥f∥ H ;<label>ð4Þ</label></formula><p>where ∥ Á ∥ Y;Y is the operator norm.</p><p>The property (i) can be easily derived from the equation ev n x y ¼ ΓðÁ; xÞy. In the property (ii), we assume that sup x∈X ∥ Γðx;</p><formula xml:id="formula_4">xÞ∥ 1=2 Y;Y ¼ s Γ o ∞.</formula><p>Similar to the scalar case, for any N∈N, fx n : n∈N N g DX , and a reproducing kernel Γ, a unique RKHS can be defined by considering the completion of the space</p><formula xml:id="formula_5">H N ¼ ∑ N n ¼ 1 ΓðÁ; x n Þc n : c n ∈Y ;<label>ð5Þ</label></formula><p>with respect to the norm induced by the inner product</p><formula xml:id="formula_6">〈f; g〉 H ¼ ∑ N i;j ¼ 1 〈Γðx j ; x i Þc i ; d j 〉 Y ;<label>ð6Þ</label></formula><formula xml:id="formula_7">for any f; g∈H N with f ¼ ∑ N i ¼ 1 ΓðÁ; x i Þc i and g ¼ ∑ N j ¼ 1 ΓðÁ; x j Þd j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vector-valued Tikhonov regularization</head><p>Regularization aims to stabilize the solution of an illconditioned problem. Given an N-sample S ¼ fðx n ; y n Þ∈X Â Y : n∈N N g of patterns, where X D R P and Y D R D are input space and output space, respectively, in order to learn a mapping f : X -Y, the vector-valued Tikhonov regularization in an RKHS H with kernel Γ minimizes a regularized risk functional <ref type="bibr" target="#b9">[10]</ref> </p><formula xml:id="formula_8">ΦðfÞ ¼ ∑ N n ¼ 1 Vðfðx n Þ; y n Þ þ λ∥f∥ 2 H ;<label>ð7Þ</label></formula><p>where the first term is called the empirical error with the loss function V : Y Â Y-½0; ∞Þ satisfying Vðy; yÞ ¼ 0, the second term is a stabilizer with a regularization parameter λ controlling the tradeoff between these two terms.</p><p>Theorem 1 (Vector-valued Representer Theorem Micchelli and Pontil <ref type="bibr" target="#b9">[10]</ref>). The optimal solution of the regularized risk functional (7) has the form:</p><formula xml:id="formula_9">f o ðxÞ ¼ ∑ N n ¼ 1 Γðx; x n Þc n ; c n ∈Y:<label>ð8Þ</label></formula><p>Hence, minimizing over the (possibly) infinite dimensional Hilbert space, boils down to find a finite set of coefficients fc n : n∈N N g.</p><p>A number of loss functions have been discussed in the literature <ref type="bibr" target="#b0">[1]</ref>. In this paper, we focus on vector-valued RLS which is a vector-valued Tikhonov regularization with an ℓ 2 loss function, i.e.,</p><formula xml:id="formula_10">Vðfðx n Þ; y n Þ ¼ p n ∥y n -fðx n Þ∥ 2 ;<label>ð9Þ</label></formula><p>where p n ≥0 is the weight, and ∥ Á ∥ denotes the ℓ 2 norm. Other common loss functions are the absolute value loss V ðfðxÞ; yÞ ¼ jfðxÞ-yj and Vapnik's ϵÀinsensitive loss VðfðxÞ; yÞ ¼ maxðjfðxÞ -yj-ϵ; 0Þ. Using the ℓ 2 loss, the coefficients c n of the optimal solution, i.e. Eq. ( <ref type="formula" target="#formula_9">8</ref>), is then determined by a linear system <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> </p><formula xml:id="formula_11">ð Γ þ λ P-1 ÞC ¼ Y;<label>ð10Þ</label></formula><p>where the kernel matrix Γ is called the Gram matrix which is an N Â N block matrix with the ði; jÞ-th block Γðx i ;</p><formula xml:id="formula_12">x j Þ. P ¼ P⊗I DÂD is a DN Â DN diagonal matrix, here P ¼ diagðp 1 ; …; p N Þ, and ⊗ denotes Kronecker product. C ¼ ðc T 1 ; …; c T N Þ T and Y ¼ ðy T 1 ; …; y T N Þ T are DN Â 1 dimensional vectors.</formula><p>Note that when the loss function V is not quadratic anymore, the solution of the regularized risk functional <ref type="bibr" target="#b6">(7)</ref> still has the form <ref type="bibr" target="#b7">(8)</ref>, but the coefficients c n cannot be found anymore by solving a linear system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sparse approximation in vector-valued regularized least-squares</head><p>Under the Representer Theorem, the optimal solution f o comes from an RKHS H N defined as in Eq. ( <ref type="formula" target="#formula_5">5</ref>). Finding the coefficients c n of the optimal solution f o in vector-valued RLS merely requires to solve the linear system <ref type="bibr" target="#b9">(10)</ref>. However, for large values of N, it may pose a serious problem due to heavy computational (i.e. scales as OðN 3 Þ) or memory (i.e. scales as OðN 2 Þ) requirements, and, even when it is implementable, one may prefer a suboptimal but simpler method. In this section, we propose an algorithm that is based on a similar kind of idea as the subset of regressors method <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> for the standard vector-valued RLS problem. Rather than searching for the optimal solution f o in H N , we use a sparse approximation and search a suboptimal solution f s in a space H M (M 5 N) with much less basis functions defined as</p><formula xml:id="formula_13">H M ¼ ∑ M m ¼ 1 ΓðÁ; xm Þc m : c m ∈Y ;<label>ð11Þ</label></formula><p>with f xm : m∈N M g D X ,<ref type="foot" target="#foot_0">1</ref> and then minimize the loss over all the training data. Yet the problem that remains is how to choose the point set f xm : m∈N M g, and accordingly find a set of coefficients fc m : m∈N M g. In the scalar case, different approaches for selecting this point set are discussed, for example, in <ref type="bibr" target="#b4">[5]</ref>. There, it was found that simply selecting an arbitrary subset of the training inputs performs no worse than more sophisticated methods. Recent progress in compressed sensing <ref type="bibr" target="#b10">[11]</ref> also demonstrated the power of sparse random basis representation. Therefore, in the interest of computational efficiency, we use the simply random sampling method to choose sparse basis functions in the vector case. And we also compare the influence of different choices of sparse basis functions in the experiment section.</p><p>According to the sparse approximation, the unique solution of the vector-valued RLS in H M has this form:</p><formula xml:id="formula_14">f s ðxÞ ¼ ∑ M m ¼ 1 Γðx; xm Þc m :<label>ð12Þ</label></formula><p>To solve the coefficients c m , we now consider the Hilbertian norm and the corresponding inner product ( <ref type="formula" target="#formula_6">6</ref>)</p><formula xml:id="formula_15">∥f∥ 2 H ¼ ∑ M i ¼ 1 ∑ M j ¼ 1 〈Γð xj ; xi Þc i ; c j 〉 Y ¼ C T ΓC;<label>ð13Þ</label></formula><p>where C ¼ ðc T 1 ; …; c T M Þ T is a DM Â 1 dimensional vector, the kernel matrix Γ is an M Â M block matrix with the ði; jÞ-th block Γð xi ; xj Þ. Thus, the minimization of the regularized risk functional <ref type="bibr" target="#b6">(7)</ref> becomes</p><formula xml:id="formula_16">min f∈H ΦðfÞ ¼ min f∈H ∑ N n ¼ 1 p n ∥y n -fðx n Þ∥ 2 þ λ∥f∥ 2 H ¼ min C f∥ P1=2 ðY-ŨCÞ∥ 2 þ λC T ΓCg;<label>ð14Þ</label></formula><p>where Ũ is an N Â M block matrix with the ði; jÞ-th block Γðx i ; xj Þ:</p><formula xml:id="formula_17">Ũ ¼ Γðx 1 ; x1 Þ ⋯ Γðx 1 ; xM Þ ⋮ ⋱ ⋮ Γðx N ; x1 Þ ⋯ Γðx N ; xM Þ 2 6 4 3 7 5:<label>ð15Þ</label></formula><p>Taking the derivative of the right hand of Eq. ( <ref type="formula" target="#formula_16">14</ref>) with respect to the coefficient matrix C and setting it to zero, we can then compute the coefficient matrix C from the following linear system:</p><formula xml:id="formula_18">ð ŨT P Ũ þ λ ΓÞC ¼ ŨT PY:<label>ð16Þ</label></formula><p>In contrast to the optimal solution f o given by the Representer Theorem, which is a linear combination of the basis functions ΓðÁ; x 1 Þ; …; ΓðÁ; x N Þ determined by the inputs x 1 ; …; x N of training samples, the suboptimal solution f s is formed by a linear combination of arbitrary M-tuples of the basis functions. Generally, this sparse approximation will yield a vast increase in speed and decrease in memory requirements with negligible decrease in accuracy.</p><p>Note that the sparse approximation is somewhat related to SVM since SVM's predictive function also depends on a few samples (i.e. support vectors). In fact, under certain conditions, sparsity leads to SVM, which is related to the Structural Risk Minimization principle <ref type="bibr" target="#b0">[1]</ref>. As derived in <ref type="bibr" target="#b19">[20]</ref>, when the data is noiseless, and the coefficient matrix C is chosen to minimize the following cost function, the sparse approximation gives the same solution of SVM:</p><formula xml:id="formula_19">QðCÞ ¼ fðxÞ-∑ N n ¼ 1 c n Γðx; x n Þ 2 H þ λ∥C∥ ℓ 1 ;<label>ð17Þ</label></formula><p>where ∥ Á ∥ ℓ 1 is the usual ℓ 1 norm. If N is very large and ΓðÁ; x n Þ is not an orthonormal basis, it is possible that many different sets of coefficients will achieve the same error on a given data set. Among all the approximating functions that achieve the same error, using the ℓ 1 norm in the second term of Eq. ( <ref type="formula" target="#formula_19">17</ref>) favors the one with the smallest number of non-zero coefficients. Our approach follows this basic idea. The difference is that in the cost function <ref type="bibr" target="#b16">(17)</ref> the sparse basis functions are chosen automatically during the optimization process, while in our approach the basis functions are chosen randomly for the purpose of reducing both time and space complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Bounds of the sparse approximation</head><p>To derive upper bounds on error of approximation of the optimal solution f o by the suboptimal one f s , we employ Maurey-Jones-Barron's theorem <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> reformulated in terms of G-variation <ref type="bibr" target="#b33">[34]</ref>: for a Hilbert space X with norm ∥ Á ∥ and f∈X, G is a bounded subset of it, the following upper bound holds</p><formula xml:id="formula_20">∥f-span n G∥≤ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ðs G ∥f∥ G Þ 2 -∥f∥ 2 n s ;<label>ð18Þ</label></formula><p>where span n G is a linear combination of n arbitrary basis functions in G, s G ¼ sup g∈G ∥g∥, and ∥ Á ∥ G denotes the G-variation which is defined as</p><formula xml:id="formula_21">∥f∥ G ¼ inf fc 4 0 : f=c∈cl convðG∪-GÞg;<label>ð19Þ</label></formula><p>with cl convðÁÞ being the closure of the convex hull of a set and -G ¼ f-g : g∈Gg.</p><p>Here the closure of a set A is defined as clA ¼ ff∈X : ð∀ϵ 4 0Þð∃g∈AÞ∥f-g∥ o ϵg. For properties of G-variation, we refer the reader to <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>.</p><p>Taking advantage of this upper bound, we derive the following proposition which compares the optimal solution f o with a suboptimal solution f s in terms of regularized risk functional ΦðfÞ.</p><formula xml:id="formula_22">Proposition 1. Let fðx n ; y n Þ∈X Â Yg N n ¼ 1 be a finite set of input- output pairs of data, Γ : X Â X -Y a matrix-valued kernel, s Γ ¼ sup x∈X ∥Γðx; xÞ∥ 1=2 Y;Y , f o ðxÞ ¼ ∑ N n ¼ 1 Γðx; x n Þc n the optimal solution of the regularized risk functional (7), f s ðxÞ ¼ ∑ M m ¼ 1 Γðx; xm Þc m a suboptimal solution, suppose ∀n∈N N , ∥f s ðx n Þ þ f o ðx n Þ-2y n ∥ Y ≤sup x∈X ∥f s ðxÞ þ f o ðxÞ∥ Y ,</formula><p>then we have the following upper bound:</p><formula xml:id="formula_23">Φðf s Þ-Φðf o Þ≤ðNs 2 Γ þ λÞ α M þ 2∥f o ∥ H ffiffiffiffi ffi α M r ;<label>ð20Þ</label></formula><formula xml:id="formula_24">where α ¼ ðs Γ ∥f o ∥ G Þ 2 -∥f o ∥ 2</formula><p>H , H is the RKHS corresponding to the reproducing kernel Γ, λ is the regularization parameter.</p><p>Proof. According to the property (ii) of an RKHS in Section 2.1, for every f∈H and x∈X we have</p><formula xml:id="formula_25">∥fðxÞ∥ Y ≤∥Γðx; xÞ∥ 1=2 Y;Y ∥f∥ H ≤s Γ ∥f∥ H ;<label>ð21Þ</label></formula><formula xml:id="formula_26">where s Γ ¼ sup x∈X ∥Γðx; xÞ∥ 1=2 Y;Y . Thus we obtain sup x∈X ∥fðxÞ∥ Y ≤s Γ ∥f∥ H :<label>ð22Þ</label></formula><p>By the last inequality and Eq. ( <ref type="formula" target="#formula_20">18</ref>), we obtain </p><formula xml:id="formula_27">Φðf s Þ-Φðf o Þ ¼ ∑ N n ¼ 1 ð∥f s ðx n Þ-y n ∥ 2 Y -∥f o ðx n Þ-y n ∥ 2 Y Þ þ λð∥f s ∥ 2 H -∥f o ∥ 2 H Þ ≤ ∑ N n ¼ 1 〈f s ðx n Þ-f o ðx n Þ; f s ðx n Þ þ f o ðx n Þ-2y n 〉 Y þλ ∥f s ∥ H -∥f o ∥ H Áð∥f s ∥ H þ ∥f o ∥ H Þ ≤ ∑ N n ¼ 1 ∥f s ðx n Þ-f o ðx n Þ∥ Y ∥f s ðx n Þ þ f o ðx n Þ-2y n ∥ Y þλ∥f s -f o ∥ H ð∥f s ∥ H þ ∥f o ∥ H Þ ≤Nsup x∈X ∥f s ðxÞ-f o ðxÞ∥ Y sup x∈X ∥f s ðxÞ þ f o ðxÞ∥ Y þλ∥f s -f o ∥ H ð∥f s ∥ H þ ∥f o ∥ H Þ ≤Ns Γ ∥f o -f s ∥ H s Γ ∥f o þ f s ∥ H -2y min j þλ∥f o -f s ∥ H ð∥f o ∥ H þ ∥f s ∥ H Þ ≤Ns Γ ffiffiffiffi ffi α M r 2s Γ ∥f o ∥ H þ s Γ ffiffiffiffi ffi α M r þ λ ffiffiffiffi ffi α M r 2∥f o ∥ H þ ffiffiffiffi ffi α M r ¼ ðNs 2 Γ þ λÞ α M þ 2∥f o ∥ H ffiffiffiffi ffi α M r ;<label>ð23Þ</label></formula><formula xml:id="formula_28">where α ¼ ðs Γ ∥f o ∥ G Þ 2 -∥f o ∥ 2 H , ∥ Á ∥ G corresponds to the G-</formula><formula xml:id="formula_29">Ns 2 Γ þ λ -2∥f o ∥ 2 H ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 þ ϵ ðNs 2 Γ þ λÞ∥f o ∥ 2 H s -1 0 @ 1 A 2 4 3 5 -1 :<label>ð24Þ</label></formula><p>3. Sparse approximation for robust vector field learning A vector field is also a vector-valued function. The Tikhonov regularization treats all samples as inliers which ignores the issue of robustness, i.e., the real-world data may often contain some unknown outliers. Recently, Zhao et al. <ref type="bibr" target="#b8">[9]</ref> present a robust vectorvalued RLS method named Vector Field Consensus (VFC) for vector field learning. In which, each sample is associated with a latent variable indicating whether it is an inlier or an outlier, and then the EM algorithm is adopted for optimization. Besides, the technique of robust vector field learning has been also adopted in Gaussian processes, basically by using the so-called t-processes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. In this section, we present a sparse approximation algorithm for VFC, and apply it to the mismatch removal problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparse vector field consensus</head><p>Given a set of observed input-output pairs S ¼ fðx n ; y n Þ : n∈N N g as samples randomly drawn from a vector field which may contain some unknown outliers, the goal is to learn a mapping f to fit the inliers well. Due to the existence of outliers, it is desirable to have a robust estimate of the mapping f. There are two choices: (i) to build a more complex model that includes the outliers-which involves modeling the outlier process using extra (hidden) variables which enable us to identify and reject outliers, or (ii) to use an estimator which is less sensitive to outliers, as described in Huber's robust statistics <ref type="bibr" target="#b38">[39]</ref>. In this paper, we use the first scenario. In the following we make an assumption that, the vector field samples in the inlier class have Gaussian noise with zero mean and uniform standard deviation s, while the ones in the outlier class are uniformly distributed 1=a with a being the volume of the output domain. Let γ be the percentage of inliers which we do not know in advance,</p><formula xml:id="formula_30">X ¼ ðx T 1 ; …; x T N Þ T and Y ¼ ðy T 1 ; …; y T N Þ T be DN Â 1 dimensional</formula><p>vectors, and θ ¼ ff; s 2 ; γg the unknown parameter set. The likelihood is then a mixture model as</p><formula xml:id="formula_31">pðYjX; θÞ ¼ ∏ N n ¼ 1 γ ð2πs 2 Þ D=2 e -∥y n -fðxnÞ∥ 2 =2s 2 þ 1-γ a ! :<label>ð25Þ</label></formula><p>We model the mapping f in a vector-valued RKHS H with reproducing kernel Γ, and impose a smoothness constraint on it, i. e. pðfÞ∝e -ðλ=2Þ∥f∥ 2 H . Therefore, we can estimate a MAP solution of θ by using the Bayes rule as θ n ¼ arg max θ pðYjX; θÞpðfÞ. An iterative EM algorithm can be used to solve this problem. We associate sample n with a latent variable z n ∈f0; 1g, where z n ¼1 indicates a Gaussian distribution and z n ¼0 points to a uniform distribution. We follow the standard notations <ref type="bibr" target="#b39">[40]</ref> and omit some terms that are independent of θ. The complete-data log posterior is then given by</p><formula xml:id="formula_32">Qðθ; θ old Þ ¼ - 1 2s 2 ∑ N n ¼ 1 Pðz n ¼ 1jx n ; y n ; θ old Þ∥y n -fðx n Þ∥ 2 - D 2 ln s 2 ∑ N n ¼ 1 Pðz n ¼ 1jx n ; y n ; θ old Þ þln ð1-γÞ ∑ N n ¼ 1 Pðz n ¼ 0jx n ; y n ; θ old Þ þln γ ∑ N n ¼ 1 Pðz n ¼ 1jx n ; y n ; θ old Þ- λ 2 ∥f∥ 2 H :<label>ð26Þ</label></formula><formula xml:id="formula_33">E-step: Denote P ¼ diagðp 1 ; …; p N Þ, where the probability p n ¼ Pðz n ¼ 1jx n ; y n ; θ old Þ can be computed by applying Bayes rule p n ¼ γe -∥y n -fðxnÞ∥ 2 =2s 2 γe -∥y n -fðxnÞ∥ 2 =2s 2 þ ð1-γÞ ð2πs 2 Þ D=2 a :<label>ð27Þ</label></formula><p>The posterior probability p n is a soft decision, which indicates to what degree the sample n agrees with the current estimated vector field f. M-step: We determine the revised parameter estimate θ new ¼ arg max θ Qðθ; θ old Þ. Taking derivative of QðθÞ with respect to s 2 and γ, and setting them to zero, we obtain</p><formula xml:id="formula_34">s 2 ¼ ðY-VÞ T PðY-VÞ D Á trðPÞ ;<label>ð28Þ</label></formula><formula xml:id="formula_35">γ ¼ trðPÞ=N;<label>ð29Þ</label></formula><formula xml:id="formula_36">where V ¼ ðfðx 1 Þ T ; …; fðx N Þ T Þ T is a DN Â 1 dimensional</formula><p>vector, and trðÁÞ denotes the trace.</p><p>Considering the terms of objective function Q in Eq. ( <ref type="formula" target="#formula_32">26</ref>) that are related to f, the vector field can be estimated from minimizing an energy function as</p><formula xml:id="formula_37">EðfÞ ¼ 1 2s 2 ∑ N n ¼ 1 p n ∥y n -fðx n Þ∥ 2 þ λ 2 ∥f∥ 2 H :<label>ð30Þ</label></formula><p>This is a regularized risk functional (7) with ℓ 2 loss function <ref type="bibr" target="#b8">(9)</ref>. To estimate the vector field f, it needs to solve a linear system similar to Eq. ( <ref type="formula" target="#formula_11">10</ref>), which takes up most of the run-time and memory requirements of the algorithm. Obviously, the sparse approximation could be used here to reduce the time and space complexity.</p><p>Using the sparse approximation, we search a suboptimal f s which has the form as in Eq. ( <ref type="formula" target="#formula_14">12</ref>) with the coefficient c n determined by a linear system similar to Eq. ( <ref type="formula" target="#formula_18">16</ref>)</p><formula xml:id="formula_38">ð ŨT P Ũ þ λs 2 ΓÞC ¼ ŨT PY:<label>ð31Þ</label></formula><p>Since it is a sparse approximation to the vector field consensus algorithm, we name our method SparseVFC. In summary, compared with the original VFC algorithm, we estimate a vector field f by solving a linear system <ref type="bibr" target="#b30">(31)</ref> in SparseVFC rather than Eq. <ref type="bibr" target="#b9">(10)</ref>  It also presented a fast implementation for VFC named FastVFC in the original paper <ref type="bibr" target="#b8">[9]</ref>. To reduce the time complexity, it first uses the low rank matrix approximation by computing the singular value decomposition (SVD) for the kernel matrix Γ of size DN Â DN, and then the Woodbury matrix identity to invert the coefficient matrix in the linear system for solving C <ref type="bibr" target="#b40">[41]</ref>.</p><p>Computational complexity. Notice that P is a diagonal matrix, we can compute ŨT P in linear system (31) by multiplying the n-th diagonal element of P to the n-th column of ŨT , and thus the time complexity of SparseVFC for mismatch removal is reduced to</p><formula xml:id="formula_39">OðmD 3 M 2 N þ mD 3 M 3 Þ</formula><p>, where m is the iterative times for EM. The space complexity of SparseVFC scales like OðD 2 MN þ D 2 M 2 Þ due to the memory requirements for storing the matrix Ũ and kernel Γ.</p><p>Typically, the required number of basis functions for sparse approximation is much less than the number of data points, i.e. M≪N. In this paper, we apply the SparseVFC to the mismatch removal problem on 2D and 3D images, in which the number of the point matches N is typically in the order of 10 3 , and the required number of basis function M is in the order of 10 1 . Therefore, both the time and space complexities of SparseVFC could be simply written as O(N). Table <ref type="table">1</ref> summarizes the time and space complexities of VFC, FastVFC and SparseVFC. Compared to VFC and FastVFC, the time and space complexities are reduced from OðN 3 Þ and OðN 2 Þ to both O(N). This is significant for large training sets. Note that the time complexity of FastVFC is still OðN 3 Þ, it is because the SVD operation of a matrix of size N Â N has time complexity OðN 3 Þ.</p><p>Relation to FastVFC. There is some relationship between our SparseVFC algorithm and the FastVFC algorithm. On the one hand, both these two algorithms use approximation to the matrixvalued kernel Γ to search a suboptimal solution rather than the optimal solution, and then reduce the time complexity. On the other hand, our algorithm is clearly superior to the FastVFC, which can be seen from both the time and space complexities as shown in Table <ref type="table">1</ref>. For FastVFC, it makes a low rank matrix approximation on the kernel matrix Γ itself, which does not change the size of Γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Comparison of computational complexity.</p><p>VFC <ref type="bibr" target="#b8">[9]</ref> FastVFC <ref type="bibr" target="#b8">[9]</ref> SparseVFC</p><formula xml:id="formula_40">Time OðN 3 Þ OðN 3 Þ O(N) Space OðN 2 Þ OðN 2 Þ O(N)</formula><p>Therefore, the memory requirement is not decreased, and it is still not implementable for large training sets. Moreover, the FastVFC algorithm has the same time complexity OðN 3 Þ as VFC, due to the SVD operation and kernel matrix inversion operation in FastVFC and VFC, respectively. The difference is that the SVD operation in FastVFC needs to perform only once while the matrix inversion operation in VFC needs to perform in each EM iteration, and hence an acceleration can be achieved in FastVFC. In contrast, the SparseVFC uses a sparse representation and chooses much less basis functions to approximate the function space, leading to a significant reduction of the size of the corresponding reproducing kernel. This sparse approximation (even with random chosen basis functions) not only significantly reduces both the time and space complexities, but also does not lead to sacrifice in accuracy, and in some situations it even gains a little better performance compared to the original VFC algorithm (as shown in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Application to mismatch removal</head><p>In this section, we focus on establishing accurate point correspondences between two images of the same scene. Many of the computer vision tasks such as building 3D models, registration, object recognition, tracking, and structure and motion recovery start by assuming that the point correspondences have been successfully recovered <ref type="bibr" target="#b41">[42]</ref>.</p><p>Point correspondences between two images are in general established by first detecting interest points and then matching the detected points based on local descriptors <ref type="bibr" target="#b42">[43]</ref>. This may result in a number of mismatches (outliers) due to viewpoint changes, occlusions, repeated structures, etc. The existence of mismatches is usually enough to ruin the traditional estimation methods. In this case, a robust estimator is desirable to remove mismatches <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>. In our SparseVFC, as shown in the last line of Algorithm 1, a sample being an inlier or outlier could be determined by its posterior probability after EM convergences. Using this property, we apply SparseVFC to the mismatch removal problem. Next, we point out some key issues.</p><p>Vector field introduced by image pairs. We first make a linear rescaling of the point correspondences so that the positions of feature points in the first and second images both have zero mean and unit variance. Let the input x∈R P be the location of a normalized point in the first image, and the output y∈R D be the corresponding displacement of that point in the second image; then the matches can be converted into motion field training set. For 2D images P ¼ D ¼ 2; for 3D surfaces P ¼ D ¼ 3. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates schematically the 2D image case.</p><p>Kernel selection. Kernel plays a central role in regularization theory as it provides a flexible and computationally feasible way to choose an RKHS. Usually, for the mismatch removal problem, the structure of the generated vector field is relatively simple. We simply choose a diagonal decomposable kernel <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>:</p><formula xml:id="formula_41">Γðx i ; x j Þ ¼ e -β∥x i -x j ∥ 2 I.</formula><p>Then we can solve a more efficient linear system instead of Eq. ( <ref type="formula" target="#formula_38">31</ref>) as</p><formula xml:id="formula_42">ðU T PU þ λs 2 ΓÞ C ¼ U T P Ỹ;<label>ð32Þ</label></formula><p>where the kernel matrix Γ∈R MÂM and Γ ij ¼ e -β∥ x i -x j ∥ 2 , U∈R NÂM and</p><formula xml:id="formula_43">U ij ¼ e -β∥x i -x j ∥ 2 , C ¼ ðc 1 ; …; c M Þ T and Ỹ ¼ ðy 1 ; …; y N Þ T are M Â D and N Â D matrices, respectively.</formula><p>Here N is the number of putative matches, and M is the number of bases. It should be noted that solving the vector field learning problem with this diagonal decomposable kernel is not equivalent to solving D independent scalar problems, since the update of the posterior probability p n and variance s 2 in Eqs. ( <ref type="formula" target="#formula_33">27</ref>) and ( <ref type="formula" target="#formula_34">28</ref>) are determined by all components of the output y n . When it is applied to mismatch removal, there is a problem which should draw attention. We must ensure that the point set f xm : m∈N M g used to construct the basis functions does not contain two same points since in this case the coefficient matrix in linear system <ref type="bibr" target="#b31">(32)</ref>, i.e. ðU T PU þ λs 2 ΓÞ, will be singular. Obviously, this may appear in the mismatch removal problem, since in the putative match set there may exist one point in the first image matched to several points in the second image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>There are mainly four parameters in the SparseVFC algorithm: γ, λ, τ and M. Parameter γ reflects our initial assumption on the amount of inliers in the correspondence sets. Parameter λ reflects the amount of the smoothness constraint which controls the trade-off between the closeness to the data and the smoothness of the solution. Parameter τ is a threshold, which is used for deciding the correctness of a match. In general, our method is very robust to these parameters. We set γ ¼ 0:9, λ ¼ 3, and τ ¼ 0:75 according to the original VFC algorithm throughout this paper. Parameter M is the number of the basis functions used for sparse approximation. The choice of M depends on both the data (i.e., the true vector field) and the assumed function space (i.e., the reproducing kernel), as shown in Eq. ( <ref type="formula" target="#formula_29">24</ref>). We will discuss it in the experiment according to the specific application.</p><p>It should be noted that in practice we do not determine the value of M according to Eq. ( <ref type="formula" target="#formula_29">24</ref>). On the one hand, it is derived in the context of providing a theoretic upper bound, and in practice to achieve a good approximation the required value of M may be much smaller than this bound. On the other hand, the upper bound in Eq. ( <ref type="formula" target="#formula_29">24</ref>) is hard to compute, and it is costly to derive a value of M for each sample set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>In our evaluation we consider synthetic 2D vector field estimation, mismatch removal on 2D real images and 3D surfaces. All the experiments are performed on a Intel Core2 2.5GHz PC with Matlab code. Next, we discuss about the datasets and evaluation criteria.</p><p>Synthetic 2D vector field: The synthetic vector field is constructed from a scalar function defined by a mixture of five Gaussians, which have the same covariance 0:25I and centered at (0, 0), (1, 0), (0, 1), (-1, 0) and (0, -1), respectively, as in <ref type="bibr" target="#b7">[8]</ref>. Its gradient and perpendicular gradient indicate a divergence-free and a curl-free field, respectively. The synthetic data is then constructed by taking a convex combination of these two vector fields. In our evaluation the combination coefficient is set to 0.5, and then we get the synthetic field as shown in Fig. <ref type="figure" target="#fig_8">2</ref>. The field is computed on a 70 Â 70 grid over the square ½-2; 2 Â ½-2; 2.</p><p>The training inliers are uniformly sampled points from the grid, and we add Gaussian noise with zero mean and uniform standard deviation 0.1 on the outputs. The outliers are generated as follows: the input x is chosen randomly from the grid; the output y is generated randomly from a uniform distribution on the square ½-2; 2 Â ½-2; 2. The performance for vector field learning is measured by an angular measure of error <ref type="bibr" target="#b49">[50]</ref> between the learned vector of VFC and the ground truth. If v g ¼ ðv 1  g ; v 2 g Þ and v e ¼ ðv 1 e ; v 2 e Þ are the ground truth and estimated fields, we consider the transformation v-ṽ ¼ 1=ð∥ðv 1 ; v 2 ; 1Þ∥Þðv 1 ; v 2 ; 1Þ. The error measure is defined as err ¼ arccosð ṽe ; ṽg Þ.</p><p>2D image datasets: We tested our method on the dataset of Mikolajczyk et al. <ref type="bibr" target="#b50">[51]</ref> and Tuytelaars and van Gool <ref type="bibr" target="#b51">[52]</ref> The open source VLFEAT toolbox <ref type="bibr" target="#b52">[53]</ref> is used to determine the initial correspondences of SIFT <ref type="bibr" target="#b42">[43]</ref>. All parameters are set as the default values except for the distance ratio threshold t. Usually, the greater value of t is, the smaller amount of matches with higher correct match percentage will be. The match correctness is determined by computing an overlap error <ref type="bibr" target="#b50">[51]</ref>, as in <ref type="bibr" target="#b8">[9]</ref>.</p><p>3D surfaces datasets: For 3D case, we consider two datasets used in <ref type="bibr" target="#b53">[54]</ref>: the Dino and Temple datasets. Each surface pair are representations of the same rigid object which can be aligned using a rotation, translation and scale.</p><p>We determine the initial correspondences by using the method of Zaharescu et al. <ref type="bibr" target="#b53">[54]</ref>. The feature point detector is called MeshDOG, which is a generalization of the difference of Gaussian (DOG) operator <ref type="bibr" target="#b42">[43]</ref>. The feature descriptor is called MeshHOG, and it is a generalization of the histogram of oriented gradients (HOG) descriptor <ref type="bibr" target="#b54">[55]</ref>. The match correctness is determined as follows. For these two datasets, the correspondences between the two surfaces can be formulated as y ¼ sRx þ t, where R 3Â3 is a rotation matrix, s is a scaling parameter, and t 3Â1 is a translation vector. We can use some robust rigid point registration methods such as the Coherent Point Drift (CPD) <ref type="bibr" target="#b40">[41]</ref> to solve these three parameters, and then the match correctness can be accordingly determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>To test the performance of the sparse approximation algorithm, we perform experiments on the proposed SparseVFC algorithm, and compare its approximate accuracy and time efficiency to VFC and FastVFC on both synthetic data and real-world images. The experiments are conducted from two aspects: (i) vector field learning performance comparison on synthetic vector field and (ii) mismatch removal performance comparison on real image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Vector field learning on synthetic vector field</head><p>We perform a representative experiment on a synthetic 2D vector field shown in Fig. <ref type="figure" target="#fig_8">2</ref>. For each cardinality of the training set, we repeat the experimental process by 10 times with different randomly drawn examples. After the vector field is learned, we use it to predict the outputs on the whole grid and compare them to the ground truth. The experimental results are evaluated by means of test error (i.e. error between prediction and ground truth on the whole grid) and run-time speedup factor (i.e. ratio of run-time of VFC to run-time of FastVFC or SparseVFC) for clarity. The matrixvalued kernel is chosen to be a convex combination of the divergence-free kernel Γ df and curl-free kernel Γ cf <ref type="bibr" target="#b55">[56]</ref> with width s ¼ 0:8, and the combination coefficient is set to 0.5. The divergence-free and curl-free kernels have the following forms:</p><formula xml:id="formula_44">Γ df ðx; x′Þ ¼ 1 s2 e -∥x-x′∥ 2 =2 s2 x-x′ s x-x′ s T " þ D-1 ð Þ- ∥x-x′∥ 2 s2 Á I ;<label>ð33Þ</label></formula><formula xml:id="formula_45">Γ cf ðx; x′Þ ¼ 1 s2 e -∥x-x′∥ 2 =2 s2 I- x-x′ s x-x′ s T " # :<label>ð34Þ</label></formula><p>In order to choose an appropriate value of M on this synthetic field, we assess the performance of SparseVFC with respect to M under different experimental setup. Rather than using the ground truth to assess performance, i.e. the mean of test error, we use the standard deviation of the estimated inliers <ref type="bibr" target="#b43">[44]</ref> </p><formula xml:id="formula_46">s n ¼ 1 jI j ∑ i∈I ∥y i -fðx i Þ∥ 2 ! 1=2 ;<label>ð35Þ</label></formula><p>where I is the estimated inlier set in the training samples, and j Á j denotes the cardinality of a set. Generally speaking, smaller value of s n indicates better estimation. The result is presented in Fig. <ref type="figure" target="#fig_2">3a</ref>, we can see that M¼60 could reach a good compromise between Fig. <ref type="figure" target="#fig_8">2</ref>. Visualization of a synthetic 2D field without noise and its 500 random samples used in experiment.</p><p>the accuracy and computational complexity. For comparison, we also present the mean of test error in Fig. <ref type="figure" target="#fig_2">3b</ref>. We can see that these two criterions tend to produce similar choices of M. For FastVFC, 150 largest eigenvalues are used for calculating the low rank matrix approximation. We first give some intuitive impression on the performance of our method, as shown in Fig. <ref type="figure">4</ref>. We see that SparseVFC can successfully recover the vector field from sparse training samples, and the performance is getting better as the number of inlier in the training set increases. Figs. <ref type="figure">5</ref> and<ref type="figure" target="#fig_4">6</ref> show the vector field learning performances of VFC, FastVFC and SparseVFC. We consider two scenarios for performance comparison: (i) fix the inlier number and change the inlier percentage and (ii) fix the inlier percentage and change the inlier number. Fig. <ref type="figure">5</ref> shows the performance comparison of the test error under different experimental setup, we see that both SparseVFC and FastVFC perform much the same as VFC. Fig. <ref type="figure" target="#fig_4">6</ref> shows a performance comparison on the average run-time. From Fig. <ref type="figure" target="#fig_4">6a</ref>, we see that the computational cost of SparseVFC is about linear with respect to the training set size. The run-time speedup factors of FastVFC and SparseVFC with respect to VFC are presented in Fig. <ref type="figure" target="#fig_4">6b</ref>. We see that the speedup factor of FastVFC is about a constant, since its time complexity is the same as VFC, both are OðN 3 Þ. However,   compared to FastVFC, the use of SparseVFC leads to an essential speedup, and this advantage will be significantly magnified with larger scale of training set.</p><p>In conclusion, when an appropriate number of basis functions are chosen, the SparseVFC algorithm can approximate the original VFC algorithm quite well, while with a significant run-time speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Mismatch removal on real images</head><p>We notice that the algorithm demonstrates strong ability on mismatch removal. Thus we apply it to the mismatch removal problem and perform experiments on a wide range of real images. The performance is characterized by precision and recall. Besides VFC and FastVFC, we use three additional mismatch removal methods for comparison: RANdom SAmple Consensus (RANSAC) <ref type="bibr" target="#b56">[57]</ref>, Maximum Likelihood Estimation SAmple Consensus (MLE-SAC) <ref type="bibr" target="#b43">[44]</ref> and Identifying point correspondences by Correspondence Function (ICF) <ref type="bibr" target="#b46">[47]</ref>. RANSAC tries to get as small an outlierfree subset as feasible to estimate a given parametric model by resampling, while its variants MLESAC adopts a new cost function using weighted voting strategy based on M-estimator, and chooses the solution which maximize the likelihood rather than the inlier count in RANSAC. The ICF method uses support vector regression to learn a correspondence function pair which maps points in one image to their corresponding points in another, and then rejects the mismatches by checking whether they are consistent with the estimated correspondence functions.</p><p>The structure of the generated motion field is relatively simple in the mismatch removal problem. We simply choose a diagonal decomposable kernel with Gaussian form in the scalar part, and we set β ¼ 0:1 according to the original VFC algorithm. Moreover, the number of basis functions M used for sparse approximation is fixed in our evaluation, since choosing M adaptively would require some pre-processing which would increase the computational cost. We manage to tune the value of M by using a small test set, and we find that using 15 basis functions for sparse approximation is accurate enough. So we set M ¼15 throughout the experiments. For FastVFC, 10 largest eigenvalues are used for calculating the low rank matrix approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Results on several 2D image pairs</head><p>Our first experiment involves mismatch removal on several image pairs, including wide three baseline image pairs (Tree, Valbonne and Mex) and three image pairs of non-rigid object (DogCat, Peacock and T-shirt). Table <ref type="table" target="#tab_3">2</ref> presents the numbers of matches as well as the initial correct match percentages in these six image pairs. The whole mismatch removal progress on these image pairs is illustrated schematically in Fig. <ref type="figure" target="#fig_5">7</ref>. The columns show the iterative progress, the level of the blue color indicates to what degree a sample belongs to inlier, and it is also the posterior probability p n in Eq. ( <ref type="formula" target="#formula_37">30</ref>). In the beginning, all the SIFT matches in the first column are assumed to be inlier. We convert them into motion field training sets which are shown in the 2nd column. As the EM iterative process continues, progressively more refined matches are shown in the 3rd, 4th and 5th columns. The 5th column shows that the algorithm almost converges to a nearly binary decision on the match correctness. The SIFT matches reserved by the algorithm are presented in the last column. It should be noted that there is an underline assumption in our method that the vector field should be smooth, i.e. the norm of the field f in Eq. ( <ref type="formula" target="#formula_15">13</ref>) should be small. However, for wide baseline image pairs such as Tree, Valbonne and Mex, the related motion fields are in general not continuous, and our method is still effective for mismatch removal. We give an explanation as follows: under the sparsity assumptions of the training data, it is not hard to seek a smooth vector field which can fit nearly all the inliers (sparse) well; if the goal is just to remove mismatches in the training data, the smoothness constraint can work well.</p><p>Next, we give a performance comparison with the other five methods in Table <ref type="table" target="#tab_4">3</ref>. The geometry model used in RANSAC and MLESAC is epipolar geometry. We see that MLESAC has slightly better precisions than the RANSAC with the cost of producing a slightly lower recall. The recall of ICF is quite low, although it has a satisfactory precision. However, VFC, FastVFC and SparseVFC can successfully distinguish inliers from outliers, and they have the best trade-off between precision and recall. The low rank matrix approximation used in FastVFC may slightly hurt the performance. While in SparseVFC, it seems that the sparse approximation does not lead to degenerated performance, on the contrary, it makes the algorithm more efficient.</p><p>Notice that we did not compare to RANSAC and MLESAC on the image pairs of non-rigid object. RANSAC and MLESAC depend on a parametric model, for example, fundamental matrix. If there exist some objects with deformation in the image pairs, they can no longer work, since the point pairs will no longer obey the epipolar geometry.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Results on a 2D image datasets</head><p>We test our method on the dataset of Mikolajczyk et al., which contains image transformations of viewpoint change, scale change, rotation, image blur, JPEG compression, and illumination. We use all the 40 image pairs, and for each pair, we set the SIFT distance ratio threshold t to 1.5, 1.3 and 1.0, respectively, as in <ref type="bibr" target="#b8">[9]</ref>. The cumulative distribution function of original correct match percentage is shown in Fig. <ref type="figure" target="#fig_7">8a</ref>. The initial average precision of all image pairs is 69.58%, and nearly 30 percent of the training sets have correct match percentage below 50%. Fig. <ref type="figure" target="#fig_7">8b</ref> presents the cumulative distribution of the number of point matches contained in the experimental image pairs. We see that most of the image pairs have large scale of point matches (i.e. in the order of 1000's).</p><p>The precision-recall pairs on this dataset are summarized in Fig. <ref type="figure" target="#fig_7">8</ref>. The average precision-recall pairs are (95.49%, 97.55%), (97.95%, 96.93%), (93.95%, 62.69%) and (98.57%, 97.78%) for RAN-SAC, MLESAC, ICF and SparseVFC, respectively. Here we choose homography as the geometry model in RANSAC and MLESAC. Note that the performances of VFC, FastVFC and SparseVFC are quite close, thus we omit the results of VFC and FastVFC in the figure for clarity. From the result, we see that ICF usually has high precision or recall, but not simultaneously. MLESAC performs a little better  than RANSAC, and they both achieve quite satisfactory performance. This can be explained by the lack of complex constraints between the elements of the homography matrix. Our method has good performance in dealing with the mismatch removal problem, and it has the best trade-off between precision and recall compared to the other three methods.</p><p>We compare the approximate accuracy and time efficiency of SparseVFC to VFC and FastVFC in Table <ref type="table" target="#tab_6">4</ref>. As shown in it, both FastVFC and SparseVFC approximate VFC quite well, especially our method SparseVFC. Moreover, SparseVFC achieves a significant speedup with respect to the original VFC algorithm, of about 300 times on average. The run-time of RANSAC is also presented in Table <ref type="table" target="#tab_6">4</ref>, and we see that SparseVFC is much more efficient than RANSAC. To prevent the efficiency of RANSAC from decreasing drastically, usually a maximum sampling number is preset in the literature. We set the maximum sampling number to 5000.</p><p>The influence of different choices of basis functions for sparse approximation is also tested on this dataset. Besides the random sampling, we consider three other different methods: (i) simply use ffiffiffiffi ffi M p Â ffiffiffiffi ffi M p uniform grid points over the bounded input space; (ii) find M clustering center of the training inputs via k-means clustering algorithm; (iii) pick M basis functions minimizing the residuals via sparse greedy matrix approximation <ref type="bibr" target="#b16">[17]</ref>. The average precision-recall pairs of these three methods are (98.58%, 97.79%), (98.57%, 97.75%) and (98.58%, 97.79%), respectively. We see that all the three approximation methods produce almost the same result as the random sampling method. Therefore, for the mismatch removal problem, it does not seem that selecting the "optimal" subset using sophisticated methods improves the performance compared to a random subset. However, in the interests of computational efficiency, we may be better off simply choosing a random subset of the training data.</p><p>Next, we study on using different kernel functions for robust learning. Two additional matrix-valued kernels are tested, such as a decomposable kernel <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_47">Γðx i ; x j Þ ¼ e -β∥x i -x j ∥ 2 A with A ¼ ω1 DÂD þð1-ωDÞI DÂD ,</formula><p>and a convex combination of the divergence-free kernel Γ df and curl-free kernel Γ cf . Note that the form kernel will degenerate into a diagonal kernel when we set ω ¼ 0. In our evaluation, the combination coefficients in these two kernels are selected via cross-validation. The results are summarized in Table <ref type="table" target="#tab_7">5</ref>. We see that SparseVFC approximate VFC quite well under different matrix-valued kernels. Moreover, using a diagonal decomposable kernel is adequate for the mismatch removal problem; it can reduce the complexity of the linear system, i.e. Eq. ( <ref type="formula" target="#formula_42">32</ref>), while with only a negligible decrease in accuracy.</p><p>Finally, we use this dataset to investigate the influence of the choice of M, the number of basis functions. Besides the default value 15, three additional values of M including 5, 10 and 20 are tested, and the results are summarized in Table <ref type="table" target="#tab_8">6</ref>. We see that SparseVFC is not very sensitive to the choice of M, and even M¼5 can achieve a satisfied performance. It should be noted that better approximate accuracy can be achieved by choosing M adaptively. For example, first compute the standard deviation of the estimated inliers s n (i.e. Eq. ( <ref type="formula" target="#formula_46">35</ref>)) under different choices of M, and then choose the one with the smallest value of s n . However, such scenario would significantly increase the computational cost, since it requires to run the algorithm once for each choice of M. Therefore, fixing the value of M to a constant large enough, i.e. 15 in this paper, will achieve a good trade-off between the approximate accuracy and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Results on 3D surface pairs</head><p>Since VFC is not influenced by the dimension of the input data, now we test the mismatch removal performance of SparseVFC on 3D surface pairs and compare it to the original algorithm. Here the parameters are set as the same values as in the 2D case.</p><p>The two surface pairs Dino and Temple are shown in Fig. <ref type="figure">9</ref>. As shown, the capability of SparseVFC is not weakened in this case. For the Dino dataset, there are 325 initial correspondences with    264 correct matches and 61 mismatches; after using the Spar-seVFC to remove mismatches, 263 matches are preserved and all of which are correct matches. That is to say, all the false matches are eliminated while discarding only 1 correct match. For the Temple dataset, there are 239 initial correspondences with 215 correct matches and 24 mismatches; after using the SparseVFC to remove mismatches, 216 matches are preserved, in which 214 are correct matches-that is, 22 of 24 false matches are eliminated while discarding only 1 correct match. Performance comparison are presented in Table <ref type="table">7</ref>, we see that both FastVFC and SparseVFC have a good approximation to the original VFC algorithm. Therefore, the sparse approximation used in our method works quite well, not only in the 2D case, but also in the 3D case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we study a sparse approximation algorithm for vector-valued regularized least-squares. It searches a suboptimal solution under an assumption that the solution space can be represented sparsely with much less basis function. The time and space complexities of our algorithm are both linear in the scale of training samples, and the number of basis functions is manually assigned. We also present a new robust vector field learning method called SparseVFC, which is a sparse approximation to VFC, and apply it to solving the mismatch removal problem. The quantitative results on various experimental data demonstrate that the sparse approximation leads to a vast increase in speed with negligible decrease in performance, and it outperforms several state-of-the-art methods such as RANSAC from the perspective of mismatch removal.</p><p>Our sparse approximation addresses a generic problem for vector field learning, and it can be applied to other methodologies which can be converted into the vector field learning problem, for example, the Coherent Point Drift algorithm <ref type="bibr" target="#b40">[41]</ref> designed for point registration. The sparse approximation of the Coherent Point Drift algorithm is described in detail in Appendix. Besides, our approach also has some limitations, for example, the sparse approximation is validated only in the case of ℓ 2 loss. Our future work shall focus on (i) determining the basis number automatically and efficiently and (ii) validating the sparse approximation under different types of loss functions.</p><p>And in the M-step, for recovering the displacement function f, it needs to solve a linear system similar to Eq. <ref type="bibr" target="#b9">(10)</ref>, which takes up most of the run-time and memory requirements of the algorithm. The sparse approximation again could be used here to reduce the time and space complexity.</p><p>In the iteration of CPD algorithm, the displacement function f can be estimated from minimizing an energy function as</p><formula xml:id="formula_48">EðfÞ ¼ 1 2s 2 ∑ L l ¼ 1 ∑ K k ¼ 1 p kl ∥x l -y k -fðy k Þ∥ 2 þ λ 2 ∥f∥ 2 H ;<label>ð36Þ</label></formula><p>where p kl is the posterior probability of the correspondence between two points y k and x l , and s is the standard deviation of the GMM components.</p><p>Using the sparse approximation, we search a suboptimal f s with the form Eq. ( <ref type="formula" target="#formula_14">12</ref>). Due to the choice of a diagonal decomposable kernel Γðy i ; y j Þ ¼ e -β∥y i -y j ∥ 2 I, Eq. ( <ref type="formula" target="#formula_48">36</ref>) becomes</p><formula xml:id="formula_49">EðCÞ ¼ 1 2s 2 ∑ L l ¼ 1 ∥ðdiagðP Á;l Þ⊗I DÂD Þ 1=2 ðX K l -Y-ŨCÞ∥ 2 þ λ 2 C T ΓC;<label>ð37Þ</label></formula><p>where P ¼ fp kl g and P Á;l denotes the l-column of P, kernel matrix Γ is a M Â M block matrix with the ði; jÞ-th block Γð ỹi ; ỹj Þ, Ũ is a K Â M block matrix with the ði; jÞ-th block Γðy i ; ỹj Þ, X K l ¼ ðx l ; …; x l Þ is a KD Â 1 dimensional vector, and C ¼ ðc 1 ; …; c M Þ is the coefficient vector.</p><p>Taking the derivative of Eq. ( <ref type="formula" target="#formula_49">37</ref>) with respect to the coefficient vector C and setting it to zero, we obtain a linear system</p><formula xml:id="formula_50">ðU T diagðP1ÞU þ λs 2 ΓÞ C ¼ U T P X-U T diagðP1Þ Ỹ;<label>ð38Þ</label></formula><p>where the kernel matrix Γ∈R MÂM and Γ ij ¼ e -β∥ ỹ i -ỹ j ∥ 2 , U∈R KÂM and U ij ¼ e -β∥y i -ỹ j ∥ 2 , 1 is a column vector of all ones, C ¼ ðc 1 ; …; c M Þ T , X ¼ ðx 1 ; …; x L Þ T and Ỹ ¼ ðy 1 ; …; y K Þ T . Here M is the number of bases.</p><p>Thus we obtain a suboptimal solution from the coefficient matrix C. This corresponds to the optimal solution f o , i.e. Eq. ( <ref type="formula" target="#formula_9">8</ref>), with the coefficient matrix C determined by the linear system <ref type="bibr" target="#b40">[41]</ref> ðΓ þ λs 2 diagðP1Þ</p><formula xml:id="formula_51">-1 Þ C ¼ diagðP1Þ -1 PX-Y;<label>ð39Þ</label></formula><p>where Γ∈R KÂK with Γ ij ¼ e -β∥y i -y j ∥ 2 , and C ¼ ðc 1 ; …; c K Þ T . Since it is a sparse approximation of the CPD algorithm, we name this approach SparseCPD. We simply summarize the Spar-seCPD algorithm in Algorithm 2. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic illustration of motion field introduced by image pairs. Left: an image pair and its putative matches; right: motion field samples introduced by the point matches in the left figure. 1 and Â indicate feature points in the first and second images, respectively.</figDesc><graphic coords="6,111.08,637.19,234.10,85.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, and several image pairs of non-rigid objects. The images in the first dataset are either of planar scenes or the camera position was fixed during acquisition. Therefore, the images are always related by a homography. The test data of Tuytelaars et al. contains several wide baseline image pairs. The image pairs of non-rigid object are made in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Experiments for choosing the sample point number M used for sparse approximation. (a) The standard deviation of the estimated inliers s n with respect to M. (b) The test error with respect to M. The inlier number in the training set is set to 500 and 800, and the inlier percentage is varied among 20%, 50% and 80%.</figDesc><graphic coords="8,134.26,380.56,317.04,155.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Reconstruction of field learning shown in Fig. 2a via SparseVFC. Left: 200 inliers contained in the training set; right: 500 inliers contained in the training set. The inlier ratios are both 0.5. The means of test error is about 0.072 and 0.044, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance comparison on run-time under the experimental setup in Fig. 5b. (a) Run-time of SparseVFC. (b) Run-time speedup of FastVFC and SparseVFC with respect to VFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results on image pairs of Tree, Valbonne, Mex, DogCat, Peacock and T-shirt. The first three rows are wide baseline image pairs, and the rest three are image pairs of nonrigid object. The columns show the iterative mismatch removal progress, and the level of the blue color indicates to what degree a sample belongs to inlier. For visibility, only 50 randomly selected matches are presented in the first column. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="10,60.26,58.67,464.94,381.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Experimental results on the dataset of Mikolajczyk et al. (a) Cumulative distribution function of original correct match percentage. (b) Cumulative distribution function of number of point matches in the image pairs. (c) Precision-recall statistics for RANSAC, MLESAC, ICF and SparseVFC on the dataset of Mikolajczyk. Our method (red circles, upper right corner) has the best precision and recall overall. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2 .</head><label>2</label><figDesc>The SparseCPD Algorithm. Input: Two point sets X and Y Output: Transformation T 1 Parameter initialization, including M and all the parameters in CPD 2 Update the coefficient vector C by solving linear system ð38Þ Update other parameters in CPD 8 until converges; 9 The transformation T is determined according to the coefficient vector C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,130.74,58.64,324.00,188.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>variation, and G corresponds to H N in our problem. □ From this upper bound, we can easily derive that to achieve an approximation accuracy ϵ, i.e. Φðf</figDesc><table /><note><p>s Þ-Φðf o Þ≤ϵ, the needed minimal number of basis functions satisfies M≤α ϵ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in VFC. Our SparseVFC algorithm is outlined in Algorithm 1. Training set S ¼ fðx n ; y n Þ : n∈N N g, matrix kernel Γ, regularization constant λ, basis function number M Output: Vector field f, inlier set I 1 Initialize a, γ, V ¼ 0 DNÂ1 , P ¼ I NÂN , and s 2 by equation (28) 2 Randomly choose M basis functions from the training inputs and construct kernel matrix Γ</figDesc><table><row><cell cols="2">Algorithm 1. The SparseVFC Algorithm.</cell></row><row><cell cols="2">3 repeat</cell></row><row><cell>4</cell><cell>E-step :</cell></row><row><cell>5</cell><cell></cell></row><row><cell>6</cell><cell></cell></row><row><cell>7</cell><cell></cell></row><row><cell>8</cell><cell></cell></row><row><cell>9</cell><cell></cell></row></table><note><p><p><p>Input:</p>Update P ¼ diagðp 1 ; …; p N Þ</p>by equation ð27Þ M-step : Update C by solving linear system ð31Þ Update V by using equation ð12Þ Update s 2 and γ by equations ð28Þandð29Þ 10 until Q converges 11 Vector field f is determined by equation (12) 12 The inlier set is I ¼ fn : p n 4 τ; n∈N N g with τ being a predefined threshold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>The numbers of matches and initial inlier percentages in the six image pairs.</figDesc><table><row><cell></cell><cell>Tree</cell><cell>Valbonne</cell><cell>Mex</cell><cell>DogCat</cell><cell>Peacock</cell><cell>T-shirt</cell></row><row><cell>No. of matches</cell><cell>167</cell><cell>126</cell><cell>158</cell><cell>113</cell><cell>236</cell><cell>300</cell></row><row><cell>Inlier pct. (%)</cell><cell>56.29</cell><cell>54.76</cell><cell>51.90</cell><cell>82.30</cell><cell>71.61</cell><cell>60.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Performance comparison with different mismatch removal algorithms. The pairs in the table are precision-recall pairs (%).</figDesc><table><row><cell></cell><cell>RANSAC [57]</cell><cell>MLESAC [44]</cell><cell>ICF [47]</cell><cell>VFC [9]</cell><cell>FastVFC [9]</cell><cell>SparseVFC</cell></row><row><cell>Tree</cell><cell>(94.68, 94.68)</cell><cell>(98.82, 89.36)</cell><cell>(92.75, 68.09)</cell><cell>(94.85, 97.87)</cell><cell>(94.79, 96.81)</cell><cell>(94.85, 97.87)</cell></row><row><cell>Valbonne</cell><cell>(94.52, 100.00)</cell><cell>(94.44, 98.55)</cell><cell>(91.67, 63.77)</cell><cell>(98.33, 85.51)</cell><cell>(98.33, 85.51)</cell><cell>(98.33, 85.51)</cell></row><row><cell>Mex</cell><cell>(91.76, 95.12)</cell><cell>(93.83, 92.68)</cell><cell>(96.15, 60.98)</cell><cell>(96.47, 100.00)</cell><cell>(96.47, 100.00)</cell><cell>(96.47, 100.00)</cell></row><row><cell>DogCat</cell><cell>-</cell><cell>-</cell><cell>(92.19, 63.44)</cell><cell>(100.00, 100.00)</cell><cell>(100.00, 100.00)</cell><cell>(100.00, 100.00)</cell></row><row><cell>Peacock</cell><cell>-</cell><cell>-</cell><cell>(99.12, 66.86)</cell><cell>(99.40, 98.82)</cell><cell>(99.40, 98.22)</cell><cell>(99.40, 98.82)</cell></row><row><cell>T-shirt</cell><cell>-</cell><cell>-</cell><cell>(99.07, 58.79)</cell><cell>(98.88, 96.70)</cell><cell>(98.84, 93.41)</cell><cell>(98.88, 96.70)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Average precision-recall and run-time comparison of RANSAC, VFC, FastVFC and SparseVFC on the dataset of Mikolajczyk.</figDesc><table><row><cell></cell><cell>RANSAC [57]</cell><cell>VFC [9]</cell><cell>FastVFC [9]</cell><cell>SparseVFC</cell></row><row><cell>(p, r)</cell><cell>(95.49, 97.55)</cell><cell>(98.57, 97.75)</cell><cell>(98.75, 96.71)</cell><cell>(98.57, 97.78)</cell></row><row><cell>t (ms)</cell><cell>3784</cell><cell>6085</cell><cell>402</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Performance comparison by using different matrix-valued kernels for robust learning. DK: decomposable kernel; DFK+CFK: combination of divergence-free and curl-free kernels.</figDesc><table><row><cell></cell><cell>DK, ω ¼ 0</cell><cell>DK, ω≠0</cell><cell>DFK+CFK</cell></row><row><cell>VFC [9]</cell><cell>(98.57, 97.75)</cell><cell>(98.66, 97.91)</cell><cell>(98.69, 97.65)</cell></row><row><cell>SparseVFC</cell><cell>(98.57, 97.78)</cell><cell>(98.66, 97.93)</cell><cell>(98.67, 97.71)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Performance comparison by choosing different numbers of basis functions.</figDesc><table><row><cell>M</cell><cell>5</cell><cell>1 0</cell><cell>1 5</cell><cell>2 0</cell></row><row><cell>(p, r)</cell><cell>(98.10, 96.88)</cell><cell>(98.57, 97.73)</cell><cell>(98.57, 97.78)</cell><cell>(98.57, 97.79)</cell></row><row><cell>t (ms)</cell><cell>12</cell><cell>15</cell><cell>21</cell><cell>49</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the point set f xm : m∈N M g may not be a subset of the input training data fxn : n∈N N g. J. Ma et al. / Pattern Recognition 46 (2013) 3519-3532</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J.Ma  et al. / Pattern Recognition 46 (2013) 3519-3532</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors gratefully acknowledge the financial supports from the National Natural Science Foundation of China (Nos. 61273279, 60903096, and 61222308) and China Scholarship Council (No. 201206160008). Xiang Bai was supported by the Program for New Century Excellent Talents in University. Zhuowen Tu was supported by NSF award IIS-0844566 and NSF award IIS-1216528.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>None</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Sparse approximation for coherent point drift</head><p>The Coherent Point Drift (CPD) algorithm <ref type="bibr" target="#b40">[41]</ref> considers alignment of two point sets as a probability density estimation problem. Given two point sets</p><p>T with D being the data point's dimension, the algorithm assumes the points in Y are the Gaussian mixture model (GMM) centroids, and the points in X are the data points generated by the GMM. It then estimates the transformation T which yields the best alignment between the transformed GMM centroids and the data points by maximizing a likelihood. There the transformation T is defined as an initial position plus a displacement function f: TðyÞ ¼ y þ fðyÞ, where f is assumed to come from an RKHS H and hence has the form of Eq. ( <ref type="formula">8</ref>).</p><p>Similar to our SparseVFC algorithm, the CPD algorithm also adopts an iterative EM algorithm to alternatively recover the spatial transformation and update the point correspondence.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Regularization networks and support vector machines</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularized least-squares classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Learning Theory: Methods, Model and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ridge regression learning algorithm in dual variables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="515" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-Output Learning via Spectral Filtering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT Computer Science and Artificial Intelligence Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A robust method for vector field learning with application to mismatch removing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2977" to="2984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On learning vector-valued functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="177" to="204" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Near-optimal signal recovery from random projections: universal encoding strategies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building support vector machines with reduced classifier complexity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1493" to="1515" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A direct method for building sparse kernel learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="603" to="624" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse approximation through boosting for learning large scale kernel machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="883" to="894" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A sparse regularized least-squares preference learning algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Tenth Scandinavian Conference on Artificial Intelligence</title>
		<meeting>the Conference on Tenth Scandinavian Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse greedy matrix approximation for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="911" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RSVM: reduced support vector machines</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient SVM training using low-rank kernel representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="243" to="264" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An equivalence between sparse approximation and support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1455" to="1480" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An interior-point method for large-scale ℓ 1 Àregularized least squares</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<title level="m">Efficient Implementation of Gaussian Processes</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Department of Physics, Cavendish Laboratory, Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods: Support Vector Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast dual algorithm for kernel logistic regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent training for L1-regularized log-linear models with cumulative penalty</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><surname>Toigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Applications</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="408" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approximation methods for gaussian process regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Ramussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-Scale Kernel Machines</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="203" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="608" to="613" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-dimensional approximation by neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kůrková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Learning Theory: Methods, Models and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="69" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of worst case errors in linear and neural network approximation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kůrková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanguineti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="264" to="275" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with generalization capability by kernel methods of bounded complexity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kůrková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanguineti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="350" to="367" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust multi-task learning with t-processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1103" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Predictive matrix-variate t models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1721" to="1728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Point set registration: coherent point drift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2262" to="2275" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MLESAC: a new robust estimator with application to estimating image geometry</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Outlier correction from uncalibrated image sequence using the triangulation method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Guided sampling via weak motion models and outlier sample generation for epipolar geometry estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="275" to="288" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rejecting mismatches by correspondence function</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mismatch removal via coherent spatial mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust estimation of nonrigid transformation for point set registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matching widely separated views based on affine invariant regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="61" to="85" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">VLFeat-an open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual ACM International Conference on Multimedia</title>
		<meeting>the 18th Annual ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Surface feature detection and description with applications to mesh matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Divergence-Free and Curl-Free Vector Fields with Matrix-Valued Kernels</title>
		<author>
			<persName><forename type="first">I</forename><surname>Macêdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Instituto Nacional de Matematica Pura e Aplicada</title>
		<meeting><address><addrLine>Brasil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with application to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">He is currently a Ph. D. student of the Institute for Pattern Recognition and Artificial Intelligence, HUST. His research interests are in the areas of computer vision and machine learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Wuhan, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics, Huazhong University of Science and Technology (HUST)</orgName>
		</respStmt>
	</monogr>
	<note>Jiayi Ma received the B.S. degree from the</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Currently he is a PhD candidate at the Institute for Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology (HUST</orgName>
		</respStmt>
	</monogr>
	<note>Ji Zhao received his BS degree in 2005 and his MS degree in 2008. His research area is computer vision</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Jinwen Tian received his PhD degree in pattern recognition and intelligent systems in 1998 from Huazhong University of Science and Technology (HUST), China</title>
		<imprint/>
	</monogr>
	<note>He is a professor and PhD supervisor of pattern recognition and artificial intelligence at HUST. His main research topics are remote sensing image analysis. wavelet analysis, image compression, computer vision, and fractal geometry</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">He is currently an Assistant Professor with the Laboratory of Neuro Imaging (LONI), Department of Neurology</title>
	</analytic>
	<monogr>
		<title level="m">Zhuowen Tu received the M.E. degree from Tsinghua University, Beijing, China, and the Ph.D. degree from</title>
		<editor>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S D</forename><surname>Ph</surname></persName>
		</editor>
		<meeting><address><addrLine>Wuhan, China; Philadelphia, PA; Columbus; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2005. 2009. January 2006 to May 2007. October 2007 to October 2008</date>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology (HUST) ; Department of Computer Science and Information, Temple University ; Ohio State University ; Department of Computer Science, University of California, Los Angeles (UCLA)</orgName>
		</respStmt>
	</monogr>
	<note>He is also affiliated with the UCLA Bioengineering Interdepartmental Program, the UCLA. Before joining LONI, he was a Member of Technical Staff with Siemens Corporate Research and a Postdoctoral Fellow with the Department of Statistics, UCLA. Prof. Tu was a recipient of the David Marr Prize in 2003</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
