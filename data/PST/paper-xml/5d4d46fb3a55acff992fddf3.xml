<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Grassland Degradation Estimation Using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiyu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>jiangy@sz.tsinghua.edu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<orgName type="laboratory">PCL Research Center of Networks and Communications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Baidu</settlement>
									<country>Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chunmei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computer Technology and Applications</orgName>
								<orgName type="institution">Qinghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<orgName type="laboratory">PCL Research Center of Networks and Communications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<orgName type="laboratory">PCL Research Center of Networks and Communications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Dong</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computer Technology and Applications</orgName>
								<orgName type="institution">Qinghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<email>zhengf@sustech.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Grassland Degradation Estimation Using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grassland degradation estimation is essential to prevent global land desertification and sandstorms. Typically, the key to such estimation is to measure the coverage of indicator plants. However, traditional methods of estimation rely heavily on human eyes and manual labor, thus inevitably leading to subjective results and high labor costs. In contrast, deep learning-based image segmentation algorithms are potentially capable of automatic assessment of the coverage of indicator plants. Nevertheless, a suitable image dataset comprising grassland images is not publicly available. To this end, we build an original Automatic Grassland Degradation Estimation Dataset (AGDE-Dataset), with a large number of grassland images captured from the wild. Based on AGDE-Dataset, we are able to propose a brand new scheme to automatically estimate grassland degradation, which mainly consists of two components. 1) Semantic segmentation: we design a deep neural network with an improved encoder-decoder structure to implement semantic segmentation of grassland images. In addition, we propose a novel Focal-Hinge loss to alleviate the class imbalance of semantics in the training stage. 2) Degradation estimation: we provide the estimation of grassland degradation based on the results of semantic segmentation. Experimental results show that the proposed method achieves satisfactory accuracy in grassland degradation estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, massive grassland ecosystem has undergone degradation because of climatic variations and overgrazing, thus resulting in multifarious ecological problems, such as desertification and sandstorms <ref type="bibr" target="#b10">[Zhan et al., 2017]</ref>. Therefore, how to estimate the stage of grassland degradation accurately is of top priority for protecting grassland ecosystem from desertification.</p><p>The emergence of indicator plants is an important sign of grassland degradation <ref type="bibr" target="#b12">[Zhao et al., 2004]</ref>. Many countries have successfully used specific plant species as indicators for estimating grassland degradation <ref type="bibr" target="#b7">[Mansour et al., 2016;</ref><ref type="bibr" target="#b7">Mansour et al., 2012]</ref>. Our case study of degrading grassland in Qinghai-Tibet Plateau demonstrates that as the grassland degrades, the coverage of Stellera chamaejasme (SC) gradually accumulates. Thus, SC is regarded as the indicator plants for grassland degradation. Specifically, grassland would go through five degradation stages before desertification, with the coverage of SC building up in each stage <ref type="bibr" target="#b12">[Zhao et al., 2004]</ref>, as shown in Table <ref type="table" target="#tab_0">1</ref>. Thus, it is intuitive to estimate the grassland degradation stage based on the coverage of SC. However, existing methods rely heavily on observations of human eyes and manual labor, thus leading to subjective results and high labor costs, which is undesirable in practice. Consequently, there is an urgent need for developing an effective and efficient method to automatically estimate the grassland degradation stage without human any interactions.</p><p>To do this, we attempt to leverage deep learning to calculate automatically the coverage of SC in real-world grassland images based on a semantic segmentation algorithm, and then estimate the stage of grassland degradation by the coverage of SC based on the results of recognition. Many challenges stand in the way of achieving an automatic estimation of degradation. First, existing public datasets <ref type="bibr" target="#b7">[Mottaghi et al., 2014;</ref><ref type="bibr" target="#b2">Cordts et al., 2016;</ref><ref type="bibr" target="#b1">Caesar et al., 2018;</ref><ref type="bibr" target="#b9">Ros et al., 2016]</ref> contain substantially insufficient grassland images and thus fail to provide us with enough samples to train the network. In addition, the aerial or satellite images used in the studies of remote sensing and environmental sciences <ref type="bibr" target="#b10">[Wang et al., 2018]</ref> are not high-resolution enough to capture such a tiny target as SC.Moreover, due to the particularity of the grassland scene, capturing images with semantic class imbalance is inevitable. Finally, existing semantic segmentation networks cannot handle directly the complex task with these challenges.</p><p>To this end, we first design a deep neural network to implement semantic segmentation that could accurately segment the foreground (SC) from the background (grassland  <ref type="bibr">and Srebro, 2005]</ref>, and propose an original Focal-Hinge loss function. Next, we calculate the coverage of SC in the grassland area through the analysis of the results from semantic segmentation of grassland images and accordingly determine the degradation stage according to the relationship between stage and coverage (Table <ref type="table" target="#tab_0">1</ref>). Through these two steps, we manage to automatically estimate grassland degradation based on deep learning. To the best of our knowledge, we are the first to leverage deep learning techniques to solve ecological problems regarding grassland ecosystem. To be more specific, we propose a brand new scheme for grassland degradation estimation using semantic segmentation by a deep neural network. Moreover, we design a Focal-Hinge loss function to train the proposed network for addressing the problem of class imbalance. Experiments of our scheme on the Automatic Grassland Degradation Estimation Dataset (AGDE-Dataset) are carried out and reveal satisfying estimation results, which substantiate the feasibility and prospect to solve the problem of automating grassland degradation estimation leveraging deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Plant Identification</head><p>Image-based plant identification is one of the most promising solutions towards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et al., 2011;</ref><ref type="bibr">Goëau et al., 2016]</ref>.</p><p>Although we need to identify indicator plants for grassland degradation, our task is much more demanding. First, the deep learning-based plant recognition of these researches are more of image classification. However, in our task concerning automatic grassland degradation estimation, we also need to tell the spatial information like the locations and areas of them in an image. In addition, these algorithms only recognize plants in an image with discernible plants and background. However, the indicator plants often lurk in the vast expanse of grassland, making themselves indistinguishable in an grassland image.</p><p>With this regard, to precisely figure out the proportion of the indicator plants in an image, we propose a semantic segmentation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Plant Density Estimation</head><p>In recent years, there has been a lot of research into plant density estimation based on image processing <ref type="bibr" target="#b7">[Liu et al., 2017a;</ref><ref type="bibr" target="#b7">Liu et al., 2017b;</ref><ref type="bibr" target="#b4">Jin et al., 2017]</ref>. For example, <ref type="bibr" target="#b7">[Liu et al., 2017b]</ref> takes wheat plant images by a high-resolution RGB camera and train Artificial Neural Networks with 10 manually extracted features to estimate the number of plants. <ref type="bibr" target="#b4">[Jin et al., 2017]</ref> captures images by a UAV and train a Support Vector Machine with 13 hand-crafted features to identify wheat plant.</p><p>In fact, plant density estimation entails the quantification of the plant within a given unit area, which is highly biased by plant distribution. However, plant coverage refers to a relative area covered by the plant species in a plot, the calculation of which is more complex than that of quantification, since the area would not necessarily scale with the quantity of plant. In addition, both <ref type="bibr" target="#b7">[Liu et al., 2017b]</ref> and <ref type="bibr">[Jin et al., 2017]</ref> treat the density estimation problem as object classification, which might work with manually extracted features. In contrast, our task is based on semantic segmentation, where hand-engineered features are not feasible, so we automatically extract features with the designed deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Segmentation</head><p>Semantic segmentation necessitates object classification at the pixel level. Recently, there are many fabulous semantic segmentation models such as <ref type="bibr">FCN [Long et al., 2015]</ref>, SegNet <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref>, <ref type="bibr">DeepLab-v3 [Chen et al., 2018a]</ref>, PSPNet <ref type="bibr" target="#b12">[Zhao et al., 2017]</ref> and etc. Leong first proposed Fully Convolutional Networks (FCN) <ref type="bibr" target="#b7">[Long et al., 2015]</ref> that is a convolutional network for dense prediction without a fully-connected layer. This model makes it possible to segment images at any size effectively, and it is much faster than traditional methods based on patch classification. However, an obtrusive problem using convolutional neural networks for semantic segmentation is that pooling layers enlarge the receptive field, aggregating contextual information while discarding location information. Therefore, in order to solve this problem, an encoder-decoder architecture is devised. The encoder gradually reduces the spatial dimensions by pooling layers, while the decoder restores the target details and spatial dimensions step by step. Among such architectures, U-Net <ref type="bibr" target="#b8">[Ronneberger et al., 2015]</ref> is a very efficient one, whose semantic segmentation model employs the architecture of encoder-decoder based on a fully convolutional neural network. At present, semantic segmentation is widely applied to the geographic information system, unmanned vehicles <ref type="bibr" target="#b7">[Menze and Geiger, 2015]</ref>, medical image analysis <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>, robots and etc. Endowed with the power of the designed encoder-decoder deep network, we manage to figure the coverage of the indicator plants in grassland images by the results of semantic segmentation. we create a labeled dataset -Automatic Grassland Degradation Estimation Dataset (AGDE-Dataset).</p><p>First of all, we capture a large number of images from grasslands on the Qinghai-Tibet Plateau, from which we sift 2,895 images and scale them down. The sizes of the resized images range from 5KB to 38KB.</p><p>Next, we manually label pixels belonging to each of the five semantic categories -grassland (background), SC, sky, water, and road -for every image with an open annotation tool -LabelMe <ref type="bibr" target="#b10">[Russell et al., 2008]</ref>. Due to the limitation on the plateau, there are inevitably quantitative differences in different semantic categories in the dataset. The number of these five semantic categories are 2,895, 2,888, 156, 48 and 32 respectively. In addition, the degradation stage for each image in AGDE-Dataset is labeled according to the coverage of SC (Table <ref type="table" target="#tab_0">1</ref>) by calculating the proportion of the area of SC in images, which will be articulated in Section 4.2. We show several labeled examples of our dataset in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Finally, we randomly divide the dataset into a training set and a test set in the ratio of 2,095:800, which is detailed in Table <ref type="table" target="#tab_1">2</ref>. All these RGB images are eventually padded to the resolution of 256 × 341. Although the dataset is not very big, the amount of the training set is enough for our network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>We address the problem of automatic grassland degradation stage estimation following two steps: 1) semantic segmentation: designing a deep network to implement semantic segmentation for grassland images, and training it using the proposed novel Focal-Hinge loss function; 2) degradation estimation: figuring out the coverage of SC according to the semantic segmentation results and further estimating the degra- dation stage of grassland.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Segmentation for Grassland Scene</head><p>Network Architecture</p><p>The proposed network is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the refined cross connections as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The encoder network consists of 5 encoder convolution groups. Each encoder in the encoder network contains several Convolution, Batch Normalization, and ReLU (Conv + BN + ReLU) layers with stride 1. Following that, max pooling with a 2 × 2 kernel and stride 2 (non-overlapping window) is performed. Therefore, the input image is totally down-sampled 2 5 (32) times through the encoder network. Symmetrically, the decoder network also contains 5 decoder up-sampling groups. Each decoder contains 1 deconvolution layer and 3 Conv + BN + ReLU layers with stride 1. The 5 decoders upsample the last encoder to 32 times, so the size of the entire network output is equal to that of the input image.</p><p>In addition, in order to enrich the representation of the encoder, we connect symmetrically the feature maps of the decoder layers to the encoder layers by refined convolution groups. For example, the feature maps of the 4th encoder are connected to the 5th decoder by the 4th refined cross connection unit, and the feature maps of the 1st encoder are connected to the 2nd decoder by the 1st refined cross connection unit. The cross connections utilize low-level features and prevent gradient disappearance in the underlying gradient. Each of refined convolution groups contains 3 Convolution layers, Batch Normalization, ReLU, and Dropout (Conv + BN + ReLU + Dropout) layers with stride 1 <ref type="bibr">[He et al., 2016]</ref>, which is similar to that in DenseNet <ref type="bibr" target="#b3">[Huang et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>We train the network on AGDE-Dataset (Section 3). However, we face the challenge of the segmentation of the 5 imbalanced classes -SC, sky, road, water, and background, with SC predominating. The problem of class imbalance is very common in semantic segmentation. In order to alleviate this problem, we devise a novel loss function -Focal-Hinge loss as the objective function for training the network. </p><note type="other">Conv+BN+ReLU Pooling Deconvolution Conv+BN+ReLU+Dropout Refine Cross Connection</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Focal-Hinge Loss</head><p>The following of this subsection details how the loss function is derived. First, in order to classify each pixel in an image, we consider the Cross Entropy loss a sigmoid activation function</p><formula xml:id="formula_0">p t (y pix ) = 1 1 + exp (−y pix ) ,<label>(1)</label></formula><p>where y pix is a heatmap pixel value of the network output, and p t (y pix ) is the probability of ground truth class. The Cross Entropy loss is represented as</p><formula xml:id="formula_1">CE(p t ) = − log(p t ).<label>(2)</label></formula><p>However, CE(p t ) with sigmoid is not capable of correctly classifying some pixels into the minority semantic classes in the training set, due to the class imbalance problem faced by many classic semantic segmentation models <ref type="bibr" target="#b7">[Long et al., 2015;</ref><ref type="bibr" target="#b3">Huang et al., 2015;</ref><ref type="bibr" target="#b12">He et al., 2017]</ref>. Considering class imbalance and easy sample overwhelming, we substitute the classic Cross Entropy loss with a new Focal Loss <ref type="bibr" target="#b6">[Lin et al., 2017]</ref> to reduce the weight of easy samples. In this way, during the training process, the model focuses more on hard samples. Focal Loss is represented as</p><formula xml:id="formula_2">FL(p t ) = −(1 − p t ) γ log(p t ),<label>(3)</label></formula><p>where γ is a focusing parameter. If γ &gt; 0, the relative loss for correctly-classified examples (p t &gt; 0.5) would decrease. Nevertheless, we find that although FL(p t ) with sigmoid in Eq. ( <ref type="formula" target="#formula_2">3</ref>) alleviates the problem of the easy sample overwhelming, the scoring on hard samples is still far from satisfying because the scores of two classes are too close to each other. In other words, the boundaries of segmentation results are not clear-cut, with one class mixed with another. In this regard, naturally, we would consider the smoothed Hinge loss <ref type="bibr">[Rennie and Srebro, 2005]</ref>, as shown in Eq. ( <ref type="formula" target="#formula_3">4</ref>), which is always utilized in maximizing the classification interval in SVM, to make the final score more discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HL(y</head><formula xml:id="formula_3">pix ) =      1 2 − t • y pix if t • y pix 0, 1 2 (1 − t • y pix ) 2 if 0 &lt; t • y pix &lt; 1, 0 if t • y pix 1, (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where t stands for the ground truth class to which y pix corresponds.</p><p>Nonetheless, the smoothed Hinge loss itself does not tackle the problem of class imbalance. Therefore, considering these two problems − insufficient class distance and class imbalance, we propose a Focal-Hinge loss (FH) as</p><formula xml:id="formula_5">FH(y pix ) =      N t (1 − p t (y pix )) γ ( 1 2 − t • y pix ) if t • y pix 0, 1 2 N t (1 − p t (y pix )) γ (1 − t • y pix ) 2 if 0 &lt; t • y pix &lt; 1, 0 if t • y pix 1, (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where N t (1 − p t ) γ represents the FL with a sigmoid activation function, and N t denotes the reciprocal of the number of classes t in one image, and γ denotes the hyper-parameter. FH is the combination of FL with sigmoid and smoothed HL. On the one hand, for the part of FL with sigmoid, the network output can be normalized to [0,1], so that the classes with more samples can receive a severer penalty based on the probability. On the other hand, the use of smoothed HL ensures that a larger score distance is obtained. We experimentally substantiate the effectiveness of proposed FH compared with CE and FL with sigmoid for alleviating class imbalance in Section 5.1. The back propagation gradient of Focal-Hinge loss (FH) is</p><formula xml:id="formula_7">∂ FH ∂ y pix =              N t (1 − p t ) γ (t − 1 2 γ • p t − γ • t • p t • y pix ), if t • y pix 0, N t (1 − p t ) γ (t • y pix − 1)[t + 1 2 γ • p t (1 − t • y pix )], if 0 &lt; t • y pix &lt; 1, 0, if t • y pix 1,<label>(6)</label></formula><p>where p t represents the function p t (y pix ). With this brand new loss function, we manage to alleviate the effect of class imbalance while keeping the distance between different classes large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimating Grassland Degradation Stage</head><p>Inputting a grassland image x into the fully trained deep network, the output image y with the results of semantic segmentation is obtained. However, to finish the grassland degradation stage estimation, we further process the image semantic segmentation results. First, we calculate the coverage of SC in grassland images. Second, we estimate the grassland degradation stage according to the corresponding relationship between the coverage and the stage (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define of the Coverage of SC</head><p>There are totally 5 semantic categories: background, SC, sky, water, and road in AGDE-Dataset. To obtain the coverage of SC (Cvg SC ), we calculate the proportion of the areas of SC and grassland background in one image, which is represented as</p><formula xml:id="formula_8">Cvg SC = A SC A im − A s − A r − A w ,<label>(7)</label></formula><p>where A SC denotes the area of SC in the image and A im denotes the area of the entire image. The areas of sky, road, and water are represented by A s , A r , and A w respectively. The denominator (A im − A s − A r − A w ) stands for the area of grassland background. Given the semantic segmentation results, A SC , A s , A r , and A w are obtained easily by quantifying pixels in an image. Thus, we could acquire the Cvg SC on the grassland by Eq. ( <ref type="formula" target="#formula_8">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of Degradation Stage</head><p>We construct a mapping relationship between SC coverage and the degradation stage (Table <ref type="table" target="#tab_0">1</ref>) <ref type="bibr" target="#b12">[Zhao et al., 2004]</ref> to obtain the degradation estimation. In this way, we accomplish the automatic estimation of grassland degradation stage leveraging deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to demonstrate the effectiveness of our proposed scheme, we evaluate the results from two aspects. First, we evaluate the performance of semantic segmentation. To be more specific, we compare the Focal-Hinge loss function with other loss functions. In addition, we also showcase the competence of our network with comparison to other classic semantic segmentation networks -FCN <ref type="bibr" target="#b7">[Long et al., 2015]</ref>, SegNet <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type="bibr" target="#b2">[Chen et al., 2018b]</ref>. Second, to test the performance of our scheme on grassland degradation stage estimation, we show its success rate on stage estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of Semantic Segmentation Implementation Details and Evaluation Criteria</head><p>We set the parameters of networks -FCN-8s, FCN-16s, FCN-32s, SegNet, and DeepLab-v3 -according to that specified in their original papers. Besides, γ in Eq. ( <ref type="formula" target="#formula_5">5</ref>) and Eq. ( <ref type="formula" target="#formula_7">6</ref>) are set to 2. The size of the input images is padded to 256 × 341, which is the largest size of images in the dataset all experiments are conducted on a GTX1080Ti. More detailed experimental parameters are specified in Supplementary Materials.</p><p>The evaluation metrics are the Pixel Accuracy (PA), Mean Pixel Accuracy (MPA), Intersection over Union (IoU) and Mean Intersection over Union (MIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Focal-Hinge Loss</head><p>Under the same conditions, using the proposed network, we juxtapose our Focal-Hinge loss with the baseline ones: Sigmoid + Cross Entropy loss (CE) and Sigmoid + Focal Loss (FL). The evaluation results are shown in Table <ref type="table" target="#tab_3">3</ref>. We can see that the Focal-Hinge loss (FH) achieves remarkably better performance, in terms of MPA and MIoU. It is notable that FH achieves satisfactory results on minority semantics.</p><p>Therefore, the results demonstrate that Focal-Hinge loss function outperforms other losses in semantic segmentation of grassland images and effectively alleviates the problem of class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of the proposed network</head><p>Employing the Focal-Hinge loss, we further compare the semantic segmentation performance of our network with that of <ref type="bibr">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017], and</ref><ref type="bibr">DeepLab-v3 [Chen et al., 2018b]</ref>. The evaluation results are shown in Table <ref type="table" target="#tab_4">4</ref>. Results show that our network, designed with a lighter structure, outperforms other networks. In addition, the proposed method outperforms FCN and SegNet on minority semantics (road and water).</p><p>The visual results are displayed in Figure <ref type="figure" target="#fig_2">3</ref>. From the results, we can see that our method outperforms other deep networks, especially on minority semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Degradation Stage Estimation</head><p>We evaluate the performance of grassland degradation estimation on the test set of AGDE-Dataset, which covers 800 images with 5 degradation stages (Table <ref type="table" target="#tab_1">2</ref>). The success rate δ i c in each stage i is calculated by the ratio of the number of correctly estimated images N i c to the total number of images N i t in the stage:</p><formula xml:id="formula_9">δ i c = N i c N i t , i ∈ {I, II, III, IV, V}.<label>(8)</label></formula><p>Accordingly, the error rate δ i j e of our scheme in each stage i mistaken as stage j is calculated by:    where N i j e represents the number of images in stage i mistaken as stage j, and N i t − N i c represents the total number of mistaken images of stage i. The success rates and error rates of the test set of AGDE-Dataset are shown in Table <ref type="table" target="#tab_5">5</ref>, from which we can see that the success rates on each degradation stage are remarkably satisfactory and the main error estimations tend to occur in two adjacent stages.</p><formula xml:id="formula_10">δ i j e = N i j e N i t − N i c , j ∈ {I, II, III, IV, V} \ {i},<label>(9)</label></formula><p>The visual results of automatic grassland degradation estimation are shown in Figure <ref type="figure" target="#fig_3">4</ref>. We can see that the segmentation results and predicted stage label approximate the ground truth.</p><p>6 Discussion and Future Work</p><p>The main contribution of this paper is to provide a scheme to achieve automatic grassland degradation estimation leveraging deep learning. Specifically, we design a deep network especially for semantic segmentation of grassland images. In addition, due to the insufficiency of grassland image samples in public datasets, we capture a large number of grassland images and build a labeled grassland dataset named AGDE-Dataset. Moreover, as for the problem of class imbalance in the dataset, we devise a new Focal-Hinge loss function. Then we calculate the coverage of indicator plants for degradation using the results of semantic segmentation of grassland images and accordingly determine the degradation stage by the mapping of between coverage and stage. Experimental results on AGDE-Dataset indicate that the proposed deep learn- ing based method achieves an remarkably satisfactory result regarding automatic grassland degradation estimation. We hope that our model will be embraced at early date by grassland preservers on automatic estimation of grassland degradation stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The labeled AGDE-Dataset with ground truth of semantic segmentation and the information of degradation stage. Cov SC in label information represents the coverage of SC in the grassland image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the network architecture in this paper. Each of the 5 encoder layers implements down-sampling with several convolution layers and max pooling, and then each of the 5 encoder layers performs up-sampling with several deconvolution layers. The feature maps of the decoder layers are connected to the corresponding that of encoder layers by refined convolution groups, each of which contains Conv + BN + ReLU + Dropout layers with stride 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visual results of semantic segmentation comparing to other deep networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visual results of semantic segmentation and grassland degradation estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The relationship between stage of grassland degradation and coverage of Stellera chamaejasme (SC). and other elements in a grassland scene) of the grassland image at the pixel level. Aiming at the problem of sample insufficiency, we create a labeled dataset for automatic grassland degradation estimation with ground-level grassland images captured from Qinghai-Tibet Plateau. To alleviate the problem of class imbalance, we combine the advantages of reducing the class imbalance in Focal Loss<ref type="bibr" target="#b6">[Lin et al., 2017]</ref> and increasing class distance in smoothed Hinge loss [Rennie</figDesc><table><row><cell cols="2">Degradation Stage Coverage of SC</cell></row><row><cell>I</cell><cell>0%-19%</cell></row><row><cell>II</cell><cell>20%-39%</cell></row><row><cell>III</cell><cell>40%-59%</cell></row><row><cell>IV</cell><cell>60%-79%</cell></row><row><cell>V</cell><cell>80%-95%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The number of images of each stage in AGDE-Dataset.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="5">Number of Images in Each Stage</cell></row><row><cell></cell><cell>I</cell><cell>II</cell><cell>III</cell><cell>IV</cell><cell>V</cell><cell>Total</cell></row><row><cell cols="7">Train Set 295 500 500 500 300 2,095</cell></row><row><cell>Test Set</cell><cell cols="5">100 200 200 200 100</cell><cell>800</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of different loss functions using proposed network measured by PA and IoU.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PA</cell><cell></cell><cell></cell><cell>MPA</cell><cell cols="2">IoU</cell><cell>MIoU</cell></row><row><cell></cell><cell>SC</cell><cell>sky</cell><cell>road</cell><cell>water</cell><cell>SC</cell><cell>sky</cell><cell>road</cell><cell>water</cell></row><row><cell>FCN -32s</cell><cell cols="8">0.454 0.722 0.284 0.229 0.338 0.432 0.720 0.238 0.202</cell><cell>0.318</cell></row><row><cell>FCN -16s</cell><cell cols="8">0.455 0.767 0.311 0.297 0.366 0.433 0.763 0.253 0.258</cell><cell>0.341</cell></row><row><cell>FCN -8s</cell><cell cols="2">0.495 0.792</cell><cell cols="2">0.326 0.336</cell><cell cols="4">0.389 0.466 0.766 0.258 0.283</cell><cell>0.369</cell></row><row><cell>SegNet</cell><cell>0.463</cell><cell>0.801</cell><cell cols="3">0.345 0.208 0.391 0.453</cell><cell>0.780</cell><cell cols="2">0.253 0.265</cell><cell>0.370</cell></row><row><cell>DeepLab-v3</cell><cell cols="8">0.551 0.742 0.422 0.363 0.421 0.459 0.743 0.299 0.298</cell><cell>0.370</cell></row><row><cell>Ours</cell><cell cols="5">0.543 0.757 0.439 0.388 0.426 0.499</cell><cell>0.753</cell><cell cols="2">0.303 0.303</cell><cell>0.372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of different neural networks using Focal-Hinge loss function measured by PA and IoU.</figDesc><table><row><cell>Stage</cell><cell>Success Rate</cell><cell>I</cell><cell>II</cell><cell cols="2">Error Rate III IV</cell><cell>V</cell><cell>other</cell></row><row><cell>I</cell><cell>90%</cell><cell>-</cell><cell>100%</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>II</cell><cell>88%</cell><cell>17%</cell><cell>-</cell><cell>83%</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>III</cell><cell>96%</cell><cell>0</cell><cell>25%</cell><cell>-</cell><cell>75%</cell><cell>0</cell><cell>0</cell></row><row><cell>IV</cell><cell>85%</cell><cell>0</cell><cell>0</cell><cell>37%</cell><cell>-</cell><cell>63%</cell><cell>0</cell></row><row><cell>V</cell><cell>84%</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>38%</cell><cell>-</cell><cell>62%</cell></row><row><cell>Avg</cell><cell>89%</cell><cell>3%</cell><cell>25%</cell><cell>24%</cell><cell>23%</cell><cell>13%</cell><cell>12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Success rates and error rates of automatic grassland degradation stage estimation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The AGDE-DatasetSince existing open image datasets contain inadequate grassland images, and even fewer datasets would cover SC images, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Proceedings of the Twenty-Eighth Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">Proceedings the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by the National Natural Science Foundation of China under Grant 61771273, the R&amp;D Program of Shenzhen under Grant JCYJ20180508152204044, the research fund of PCL Future Regional Network Facilities for Large-scale Experiments and Applications (PCL2018KP001), and the Program for University Key Laboratory of Guangdong Province (Grant No. 2017KSYS008).</p><p>Special thanks for the support of the Research Program of Science and Technology Department of Qinghai Province (Grant No. 2016-ZJ-774), and also for Prof. Li Chunmei and the students from Qinghai University: Dong Shuo, Pi Wei, and Li Zhao, they have made a great contribution to the collection and annotation of the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for scene segmentation</title>
		<author>
			<persName><surname>Badrinarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PP</title>
		<imprint>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Antoine Vacavant, and Didier Coquin. A parametric active polygon for leaf segmentation and shape estimation</title>
		<author>
			<persName><forename type="first">Caesar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2018. 2018. 2011. 2011</date>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
	<note>ISVC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Labs of the Evaluation forum</title>
				<meeting><address><addrLine>Georgia Gkioxari</addrLine></address></meeting>
		<imprint>
			<publisher>Kaiming He</publisher>
			<date type="published" when="2016">2018a. 2018. 2018b. 2018. 2016. 2016. 2016. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>CVPR. Piotr Dollar, and Ross Girshick. Mask r-cnn. TPAMI, PP(99</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Gao Huang, Zhuang Liu</publisher>
			<date type="published" when="2015">2015. 2015. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Densebox: Unifying landmark localization with end to end object detection</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Estimates of plant density of wheat crops at emergence from very low altitude uav imagery. Remote Sensing of Environment</title>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName><surname>Kebapci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hanife Kebapci, Berrin Yanikoglu, and Gozde Unal</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PP</title>
		<imprint>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="2999" to="3007" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminating indicator grass species for rangeland degradation assessment using hyperspectral data resampled to aisa eagle resolution</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sanja Fidler, Raquel Urtasun, and Alan Yuille</title>
				<editor>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nam</forename><forename type="middle">Gyu</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Seong</forename><forename type="middle">Whan</forename><surname>Lee</surname></persName>
		</editor>
		<imprint>
			<publisher>Menze and Geiger</publisher>
			<date type="published" when="2005">2017a. 2017. 2017b. 2017. 2015. 2015. 2012. 2012. 2016. 2016. 2015. 2015. 2014. 2014. 2005</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="180" to="186" />
		</imprint>
	</monogr>
	<note>IJCAI workshop</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">Ros</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of modeling grassland degradation with and without considering localized spatial associations in vegetation changing patterns</title>
		<author>
			<persName><forename type="first">Russell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="2008">2008. 2008. 2018. 2018. 2017. 2017</date>
			<publisher>Catena</publisher>
		</imprint>
	</monogr>
	<note>What is the main cause of grassland degradation? a case study of grassland ecosystem service in the middle-south inner mongolia</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mdnet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3549" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Study on vegetation community&apos;s structure of degraded grassland of noxious and miscellaneous grass type</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2004">2004. 2004. 2017. 2017</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
	<note>Pyramid scene parsing network</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
