<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-20">20 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
							<email>fulifeng93@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">• F. Feng and TS. Chua are with School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>Computing 1, Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">• F. Feng and TS. Chua are with School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>Computing 1, Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">• F. Feng and TS. Chua are with School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>Computing 1, Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">• F. Feng and TS. Chua are with School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>Computing 1, Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">X</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-20">20 Feb 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1902.08226v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial Training</term>
					<term>Graph-based Learning</term>
					<term>Graph Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (e.g., articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. Adversarial Training (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples. In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose Graph Adversarial Training (GAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GAT, we employ it on a state-of-the-art graph neural network model -Graph Convolutional Network (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GAT which outperforms normal training on GCN by 4.51% in node classification accuracy. Codes will be released upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-based learning makes predictions by accounting for both input features of examples and the relations between examples. It is remarkably effective for a wide range of applications, such as predicting the profiles and interests of social network users <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, predicting the role of a protein in biological interaction graph <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and classifying contents like documents, videos, and webpages based on their interlinks <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. In addition to the supervised loss on labeled examples, graph-based learning also optimizes the smoothness of predictions over the graph structure, that is, closely connected examples are encouraged to have similar predictions <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Recently, owing to the extraordinary representation ability, deep neural networks become prevalent models for graph-based learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>Despite promising performance, we argue that graph neural networks are vulnerable to small but intentional perturbations on the input features <ref type="bibr" target="#b12">[13]</ref>, and this could even be more serious than the standard neural networks that do not model the graph structure. The reasons are twofold: 1) graph neural networks also optimize the supervised loss on labeled data, thus it will face the same vulnerability issue as the standard neural networks <ref type="bibr" target="#b13">[14]</ref>, and 2) the additional smoothness constraint will exacerbate the impact of perturbations, since smoothing across connected nodes 1 would aggregate the impact of perturbations from nodes connected to the target node (i.e., the node that we apply perturbations with the aim of changing its prediction). Figure <ref type="figure">1</ref> illustrates the impact of perturbations on node features with an intuitive example of a graph with 4 nodes. A graph neural network model predicts node labels (3 in total) for clean input features and features with applied perturbations, respectively. Here perturbations are intentionally applied to the features of nodes 1, 2, 4. Consequently, the graph neural network model is fooled to make wrong predictions on nodes 1 and 2 as with standard neural networks. Moreover, by propagating the node embeddings, the model aggregates the influence of perturbations to node 3, from which its prediction is also affected. In real-world applications, small perturbations like the update of node features may frequently happen, but should not change the predictions much. As such, we believe that there is a strong need to stabilize the graph neural network models during training.</p><p>Adversarial Training (AT) is a dynamic regularization technique that proactively simulates the perturbations during the training phase <ref type="bibr" target="#b13">[14]</ref>. It has been empirically shown to be able to stabilize neural networks, and enhance their robustness against perturbations in standard classification tasks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Therefore, employing a similar approach to that of AT on a graph neural network model would also be helpful to the model's robustness. However, directly employing AT on graph neural network is insufficient, since Fig. <ref type="figure">1</ref>: An intuitive example to illustrate the impact of applying perturbations to the input node features on the prediction of graph neural networks. Here the model implements the graph smoothness constraint via propagating node embeddings over the graph. On the right, the model propagates the applied perturbations on the connected nodes of the target node 3, leading to a wrong prediction. Moreover, the perturbations on node 1 and 2 directly lead to the wrong associated predictions like in the standard neural networks.</p><p>it treats examples as independent of each other and does not consider the impacts from connected examples. As such, we propose a new adversarial training method, named Graph Adversarial Training (GAT), which learns to construct and resist perturbations by taking the graph structure into account.</p><p>The key idea of GAT is that, when generating perturbations on a target example, it maximizes the divergence between the prediction of the target example and its connected examples. That is, the adversarial perturbations should attack the graph smoothness constraint as much as possible. Then, GAT updates model parameters by additionally minimizing a graph adversarial regularizer, reducing the prediction divergence between the perturbed target example and its connected examples. Through this way, GAT can resist the worst-case perturbations on graphbased learning and enhance model robustness. To efficiently calculate the adversarial perturbations, we further devise a linear approximation method based on back-propagation.</p><p>To demonstrate GAT, we employ it on a well-established graph neural network model, Graph Convolutional Network (GCN) <ref type="bibr" target="#b6">[7]</ref>, which implements the smoothness constraint by performing embedding propagation. We study the method's performance on node classification, one of the most popular tasks on graph-based learning. Extensive experiments on three public benchmarks (two citation graphs and a knowledge graph) verify the strengths of GAT -compared to normal training on GCN, GAT leads to 4.51% accuracy improvement. Moreover, the improvements on less popular nodes (with a small degree) are more significant, highlighting the necessity of performing AT with the graph structure considered.</p><p>The main contributions of this paper are summarized as:</p><p>• We formulate Graph Adversarial Training, a new optimization method for graph neural networks that can enhance the model's robustness against perturbations on node input features. • We devise a graph adversarial regularizer that encourages the model to generate similar predictions on the perturbed target example and its connected examples, and develop an efficient algorithm to construct perturbations.</p><p>• We demonstrate the effectiveness of GAT on GCN, con-ducting experiments on three datasets which show that our method achieves state-of-the-art performance for node classification. Codes will be available to facilitate the community. In the remainder of this paper, we first discuss related work in Section 2, followed by the problem formulation and preliminaries in Section 3. In Section 4 and 5, we elaborate the method and experimental results, respectively. We conclude the paper and envision future directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss the existing research on graphbased learning and adversarial learning, which are closely related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-based Learning</head><p>Graph, a natural representation of relational data, in which nodes and edges represent entities and their relations, is widely used in the analysis of social networks, transaction records, biological interactions, collections of interlinked documents, web pages, and multimedia contents, etc.. On such graphs, one of the most popular tasks is node classification targeting to predicting the label of nodes in the graph by accounting for node features and the graph structure. The existing work on node classification mainly fall into two broad categories: graph Laplacian regularization and graph embedding-based methods. Methods lying in the former category explicitly encode the graph structure as a regularization term to smooth the predictions over the graph, i.e., the regularization incurs a large penalty when similar nodes (e.g., closely connected) are predicted with different labels <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, graph embedding-based methods, which learn node embeddings that encodes the graph data, have become promising solution. Most of embedding-based methods fall into two broad categories: skip-gram based methods and convolution based methods, depending on how the graph data are modeled. The skip-gram based methods learn node embeddings via using the embedding of a node to predict node context that are generated by performing random walk on the graph so as the embeddings of "connected" nodes are associated to each other <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Inspired by the idea of convolution in computer vision, which aggregates contextual signals in a local window, convolution based methods iteratively aggregate representation of neighbor nodes to learn a node embedding <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>.</p><p>In both of the two categories, methods leveraging the advanced representation ability of deep neural networks (neural graph-based learning methods) have shown remarkably effective in solving the node classification task. However, the neural graph-based learning models are vulnerable to intentionally designed perturbations indicating the unstability in generalization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and little attention has been paid to enhance the robustness of these methods, which is the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Adversarial Training</head><p>In order tackle the vulnerability to intentional perturbations of deep neural networks, researchers proposed adversarial training which is an alternative minimax process <ref type="bibr" target="#b24">[25]</ref>. The adversarial training methods augment the training process by dynamically generating adversarial examples from clean examples with perturbations maximally attacking the training objective, and then learn over these adversarial examples by minimizing an additional regularization term <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b30">[31]</ref>. The adversarial training methods mainly fall into supervised and semi-supervised ones regarding the target of the training objective. In supervised learning tasks such as visual recognition <ref type="bibr" target="#b13">[14]</ref>, supervised loss <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and its surrogates <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> over adversarial examples are designed as the target of the maximization and minimization. For semi-supervised learning where partial examples are labeled, divergence of predictions for inputs around each examples is adopted as the target. Generally speaking, the philosophy of adversarial training methods is to smooth the prediction around individual inputs in a dynamical fashion.</p><p>Our work is inspired by these adversarial training methods. In addition to the local smoothness of individual examples, our method further accounts for relation between examples (i.e., the graph structure) in the target of the minimax process so as to learn robust classifiers predicting smoothly over the graph structure. To the best of our knowledge, this is the first attempt to incorporate graph structure in adversarial training.</p><p>Another emerging research topic related to our work is generating adversarial perturbations attacking neural graph-based learning models where <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b12">[13]</ref> are the only published work. However, methods in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b12">[13]</ref> are not suitable for constructing adversarial examples in graph adversarial training. This is because these methods generate a new graph as the adversarial example for each individual node, i.e., they would generate N graphs when the number of nodes is N leading to unaffordable memory overhead. In this work, we devise an efficient method to generate adversarial examples for graph adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Generative Adversarial Networks</head><p>Generative adversarial networks (GAN) is a machine learning framework with two different networks as a generator and a discriminator playing minimax game on generating and detecting fake examples. Recently, several GAN-based models are proposed to learn graph embeddings, which either generate fake nodes and edges to augment embedding learning <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> or smooth the leaned embeddings to follow a prior distribution <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>. However, using two different networks inevitably doubles the computation of model training and the labor of parameter tuning of GAN-based methods. Moreover, for different applications, one may need to build GAN from scratch, whereas our method is a generic solution can be seamlessly applied to enhance the existing graph neural network models with less computing and tuning overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We first introduce some notations used in the following sections. We use bold capital letters (e.g. X) and bold lowercase letters (e.g. x) to denote matrices and vectors, respectively. Note that all vectors are in a column form if not otherwise specified, and X ij denotes the entry of matrix X at the row i and column j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Representation</head><p>The nodes and edges of a graph represent the entities of interest and their relations, respectively. First, the edges in a graph with N nodes are typically represented as an adjacency matrix A ∈ R N ×N . In this work, we mainly study unweighted graphs where A is a binary matrix. A ij = 1 if there is an edge between node i and j, otherwise A ij = 0. Moreover, we use a diagonal matrix D ∈ R N ×N to denote the degrees of nodes, i.e., D ii = N j=1 A ij . For an attributed graph, where each node is associated with a feature vector, we use a matrix X = [x 1 , x 2 , • • • , x N ] T ∈ R N ×F to represent the feature vectors of all nodes, where F is the dimension of the features. Finally, an attributed graph is denoted as G = (A, D, X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Classification</head><p>On graph data, node classification is one of the most popular tasks. In the general problem setting of node classification, a graph G with N nodes is given, associated with labels (Y ) of a some portion of nodes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. This setting is transductive since testing nodes are observed (only features and associated edges) during training, and is the focus of this work. Here,</p><formula xml:id="formula_0">Y = [y 1 , y 2 , • • • , y M ] T ∈ R M×L</formula><p>are the labels, where M and L are the numbers of labeled nodes and node classes, respectively, and y i is the one-hot encoding of node i's label. Note that, without loss of generality, we index the labeled nodes and unlabeled nodes in the range of [1, M ] and (N − M, N ], respectively. The target of node classification is to learn a prediction function (classifier) ŷi = f (x i , G||×), to forecast the label of the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-based Learning</head><p>Graph-based learning methods have been shown remarkably effective on solving the node classification task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Generally, most of the models jointly optimize two objectives: 1) supervised loss on labeled nodes and 2) graph smoothness constraint, which can be summarized as:</p><formula xml:id="formula_1">Γ = Ω + λΦ,<label>(1)</label></formula><p>where Ω is a classification loss (e.g., log loss, hinge loss, and cross-entropy loss) that measures the discrepancy between prediction and ground-truth of labeled nodes. Φ encourages smoothness of predictions over the graph structure, which is based on the assumption that closely connected nodes tend to have similar predictions. For instance, Φ could be a graph Laplacian term,</p><formula xml:id="formula_2">N i,j=1</formula><p>A ij ŷi − ŷj 2 , which directly regulates the predictions of connected nodes to be similar <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The assumption could also be implicitly implemented by iteratively propagating node embeddings through the graph so that connected nodes obtain close embeddings and are predicted similarly <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Here, λ is a hyperparameter to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first introduce the formulation of graph adversarial training, followed by the introduction of GATV, an extension of GAT, which incorporates the virtual adversarial regularization <ref type="bibr" target="#b27">[28]</ref>. We then present two solutions for the node classification task, GCN-GAT and GCN-GATV, which employ GAT and GATV to train GCN <ref type="bibr" target="#b6">[7]</ref>, respectively. Finally, we analyze the time complexity of the two solutions and present the important implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Adversarial Training</head><p>Recent advances of adversarial training (AT) has been successful in learning deep neural network-based classifiers, making them robust against perturbations for a wide range of standard classification tasks such as visual recognition <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref> and text classification <ref type="bibr" target="#b15">[16]</ref>. Generally, applying AT would regulate the model parameters to smooth the output distribution. Specifically, for each clean example in the dataset, adversarial training encourages the model to assign similar outputs to the artificial input (i.e., the adversarial example) derived from the clean example. Inspired by the philosophy of standard AT, we develop graph adversarial training, which trains graph neural network modules in the manner of generating adversarial examples and optimizing additional regularization terms over the adversarial examples, so as to prevent the adverse effects of perturbations. Here the focus is to prevent perturbations propagated through node connections (as illustrated in Figure <ref type="figure">1</ref>), i.e., accounting for graph structure in adversarial training. Generally, the formulation of graph adversarial training is:</p><formula xml:id="formula_3">min: Γ GAT = Γ + β N i=1 j∈Ni D(f (x i + r g i , G|Θ), f (x j , G|Θ)), max: r g i = arg max ri, ri ≤ǫ j∈Ni D(f (x i + r i , G| Θ), f (x j , G| Θ)),<label>(2)</label></formula><p>where Γ GAT is the training objective function with two terms: the standard objective function of the origin graphbased learning model (e.g., Equation <ref type="formula" target="#formula_1">1</ref>) and graph adversarial regularizer. The second term encourages the graph adversarial examples to be classified similarly as connected examples where Θ denotes the parameters to be learned, and D is a nonnegative function that measures the divergence (e.g., Kullback-Leibler divergence <ref type="bibr" target="#b37">[38]</ref>) between two predictions. r g i denotes the graph adversarial perturbation, which is applied to the input feature of the clean example i to construct a graph adversarial example.</p><p>The graph adversarial perturbation is calculated by maximizing the graph adversarial regularizer under current value of model parameters. That is to say, the graph adversarial perturbation is the direction of changes on the input feature, which can maximally attack the graph adversarial regularizer, i.e., the worst case of perturbations propagated from neighbor nodes. ǫ is a hyperparameter controling the magnitude of perturbations, which is typically set as small values so that the feature distribution of adversarial examples is close to that of clean examples.</p><p>Generally, similar to the standard adversarial training, each iteration of GAT can also be viewed as playing a minimax game: As such, the model becomes robust against perturbations propagated through the graph. While the traditional graph-based regularizations (e.g., the graph Laplacian term) also encourage the smoothness of predictions over the graph structure, GAT is believed to be a more advanced regulation for two reasons: 1) the regularization performed by GAT is dynamic since the adversarial examples are adaptively generated according to the current parameters and predictions of the model whereas the standard graph-based regularizations are static; and 2) GAT to some extent augments the training data, since the generated adversarial examples have not occurred in the training data, which is beneficial to model generalization.</p><p>Approximation. It is non-trivial to obtain the closedform solution of r g i . Inspired by the linear approximation method proposed in <ref type="bibr" target="#b13">[14]</ref> for standard adversarial training, we also design a linear approximation method to calculate the graph adversarial perturbations in GAT, of which the formulation is:</p><formula xml:id="formula_4">r g i ≈ ǫ g g , where g = ∇ xi j∈Ni D(f (x i , G| Θ), f (x j , G| Θ)),<label>(3)</label></formula><p>where g is the gradient w.r.t. the input x i . For graph neural network models, the gradient can be efficiently calculated by one backpropagation. Note that Θ is a constant set denoting the current model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Virtual Adversarial Training</head><p>Considering that node classification is a task of semisupervised learning by nature, we further devise an ex-tended version of GAT (GATV), which additionally smooths the distribution of predictions around each clean example to further enhance the model robustness. Inspired by the idea of virtual adversarial training <ref type="bibr" target="#b27">[28]</ref>, we further add a virtual adversarial regularizer into the training objective function and construct virtual adversarial examples to attack the local smoothness of predictions. The formulation of GATV is:</p><formula xml:id="formula_5">min: Γ GAT V = Γ + α N i=1 D(f (x i + r v i , G|Θ), ỹi ) virtual adversarial regularizer + β N i=1 j∈Ni D(f (x i + r g i , G|Θ), f (x j , G|Θ))</formula><p>graph adversarial regularizer , max:</p><formula xml:id="formula_6">r v i = arg max r ′ i , r ′ i ≤ǫ ′ D(f (x i + r ′ i , G| Θ), ỹi ),<label>(4)</label></formula><p>where r ′ i denotes the virtual adversarial perturbation, the direction that leads to the largest change on the model prediction of x i . For labeled nodes and unlabeled nodes, ỹi denotes ground truth label and model prediction, respec- tively. That is,</p><formula xml:id="formula_7">ỹi = ŷi , i ≤ M (labeled node), f (x i , G| Θ), M &lt; i ≤ N (unlabeled node).</formula><p>Note that GATV can be seen as jointly playing two minimax games with three players, where the two maximum players Approximation. For labeled nodes, r ′ i can be easily evaluated via linear approximation <ref type="bibr" target="#b13">[14]</ref>, i.e., calculating the gradient of D(f (x i , G| Θ), ỹi ) w.r.t. x i . For unlabeled nodes, such approximation is infeasible since the gradient will always be zero. This is because D(f (x i , G| Θ), ỹi ) achieves the minimum value (0) at x i (note that ỹi = f (x i , G| Θ) for unlabeled data). Realizing that the first-order gradient is always zero, we estimate r ′ i from the second-order Taylor approximation of D(f</p><formula xml:id="formula_8">(x i + r ′ i , G| Θ), ỹi ). That is, r v i ≈ arg max r ′ i , r ′ i ≤ǫ ′ 1 2 r ′T i Hr ′ i where H is the Hessian matrix of D(f (x i + r ′ i , G| Θ), ỹi ).</formula><p>For the consideration of efficiency, we calculate r v i via the power iteration approximation <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_9">r v i ≈ ǫ ′ g g , where g = ∇ ri D(f (x i + r i , G| Θ, ỹi )) | ri=ξd ,<label>(5)</label></formula><p>where d is a random vector. Detailed derivation of the method is referred to <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Convolution Network</head><p>Inspired by the extraordinary representation ability, many neural networks have been used as the predictive model f (x i , G|Θ) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Under the transductive setting, Graph Convolutional Network <ref type="bibr" target="#b6">[7]</ref> is a state-of-the-art model. Specifically, GCN stacks multiple graph convolution layers, which is formulated:</p><formula xml:id="formula_10">H l = σ D − 1 2 A D − 1 2 H l−1 W l + b l .<label>(6)</label></formula><p>Specifically, the l-th graph convolution layer conducts three operations to project H l−1 ∈ R N ×D l−1 (the output of the (l − 1)-th layer or the node features X) into H l ∈ R N ×D l , where D l−1 and D l are the output dimension of layer l − 1 and l, respectively.</p><p>• Similar as the fully connected layer, the graph convolution layer first projects the input (H l−1 ) into latent representations with</p><formula xml:id="formula_11">W l ∈ R D l−1 ×D l and b l ∈ R D l . • It then propagates the latent representations (H l−1 W l +b l )</formula><p>through the normalizied adjacency matrix</p><formula xml:id="formula_12">D − 1 2 A D − 1 2</formula><p>with self-connections, where D = D + I and A = A + I (I ∈ R N ×N is an identity matrix). Here, the representation of node i in H is the aggregation of latent representations in (H l−1 W l + b l ) of nodes connected to i (including itself due to the self-connection). • Finally, a non-linear activation function σ (e.g., the sigmoid, hyperbolic tangent, and rectifier functions) is applied to allow non-linearity.</p><p>The original objective function of GCN is,</p><formula xml:id="formula_13">M i=1 cross-entropy(f (x i , G|Θ), y i ) + λ Θ 2 F ,<label>(7)</label></formula><p>where the second term is L 2 -norm to prevent overfitting.</p><p>To train GCN with our proposed GAT and GATV, we set the Γ term in Equation 2 and 4 as the cross-entropy loss in Equation <ref type="formula" target="#formula_13">7</ref>, which are minimized to update the parameter of GCN, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Time Complexity and Implementation</head><p>Time Complexity. As compared to GCN with standard training, the additional computation of GCN-GAT is twofold: 1) generating graph adversarial perturbations ({r g i , i &lt; N }) with Equation 3 and 2) calculating the value of graph adversarial regularizer (</p><formula xml:id="formula_14">N i=1 j∈Ni D(f (x i + r g i , G|Θ), f (x j , G|Θ))).</formula><p>Considering that they can be accomplished with a back-propagation and a forwardpropagation (to calculate f (x i + r g i , G|Θ)), the computation overhead of GCN-GAT is acceptable. Additionally, GCN-GATV computes virtual adversarial perturbations and virtual adversarial regularizer, which can be performed with one back-propagation and one forward-propagation, respectively <ref type="bibr" target="#b27">[28]</ref>. It indicates that the overhead of GCN-GATV is still acceptable <ref type="bibr" target="#b27">[28]</ref>. Running time comparison in Section 5.4.2 further demonstrate the efficiency of GCN-GAT and GCN-GATV.</p><p>Implementation. Noting that number of connected nodes varies a lot across the nodes in the graph, we sample K neighbors for each node to generate adversarial examples and calculate the graph adversarial regularizer to facilitate the calculation. Here, the following sampling strategies are considered:</p><p>• Uniform: neighbors are selected uniformly. • Degree: the probability of selecting a node is proportional to the normalized node degree.</p><p>• Degree-Reverse: on the contrary, the probability is the reciprocal of node degree (also normalized to sum to unity). • PageRank: it performs PageRank <ref type="bibr" target="#b38">[39]</ref> on the graph and takes the normalized pagerank score as the sampling probability. Note that advanced but complex sampling strategies (e.g., the one in <ref type="bibr" target="#b22">[23]</ref>) are not considered due to efficiency consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We follow the same experimental settings as in <ref type="bibr" target="#b6">[7]</ref> and conduct experiments on two types of node classification datasets: citation network datasets (Citeseer and Cora <ref type="bibr" target="#b39">[40]</ref>) and knowledge graph (NELL <ref type="bibr" target="#b11">[12]</ref>) 2 , of which the statistics are summarized in Table <ref type="table" target="#tab_2">1</ref>.</p><p>• In the citation networks, nodes and edges represent documents and citation links between documents, respectively. Note that the direction of edge is omitted since a citation is assumed to have equally impacts on the prediction of the associated two documents. Each document is associated with a normalized bag-of-words feature vector and a class label. During training, we use features of all nodes, but only 20 labels per class. 500 and 1,000 of the remaining nodes are used as validation and testing, respectively. • NELL is a bipartite graph of 55,864 relation nodes and 9,891 entity nodes, extracted from a knowledge graph which is a set of triplets in the format of (e 1 , r, e 2 ). Here e 1 and e 2 are entities, and r is the connected relation between them. Following <ref type="bibr" target="#b6">[7]</ref>, each relation r is split into two relation nodes (r 1 and r 2 ), from which two edges (e 1 , r 1 ) and (e 2 , r 2 ) are constructed. Entity nodes and relation nodes are described by bag-of-words feature vectors (normalized) and one-hot encodings, respectively. Note that we pad zero values to align the feature vectors of entity and relation nodes. Here only labels of entity nodes are availabe. During training, only 0.001 of entities under each class are labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines</head><p>We compare the following baselines:</p><p>• LP <ref type="bibr" target="#b7">[8]</ref>: Label propagation ignores node features and only propagates labels to unlabeled nodes with a graph Laplacian term.</p><p>• DeepWalk <ref type="bibr" target="#b4">[5]</ref>: DeepWalk is a skip-gram based method to learn graph embeddings, which uses the embedding 2. https://github.com/kimiyoung/planetoid.</p><p>of a node to predict node contexts that are generated by performing random walk on the graph. • SemiEmb <ref type="bibr" target="#b40">[41]</ref>: SemiEmb learns embeddings for nodes from node features and leverages Laplacian regularization to encourage connected nodes have close embeddings.</p><p>• Planetoid <ref type="bibr" target="#b11">[12]</ref>: Planetoid also learns node embeddings from input features but accounts for the graph structure in the fashion of DeepWalk, i.e., predicting node context. • GCN <ref type="bibr" target="#b6">[7]</ref>: GCN stacks two graph convolution layers to project node features into labels and propagates node representations and predictions over the graph structure to smooth the output.</p><p>• GraphSGAN <ref type="bibr" target="#b32">[33]</ref>: GraphSGAN is a semi-supervised generative adversarial network which encodes the density signal of the graph during generation of fake nodes. Since LP, DeepWalk, SemiEmb, and Planetoid are all baselines in the paper of GCN, we exactly follow their settings in <ref type="bibr" target="#b6">[7]</ref>. In addition, the setting of GraphSGAN is same as the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Parameter Settings</head><p>We implement GCN-GAT and GCN-GATV, which train GCN with different versions of graph adversarial training, respectively, with Tensorflow (the implementations are available via https://anonymous.com). In total, GCN-GAT has six hyperparameters: D 1 size of hidden layer (GCN), λ weight for L 2 -norm (GCN), dropout ratio (GCN), ǫ the scale of graph adversarial perturbations (GAT), β weight for graph adversarial regularizer (GAT), and K the number of sampled neighbors (GAT). For fair comparison and simplification, we set D 1 , λ as the optimal values of standard GCN. But we set dropout ratio as zero in GCN-GAT for stable training. For the remaining three parameters, ǫ, β, and K, we performed grid-search within the ranges of [0.01, 0.05, 0.1, 0.5, 1], [0.01, 0.05, 0.1, 0.5, 1, 5], <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, respectively, on the validation set.</p><p>Additionally, GCN-GATV has three more hyperparameters: ǫ ′ the scale of virtual adversarial perturbations, α weight for virtual adversarial regularizer as well as P and ξ in the approximation of virtual adversarial perturbations. Again, for simplification, we first set the other parameters with the optimal value of GCN-GAT, and empirically set P = 1 since previous work demonstrated that increasing P would not bring substantial improvements <ref type="bibr" target="#b27">[28]</ref>. We then perform grid-search within the ranges of [0.01, 0.05, 0.1, 0.5, 1], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5], [1e-6, 1e-5, 1e-4], respectively. It should be noted that the uniform strategy is adopted to sample neighbor nodes if not other specified.</p><p>The selected values for hyperparameters of both GCN-GAT and GCN-GATV would be released together with the implementation. Moreover, similar as standard GCN, we train the models via Adam <ref type="bibr" target="#b41">[42]</ref> with a learning rate of 0.01 and early stopping with a window size of 10, i.e. training stops if the validation loss does not decrease for 10 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Model Comparison</head><p>We first investigate how effective is the proposed graph adversarial training via comparing the performance of GCN- GATV with state-of-the-art node classification methods. Table <ref type="table" target="#tab_3">2</ref> shows the classification performance of the compared methods on the three datasets regarding accuracy. The performance of LP, DeepWalk, SemiEmb, and Planetoid are taken from the GCN paper <ref type="bibr" target="#b6">[7]</ref> since we exactly followed its settings. We employ the public implementation 3 of GCN with same settings as the origin paper to report its performance on Citeseer and Cora. For the performance of GCN on NELL, we tune its hyperparameters with grid search since the setting released in the GCN paper <ref type="bibr" target="#b6">[7]</ref> achieves performance (lower than 50.0) much worse than expected 4 .</p><p>In <ref type="bibr" target="#b32">[33]</ref> GraphSGAN is also evaluated on the Citeseer and Cora datasets with a similar setting, we hence directly copy the reported performance.</p><p>From the results, we have the following observations: • GCN-GATV significantly outperforms the standard GCN, exhibiting relative improvements of 6.35%, 1.47%, and 5.72% on the Citeseer, Cora, and NELL datasets, respectively. As the only difference between GCN-GATV and GCN is applying the proposed graph adversarial training, the improvements are attributed to the proposed training method which would enhance the stabilization and generalization of the standard GCN. Besides, the results justify that GCN-GATV is effective in solving the node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• GCN-GATV achieves comparable performance as</head><p>GraphSGAN, which is the state-of-the-art of the node classification task, demonstrating the efficacy of the proposed method. Moreover, our method is believed to be a more feasible solution for two reasons: 1) GraphSGAN is in the fashion of generative adversarial networks, which explicitly play a mini-max game between a discriminator and a generator (two different networks), inevitably doubling the computation of model training and the labor of parameter tuning. 2) For different applications, one may need to build GraphSGAN from scratch, whereas our GCN-GATV is a generic solution can be seamlessly applied to enhance the existing models of the applications. • GCN-GATV and GraphSGAN achieve better results in all the cases as compared to the other baselines. On the Citeseer, Cora, and NELL datasets, the relative improvements are at least 6.35%, 1.97%, and 4.52%, respectively. This indicates the effectiveness of adversarial learning, i.e., dynamically playing a mini-max game either implicitly (GCN-GATV) and explicitly (GraphSGAN) in the training phase. Moreover, the results are consistent with findings in previous work <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>• Among the baselines, 1) methods jointly account for the graph structure and node features (in the category of +Node Features) outperforms LP and DeepWalk that only consider graph structure. This suggests further exploration of how to combine the connection patterns and node features more appropriately. 2) As compared to SemiEmb, a shallow model, Planetoid and GCN achieves significant improvements (from 8.56% to 131.8%) in all cases. The improvement is reasonable and attributed to the strong representation ability of neural networks. It suggests that neural networks would be beneficial once node features are incorporated. As such, methods targeting to enhance the graph neural network models, such as the graph adversarial training, will be meaningful and influential in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance w.r.t. Node Degree</head><p>We then study how the graph adversarial training performs on nodes with different densities of connections so as to understand where this regularization technique is suitable for. We empirically split the nodes into three groups according to node degree (i.e., the number of neighbors), where node degrees are in ranges of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, [6, N ], respectively. Figure <ref type="figure">2</ref> illustrates the distribution of nodes in the three datasets over the groups. As can be seen, in all the three datasets, a great number of nodes are sparsely connected (i.e., with degrees smaller than three), and only about ten percent of the nodes are densely connected with degrees bigger than five. Note that we omit the distribution of testing nodes since they are randomly sampled from the whole node set and roughly follow the same distributions. By separately counting the accuracy of standard GCN and Fig. <ref type="figure">2</ref>: Percentage of nodes with degrees in different groups in the three datasets.</p><p>GCN-GATV over nodes in different groups, we obtain the group-oriented performance on the three datasets, which is depicted in Figure <ref type="figure" target="#fig_0">3</ref>. From the results, we observe that:</p><p>• In all the three datasets, both of GCN and GCN-GATV achieves the best performance on the group of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The relatively worse performance on the group of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> could be attributed to that the nodes in the group are sparsely connected and lacks enough signals propagated from neighbors, which are helpful for the classification <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In addition, we postulate the reason of the worse performance over nodes with degrees in [6, N ] as  such nodes are harder to be classified. This is because such nodes typically represent more general entities, for instance, an entity having connections to other entities with different types of relations might be a more general concept, and hard to be accurately classified into a specific category. • In most cases (except the [6, N ] group of Cora and NELL), GCN-GATV outperforms the standard GCN, which indicates that graph adversarial training would benefit the prediction of nodes with different degrees and is roughly not sensitive to the density of graph. For one of the exceptions (the [6, N ] group of NELL), we speculated that the reason is the under-fitting of standard GCN on such nodes (note that the performance of GCN on [6, N ] is averagely 27.7% worse than the other two groups), where additional regularization performed by graph adversarial training worsens it. Investigating the reason of the other exception (the [6, N ] group of Cora) is left to future work. • GCN-GATV significantly and consistently outperforms GCN on the group of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> across all three datasets, with an average improvement of 5.45%. The result indicates that the graph adversarial training would be more effective on sparse part of the graph. It should be noted that most of the graphs are sparse in real world applications <ref type="bibr" target="#b44">[45]</ref>. As such this result further demonstrates the potential of the proposed methods in solving more real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Method Ablation</head><p>Recall that we design two versions of graph adversarial training: 1) basic GAT (Equation <ref type="formula" target="#formula_3">2</ref>) and 2) incorporating virtual adversarial training (Equation <ref type="formula" target="#formula_6">4</ref>). To evaluate the contribution of these two types of regularizations, we compare the performance of the following solutions built upon GCN: Table <ref type="table" target="#tab_5">3</ref> shows the performance of compared methods on the three datasets w.r.t. accuracy. As can be seen:  be the reason why GCN-GAT outperforms standard GCN on NELL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Effect of Sampling Strategies</head><p>As mentioned in Section 4.4, different sampling strategies could be adopted to sample neighbor nodes for the generation of graph adversarial perturbations and the calculation of graph adversarial regularizer. Here, we investigate the effect of sampling strategies via comparing the results of GCN-GAT performing different samplings. It should be noted that we separately tune the hyperparameters of GCN-GAT when different samplings are employed. Table <ref type="table" target="#tab_6">4</ref> shows the corresponding performance, from which we can observe that the performance of different sampling strategies are comparable to each other. It indicates that the efficacy of GCN-GAT is not sensitive to sampling strategies, as such, Uniform would be a suitable selection since it will not bring any additional computation as compared to the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Sensitivity</head><p>We then investigate how the value of hyperparameters effects the performance of the proposed method. Given a hyperparameter, we evaluate model performance when adjusting its value and fixing the other hyperparameters with optimal values. Considering that effect of hyperparameters relevant to virtual adversarial training, the weight for virtual adversarial regularizer (α), the magnitude of virtual adversarial perturbations (ǫ ′ ), and the magnitude of input to calculate the perturbations (ξ), has been studied in previous work <ref type="bibr" target="#b27">[28]</ref>, we focus on the remaining ones: a) weight of graph adversarial regularizer (β), b) scale of graph adversarial perturbations (ǫ), and c) number of sampled neighbors (k), and used GCN-GAT to report the performance. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the performance of GCN-GAT on the validation and testing of the three datasets when varying the value of β, ǫ, and k. From the figures, we have the following observations:</p><p>• Under most cases, the performance of GCN-GAT changes smoothly near the optimal value of the selected hyperparameter, which indicates that GCN-GAT is not sensitive to hyperparameters. The only exception is that GCN-GAT performs significantly worse when k = 3 and k = 5 as compared to the performance with other values of k.</p><p>We check the training procedure and observe that both of them are caused by triggering early stopping at the early stage of the training (dozens of epochs), which is occasional and would converge to an expected performance if disable early stopping. • For individual parameter, a) GCN-GAT achieves best performance with k around 0.1, which roughly balance the contribution of the supervised loss and the graph adversarial regularizer (note that the supervised loss decreases fast in the early epochs). Larger value of k (stronger regularization) will harm GCN-GAT since the model could suffer from underfitting. b) GCN-GAT performs well when ǫ is in the range of [1e-4, 1e-2], but the performance decrease significantly as increasing ǫ. This justifies the assumption that perturbations have to be in small scale so that the constructed adversarial examples have similar feature distributions as real data. c) On all the three datasets, GCN-GAT performs best when k = 1 or k = 2, which is somehow coherent with the result in Figure <ref type="figure" target="#fig_0">3</ref> that graph adversarial training are more effective to nodes with degree in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The specific reasons of this result are left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Tuning ǫ Only</head><p>Considering that the number of candidate combinations exponentially increases with the number of hyperparameters, we explore whether comparable performance could be achieved when tune one hyperparameter alone and fix the others with empirical values. It should be noted that previous work <ref type="bibr" target="#b27">[28]</ref> has shown that tuning ǫ ′ alone could suffice for achieving satisfactory performance of VAT. Similarly, we tune ǫ with β = 1 and k = 1 and summarize the performance of GCN-GAT in Table <ref type="table" target="#tab_7">5</ref>. As can be seen, on the citation graphs, tuning ǫ alone achieves satisfactory performance, whereas the performance on NELL is not desirable. We find that the graph adversarial regularizer would get much larger value on the NELL dataset as compared to the other two citation datasets, which might caused by the larger number of classes (210 in NELL). By setting β = 0.01 and β = 0.1, which roughly balance the supervised loss and the regularizer, we obtain satisfactory performance when tune ǫ alone. Therefore, we would conclude that the hyperparameter search for only ǫ suffices for achieving satisfactory performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Graph Adversarial Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Training Process</head><p>By taking the basic version of graph adversarial training GCN-GAT as example, we then study the effect of GAT on the training process. Specifically, we observe the performance of GCN and GCN-GAT on the validation and testing of Citeseer and Cora, which is depicted in Figure <ref type="figure" target="#fig_4">5</ref>. Note that we omit the performance on NELL, which shows the same trend, for saving space. As can be seen, 1) On the two datasets, the performance of both GCN and GCN-GAT becomes stable after 100 epochs, which indicates that GAT will not affect the convergence speed of GCN. 2) It is interesting to see that the performance of GCN-GAT    increases faster than standard GCN during the initial several epochs. Considering that the supervised loss is typically much larger (about 1e5 times) than the value of graph adversarial regularizer in the initial epochs since all nodes are assigned predictions close to random leading to tiny divergence between connected nodes, the acceleration of performance increase is believed to be the effect data augmentation (additional adversarial examples) rather then the regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Training Time</head><p>Here, we discuss the overhead of graph adversarial training via comparing the training time of GCN, GCN-GAT, and GCN-GATV, of which the average times of 50 epochs are summarized in Table <ref type="table" target="#tab_8">6</ref>. It should be noted that we conduct the experiment on a server equipped with two Intel(R) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Robustness against Adversarial Perturbations</head><p>Recall that our target is to enhance the robustness of graph neural networks. Table <ref type="table" target="#tab_9">7</ref> shows relative performance decrease of GCN and GCN-GAT on adversarial examples as compared to clean examples. As can be seen, by training GCN with GAT, the model becomes less sensitive to adversarial perturbations. For example, on the citation graphs, graph adversarial perturbations in the scale of 0.01 (i.e., ǫ = 0.01) decreases accuracy of GCN by 13.7%, while the number is only 2.7% for GCN-GAT. It justifies that the graph adversarial training technique could enhance the robustness of the GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Effect of GAT on Divergence of Neighbor Nodes</head><p>We retrospect the intuition of the graph adversarial regularizer is to encourage connected nodes to be predicted similarly. Table <ref type="table" target="#tab_10">8</ref> shows the effect of applying GAT to train GCN, from which we can see that GAT reduces the divergence between connected as expected. These results verify that the predictions of GCN-GAT are more smooth over the graph structure, which indicates the trained model would be more robust and have stronger generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In In future, we will explore we are interested to explore the effectiveness of GAT on more graph neural network models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Moreover, we are interested to investigate the effect of GAT on other graph-based learning tasks such as link prediction and community detection. As focusing on graph-based learning with only one graph in this paper, one potential future work is to investigate the effectiveness of graph adversarial training for graph-based learning methods simultaneously handling multiple graphs. In addition, we are interested in testing the performance of graph adversarial training on graphs with specifical structures, for instance, hyper-graphs and heterogeneous information graphs. Moreover, we would like to incorporate techniques like robust optimization <ref type="bibr" target="#b45">[46]</ref> and adversarial dropout <ref type="bibr" target="#b46">[47]</ref> into the proposed method to further enhance its ability of stabilizing graph neural network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Performance comparison between GCN and GCN-GATV on nodes with different degrees in the Citeseer (a), Cora (b), and NELL (c) datasets.</figDesc><graphic url="image-3.png" coords="8,48.69,43.79,175.42,131.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>GCN: It learns the parameters of GCN standard training, optimizing Equation 7. • GCN-VAT: Virtual adversarial training, which performs perturbations by considering node features only, is employed to train GCN, i.e., optimizing Equation 4 with β = 0. • GCN-GAT: It trains GCN by the pure GAT, of which the perturbations only focus on only graph structure,i.e., optimizing Equation 4 with α = 0. • GCN-GATV: It accounts for both the virtual and graph adversarial regularizations during the training of GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>In most of the cases, GCN performs worse than the other approaches, which indicates that adversarial training could enhance the node classification model as compared to the standard training. That is, by intentionally and dynamically generating perturbations and optimizing additional regularizers, the trained model could by more accurate.• GCN-GATV achieves the best performance in all cases. It justifies that perturbations targeting on individual nodes (virtual adversarial perturbations) and connected nodes (graph adversarial perturbations) both benefits the training of graph neural network model. Moreover, it suggests joint consideration of node features and the graph structure in adversarial training on graph data. • Compared to GCN-VAT, GCN-GAT achieves improvements of 1.38% and 4.04% on the Citeseer and Cora datasets, which signifies the benefit of accounting for the graph structure in adversarial training of graph neural networks. However, on the NELL dataset, the performance of GCN-GAT is 1.58% worse than GCN-VAT, which is reasonable. We speculate that the decrease is mainly because NELL is a bipartite graph where the connected nodes of an entity node are all relation nodes without bag-of-words descriptions and predictions as meaningful as entity nodes. Therefore, as compared to standard graph with homogeneous nodes, the generated graph adversarial perturbations according to the predictions of connected relation nodes are less effective. It should be noted that, by resisting such perturbations, GAT still implicitly encourages smooth predictions of entity nodes connected by the same relation node, which could</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Performance of GCN-GAT with different values of hyperparameters: (a) weight of graph adversarial regularizer (β), (b) scale of graph adversarial perturbations (ǫ), and (c) number of sampled neighbors (k) on the validation and testing of the three datasets (When investigating the effect of a hyperparameter, the other two are set as the optimal values).</figDesc><graphic url="image-10.png" coords="10,175.49,232.99,134.12,100.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Training curves of the GCN and GCN-GAT on the validation and testing of Citeseer and Cora.</figDesc><graphic url="image-9.png" coords="10,48.69,232.99,134.12,100.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,51.84,43.52,505.81,124.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the experiment datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell>#Classes</cell><cell cols="2">#Features Label rate</cell></row><row><cell>Citeseer</cell><cell>3,312</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell>NELL</cell><cell>65,755</cell><cell>266,144</cell><cell>210</cell><cell>5,414</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Performance of the compared methods on the three datasets w.r.t. accuracy.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell cols="2">Citeseer Cora</cell><cell>NELL</cell></row><row><cell>Graph</cell><cell>LP DeepWalk</cell><cell>45.3 43.2</cell><cell>68.0 67.2</cell><cell>26.5 58.1</cell></row><row><cell>+Node Features</cell><cell>SemiEmb Planetoid GCN</cell><cell>59.6 64.7 69.3</cell><cell>59.0 75.7 81.4</cell><cell>26.7 61.9 61.2</cell></row><row><cell>+Adversarial</cell><cell>GraphSGAN GCN-GATV</cell><cell>73.1 73.7</cell><cell>83.0 82.6</cell><cell>-64.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>//github.com/tkipf/gcn/issues/14), we are not the first one struggling for reproducing the performance. And the author of the GCN paper suggests us to tune the hyperparameters by ourselves.</figDesc><table><row><cell>4. According</cell><cell>to</cell><cell>the</cell><cell>record</cell><cell>on</cell><cell>GitHub</cell></row><row><cell>(https:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3. https://github.com/tkipf/gcn.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Effect of graph adversarial regularization and virtual adversarial regularization.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Citeseer</cell><cell>Cora</cell><cell>NELL</cell></row><row><cell>Standard Training</cell><cell>GCN</cell><cell>69.3</cell><cell>81.4</cell><cell>61.2</cell></row><row><cell>Adversarial Training</cell><cell>GCN-VAT GCN-GAT GCN-GATV</cell><cell>72.4 73.4 73.7</cell><cell>79.3 82.5 82.6</cell><cell>63.3 62.3 64.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Performance comparison of GCN-GAT with different strategies to sample neighbors during adversarial example generation.</figDesc><table><row><cell cols="3">Sampling Strategy Citeseer Cora</cell><cell>NELL</cell></row><row><cell>Uniform</cell><cell>73.4</cell><cell>82.5</cell><cell>62.3</cell></row><row><cell>Degree</cell><cell>73.0</cell><cell>82.9</cell><cell>61.8</cell></row><row><cell>Degree-Reverse</cell><cell>73.8</cell><cell>82.4</cell><cell>62.1</cell></row><row><cell>PageRank</cell><cell>72.6</cell><cell>83.1</cell><cell>62.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Performance of GCN-GAT as tuning all hyperparameters (i.e., β, ǫ, and k) and tuning ǫ with fixed β = 1.0 and k = 1.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">Citeseer Cora</cell><cell>NELL</cell></row><row><cell>{β, ǫ, k}</cell><cell>73.4</cell><cell>82.5</cell><cell>62.3</cell></row><row><cell>{ǫ}</cell><cell>73.6</cell><cell>82.5</cell><cell>45.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">Citeseer</cell><cell></cell><cell>Cora</cell><cell cols="2">NELL</cell></row><row><cell></cell><cell>Time</cell><cell>Slower</cell><cell>Time</cell><cell>Slower</cell><cell>Time</cell><cell>Slower</cell></row><row><cell>GCN</cell><cell>0.042</cell><cell>-</cell><cell>0.024</cell><cell>-</cell><cell>1.170</cell><cell>-</cell></row><row><cell>GCN-GAT</cell><cell>0.489</cell><cell>11.7x</cell><cell>0.126</cell><cell>5.2x</cell><cell>4.620</cell><cell>3.9x</cell></row><row><cell cols="2">GCN-GATV 0.800</cell><cell>19.1x</cell><cell>0.216</cell><cell>8.9x</cell><cell>5.725</cell><cell>4.9x</cell></row></table><note>Average training time per epoch of GCN, GCN-GAT, and GCN-GATV w.r.t. seconds. Slower means how many times the method is slower than GCN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>The impact of adding graph adversarial perturbations to GCN and GCN-GAT. The number shows the relative decrease of accuracy.</figDesc><table><row><cell>Method</cell><cell>Citeseer</cell><cell>Cora</cell><cell>NELL</cell></row><row><cell>GCN</cell><cell>-21.1%</cell><cell cols="2">-6.6% -54.8%</cell></row><row><cell>GCN-GAT</cell><cell>-4.1%</cell><cell cols="2">-1.6% -53.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Average Kullback-Leibler divergence between connected node pairs calculated from predictions of GCN and GCN-GAT, of which small value indicates close predictions. We can see that adversarial training averagely decelerates the training of GCN 4.4 times on the NELL dataset, which is acceptable considering that each epoch still takes several seconds only. Besides, the additional computation on smaller datasets (i.e., Citeseer and Cora) is negligible since all the methods are much faster.</figDesc><table><row><cell>Method</cell><cell cols="2">Citeseer</cell><cell>Cora</cell></row><row><cell></cell><cell>Test</cell><cell>All</cell><cell>Test</cell><cell>All</cell></row><row><cell>GCN</cell><cell cols="2">0.132 0.137</cell><cell cols="2">0.345 0.333</cell></row><row><cell>GCN-GAT</cell><cell cols="2">0.127 0.130</cell><cell cols="2">0.308 0.299</cell></row><row><cell cols="2">Xeon(R) CPU E5-2620 V3.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>this work, we proposed a new learning method, named graph adversarial training, which additional accounts for relation between examples as compared to standard adversarial training. By iteratively generating adversarial examples attacking the graph smoothness constraint and learning over adversarial examples, the proposed method encourages the smoothness of predictions over the given graph, a property indicating good generalization of the model. As can be seen as a dynamic regularization technique, our method is generic to be applied to train most graph neural network models. We trained one well-established model, GCN, with the proposed method to solve the node classification task. By conducting experiments on three benchmark datasets, we demonstrated that training GCN with our method is remarkably effective, achieving an average improvement of 4.51%. Moreover, it also beats GCN trained with VAT, indicating the necessity of performing AT with graph structure considered</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-regularized deep multi-network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">New regularized algorithms for transductive learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="442" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning on partialorder hypergraphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-I. Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graphgan: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Aaane: Attention-based adversarial autoencoder for multi-scale network embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep network representations with adversarially regularized autoencoders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial network embedding</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Kullback-leibler divergence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Joyce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Alphascript Publishing</publisher>
			<biblScope unit="page">844</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab, Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial personalized ranking for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SI-GIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
