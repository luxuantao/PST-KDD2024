<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TurkDeck: Physical Virtual Reality Based on People</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lung-Pan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thijs</forename><surname>Roumen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hannes</forename><surname>Rantzsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sven</forename><surname>KÃ¶hler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Kovacs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Jasper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Kemper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TurkDeck: Physical Virtual Reality Based on People</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B9A8F127E7F598BD235B2FD9EC4F2B0</idno>
					<idno type="DOI">10.1145/2807442.2807467</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prop-based virtual reality; passive virtual reality H5</term>
					<term>2 [Information interfaces and presentation]: User Interfaces</term>
					<term>-Graphical user interfaces</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: TurkDeck is a prop-based virtual reality system that let's users not only (a) see and hear a virtual world, but also (b) feel it. Conceptually, the user is in a fully populated physical world. (c) In reality, however, TurkDeck's physical room is almost empty. "Human actuators" present and operate props only when and where the user can actually reach them. (d) By reusing generic props, TurkDeck minimizes the required props to what human actuators can carry, still allows producing virtual worlds of arbitrary size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Ever since its conception in the 1960's, head-mounted virtual reality systems have been primarily concerned with the user's visual senses <ref type="bibr" target="#b26">[28]</ref> and optionally spatial audio <ref type="bibr" target="#b0">[1]</ref>. As the next step towards realism and immersion, however, many researchers argue that the next sense such a system should support is the haptic sense, in order to convey the physicality of the virtual world <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In the past, researchers have pursued two different approaches. On the one hand, researchers use mechanical machinery, such as motion platforms <ref type="bibr" target="#b25">[27]</ref> and exoskeletons <ref type="bibr" target="#b2">[3]</ref> to apply forces to the user. While these approaches have been very successful at giving users the experience of walking, they are not well suited for recreating the experience of touching objects, such as grabbing a door handle or slamming against a wall. Researchers therefore proposed using physical props. Simple prop-based systems used a single hand-held prop (Ortega et al <ref type="bibr" target="#b18">[19]</ref>). The more elaborate systems supported "real walking" <ref type="bibr" target="#b28">[30]</ref> in a space where all walls were physical (with projection <ref type="bibr" target="#b13">[14]</ref> or head-mounted displays <ref type="bibr" target="#b9">[10]</ref>) allowing users to experience the full physicality of the room. Unfortunately, simulating one room worth of a virtual world using the prop-based approach requires one room worth of physical space, as even redirected walking allows reusing only isolated props <ref type="bibr" target="#b12">[13]</ref>. This makes prop-based approaches very space-inefficient, stationary, and limits the size of the virtual worlds they can render. In this paper, we present an approach to scaling prop-based virtual reality. We achieve this by leveraging the concept of human workers that perform physical labor (haptic turk <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TURKDECK</head><p>TurkDeck is a physical virtual reality system that reproduces not only what users see or hear, but also what users feel. Its key idea is to use "human actuators" to present and Figure <ref type="figure">1a</ref> shows a user immersed in a virtual reality experience in TurkDeck. The user can navigate freely, and can walk, sit, or lie, or turn and tilt their head. A head-mounted display (Oculus Rift <ref type="bibr" target="#b17">[18]</ref>) continuously provides the user with a first-person stereoscopic view into the virtual world.</p><p>A stereo headset provides spatial audio and prevents users from hearing what happens in the physical space around.</p><p>The key element of TurkDeck, however, is that it provides the user not only with a visual and auditory experience, but also a physical/haptic experience. TurkDeck achieves this using a prop-based approach, i.e., whenever users touch an object in the virtual world, they also touch an object in the physical world. On a conceptual level, the user is thus immersed in the physical world as shown in Figure <ref type="figure">1b</ref>, which illustrates how traditional prop-based systems have tackled the challenge (e.g. <ref type="bibr" target="#b9">[10]</ref>). Figure <ref type="figure">1c</ref>, in contrast, shows the physical world actually created by TurkDeck. TurkDeck recreates the physical world only when and where the user can actually reach the elements. It creates this partial physical world on the fly with the help of human workers, which we call human actuators. In the shown scene there are eight of them. Managed by TurkDeck, they position and operate props where required and just in time. TurkDeck's props are designed to be generic; human actuators can thus turn a prop that just served as a balancing beam into a wall, a table, a chair, etc. as necessary. Figure <ref type="figure">1d</ref>: The main benefit of our approach of constantly rearranging a small set of generic props is that it allows TurkDeck to create arbitrarily large, animated, and responsive virtual worlds in finite space and from limited physical resources. This is unlike previous prop-based systems that require physical space and physical props proportional to the amount of virtual space. The central element of TurkDeck is the software system that manages human actuators to make them position and operate these props where required and just in time. Turk-Deck does so by providing human actuators with timed instructions of when and where to place its custom props and how to actuate them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TurkDeck is Based on Rearranging Props</head><p>The following figures illustrate how TurkDeck uses human actuators to rearrange props-the main mechanism behind TurkDeck. As an example, we use of the first few seconds of our demo experience called the Lighthouse.</p><p>The user enters the world on a thin ledge-apparently in the process of rock climbing. Through a head-worn display, the user experiences the world in first person as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In the interest of visual clarity, however, we will show all remaining images in 3 rd person.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows how TurkDeck allows the user to feel the vertical surface he is pressed against and the void under his heels using props operated by 5 human actuators. 2 human actuators have laid down their boards to form the ledge the user stands on and 4 human actuators position their boards to simulate the segment of the cliff the user is holding on to.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Prop System</head><p>The objective behind TurkDeck's prop design is to get by with the smallest possible number of generic props-as that makes the system small and potentially even portable. Each human actuator carries only two main props, i.e., a board and a stick. With the help of the human actuators, TurkDeck constantly re-uses these props in different locations, orientations, and combinations, turning these few elements into a complete physical world.</p><p>We have created two sets of boards. Inspired by work in prop-based spatial augmented reality <ref type="bibr" target="#b13">[14]</ref>), we made one version from Styrofoam; the resulting four-panel boards weigh 1 kg. To offer users an even better experience, we also created the 12 kg high-fidelity version shown in Figure <ref type="figure" target="#fig_6">5</ref>. These boards consist of the surfaces of four coffee tables (IKEA), which we connected to one another using hinges. The high-fidelity boards are very sturdy and their hardness and surface conveys the sensation of walls and steps convincingly, while the use of finger holes keeps the weight manageable for the human actuators as shown in Figure <ref type="figure" target="#fig_7">6</ref>. Each board features a four-segment folding mechanism and magnetic connectors along all relevant edges. Hinges and magnets along the inside edges allows boards to assume a wide range of shapes, some of which are shown in Figure <ref type="figure" target="#fig_8">7</ref>. Magnetic connectors along the outside edges allow human actuators to combine boards into larger surfaces, such as the multi-segment walls in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The same mechanism allows boards to collapse into the 90x55x20cm package shown in Figure <ref type="figure" target="#fig_9">8</ref>. A built-in handle allows human actuators to carry their board and stick like a suitcase. As each human actuator has only one board and stick, the TurkDeck props can travel with the team.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Display System</head><p>As illustrated by Figure <ref type="figure" target="#fig_11">10</ref>, a projector mounted 8m above projects instructions for the human actuators onto the interaction area. We use a laser projector for this purpose (SmartLine RGB 1800), i.e., a vector display consisting of a laser, mirror, and galvanometer scanners, as used in laser shows. We opted for this type of display device because it delivers sufficient brightness to work in daylight, independently of the size of interaction area. We measured the power per square inch that reaches human actuators as the result of our specific display configuration and found safe levels. We encourage readers replicating our work to conduct similar measurements before using such a projection system. In accordance with our display system, we designed the visual language of the human actuator instructions to consist of vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The TurkDeck Instruction System</head><p>Human actuators perform their actions under the management of TurkDeck. As shown in Figure <ref type="figure" target="#fig_12">11</ref>, TurkDeck talks to human actuators by projecting visual instructions on the ground. The use of a public projection system gives all human actuators shared understanding of the system status (unlike, for example, individual head mounted displays).  In order to deal with occlusion and to maximize speed, TurkDeck starts projecting instructions a few seconds before they are required. Furthermore, TurkDeck uses redundant cues when directing human actuators. In addition to the projected instructions, (a) a small number of visual landmarks on the ground (masking tape, see Figure <ref type="figure" target="#fig_12">11</ref>) serve as visual reference whenever projection is occluded, (b) overview maps of the experience give human actuators a preview of their upcoming tasks, and (c) auditory alerts instruct actuators to move to the next position. The audio instruction "3B", for example, instructs human actuator "3" to move on to the location labeled "B".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WALKTHROUGH OF SELECTED MECHANISMS</head><p>We now demonstrate the resulting functionality of Turk-Deck by showing selected mechanisms from the Lighthouse demo experience. The actual experience takes about 10 minutes and requires 65 physical props of 12 different types to be placed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive Mechanisms</head><p>Following the ledge from Figure <ref type="figure" target="#fig_2">3</ref> leads to a little alcove where the user finds a switch to open a door (Figure <ref type="figure" target="#fig_14">13</ref>). the path a physical knob would perform. The use of human actuators provides TurkDeck with substantial freedom in rendering the knob's response, including a wide range of motion paths, animated behavior, and force feedback. Marks on the stick and the adjacent boards serve as visual reference for knob height and the default angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human actuator as sensing system</head><p>Conceptually, interactive mechanisms such as the switch require a sensing mechanism that detects the state of the prop and updates the virtual world accordingly, e.g., an accelerometer built into the stick. While that is of course an option, TurkDeck bypasses sensing/tracking/recognition questions by letting human actuators perform the job. In the case of the switch, TurkDeck tasks one of the human actuators to watch the human actuator enacting the switch. This human actuator simply triggers a predefined animation sequence by pressing a button on a wireless presenter tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bodies as Props</head><p>Next, the user finds a body and strips it off two bracelets (Figure <ref type="figure" target="#fig_15">14</ref>). TurkDeck renders the body using a human actuator itself as prop. To help the human actuator assume the correct pose it displays the body's outline on the board. Next, the user has to get past the security mechanism shown in Figure <ref type="figure" target="#fig_16">15</ref>: a "stomper". This is an example of an animated mechanism. When the user enters into the security mechanism, the stomper starts actuating, pushing the user against the wall. TurkDeck implements this using four human actuators, as shown.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Feedback is Independent of Visual Feedback</head><p>While all mechanisms presented so far combine the concept of physical props with the visuals provided by the headmounted display, the physical reality created by TurkDeck can be used stand-alone. To illustrate this we created a room in which users are forced to turn off the power to get past a laser barrier security mechanism, which also turns off the lights in the room, leaving the user in complete darkness. Based on the physical sensation provided by TurkDeck alone, the user now navigates to the other end of the room, where the user can turn the lights back on and progress in the experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stationary Movement ("Vehicles")</head><p>Figure <ref type="figure" target="#fig_19">17</ref>  On an abstract level, vehicles create an experience as it could have been created using Haptic Turk <ref type="bibr" target="#b4">[5]</ref>. Another way of thinking of TurkDeck's vehicle mechanism is thus as a means for embedding Haptic Turk experiences into a larger, overarching TurkDeck experience. The resulting worlds have the form of node-link diagrams with realwalking areas as nodes and vehicles forming the links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Effects</head><p>Any TurkDeck mechanism can be and typically is complemented with physical effects. Riding the zip line, for example, is accompanied by wind, which TurkDeck implements using human actuators waving their boards. Other physical effects, such as spraying water from waterfalls, brushing leaves, etc. can be added to as required by the experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCHEDULING AND TRACKING</head><p>The key objective of TurkDeck is to get the timing and the location of physical actuation right. TurkDeck's underlying system accomplishes this by scheduling human actuators, tracking users, and by coordinating the two with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Scheduler</head><p>TurkDeck's scheduler makes sure that human actuators physically "render" mechanisms before the user can touch them. Since this just-in-time approach is the key to Turk-Deck's contribution of delivering large physical worlds with limited resources, getting the timing right is a key objective for the system. We made good experiences with this very simple queue algorithm. (1) When the experience is launched, all mechanisms register with the scheduler. The system attaches a "collider" object to every mechanism, i.e., an invisible hull surrounding the object that triggers an event whenever the user enters or leaves. The scheduler sorts mechanisms by their expected chronological order. (2) At the beginning of the experience, the scheduler assigns the n human actuators to the n mechanisms the user will encounter first.</p><p>(3) During operation, whenever the user leaves a collider, the scheduler frees up the associated human actuator and instantly reuses it for the next upcoming mechanism.</p><p>This scheduler also works across rooms. While TurkDeck uses portals [21] to "fold" large virtual worlds into small physical spaces, the scheduler already starts tearing down the current room and sets up the room behind the portal, as early as the user starts walking towards the portal. The moment the user passes the portal, the room on the other side is already "there". Note how our algorithm very aggressively allocates all its resources for future events, maximizing the time human actuators have to set up. This maximizes the speed at which a user can traverse the virtual world, at the obvious expense of reduced responsiveness for unexpected user behavior, such as walking backwards. Giving users additional freedom, such as the option to instantaneously turn around and go back, is possible, but requires additional human actuators. The same holds for bifurcations and large rooms. We can reduce this need for human actuators by trading it in for responsiveness, as we describe in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Delay Mechanisms</head><p>If a room contains more mechanisms than the human actuators can set up in time, we protect the room with what we call a delay mechanism, i.e., a simple mechanism that can be set up quickly, but that requires users a specified time to get past. It consists of (1) a barrier that prevents access to the complex mechanism and (2) a mechanism that keeps the user occupied until a predefined duration has passed. Figure <ref type="figure" target="#fig_20">18</ref> shows an example. A user reaching the zip line finds access blocked by a glass door. In order to unlock the door, the user has to crank the hydraulic mechanism that powers the door. Once oil pressure has been restored, the door slides open and the user can access the zip line. Note that the speed at which pressure builds up is designed to be non-obvious to the user. In reality, a human actuator determines the build-up of pressure-as with all interactive elements. This gives the human actuators control over the timing, allowing the user's task to complete just as the human actuators have completed the zip line mechanism. In essence, the delay element thus serves as a progress bar. One that, however, feels like it is under the user's control. Within limits, adding delay mechanisms allows adapting an experience to the number of human actuators available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking the User with Precision</head><p>TurkDeck uses a hybrid approach to tracking the user. First, a ceiling-mounted camera observes the user from above, tracking the user's head-worn ALVAR marker <ref type="bibr" target="#b1">[2]</ref> (Figure <ref type="figure" target="#fig_11">10c</ref>). This perspective from above works reliably, while human actuators and their boards tend to block almost any other angle. Second, the inertial measurement unit (IMU) in the Oculus Rift determines roll/pitch/yaw. We aggregate this IMU data with the ALVAR data in order to track the user during fast movements. Some mechanisms require additional accuracy. Figure <ref type="figure" target="#fig_22">19</ref> illustrates this at the example of the user stepping onto a physical prop; accuracy matters here to make sure the user does not trip. TurkDeck addresses this with a variant of the delay mechanism. (a) When the user arrives at the pit, there is no way to cross. This forces the user to stand on a virtual footswitch until a balancing beam is slowly raised into position. (b) In the physical world, there is no balancing beam. As the human actuators are setting it up, the welldefined position of the user serves as additional reference, allowing the actuators to place the beam in precise reference to the user's feet. (c) When the physical beam is set up, also the physical beam reaches its goal position. The user can now step forward and reliably hit the center of the beam without risk of tripping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System overview</head><p>Figure <ref type="figure" target="#fig_23">20</ref> summarizes TurkDeck's system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTRIBUTION, BENEFITS, LIMITATIONS</head><p>The main contribution of TurkDeck is that it "scales" propbased virtual reality, i.e., it can produce arbitrarily large virtual worlds in finite space and with a finite amount of props. TurkDeck achieves this as follows: (1) By using On the flipside, experiences created by TurkDeck are limited by the speed and accuracy of the human actuators and in our case also the tracking system. Readers attempting to replicate our work can achieve better results by tracking the user using an optical motion capture system instead (e.g. Vicon or HTC/Valve's Lighthouse), as well as helping human actuators position props with alignment aids, such as a "kickstands" at the bottom of each board that help human actuators create a right angle between board and ground. Finally, the system presented in this paper is limited in that our lighthouse experience was created by hand. The mechanisms the experience is composed of, however, are reusable. They are also intended to give readers a sense of how to design their own experiences for a given number of human actuators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>The work presented in this paper attempts to bring a physical experience into virtual reality. Thus it is related to research on virtual reality hardware, in particular prop-based virtual reality in the context of real walking environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand-Held and Other Props</head><p>The simplest use of props involves hand-held props. Hinckley et al. used a doll's head and a rectangle plate to allow users to explore three-dimensional neurosurgical visualiza- tions <ref type="bibr" target="#b5">[6]</ref>. This type of use of props has started to enjoy adoption in the medical field, such as virtual surgery <ref type="bibr" target="#b0">[1]</ref>. Similarly, users of the Personal Interaction Panel use a pen and a pad to help manipulate objects in see-through augmented reality <ref type="bibr" target="#b27">[29]</ref>. Sheng et al used a sponge-like prop to create a range of interaction techniques <ref type="bibr" target="#b24">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiencing Motion</head><p>In the class of devices that support whole-body experiences we find motion simulators; they shake, lift or tilt users sitting or standing on them. Motion simulators are used in driving and flight simulation for both training and entertainment purposes <ref type="bibr" target="#b15">[16]</ref>. Most of them are based on a Stewart platform, which offers six degrees of freedom driven, e.g., by six hydraulic cylinders as actuators <ref type="bibr" target="#b25">[27]</ref>. TurkDeck builds on Haptic Turk <ref type="bibr" target="#b4">[5]</ref>, a system that introduced the concept of human actuators to simulate a motion platform. The system allows a user to enjoy an interactive experience, such as a flight simulation, while human actuators manually lift, tilt, and push the user's limbs or torso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locomotion Devices and Simulating Terrain</head><p>The next level of realism uses devices that simulate the experience of walking-while keeping the user stationary.</p><p>A range of locomotion devices increases realism by simulating terrain texture. The ground surface simulator, for example, is a treadmill equipped with individually heightadjustable elements of up to 6 cm that simulate bumpy terrain and virtual slopes <ref type="bibr" target="#b16">[17]</ref>. Torso force feedback pulls users walking on a treadmill using an active mechanical link, simulating a slope <ref type="bibr" target="#b7">[8]</ref>. Iwata et al.'s system provides users with additional freedom in that it allows users to "walk". The system, however, captures the user again on every step using motion platforms that position themselves where the user is expected to step next <ref type="bibr" target="#b11">[12]</ref>. CirculaFloor builds on the same concept, but uses four robot units that place themselves under the user's steps <ref type="bibr" target="#b10">[11]</ref>. Usoh et al. argue that real walking produces a higher sense of presence than the more space-efficient locomotion devices <ref type="bibr" target="#b28">[30]</ref>. Building on this, Level-Ups simulate terrain in realwalking interfaces <ref type="bibr" target="#b22">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passive haptics and real walking</head><p>Several "passive haptic" systems combine the concepts of props with real walking. Low et al., for example, project augmented reality experiences onto Styrofoam walls <ref type="bibr" target="#b13">[14]</ref>, allowing users to experience different virtual worlds in the same physical room. Similarly, Mixed Reality for military operations in urban terrain <ref type="bibr" target="#b8">[9]</ref> uses passive haptics to create a haptic sense of objects, terrain, and walls in the virtual world <ref type="bibr" target="#b9">[10]</ref>. FlatWorld integrates large props into a physical world; between experiences these props can be rearranged to match the next virtual world <ref type="bibr" target="#b19">[20]</ref>. Kohli et al. use redirected walking <ref type="bibr" target="#b20">[22]</ref> to allow users to encounter a stationary prop at different virtual locations <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDY</head><p>To validate our approach, we conducted a user study. Every participant explored a sequence out of the Lighthouse experience in two conditions: with props placed and actuated by human actuators and without. Participants rated both experiences on a series of Likert scales. Our main hypothesis was that the props and human actuation would contribute to a more realistic and enjoyable user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experience and Task</head><p>Participants explored a sequence out of the lighthouse experience and their task was to simply reach the other end of the experience. The experience included a range of elements and mechanisms, including the ledge, a door, a dead body, multiple portals, a stomper, two balancing beams (as shown in <ref type="bibr">Figures 1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20)</ref>, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Interface Conditions</head><p>There were two user interface conditions. In both conditions, participants could walk around the world freely and they experienced the virtual world by means of a head mounted display (Oculus Rift V2 <ref type="bibr" target="#b17">[18]</ref>) and earphones.</p><p>In the TurkDeck condition, shown in Figure <ref type="figure" target="#fig_25">21</ref>, the lighthouse experience encompassed a simulation of its physicality, administered by ten human actuators. Combined, human actuators placed and operated a total of 65 props. In the Baseline condition, shown in Figure <ref type="figure" target="#fig_26">22</ref>, participants experienced the lighthouse through the head-mounted display and the closed earphones only; there were neither props nor human actuation. We used a within-subject design, so that each participant experienced the lighthouse world twice, i.e., once in the TurkDeck condition and once in the Baseline condition, in counterbalanced order. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 11 participants (3 females) from our institution. Their age ranged from 19 to 29 years (mean 23.0). They had never experienced TurkDeck before. One participant lacked the ability to see in stereo.</p><p>In addition, we recruited two groups of actuator participants to serve as human actuators in two sessions, i.e., one group of four and one group of three. The experimenter filled in additional human actuators to fill the ten-actuator slots required for the experience. Each actuator participant assisted in 3-8 experiences.</p><p>The two groups of participants were distinct, i.e., participants either served as participant or as actuator participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>For each participant the experiment started by dressing up in the head-mounted display (Oculus Rift), a set of earphones, a marker hat, and a backpack that contained the MacBook. After we calibrated the system, participants explored the lighthouse experience until they reached the other end, which took between six and eight minutes. They then repeated the experience in the other interface condition, and finally filled in a questionnaire.</p><p>For each of the two groups of actuator participants, we provided 30 minutes of training ahead of time during which we explained the handling of the boards and the individual actuator displays (projection, sheets, audio). Then actuator participants assisted in 3-8 TurkDeck experiences; they used the in-between baseline conditions to rest (10-15 minutes each). After they were done actuating their respective group of participants, all actuator participants filled in questionnaires about their experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 23 summarizes our results.</head><p>User participants rated their experience on average as 5.82 (SD=1.16) on a 7-point Likert scale (7=fun)-so clearly as enjoyable. This was also reflected by their ratings of realism 4.91 vs. 3.63. Feedback about the TurkDeck condition was overall very positive. One participant explained with respect to the TurkDeck condition "What I mean is, that it was that realistic, that I was truly fearful standing on the edge or before the fire." Another participant said, "I really felt I was there, and I could only calm myself down by telling me it was not real." Some participant mentioned favorite moments, such as standing on the ledge. User participants agreed that the human actuators were responsible for the added realism (6.0/7). The limiting factor in the experience turned out to be the calibration of the tracking system, which occasionally was off by up to 15cm. In the TurkDeck condition, this affected the sync between props and visuals, while it left the Baseline condition largely unaffected. This caused one participant to rate the TurkDeck condition lower than the baseline (3 vs. 5) while all participants agreed that "better tracking would make this more realistic" (6.0/7). An optical motion capture system can fix these issues, as mentioned earlier. As actuators, participants rated the experience as less enjoyable than as user participants, yet still as "enjoyable" (M=5.2, SD=1.9). The actuator participants explained that their experience was driven by two factors: their desire to support of users' experience and the team experience among the human actuators.</p><p>Actuator participants rated all four components of the instruction system as useful (projection 6.1/7, auditory 6.0/7, sheets 5.0/7, landmarks 3.8/7) confirming our design decisions. In addition, we observed human actuators use boards placed by other human actuators as a spatial reference. Five actuator participants mentioned fatigue-an expected effect given the use of the heavier high-fidelity boards.</p><p>In summary, our user study provides initial validation for the TurkDeck concept. Most importantly, participants enjoyed their experience in the TurkDeck condition more than baseline and rated it as more realistic. The actuator experience turned out to be enjoyable as well, primarily caused by the social nature of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this paper, we presented TurkDeck, physical virtual reality based on people. Our main contribution is that TurkDeck allows prop-based virtual reality to scale, i.e., it allows producing arbitrarily large virtual worlds in finite space and with a finite amount of props. TurkDeck achieves this by using human actuators to re-arrange props. Arguably, the key remaining question is: "shouldn't this all be automated in the future; shouldn't all actuation be performed by machines?" We argue 'no'. The idea of automation is to reduce the cost of a heavily repeated process. For immersive experiences, however, this assumption is simply not justified. Users may repeat an experience once or twice, but certainly not hundreds of times. Since the reason for automation does not apply to immersive experiences, we argue that going "back" to manual labor is the valid approach.</p><p>Ultimately, we think of TurkDeck as complementary to traditional prop-based installations that have been deployed, for example, by the military. The military use case is characterized by the availability of large spaces, large budgets, and the availability of thousands of users who can be brought to the installation to experience the exact same experience. TurkDeck in contrast, we see in the ecosystem resulting from the currently ongoing mass-availability of virtual reality consumer headsets, such as the Oculus Rift. These are democratizing the field by bringing VR to consumers and HTC/Valve's Lighthouse could do the same for real-walking VR. The resulting consumer use cases lack all the aspects that characterized the military use case in that space and budgets are limited and travel to an installation just does not seem justified. Consequently, we expect to see many VR installations in the future, all of which serve only very few users. This new ecosystem requires a new take on VR, including prop-based VR. Where the traditional approach builds on flying in thousands of users, to a military base or Disney World, the new eco system will bring immersive virtual experiences to people. We designed TurkDeck to be a part of this. We expect it to be used in social settings, with users recruiting human actuators from among their friends, rewarded simply by taking turns as players. Arguably, human actuators could also be paid workers, even though this is not what we think of as the primary use of such a system. As future work, we plan on exploring more elaborate schedulers, as well as creating an interactive editor for creating TurkDeck experiences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>when and where the user can actually reach them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: User's view through the head-worn display (Oculus Rift [18]) as the user is coming to the ledge.</figDesc><graphic coords="2,55.19,565.46,238.41,128.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The user comes to, standing on a ledge, which is (b) simulated by 5 human actuators using 6 boards</figDesc><graphic coords="2,358.00,199.56,254.80,161.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: In order to locate a place to go, the user is inching towards the left, still pressed against the cliff. As shown, human actuators create two new segments of the cliff on the fly. At the same time, the wall segments on the right are now out of the user's reach; TurkDeck thus makes the respective human actuators tear them down.</figDesc><graphic coords="2,91.70,198.51,398.50,224.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: As the user is inching towards the left, human actuators set up new wall segments there, tear down on the right.</figDesc><graphic coords="2,376.50,421.99,235.94,147.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Board folds in four. Joints and magnet connectors help human actuators help reconfigure boards.</figDesc><graphic coords="3,54.30,124.99,243.85,162.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Finger holes enable actuators to lift boards easily</figDesc><graphic coords="3,320.35,415.83,241.00,152.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Board can be arranged into many different shapes.</figDesc><graphic coords="3,54.50,548.84,239.55,147.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Boards collapse into a 90x55x20cm package that human actuators can carry like a suitcase. Some experiences also use the stick shown in Figure 9. It is made from a metal pipe and it bears a male and a female mount at its two ends, allowing it to attach to other sticks. (b) This allows combining sticks into larger structures, such as the zip line mechanism shown in Figure 17. (c) Optional experience-specific attachments allow sticks to perform a wide range of functions; a Styrofoam knob, for example, turns a stick into a lever.</figDesc><graphic coords="3,320.35,148.10,239.73,161.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (a) Sticks bear mounts at their two ends. (b) This allows sticks to connect (c) to experience-specific attachments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: TurkDeck projects instructions human actuators onto the ground using a 200 mW laser projector.</figDesc><graphic coords="4,158.35,260.12,150.12,100.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The projected human actuator interface consists of green board outlines and blue human actuator IDs. Each instruction is a pair consisting of (1) an outline representing the requested prop, shown in the desired position and orientation and (2) the ID of the human actuator intended to perform the job, shown in the position and orientation the in which the human actuator is supposed to perform the task. The respective human actuator responds by standing on the ID, picking the respective prop from his/her personal prop repertoire, and transforming it to the shown shape. As illustrated by Figure 12, TurkDeck depicts each</figDesc><graphic coords="4,264.82,484.14,203.55,199.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: TurkDeck renders props as their outline.</figDesc><graphic coords="4,186.95,138.49,188.70,125.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: A switch; this human actuator also serves as sensorTurkDeck renders the switch using a stick with a ball attachment held by a human actuator. As the user pushes down, the human actuator follows the motion, simulating</figDesc><graphic coords="4,344.14,511.69,266.48,177.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: TurkDeck using body of a human actuator as prop Animated Mechanisms</figDesc><graphic coords="5,1.70,303.76,170.36,159.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: This "stomper" is an animated mechanism</figDesc><graphic coords="5,-70.02,419.12,325.52,349.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16</head><label>16</label><figDesc>Figure 16 shows how TurkDeck coordinates the human animation. (a) TurkDeck prepares human actuators by showing a pair of arrow icons that indicate the upcoming animation. Right before the animation starts the arrows perform a visible countdown (they blink three times). (b) Now the wall icon animates along the trajectory shown using the arrows. Because human actuators had the advance warning, they are ready to follow the movement of the wall in correct timing. TurkDeck uses the same visual language for all animations. Figure 16c, for example, shows a door (next to a switch) that will later rotate to open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Animation based on human actuators: (a) the pair of (half-) arrows indicates that this wall will animate at some point in time. (b) During the animation, the wall moves along the arrows. (c) TurkDeck uses the same visual language for all animations; this is a door that will later open to the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure17shows a different type of animation. (a) In the shown scene, the user climbs a step, grabs the handle overhead, and rides the zip line. (b) TurkDeck implements the ride by making human actuators remove the step from under the user's feet, leaving the user dangling in the air. TurkDeck now plays the visuals of the world animating past the user-while the user remains stationary. (c) A few seconds later, the user reaches the end of the zip line, and, typically feet-forward, bumps into the wall. TurkDeck implements this by two human actuators carrying wall segments walking into the user. The instructions for the human actuators use the visual language described earlier, i.e., first arrows indicate that the wall at the end of the zip line is going to animate, then the blueprint of the entire world animates past the user. The zip line is an example of a class of mechanisms we call "vehicles". Normally, users move though a stationary world ("real-walking"<ref type="bibr" target="#b28">[30]</ref>). While in a vehicle, in contrast, users remain stationary and the world moves past them. TurkDeck implements vehicles this way, as it allows transporting users by an arbitrary distance-including vertical displacement-with constant space requirements and constant effort for props.</figDesc><graphic coords="5,83.75,282.62,308.68,205.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: This delay mechanism allows human actuators to delay the user until the zip line mechanism has been set up.</figDesc><graphic coords="6,371.63,424.90,236.87,157.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: This position &amp; delay mechanism allows human actuators to place the physical beam in reference to the user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: System diagramhuman actuators to re-arrange props, TurkDeck reuses props, allowing it to simulate large virtual worlds with a small set of physical props. (2) By designing props to be generic, TurkDeck is able to use the same props to represent a wide range of physical objects and effects. (3) By physically rendering only the user's immediate periphery, TurkDeck can tear down or rebuilt the world in a constant amount of time. This allows TurkDeck to handle situations of abrupt change to the virtual world, e.g., when the user walks through a portal or when loading a new experience. (4) By exploiting the smarts of human actuators, TurkDeck can animate objects and sense user actions. On the flipside, experiences created by TurkDeck are limited by the speed and accuracy of the human actuators and in our case also the tracking system. Readers attempting to replicate our work can achieve better results by tracking the user using an optical motion capture system instead (e.g. Vicon or HTC/Valve's Lighthouse), as well as helping human actuators position props with alignment aids, such as a "kickstands" at the bottom of each board that help human actuators create a right angle between board and ground. Finally, the system presented in this paper is limited in that our lighthouse experience was created by hand. The mechanisms the experience is composed of, however, are reusable. They are also intended to give readers a sense of how to design their own experiences for a given number of human actuators.</figDesc><graphic coords="7,45.37,290.03,416.72,234.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: A participant in the TurkDeck condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: A participant in the Baseline condition. The projected instructions here serve only as frame of reference for observers, as there were no props and no human actuation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: User participants rated their experience as more fun and realistic when in the TurkDeck condition. Error bars are +/-1 standard error of the mean.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Neda Rajaeean, Marie Havemann, Ralf Lange, Moritz Peter, Tobias Rohr, Huel Fuchsberger, and Thomas Langhanki. We thank Patrick LÃ¼hne for his contribution to early discussions on the topic. We thank repeat human actuators: Jack Lindsey, Saiganesch Agrawala, Patrick Weis, and Lisa Pfisterer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Virtual reality training in neurosurgery: review of current status and future applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lemole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Finkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yudkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luciano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Charbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical neurology international</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Alvar</surname></persName>
		</author>
		<ptr target="http://virtual.vtt.fi/virtual/proj2/multimedia/alvar" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The GLAD-IN-ART Project. Virtual Reality SE-19</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bergamasco</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s real about virtual reality? Computer Graphics and Applications</title>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frederick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="16" to="27" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Haptic Turk : a Motion Platform Based on People</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>LÃ¼hne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sterz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;14</title>
		<meeting>CHI &apos;14</meeting>
		<imprint>
			<biblScope unit="page" from="3463" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Passive Real-world Interface Props for Neurosurgical Visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Kassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;94</title>
		<meeting>CHI &apos;94</meeting>
		<imprint>
			<biblScope unit="page" from="452" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE VRAIS &apos;98</title>
		<meeting>IEEE VRAIS &apos;98</meeting>
		<imprint>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Torso Force Feedback Realistically Simulates Slope on Treadmill-Style Locomotion Interfaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hollerbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="939" to="952" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixed reality in education, entertainment, and training</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="24" to="30" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Passive haptics significantly enhances virtual environments</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Insko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation at</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CirculaFloor: A Locomotion Interface Using Circulation of Movable Tiles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Virtual Reality&apos;05</title>
		<meeting>Virtual Reality&apos;05</meeting>
		<imprint>
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gait Master: a Versatile Locomotion Interface for Uneven Virtual Terrain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nakaizumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virtual Reality</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining passive haptics with redirected walking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICAT&apos;05</title>
		<meeting>ICAT&apos;05</meeting>
		<imprint>
			<biblScope unit="page" from="253" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Life-Size Projector-Based Dioramas</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">- L</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VRST&apos;01</title>
		<meeting>VRST&apos;01</meeting>
		<imprint>
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Immediate Usability of Graffiti</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GI &apos;97</title>
		<meeting>GI &apos;97</meeting>
		<imprint>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="http://www.nasa.gov/centers/ames/research/technology-onepagers/vms.html" />
		<title level="m">NASA Vertical Motion Simulator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Development of Ground Surface Simulator for Tel-E-Merge system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sugihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VR&apos;00</title>
		<meeting>VR&apos;00</meeting>
		<imprint>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Oculus Rift, www.oculusvr.com</orgName>
		</author>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prop-based haptic interaction with co-location and immersion: an automotive application</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coquillart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HAVE&apos;05</title>
		<meeting>HAVE&apos;05</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining Hollywood Set-Design Techniques with VR</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Piepol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Swartout</surname></persName>
		</author>
		<author>
			<persName><surname>Flatworld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="12" to="15" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sharif</forename><surname>Razzaque</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note>Redirected walking</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facetons: face primitives with adaptive bounds for building 3D architectural models in virtual environment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VRST &apos;13</title>
		<meeting>VRST &apos;13</meeting>
		<imprint>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>KÃ¶hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;15</title>
		<meeting>CHI&apos;15</meeting>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HapticWalker-a Novel Haptic Foot Device</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KrÃ¼ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="166" to="180" />
			<date type="published" when="2005-02">2 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An interface for virtual 3D sculpting via physical proxy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GRAPHITE&apos;06</title>
		<meeting>GRAPHITE&apos;06</meeting>
		<imprint>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Platform with Six Degrees of Freedom</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Institution of Mechanical Engineers</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="371" to="386" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Head-mounted Three Dimensional Display</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AFIPS &apos;68</title>
		<meeting>AFIPS &apos;68</meeting>
		<imprint>
			<biblScope unit="page" from="757" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Personal Interaction Panel -a Two-Handed Interface for Augmented Reality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>SzalavÃ¡ri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gervautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Walking &gt; Walking-in-Place &gt; Flying, in Virtual Environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Usoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Whitton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH&apos;99</title>
		<meeting>SIGGRAPH&apos;99</meeting>
		<imprint>
			<biblScope unit="page" from="359" to="364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
