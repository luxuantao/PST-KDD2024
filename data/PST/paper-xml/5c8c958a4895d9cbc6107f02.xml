<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Adversarial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-05">5 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
							<email>samotian@mix.wvu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering West</orgName>
								<orgName type="institution">Virginia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Seyed</roleName><forename type="first">Quinn</forename><surname>Jones</surname></persName>
							<email>qjones1@mix.wvu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering West</orgName>
								<orgName type="institution">Virginia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Iranmanesh</surname></persName>
							<email>seiranmanesh@mix.wvu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering West</orgName>
								<orgName type="institution">Virginia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
							<email>gidoretto@mix.wvu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering West</orgName>
								<orgName type="institution">Virginia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Adversarial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-05">5 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">CEDE3EF45C6D6B7C0B0729AFA0C862B7</idno>
					<idno type="arXiv">arXiv:1711.02536v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high "speed" of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As deep learning approaches have gained prominence in computer vision we have seen tasks that have large amounts of available labeled data flourish with improved results. There are still many problems worth solving where labeled data on an equally large scale is too expensive to collect, annotate, or both, and by extension a straightforward deep learning approach would not be feasible. Typically, in such a scenario, practitioners will train or reuse a model from a closely related dataset with a large amount of samples, here called the source domain, and then train with the much smaller dataset of interest, referred to as the target domain. This process is well-known under the name finetuning. Finetuning, while simple to implement, has been found to be sub-optimal when compared to later techniques such as domain adaptation <ref type="bibr" target="#b4">[5]</ref>. Domain Adaptation can be supervised <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b26">27]</ref>, unsupervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>, or semi-supervised <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b62">63]</ref>, depending on what data is available in a labeled format and how much can be collected.</p><p>Unsupervised domain adaptation (UDA) algorithms do not need any target data labels, but they require large amounts of target training samples, which may not always be available. Conversely, supervised domain adaptation (SDA) algorithms do require labeled target data, and because labeling information is available, for the same quantity of target data, SDA outperforms UDA <ref type="bibr" target="#b37">[38]</ref>. Therefore, if the available target data is scarce, SDA becomes attractive, even if the labeling process is expensive, because only few samples need to be processed.</p><p>Most domain adaptation approaches try to find a feature space such that the confusion between source and target distributions in that space is maximum (domain confusion). Because of that, it is hard to say whether a sample in the feature space has come from the source distribution or the target distribution. Recently, generative adversarial networks <ref type="bibr" target="#b17">[18]</ref> have been introduced for image generation which can also be used for domain adaptation. In <ref type="bibr" target="#b17">[18]</ref>, the goal is to learn a discriminator 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</p><formula xml:id="formula_0">G 2 G 3 G 4 G 1</formula><p>Figure <ref type="figure">1</ref>: Examples from MNIST <ref type="bibr" target="#b31">[32]</ref> and SVHN <ref type="bibr" target="#b39">[40]</ref> of grouped sample pairs. G 1 is composed of samples of the same class from the source dataset in this case MNIST. G 2 is composed of samples of the same class, but one is from the source dataset and the other is from the target dataset. In G 3 the samples in each pair are from the source dataset but with differing class labels. Finally, pairs in G 4 are composed of samples from the target and source datasets with differing class labels.</p><p>to distinguish between real samples and generated (fake) samples and then to learn a generator which best confuses the discriminator. Domain adaptation can also be seen as a generative adversarial network with one difference, in domain adaptation there is no need to generate samples, instead, the generator network is replaced with an inference network. Since the discriminator cannot determine if a sample is from the source or the target distribution the inference becomes optimal in terms of creating a joint latent space. In this manner, generative adversarial learning has been successfully modified for UDA <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49]</ref> and provided very promising results.</p><p>Here instead, we are interested in adapting adversarial learning for SDA which we are calling fewshot adversarial domain adaptation (FADA) for cases when there are very few labeled target samples available in training. In this few-shot learning regime, our SDA method has proven capable of increasing a model's performance at a very high rate with respect to the inclusion of additional samples. Indeed, even one additional sample can significantly increase performance.</p><p>Our first contribution is to handle this scarce data while providing effective training. Our second contribution is to extend adversarial learning <ref type="bibr" target="#b17">[18]</ref> to exploit the label information of target samples. We propose a novel way of creating pairs of samples using source and target samples to address the first challenge. We assign a group label to a pair according to the following procedure: 0 if samples of a pair come from the source distribution and the same class label, 1 if they come from the source and target distributions but the same class label, 2 if they come from the source distribution but different class labels, and 3 if they come from the source and target distributions and have different class labels. The second challenge is addressed by using adversarial learning <ref type="bibr" target="#b17">[18]</ref> to train a deep inference function, which confuses a well-trained domain-class discriminator (DCD) while maintaining a high classification accuracy for the source samples. The DCD is a multi-class classifier that takes pairs of samples as input and classifies them into the above four groups. Confusing the DCD will encourage domain confusion, as well as the semantic alignment of classes. Our third contribution is an extensive validation of FADA against the state-of-the-art. Although our method is general, and can be used for all domain adaptation applications, we focus on visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Naively training a classifier on one dataset for testing on another is known to produce sub-optimal results, because an effect known as dataset bias <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56]</ref>, or covariate shift <ref type="bibr" target="#b50">[51]</ref>, occurs due to a difference in the distributions of the images between the datasets.</p><p>Prior work in domain adaptation has minimized this shift largely in three ways. Some try to find a function which can map from the source domain to the target domain <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b51">52]</ref>. Others find a shared latent space that both domains can be mapped to before classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Finally, some use regularization to improve the fit on the target domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. UDA can leverage the first two approaches while SDA uses the second, third, or a combination of the two approaches. In addition to these methods, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50]</ref> have addressed UDA when an auxiliary data view <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>, is available during training, but that is beyond the scope of this work.</p><p>For this approach we are focused on finding a shared subspace for both the source and target distributions. Siamese networks <ref type="bibr" target="#b6">[7]</ref> work well for subspace learning and have worked very well with deep  convolutional neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b60">61]</ref>. Siamese networks have also been useful in domain adaptation recently. In <ref type="bibr" target="#b57">[58]</ref>, which is a deep SDA approach, unlabeled and sparsely labeled target domain data are used to optimize for domain invariance to facilitate domain transfer while using a soft label distribution matching loss. In <ref type="bibr" target="#b53">[54]</ref>, which is a deep UDA approach, unlabeled target data is used to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Some approaches went beyond Siamese weight-sharing and used couple networks for DA.</p><p>[27] uses two CNN streams, for source and target, fused at the classifier level. <ref type="bibr" target="#b44">[45]</ref>, which is a deep UDA approach and can be seen as an SDA after fine-tuning, also uses a two-streams architecture, for source and target, with related but not shared weights. <ref type="bibr" target="#b37">[38]</ref>, which is an SDA approach, creates positive and negative pairs using source and target data and then finds a shared feature space between source and target by bringing together the positive pairs and pushing apart the negative pairs.</p><p>Recently, adversarial learning <ref type="bibr" target="#b17">[18]</ref> has shown promising results in domain adaptation and can be seen as examples of the second category. <ref type="bibr" target="#b32">[33]</ref> introduced a coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images for different applications including UDA. <ref type="bibr" target="#b58">[59]</ref> has used the adversarial loss for discriminative UDA. <ref type="bibr" target="#b48">[49]</ref> introduces an approach that leverages unlabeled data to bring the source and target distributions closer by inducing a symbiotic relationship between the learned embedding and a generative adversarial framework.</p><p>Here we use adversarial learning to train inference networks such that samples from different distributions are not distinguishable. We consider the task where very few labeled target data are available in training. With this assumption, it is not possible to use the standard adversarial loss used in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49]</ref>, because the training target data would be insufficient. We address that problem by modifying the usual pairing technique used in many applications such as learning similarity metrics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Our pairing technique encodes domain labels as well as class labels of the training data (source and target samples), producing four groups of pairs. We then introduce a multi-class discriminator with four outputs and design an adversarial learning strategy to find a shared feature space. Our method also encourages the semantic alignment of classes, while other adversarial UDA approaches do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Few-shot adversarial domain adaptation</head><p>In this section we describe the model we propose to address supervised domain adaptation (SDA).</p><p>We are given a training dataset made of pairs D s = {(x s i , y s i )} N i=1 . The feature x s i ∈ X is a realization from a random variable X s , and the label y s i ∈ Y is a realization from a random variable Y s . In addition, we are also given the training data</p><formula xml:id="formula_1">D t = {(x t i , y t i )} M i=1</formula><p>, where x t i ∈ X is a realization from a random variable X t , and the labels y t i ∈ Y. We assume that there is a covariate shift <ref type="bibr" target="#b50">[51]</ref> between X s and X t , i.e., there is a difference between the probability distributions p(X s ) and p(X t ). We say that X s represents the source domain and that X t represents the target domain. Under this settings the goal is to learn a prediction function f : X → Y that during testing is going to perform well on data from the target domain.</p><p>The problem formulated thus far is typically referred to as supervised domain adaptation. In this work we are especially concerned with the version of this problem where only very few target Algorithm 1 FADA algorithm 1: Train g and h on Ds using (1). 2: Uniformly sample G1,G3 from DsxDs. 3: Uniformly sample G2,G4 from DsxDt. 4: Train DCD w.r.t. gt = gs = g using (3). 5: while not convergent do 6:</p><p>Update g and h by minimizing (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update DCD by minimizing (3). 8: end while labeled samples per class are available. We aim at handling cases where there is only one target labeled sample, and there can even be some classes with no target samples at all. In absence of covariate shift a visual classifier f is trained by minimizing a classification loss</p><formula xml:id="formula_2">L C (f ) = E[ℓ(f (X s ), Y )] ,<label>(1)</label></formula><p>where In general, f could be modeled by the composition of two functions, i.e., f = h•g. Here g : X → Z would be an inference from the input space X to a feature or inference space Z, and h : Z → Y would be a function for predicting from the feature space. With this notation we would have f s = h s • g s and f t = h t • g t , and the SDA problem would be about finding the best approximation for g t and h t , given the constraints on the available data.</p><p>If g s and g t are able to embed source and target samples, respectively, to a domain invariant space, it is safe to assume from the feature to the label space that h t = h s = h. Therefore, domain adaptation paradigms are looking for such inference functions so that they can use the prediction function h s for target samples.</p><p>Traditional unsupervised DA (UDA) paradigms try to align the distributions of the features in the feature space, mapped from the source and the target domains using a metric between distributions, Maximum Mean Discrepancy <ref type="bibr" target="#b19">[20]</ref> being a popular one and other metrics like Kullback Leibler divergence <ref type="bibr" target="#b28">[29]</ref> and Jensen-Shannon <ref type="bibr" target="#b17">[18]</ref> divergence becoming popular when using adversarial learning. Once they are aligned, a classifier function would no longer be able to tell whether a sample is coming from the source or the target domain. Recent UDA paradigms try to find inference functions to satisfy this important goal using adversarial learning. Adversarial training looks for a domain discriminator D that is able to distinguish between samples of source and target distributions.</p><p>In this case D is a binary classifier trained with the standard cross-entropy loss</p><formula xml:id="formula_3">L adv-D (X s , X t , g s , g t ) = -E[log(D(g s (X s )))] -E[log(1 -D(g t (X t )))] .<label>(2)</label></formula><p>Once the discriminator is learned, adversarial learning tries to update the target inference function g t in order to confuse the discriminator. In other words, the adversarial training is looking for an inference function g t that is able to map a target sample to a feature space such that the discriminator D will no longer distinguish it from a source sample.</p><p>From the above discussion it is clear that in order to perform well, UDA needs to align the distributions effectively in order to be successful. This can happen only if distributions are represented by a sufficiently large dataset. Therefore, UDA approaches are in a position of weakness when we assume D t to be small. Moreover, UDA approaches have also another intrinsic limitation; even with perfect confusion alignment, there is no guarantee that samples from different domains but with the same class label will map nearby in the feature space. This lack of semantic alignment is a major source of performance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Handling Scarce Target Data</head><p>We are interested in the case where very few labeled target samples (as low as 1 sample per class) are available. We are facing two challenges in this setting. First, since the size of D t is small, we need to find a way to augment it. Second, we need to somehow use the label information of D t . Therefore, we create pairs of samples. In this way, we are able to alleviate the lack of training target samples by pairing them with each training source sample. In <ref type="bibr" target="#b37">[38]</ref>, we have shown that creating positive and negative pairs using source and target data is very effective for SDA. Since the method proposed in <ref type="bibr" target="#b37">[38]</ref> does not encode the domain information of the samples, it cannot be used in adversarial learning. Here we extend <ref type="bibr" target="#b37">[38]</ref> by creating 4 groups of pairs (G i , i = 1, 2, 3, 4) as follows: we break down the positive pairs into two groups (Groups 1 and 2), where pairs of the first group consist of samples from the source distribution with the same class labels, while pairs of the second group also have the same class label but come from different distributions (one from the source and one from the target distribution). This is important because we can encode both label and domain information of training samples. Similarly, we break down the negative pairs into two groups (Groups 3 and 4), where pairs of the third group consist of samples from the source distribution with different class labels, while pairs of the forth group come from different class labels and different distributions (one from the source and one from the target distributions). See Figure <ref type="figure">1</ref>. In order to give each group the same amount of members we use all possible pairs from G 2 , as it is the smallest, and then uniformly sample from the pairs in G 1 , G 3 , and G 4 to match the size of G 2 . Any reasonable amount of portions between the numbers of the pairs can also be used.</p><p>In classical adversarial learning we would at this point learn a domain discriminator, but since we have semantic information to consider as well, we are interested in learning a multi-class discriminator (we call it domain-class discriminator (DCD)) in order to introduce semantic alignment of the source and target domains. By expanding the binary classifier to its multiclass equivalent, we can train a classifier that will evaluate which of the 4 groups a given sample pair belongs to. We model the DCD with 2 fully connected layers with a softmax activation in the last layer which we can train with the standard categorical cross-entropy loss</p><formula xml:id="formula_4">L F ADA-D = -E[ 4 i=1 y Gi log(D(φ(G i )))] ,<label>(3)</label></formula><p>where y Gi is the label of G i and D is the DCD function. φ is a symbolic function that takes a pair as input and outputs the concatenation of the results of the appropriate inference functions. The output of φ is passed to the DCD (Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>In the second step, we are interested in updating g t in order to confuse the DCD in such a way that the DCD can no longer distinguish between groups 1 and 2, and also between groups 3 and 4 using the loss</p><formula xml:id="formula_5">L F ADA-g = -E[y G1 log(D(φ(G 2 ))) -y G3 log(D(φ(G 4 )))] .<label>(4)</label></formula><p>(4) is inspired by the non-saturating game <ref type="bibr" target="#b16">[17]</ref> and will force the inference function g t to embed target samples in a space that DCD will no longer be able to distinguish between them.</p><p>Connection with multi-class discriminators: Consider an image generation task where training samples come from k classes. Learning the image generator can be done by any standard k-class classifier and adding generated samples as a new class (generated class) and correspondingly increasing the dimension of the classifier output from k to k + 1. During the adversarial learning, only the generated class is confused. This has proven effective for image generation <ref type="bibr" target="#b47">[48]</ref> and other tasks. However, this is different than the proposed DCD, where group 1 is confused with 2, and group 3 is confused with 4. Inspired by <ref type="bibr" target="#b47">[48]</ref>, we are able to create a k + 4 classifier to also guarantee a high classification accuracy. Therefore, we suggest that (4) needs to be minimized together with the main classifier loss</p><formula xml:id="formula_6">L F ADA-g = -γE[y G1 log(D(g(G 2 )))-y G3 log(D(g(G 4 )))]+E[ℓ(f (X s ), Y )]+E[ℓ(f (X t ), Y )] ,<label>(5)</label></formula><p>where γ strikes the balance between classification and confusion. Misclassifying pairs from group 2 as group 1 and likewise for groups 4 and 3, means that the DCD is no longer able to distinguish positive or negative pairs of different distributions from positive or negative pairs of the source distribution, while the classifier is still able to discriminate positive pairs from negative pairs. This simultaneously satisfies the two main goals of SDA, domain confusion and class separability in the feature space. UDA only looks for domain confusion and does not address class separability, because of the lack of labeled target samples. Connection with conditional GANs: Concatenation of outputs of different inferences has been done before in conditional GANs. For example, <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b63">64]</ref> concatenate the input text to the penultimate layers of the discriminators. <ref type="bibr" target="#b24">[25]</ref> concatenates positive and negative pairs before passing them to the discriminator. However, all of them use the vanilla binary discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship between g s and g t :</head><p>There is no restriction for g s and g t and they can be constrained or unconstrained. An obvious choice of constraint is equality (weight-sharing) which makes the inference functions symmetric. This can be seen as a regularizer and will reduce overfitting <ref type="bibr" target="#b37">[38]</ref>. Another approach would be learning an asymmetric inference function <ref type="bibr" target="#b44">[45]</ref>. Since we have access to very few target samples, we use weight-sharing (g s = g t = g). Choice of g s , g t , and h: Since we are interested in visual recognition, the inference functions g s and g t are modeled by a convolutional neural network (CNN) with some initial convolutional layers, followed by some fully connected layers which are described specifically in the experiments section. In addition, the prediction function h is modeled by fully connected layers with a softmax activation function for the last layer.</p><p>Training Process: Here we discuss the training process for the weight-sharing regularizer (g s = g t = g). Once the inference functions g and the prediction function h are chosen, FADA takes the following steps: First, g and h are initialized using the source dataset D s . Then, the mentioned four groups of pairs should be created using D s and D t . The next step is training DCD using the four groups of pairs. This should be done by freezing g. In the next step, the inference function g and prediction function h should be updated in order to confuse DCD and maintain high classification accuracy. This should be done by freezing DCD. See Algorithm 1 and Figure <ref type="figure" target="#fig_1">2</ref>. The training process for the non weight-sharing case can be derived similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present results using the Office dataset <ref type="bibr" target="#b46">[47]</ref>, the MNIST dataset <ref type="bibr" target="#b31">[32]</ref>, the USPS dataset <ref type="bibr" target="#b23">[24]</ref>, and the SVHN dataset <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST-USPS-SVHN Datasets</head><p>The MNIST (M), USPS (U), and SVHN (S) datasets have recently been used for domain adaptation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b58">59]</ref>. They contain images of digits from 0 to 9 in various different environments including in the wild in the case of SVHN <ref type="bibr" target="#b39">[40]</ref>. We considered six cross-domain tasks. The first two tasks include M → U, U → M, and followed the experimental setting in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49]</ref>, which involves randomly selecting 2000 images from MNIST and 1800 images from USPS. For the rest of the cross-domain tasks, M → S, S → M, U → S, and S → U, we used all training samples of the source domain for training and all testing samples of the target domain for testing. Table <ref type="table">2</ref>: Office dataset. Classification accuracy for domain adaptation over the 31 categories of the Office dataset. A, W, and D stand for Amazon, Webcam, and DSLR domain. LB is our base model without adaptation. Since we do not train any convolutional layers and only use pre-computed DeCaF-fc7 features as input, we expect a more challenging task compared to <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b26">27]</ref>.   Since <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49]</ref> introduced unsupervised methods, they used all samples of a target domain as unlabeled data in training. Here instead, we randomly selected n labeled samples per class from target domain data and used them in training. We evaluated our approach for n ranging from 1 to 4 and repeated each experiment 10 times (we only show the mean of the accuracies for this experiment because standard deviation is very small).</p><p>Since the images of the USPS dataset have 16 × 16 pixels, we resized the images of the MNIST and SVHN datasets to 16 × 16 pixels. We assume g s and g t share weights (g = g s = g t ) for this experiment. Similar to <ref type="bibr" target="#b31">[32]</ref>, we used 2 convolutional layers with 6 and 16 filters of 5 × 5 kernels followed by max-pooling layers and 2 fully connected layers with size 120 and 84 as the inference function g, and one fully connected layer with softmax activation as the prediction function h. Also, we used 2 fully connected layers with size 64 and 4 as DCD (4 groups classifier). Training for each stage was done using the Adam Optimizer <ref type="bibr" target="#b25">[26]</ref>. We compare our method with 1 SDA method, under the same condition, and 6 recent UDA methods. UDA methods use all target samples in their training stage, while we only use very few labeled target samples per category in training.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the classification accuracies, where FADA -n stands for our method when we use n labeled target samples per category in training. FADA works well even when only one target sample per category (n = 1) is available in training. Also, we can see that by increasing n, the accuracy goes up. This is interesting because we can get comparable accuracies with the state-of-the-art using only 10 labeled target samples (one sample per class) instead of using more than thousands unlabeled target samples. We also report the lower bound (LB) of our model which corresponds to training the base model using only source samples. Moreover, we report the accuracies obtained by fine-tuning (FT) the base model on available target data. Although Table <ref type="table" target="#tab_1">1</ref> shows that FT increases the accuracies over LB, it has reduce performance compared to SDA methods.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows how much improvement can be obtained with respect to the base model. The base model is the lower bound LB. This is simply obtained by training g and h with only the classification loss and source training data; so, no adaptation is performed.</p><p>Weight-Sharing. As we discussed earlier, weight-sharing can be seen as a regularizer that prevents the target network g t from overfitting. This is important because g t can be easily overfitted since target data is scarce. We repeated the experiment for the U → M with n = 5 without sharing weights. This provides an average accuracy of 84.1 over 10 repetitions, which is less than the weight-sharing case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Office Dataset</head><p>The office dataset is a standard benchmark dataset for visual domain adaptation. It contains 31 object classes for three domains: Amazon, Webcam, and DSLR, indicated as A, W, and D, for a total of 4,652 images. The first domain A, consists of images downloaded from online merchants, the second W, consists of low resolution images acquired by webcams, the third D, consists of high resolution images collected with digital SLRs. We consider four domain shifts using the three domains (A → W, A → D, W → A, and D → A). Since there is not a considerable domain shift between W and D, we exclude W → D and D → W.</p><p>We followed the setting described in <ref type="bibr" target="#b57">[58]</ref>. All classes of the office dataset and 5 train-test splits are considered. For the source domain, 20 examples per category for the Amazon domain, and 8  <ref type="table" target="#tab_1">1</ref> for the corresponding domain pair. The middle bar is the improvement of finetuning FT the base model using the available target data reported in Table <ref type="table" target="#tab_1">1</ref>. The top bar is the improvement of FADA over FT, also reported in Table <ref type="table" target="#tab_1">1</ref>.</p><p>examples per category for the DSLR and Webcam domains are randomly selected for training for each split. Also, 3 labeled examples are randomly selected for each category in the target domain for training for each split. The rest of the target samples are used for testing. Note that we used the same splits generated by <ref type="bibr" target="#b57">[58]</ref>.</p><p>In addition to the SDA algorithms, we report the results of some recent UDA algorithms. They follow a different experimental protocol compared to the SDA algorithms, and use all samples of the target domain in training as unlabeled data together with all samples of the source domain. So, we cannot make an exact comparison between results. However, since UDA algorithms use all samples of the target domain in training and we use only very few of them (3 per class), we think it is still worth looking at how they differ.</p><p>Here we are interested in the case where g s and g t share weights (g s = g t = g). For the inference function g, we used the convolutional layers of the VGG-16 architecture <ref type="bibr" target="#b52">[53]</ref> followed by 2 fully connected layers with output size of 1024 and 128, respectively. For the prediction function h, we used a fully connected layer with softmax activation. Similar to <ref type="bibr" target="#b57">[58]</ref>, we used the weights pretrained on the ImageNet dataset <ref type="bibr" target="#b45">[46]</ref> for the convolutional layers, and initialized the fully connected layers using all the source domain data. We model the DCD with 2 fully connected layers with a softmax activation in the last layer.</p><p>Table <ref type="table">2</ref> reports the classification accuracy over 31 classes for the Office dataset and shows that FADA has performance comparable to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced a deep model combining a classification and an adversarial loss to address SDA in few-shot learning regime. We have shown that adversarial learning can be augmented to address SDA. The approach is general in the sense that the architecture sub-components can be changed. We found that addressing the semantic distribution alignments with point-wise surrogates of distribution distances and similarities for SDA works very effectively, even when labeled target samples are very few. In addition, we found the SDA accuracy to converge very quickly as more labeled target samples per category are available. The approach shows clear promise as it sets new state-of-the-art performance in the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Few-shot adversarial domain adaptation. For simplicity we show our networks in the case of weight sharing (g s = g t = g). (a) In the first step, we initialized g and h using the source samples D s . (b) We freeze g and train a DCD. The picture shows a pair from the second group G 2 when the samples come from two different distributions but the same class label. (c) We freeze the DCD and update g and h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 53.1 ± 0.3 54.09 ± 0.5 65.0 ± 0.5 65.7 ± 1.7 72.1 ± 1.0 71.1 ± 0.9 W → D 95.6 ± 0.7 98.5 ± 0.4 99.0 ± 0.2 99.0 ± 0.2 97.6 ± 0.2 97.5 ± 0.7 97.6 ± 0.4 97.5 ± 0.6 D → A 58.5 ± 0.8 52.1 ± 0.8 54.0 ± 0.4 56.0 ± 0.5 66.2 ± 0.3 66.5 ± 1.0 71.8 ± 0.5 68.1 ± 06 D → W 80.1 ± 0.6 95.0 ± 0.5 96.0 ± 0.3 96.4 ± 0.3 95.7 ± 0.5 95.5 ± 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MNIST-USPS-SVHN summary. The lower bar of each column represents the LB as reported in Table1for the corresponding domain pair. The middle bar is the improvement of finetuning FT the base model using the available target data reported in Table1. The top bar is the improvement of FADA over FT, also reported in Table1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>E[•] denotes statistical expectation and ℓ could be any appropriate loss function. When the distributions of X s and X t are different, a deep model f s trained with D s will have reduced performance on the target domain. Increasing it would be trivial by simply training a new model f t with data D t . However, D t is small and deep models require large amounts of labeled data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>MNIST-USPS-SVHN datasets. Classification accuracy for domain adaptation over the MNIST, USPS, and SVHN datasets. M, U, and S stand for MNIST, USPS, and SVHN domain. LB is our base model without adaptation. FT and FADA stand for fine-tuning and our method, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Traditional UDA</cell><cell cols="3">Adversarial UDA</cell><cell></cell></row><row><cell></cell><cell cols="8">LB [60] [45] [15] [33] [59] [49] SDA</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FT</cell><cell>82.3 84.9 85.7 86.5 87.2 88.4 88.6</cell></row><row><cell cols="2">M → U 65.4</cell><cell>47.8</cell><cell>60.7</cell><cell>91.8</cell><cell>91.2</cell><cell>89.4</cell><cell>92.5</cell><cell cols="2">[38] 85.0 89.0 90.1 91.4 92.4 93.0 92.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FADA 89.1 91.3 91.9 93.3 93.4 94.0 94.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FT</cell><cell>72.6 78.2 81.9 83.1 83.4 83.6 84.0</cell></row><row><cell cols="2">U → M 58.6</cell><cell>63.1</cell><cell>67.3</cell><cell>73.7</cell><cell>89.1</cell><cell>90.1</cell><cell>90.8</cell><cell cols="2">[38] 78.4 82.2 85.8 86.1 88.8 89.6 89.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FADA 81.1 84.2 87.5 89.9 91.1 91.2 91.5</cell></row><row><cell cols="2">S → M 60.1</cell><cell>-</cell><cell>-</cell><cell>82.0</cell><cell>76.0</cell><cell>-</cell><cell>84.7</cell><cell cols="2">FT FADA 72.8 81.8 82.6 85.1 86.1 86.8 87.2 65.5 68.6 70.7 73.3 74.5 74.6 75.4</cell></row><row><cell cols="2">M → S 20.3</cell><cell>-</cell><cell>-</cell><cell>40.1</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell cols="2">FT FADA 37.7 40.5 42.9 46.3 46.1 46.8 47.0 29.7 31.2 36.1 36.7 38.1 38.3 39.1</cell></row><row><cell>S → U</cell><cell>66.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">FT FADA 78.3 83.2 85.2 85.7 86.2 87.1 87.5 69.4 71.8 74.3 76.2 78.1 77.9 78.9</cell></row><row><cell>U → S</cell><cell>15.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">FT FADA 27.5 29.8 34.5 36.0 37.9 41.3 42.9 19.9 22.2 22.8 24.6 25.4 25.4 25.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2252" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-linear domain adaptation with boosting</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="485" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing RGB images by learning from RGB-D data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="1418" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DeCAF: a deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain transfer svm for video concept detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1375" to="1381" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint cross-domain classification and subspace learning for unsupervised adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaarsc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Pattern Recogition Letters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<title level="m">Unsupervised domain adaptation by backpropagation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A kernel method for the twosample-problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross language text classification via subspace co-regularized multi-view learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the 29th International Conference on Machine Learning, ICML 2012<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-01">June 26 -July 1, 2012, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08195</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1785" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning using privileged information: SVM+ and weighted SVM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer sparse coding for robust image representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Information bottleneck domain adaptation with privileged information for visual recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Information bottleneck learning using privileged information for visual recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1496" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dataset issues in object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward category-level object recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Beyond sharing weights for deep domain adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06432</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01705</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adaptive svm+: Learning with privileged information for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vrigkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning the roots of visual domain shift</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A deeper look at dataset bias</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="504" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adapting svm classifiers to data with shifted distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops, 2007. ICDM Workshops 2007</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
	<note>Seventh IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation with subspace learning for visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
