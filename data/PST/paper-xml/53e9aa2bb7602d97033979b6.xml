<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Computing Group Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhixun</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76BACF7791E5C4044030508FB95D5C1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve lowrank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n 3 ) of the original ADM based method to O(rn 2 ), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, compressive sensing <ref type="bibr" target="#b4">[5]</ref> and sparse representation <ref type="bibr" target="#b18">[19]</ref> have been hot research topics and also have found abundant applications in signal processing and machine learning. Many of the problems in these fields can be formulated as the following linearly constrained convex programs:</p><formula xml:id="formula_0">min x,y f (x) + g(y), s.t. A(x) + B(y) = c,<label>(1)</label></formula><p>where x, y and c could be either vectors or matrices, f and g are convex functions (e.g., the nuclear norm ∥ • ∥ * <ref type="bibr" target="#b1">[2]</ref>, Frobenius norm ∥ • ∥, l 2,1 norm ∥ • ∥ 2,1 <ref type="bibr" target="#b12">[13]</ref>, and l 1 norm ∥ • ∥ 1 ), and A and B are linear mappings.</p><p>Although the interior point method can be used to solve many convex programs, it may suffer from unbearably high computation cost when handling large scale problems. For example, when using CVX, an interior point based toolbox, to solve nuclear norm minimization (namely, f (X) = ∥X∥ * in (1)) problems, such as matrix completion <ref type="bibr" target="#b3">[4]</ref>, robust principal component analysis <ref type="bibr" target="#b17">[18]</ref> and their combination <ref type="bibr" target="#b2">[3]</ref>, the complexity of each iteration is O(n 6 ), where n × n is the matrix size. To overcome this issue, first-order methods are often preferred. The accelerated proximal gradient (APG) algorithm <ref type="bibr" target="#b15">[16]</ref> is a popular technique due to its guaranteed O(k -2 ) convergence rate, where k is the iteration number. The alternating direction method (ADM) has also regained a lot of attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. It updates the variables alternately by minimizing the augmented Lagrangian function with respect to the variables in a Gauss-Seidel manner. While APG has to convert (1) into an approximate unconstrained problem by adding the linear constraints to the objective function as a penalty, hence only producing an approximate solution to <ref type="bibr" target="#b0">(1)</ref>, ADM can solve (1) exactly. However, when A or B is not the identity mapping, the subproblems in ADM may not have closed form solutions. So solving them is cumbersome.</p><p>In this paper, we propose a linearized version of ADM (LADM) to overcome the difficulty in solving subproblems. It is to replace the quadratic penalty term by linearizing the penalty term and adding a proximal term. We also allow the penalty parameter to change adaptively and propose a novel and simple rule to update it. Linearization makes the auxiliary variables unnecessary, hence saving memory and waiving the expensive matrix inversions to update the auxiliary variables. Moreover, without the extra constraints introduced by the auxiliary variables, the convergence is also faster. Using a variable penalty parameter further speeds up the convergence. The global convergence of LADM with adaptive penalty (LADMAP) is also proven.</p><p>As an example, we apply our LADMAP to solve the low-rank representation (LRR) problem <ref type="bibr" target="#b11">[12]</ref> <ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_1">min Z,E ∥Z∥ * + µ∥E∥ 2,1 , s.t. X = XZ + E, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where X is the data matrix. LRR is an important robust subspace clustering technique and has found wide applications in machine learning and computer vision, e.g., motion segmentation, face clustering, and temporal segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6]</ref>. However, the existing LRR solver <ref type="bibr" target="#b11">[12]</ref> is based on ADM, which suffers from O(n 3 ) computation complexity due to the matrix-matrix multiplications and matrix inversions. Moreover, introducing auxiliary variables also slows down the convergence, as there are more variables and constraints. Such a heavy computation load prevents LRR from large scale applications. It is LRR that motivated us to develop LADMAP. We show that LADMAP can be successfully applied to LRR, obtaining faster convergence speed than the original solver. By further representing Z as its skinny SVD and utilizing an advanced functionality of the PROPACK <ref type="bibr" target="#b8">[9]</ref> package, the complexity of solving LRR by LADMAP becomes only O(rn 2 ), as there is no full sized matrix-matrix multiplications, where r is the rank of the optimal Z. Numerical experiments show the great speed advantage of our LADMAP based methods for solving LRR.</p><p>Our work is inspired by Yang et al. <ref type="bibr" target="#b19">[20]</ref>. Nonetheless, the difference of our work from theirs is distinct. First, they only proved the convergence of LADM for a specific problem, namely nuclear norm regularization. Their proof utilized some special properties of the nuclear norm, while we prove the convergence of LADM for general problems in <ref type="bibr" target="#b0">(1)</ref>. Second, they only proved in the case of fixed penalty, while we prove in the case of variable penalty. Although they mentioned the dynamic updating rule proposed in <ref type="bibr" target="#b7">[8]</ref>, their proof cannot be straightforwardly applied to the case of variable penalty. Moreover, that rule is for ADM only. Third, the convergence speed of LADM heavily depends on the choice of penalty. So it is difficult to choose an optimal fixed penalty that fits for different problems and problem sizes, while our novel updating rule for the penalty, although simple, is effective for different problems and problem sizes. The linearization technique has also been used in other optimization methods. For example, Yin <ref type="bibr" target="#b21">[22]</ref> applied this technique to the Bregman iteration for solving compressive sensing problems and proved that the linearized Bregman method converges to an exact solution conditionally. In comparison, LADM (and LADMAP) always converges to an exact solution.</p><p>2 Linearized Alternating Direction Method with Adaptive Penalty</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Alternating Direction Method</head><p>ADM is now very popular in solving large scale machine learning problems <ref type="bibr" target="#b0">[1]</ref>. When solving (1) by ADM, one operates on the following augmented Lagrangian function:</p><formula xml:id="formula_3">L(x, y, λ) = f (x) + g(y) + ⟨λ, A(x) + B(y) -c⟩ + β 2 ∥A(x) + B(y) -c∥ 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where λ is the Lagrange multiplier, ⟨•, •⟩ is the inner product, and β &gt; 0 is the penalty parameter. The usual augmented Lagrange multiplier method is to minimize L w.r.t. x and y simultaneously. This is usually difficult and does not exploit the fact that the objective function is separable. To remedy this issue, ADM decomposes the minimization of L w.r.t. (x, y) into two subproblems that minimize w.r.t. x and y, respectively. More specifically, the iterations of ADM go as follows:</p><formula xml:id="formula_5">x k+1 = arg min x L(x, y k , λ k ) = arg min x f (x) + β 2 ∥A(x) + B(y k ) -c + λ k /β∥ 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">y k+1 = arg min y L(x k+1 , y, λ k ) = arg min y g(y) + β 2 ∥B(y) + A(x k+1 ) -c + λ k /β∥ 2 , (<label>5</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">λ k+1 = λ k + β[A(x k+1 ) + B(y k+1 ) -c].<label>(6)</label></formula><p>In many machine learning problems, as f and g are matrix or vector norms, the subproblems ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_7">5</ref>) usually have closed form solutions when A and B are identities <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. In this case, ADM is appealing. However, in many problems A and B are not identities. For example, in matrix completion A can be a selection matrix, and in LRR and 1D sparse representation A can be a general matrix. In this case, there are no closed form solutions to ( <ref type="formula" target="#formula_5">4</ref>) and <ref type="bibr" target="#b4">(5)</ref>. Then ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_7">5</ref>) have to be solved iteratively. To overcome this difficulty, a common strategy is to introduce auxiliary variables <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref> u and v and reformulate problem ( <ref type="formula" target="#formula_0">1</ref>) into an equivalent one:</p><formula xml:id="formula_10">min x,y,u,v f (x) + g(y), s.t. A(u) + B(v) = c, x = u, y = v,<label>(7)</label></formula><p>and the corresponding ADM iterations analogous to ( <ref type="formula" target="#formula_5">4</ref>)-( <ref type="formula" target="#formula_9">6</ref>) can be deduced. With more variables and more constraints, more memory is required and the convergence of ADM also becomes slower. Moreover, to update u and v, whose subproblems are least squares problems, expensive matrix inversions are often necessary. Even worse, the convergence of ADM with more than two variables is not guaranteed <ref type="bibr" target="#b6">[7]</ref>.</p><p>To avoid introducing auxiliary variables and still solve subproblems ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_7">5</ref>) efficiently, inspired by Yang et al. <ref type="bibr" target="#b19">[20]</ref>, we propose a linearization technique for ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_7">5</ref>). To further accelerate the convergence of the algorithm, we also propose an adaptive rule for updating the penalty parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linearized ADM</head><p>By linearizing the quadratic term in (4) at x k and adding a proximal term, we have the following approximation:</p><formula xml:id="formula_11">x k+1 = arg min x f (x) + ⟨A * (λ k ) + βA * (A(x k ) + B(y k ) -c), x -x k ⟩ + βηA 2 ∥x -x k ∥ 2 = arg min x f (x) + βη A 2 ∥x -x k + A * (λ k + β(A(x k ) + B(y k ) -c))/(βη A )∥ 2 ,</formula><p>(8) where A * is the adjoint of A and η A &gt; 0 is a parameter whose proper value will be analyzed later. The above approximation resembles that of APG <ref type="bibr" target="#b15">[16]</ref>, but we do not use APG to solve (4) iteratively.</p><p>Similarly, subproblem (5) can be approximated by</p><formula xml:id="formula_12">y k+1 = arg min y g(y) + βη B 2 ∥y -y k + B * (λ k + β(A(x k+1 ) + B(y k ) -c))/(βη B )∥ 2 . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>The update of Lagrange multiplier still goes as (6)<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adaptive Penalty</head><p>In previous ADM and LADM approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>, the penalty parameter β is fixed. Some scholars have observed that ADM with a fixed β can converge very slowly and it is nontrivial to choose an optimal fixed β. So is LADM. Thus a dynamic β is preferred in real applications. Although Tao et al. <ref type="bibr" target="#b14">[15]</ref> and Yang et al. <ref type="bibr" target="#b19">[20]</ref> mentioned He et al.'s adaptive updating rule <ref type="bibr" target="#b7">[8]</ref> in their papers, the rule is for ADM only. We propose the following adaptive updating strategy for the penalty parameter β:</p><formula xml:id="formula_14">β k+1 = min(β max , ρβ k ),<label>(10)</label></formula><p>where β max is an upper bound of {β k }. The value of ρ is defined as</p><formula xml:id="formula_15">ρ = { ρ 0 , if β k max( √ η A ∥x k+1 -x k ∥, √ η B ∥y k+1 -y k ∥)/∥c∥ &lt; ε 2 , 1, otherwise,<label>(11)</label></formula><p>where ρ 0 ≥ 1 is a constant. The condition to assign ρ = ρ 0 comes from the analysis on the stopping criteria (see Section 2.5). We recommend that β 0 = αε 2 , where α depends on the size of c. Our updating rule is fundamentally different from He et al.'s for ADM <ref type="bibr" target="#b7">[8]</ref>, which aims at balancing the errors in the stopping criteria and involves several parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convergence of LADMAP</head><p>To prove the convergence of LADMAP, we first have the following propositions.</p><p>Proposition 1</p><formula xml:id="formula_16">-β k η A (x k+1 -x k )-A * ( λk+1 ) ∈ ∂f (x k+1 ), -β k η B (y k+1 -y k )-B * ( λk+1 ) ∈ ∂g(y k+1 ), (<label>12</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">where λk+1 = λ k + β k [A(x k ) + B(y k ) -c], λk+1 = λ k + β k [A(x k+1 ) + B(y k ) -c],</formula><p>and ∂f and ∂g are subgradients of f and g, respectively.</p><p>This can easily proved by checking the optimality conditions of ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_12">9</ref>).  <ref type="formula" target="#formula_20">13</ref>)-( <ref type="formula" target="#formula_21">14</ref>)), then: <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_19">{η A ∥x k -x * ∥ 2 -∥A(x k -x * )∥ 2 + η B ∥y k -y * ∥ 2 + β -2 k ∥λ k -λ * ∥ 2 } is non-increasing. (2). ∥x k+1 -x k ∥ → 0, ∥y k+1 -y k ∥ → 0, ∥λ k+1 -λ k ∥ → 0.</formula><p>The proof can be found in Supplementary Material. Then we can prove the convergence of LADMAP, as stated in the following theorem.</p><p>Theorem 3 If {β k } is non-decreasing and upper bounded, η A &gt; ∥A∥ 2 , and η B &gt; ∥B∥ 2 , then the sequence {(x k , y k , λ k )} generated by LADMAP converges to a KKT point of problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>The proof can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Stopping Criteria</head><p>The KKT conditions of problem <ref type="bibr" target="#b0">(1)</ref> are that there exists a triple (x * , y * , λ * ) such that</p><formula xml:id="formula_20">A(x * ) + B(y * ) -c = 0, (<label>13</label></formula><formula xml:id="formula_21">) -A * (λ * ) ∈ ∂f (x * ), -B * (λ * ) ∈ ∂g(y * ).<label>(14)</label></formula><p>The triple (x * , y * , λ * ) is called a KKT point. So the first stopping criterion is the feasibility:</p><formula xml:id="formula_22">∥A(x k+1 ) + B(y k+1 ) -c∥/∥c∥ &lt; ε 1 . (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>As for the second KKT condition, we rewrite the second part of Proposition 1 as follows</p><formula xml:id="formula_24">-β k [η B (y k+1 -y k ) + B * (A(x k+1 -x k ))] -B * ( λk+1 ) ∈ ∂g(y k+1 ).<label>(16)</label></formula><p>So for λk+1 to satisfy the second KKT condition, both</p><formula xml:id="formula_25">β k η A ∥x k+1 -x k ∥ and β k ∥η B (y k+1 -y k ) + B * (A(x k+1 -x k ))</formula><p>∥ should be small enough. This leads to the second stopping criterion:</p><formula xml:id="formula_26">β k max(η A ∥x k+1 -x k ∥/∥A * (c)∥, η B ∥y k+1 -y k ∥/∥B * (c)∥) ≤ ε ′ 2 . (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>By estimating ∥A * (c)∥ and ∥B * (c)∥ by √ η A ∥c∥ and √ η B ∥c∥, respectively, we arrive at the second stopping criterion in use:</p><formula xml:id="formula_28">β k max( √ η A ∥x k+1 -x k ∥, √ η B ∥y k+1 -y k ∥)/∥c∥ ≤ ε 2 . (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>Finally, we summarize our LADMAP algorithm in Algorithm 1.</p><p>Algorithm 1 LADMAP for Problem (1)</p><formula xml:id="formula_30">Initialize: Set ε 1 &gt; 0, ε 2 &gt; 0, β max ≫ β 0 &gt; 0, η A &gt; ∥A∥ 2 , η B &gt; ∥B∥ 2 ,</formula><p>x 0 , y 0 , λ 0 , and k ← 0. while (15) or ( <ref type="formula" target="#formula_28">18</ref>) is not satisfied do</p><p>Step 1: Update x by solving (8).</p><p>Step 2: Update y by solving <ref type="bibr" target="#b8">(9)</ref>.</p><p>Step 3: Update λ by <ref type="bibr" target="#b5">(6)</ref>.</p><p>Step 4: Update β by ( <ref type="formula" target="#formula_14">10</ref>) and <ref type="bibr" target="#b10">(11)</ref>.</p><p>Step 5: k ← k + 1. end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applying LADMAP to LRR</head><p>In this section, we apply LADMAP to solve the LRR problem (2). We further introduce acceleration tricks to reduce the computation complexity of each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solving LRR by LADMAP</head><p>As the LRR problem ( <ref type="formula" target="#formula_1">2</ref>) is a special case of problem (1), PADM can be directly applied to it. The two subproblems both have closed form solutions. In the subproblem for updating E, one may apply the l 2,1 -norm shrinkage operator <ref type="bibr" target="#b11">[12]</ref>, with a threshold</p><formula xml:id="formula_31">β -1 k , to matrix M k = -XZ k + X -Λ k /β k .</formula><p>In the subproblem for updating Z, one has to apply the singular value shrinkage operator <ref type="bibr" target="#b1">[2]</ref>, with a threshold</p><formula xml:id="formula_32">(β k η X ) -1 , to matrix N k = Z k -η -1 X X T (XZ k + E k+1 -X + Λ k /β k ), where η X &gt; σ 2 max (X).</formula><p>If N k is formed explicitly, the usual technique of partial SVD, using PROPACK <ref type="bibr" target="#b8">[9]</ref> and rank prediction <ref type="foot" target="#foot_2">3</ref> , can be utilized to compute the leading r singular values and associated vectors of N k efficiently, making the complexity of SVD computation O(rn 2 ), where r is the predicted rank of Z k+1 and n is the column number of X. Note that as β k is non-decreasing, the predicted rank is almost non-decreasing, making the iterations computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acceleration Tricks for LRR</head><p>Up to now, LADMAP for LRR is still of complexity O(n 3 ), although partial SVD is already used. This is because forming M k and N k requires full sized matrix-matrix multiplications, e.g., XZ k . To break this complexity bound, we introduce a decomposition technique to further accelerate LADMAP for LRR. By representing Z k as its skinny SVD:</p><formula xml:id="formula_33">Z k = U k Σ k V T</formula><p>k , some of the full sized matrix-matrix multiplications are gone: they are replaced by successive reduced sized matrix-matrix multiplications. For example, when updating E, XZ k is computed as</p><formula xml:id="formula_34">((XU k )Σ k )V T</formula><p>k , reducing the complexity to O(rn 2 ). When computing the partial SVD of N k , things are more complicated. If we form N k explicitly, we will face with computing X T (X + Λ k /β k ), which is neither low-rank nor sparse <ref type="foot" target="#foot_3">4</ref> . Fortunately, in PROPACK the bi-diagonalizing process of N k is done by the Lanczos procedure <ref type="bibr" target="#b8">[9]</ref>, which only requires to compute matrix-vector multiplications N k v and u T N k , where u and v are some vectors in the Lanczos procedure. So we may compute N k v and u T N k by multiplying the vectors u and v successively with the component matrices in N k , rather than forming N k explicitly. So the computation complexity of partial SVD of N k is still O(rn 2 ). Consequently, with our acceleration techniques, the complexity of our accelerated LADMAP (denoted as LADMAP(A) for short) for LRR is O(rn 2 ). LADMAP(A) is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Accelerated LADMAP for LRR (2)</head><p>Input: Observation matrix X and parameter µ &gt; 0.</p><p>Initialize: Set E 0 , Z 0 and Λ 0 to zero matrices, where Z 0 is represented as</p><formula xml:id="formula_35">(U 0 , Σ 0 , V 0 ) ← (0, 0, 0). Set ε 1 &gt; 0, ε 2 &gt; 0, β max ≫ β 0 &gt; 0, η X &gt; σ 2 max (X), r = 5</formula><p>, and k ← 0. while (15) or ( <ref type="formula" target="#formula_28">18</ref>) is not satisfied do</p><p>Step 1:</p><formula xml:id="formula_36">Update E k+1 = arg min E µ∥E∥ 2,1 + β k 2 ∥E + (XU k )Σ k V T k -X + Λ k /β k ∥ 2</formula><p>. This subproblem can be solved by using Lemma 3.2 in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Step 2: Update the skinny SVD (U k+1 , Σ k+1 , V k+1 ) of Z k+1 . First, compute the partial SVD Ũr Σr ṼT r of the implicit matrix N k , which is bi-diagonalized by the successive matrixvector multiplication technique described in Section 3.1. Second, U k+1 = Ũr (:, 1 : r ′ ),</p><formula xml:id="formula_37">Σ k+1 = Σr (1 : r ′ , 1 : r ′ ) -(β k η X ) -1 I, V k+1 = Ṽr (:, 1 : r ′ )</formula><p>, where r ′ is the number of singular values in Σ r that are greater than (β k η X ) -1 .</p><p>Step 3: Update the predicted rank r:</p><formula xml:id="formula_38">If r ′ &lt; r, then r = min(r ′ + 1, n); otherwise, r = min(r ′ + round(0.05n), n). Step 4: Update Λ k+1 = Λ k + β k ((XU k+1 )Σ k+1 V T k+1 + E k+1 -X).</formula><p>Step 5: Update β k+1 by ( <ref type="formula" target="#formula_14">10</ref>)- <ref type="bibr" target="#b10">(11)</ref>.</p><p>Step 6: k ← k + 1. end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we report numerical results on LADMAP, LADMAP(A) and other state-of-the-art algorithms, including APG <ref type="foot" target="#foot_4">5</ref> , ADM <ref type="foot" target="#foot_5">6</ref> and LADM, for LRR based data clustering problems. APG, ADM, LADM and LADMAP all utilize the Matlab version of PROPACK <ref type="bibr" target="#b8">[9]</ref>. For LADMAP(A), we provide two function handles to PROPACK which fulfils the successive matrix-vector multiplications. All experiments are run and timed on a PC with an Intel Core i5 CPU at 2.67GHz and with 4GB of memory, running Windows 7 and Matlab version 7.10.</p><p>We test and compare these solvers on both synthetic multiple subspaces data and the real world motion data (Hopkin155 motion segmentation database <ref type="bibr" target="#b16">[17]</ref>). For APG, we set the parameters β 0 = 0.01, β min = 10 -10 , θ = 0.9 in its continuation technique and the Lipschitz constant τ = σ 2 max (X). The parameters of ADM and LADM are the same as those in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>, respectively. In particular, for LADM the penalty is fixed at β = 2.5/ min(m, n), where m × n is the size of X. For LADMAP, we set ε 1 = 10 -4 , ε 2 = 10 -5 , β 0 = min(m, n)ε 2 , β max = 10 10 , ρ 0 = 1.9, and η X = 1.02σ 2 max (X). As the code of ADM was downloaded, its stopping criteria,</p><formula xml:id="formula_39">∥XZ k + E k -X∥/∥X∥ ≤ ε 1 and max(∥E k -E k-1 ∥/∥X∥, ∥Z k -Z k-1 ∥/∥X∥) ≤ ε 2 ,</formula><p>are used in all our experiments <ref type="foot" target="#foot_6">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">On Synthetic Data</head><p>The synthetic test data, parameterized as (s, p, d, r), is created by the same procedure in <ref type="bibr" target="#b11">[12]</ref>. s independent subspaces {S i } s i=1 are constructed, whose bases {U i } s i=1 are generated by</p><formula xml:id="formula_40">U i+1 = TU i , 1 ≤ i ≤ s -1,</formula><p>where T is a random rotation and U 1 is a d × r random orthogonal matrix. So each subspace has a rank of r and the data has an ambient dimension of d. Then p data points are sampled from each subspace by X i = U i Q i , 1 ≤ i ≤ s, with Q i being an r × p i.i.d. zero mean unit variance Gaussian matrix N (0, 1). 20% samples are randomly chosen to be corrupted by adding Gaussian noise with zero mean and standard deviation 0.1∥x∥. We empirically find that LRR achieves the best clustering performance on this data set when µ = 0.1. So we test all algorithms with µ = 0.1 in this experiment. To measure the relative errors in the solutions, we run LADMAP 2000 iterations with β max = 10 3 to establish the ground truth solution (E 0 , Z 0 ). The computational comparison is summarized in Table <ref type="table" target="#tab_1">1</ref>. We can see that the iteration numbers and the CPU times of both LADMAP and LADMAP(A) are much less than those of other methods, and LADMAP(A) is further much faster than LADMAP. Moreover, the advantage of LADMAP(A) is even greater when the ratio r/p, which is roughly the ratio of the rank of Z 0 to the size of Z 0 , is smaller, which testifies to the complexity estimations on LADMAP and LADMAP(A) for LRR. It is noteworthy that the iteration numbers of ADM and LADM seem to grow with the problem sizes, while that of LADMAP is rather constant. Moreover, LADM is not faster than ADM. In particular, on the last data we were unable to wait until LADM stopped. Finally, as APG converges to an approximate solution to (2), its relative errors are larger and its clustering accuracy is lower than ADM and LADM based methods. We further test the performance of these algorithms on the Hopkins155 database <ref type="bibr" target="#b16">[17]</ref>. This database consists of 156 sequences, each of which has 39 to 550 data vectors drawn from two or three motions. For computational efficiency, we preprocess the data by projecting it to be 5-dimensional using PCA. As µ = 2.4 is the best parameter for this database <ref type="bibr" target="#b11">[12]</ref>, we test all algorithms with µ = 2.4.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the comparison among APG, ADM, LADM, LADMAP and LADMAP(A) on this database. We can also see that LADMAP and LADMAP(A) are much faster than APG, ADM, and LADM, and LADMAP(A) is also faster than LADMAP. However, in this experiment the advantage of LADMAP(A) over LADMAP is not as dramatic as that in Table <ref type="table" target="#tab_1">1</ref>. This is because on this data µ is chosen as 2.4, which cannot make the rank of the ground truth solution Z 0 much smaller than the size of Z 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a linearized alternating direction method with adaptive penalty for solving subproblems in ADM conveniently. With LADMAP, no auxiliary variables are required and the convergence is also much faster. We further apply it to solve the LRR problem and combine it with an acceleration trick so that the computation complexity is reduced from O(n 3 ) to O(rn 2 ), which is highly advantageous over the existing LRR solvers. Although we only present results on LRR, LADMAP is actually a general method that can be applied to other convex programs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison among APG, ADM, LADM, LADMAP and LADMAP(A) on the synthetic data. For each quadruple (s, p, d, r), the LRR problem, with µ = 0.1, was solved for the same data using different algorithms. We present typical running time (in ×10 3 seconds), iteration number, relative error (%) of output solution ( Ê, Ẑ) and the clustering accuracy (%) of tested algorithms, respectively.</figDesc><table><row><cell>Size (s, p, d, r)</cell><cell>Method</cell><cell>Time</cell><cell>Iter.</cell><cell>∥ Ẑ-Z0∥ ∥Z0∥</cell><cell>∥ Ê-E0∥ ∥E0∥</cell><cell>Acc.</cell></row><row><cell></cell><cell>APG</cell><cell>0.0332</cell><cell>110</cell><cell>2.2079</cell><cell>1.5096</cell><cell>81.5</cell></row><row><cell></cell><cell>ADM</cell><cell>0.0529</cell><cell>176</cell><cell>0.5491</cell><cell>0.5093</cell><cell>90.0</cell></row><row><cell>(10, 20,200, 5)</cell><cell>LADM</cell><cell>0.0603</cell><cell>194</cell><cell>0.5480</cell><cell>0.5024</cell><cell>90.0</cell></row><row><cell></cell><cell>LADMAP</cell><cell>0.0145</cell><cell>46</cell><cell>0.5480</cell><cell>0.5024</cell><cell>90.0</cell></row><row><cell></cell><cell cols="2">LADMAP(A) 0.0010</cell><cell>46</cell><cell>0.5480</cell><cell>0.5024</cell><cell>90.0</cell></row><row><cell></cell><cell>APG</cell><cell>0.0869</cell><cell>106</cell><cell>2.4824</cell><cell>1.0341</cell><cell>80.0</cell></row><row><cell></cell><cell>ADM</cell><cell>0.1526</cell><cell>185</cell><cell>0.6519</cell><cell>0.4078</cell><cell>83.7</cell></row><row><cell>(15, 20,300, 5)</cell><cell>LADM</cell><cell>0.2943</cell><cell>363</cell><cell>0.6518</cell><cell>0.4076</cell><cell>86.7</cell></row><row><cell></cell><cell>LADMAP</cell><cell>0.0336</cell><cell>41</cell><cell>0.6518</cell><cell>0.4076</cell><cell>86.7</cell></row><row><cell></cell><cell cols="2">LADMAP(A) 0.0015</cell><cell>41</cell><cell>0.6518</cell><cell>0.4076</cell><cell>86.7</cell></row><row><cell></cell><cell>APG</cell><cell>1.8837</cell><cell>117</cell><cell>2.8905</cell><cell>2.4017</cell><cell>72.4</cell></row><row><cell></cell><cell>ADM</cell><cell>3.7139</cell><cell>225</cell><cell>1.1191</cell><cell>1.0170</cell><cell>80.0</cell></row><row><cell>(20, 25, 500, 5)</cell><cell>LADM</cell><cell>8.1574</cell><cell>508</cell><cell>0.6379</cell><cell>0.4268</cell><cell>80.0</cell></row><row><cell></cell><cell>LADMAP</cell><cell>0.7762</cell><cell>40</cell><cell>0.6379</cell><cell>0.4268</cell><cell>84.6</cell></row><row><cell></cell><cell cols="2">LADMAP(A) 0.0053</cell><cell>40</cell><cell>0.6379</cell><cell>0.4268</cell><cell>84.6</cell></row><row><cell></cell><cell>APG</cell><cell>6.1252</cell><cell>116</cell><cell>3.0667</cell><cell>0.9199</cell><cell>69.4</cell></row><row><cell></cell><cell>ADM</cell><cell cols="2">11.7185 220</cell><cell>0.6865</cell><cell>0.4866</cell><cell>76.0</cell></row><row><cell>(30, 30, 900, 5)</cell><cell>LADM</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell></cell><cell>LADMAP</cell><cell>2.3891</cell><cell>44</cell><cell>0.6864</cell><cell>0.4294</cell><cell>80.1</cell></row><row><cell></cell><cell cols="2">LADMAP(A) 0.0058</cell><cell>44</cell><cell>0.6864</cell><cell>0.4294</cell><cell>80.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison among APG, ADM, LADM, LADMAP and LADMAP(A) on the Hopkins155 database. We present their average computing time (in seconds), average number of iterations and average classification errors (%) on all 156 sequences.</figDesc><table><row><cell></cell><cell cols="2">Two Motion</cell><cell></cell><cell cols="2">Three Motion</cell><cell></cell><cell>All</cell></row><row><cell></cell><cell>Time</cell><cell cols="2">Iter. CErr.</cell><cell>Time</cell><cell>Iter. CErr.</cell><cell>Time</cell><cell cols="2">Iter. CErr.</cell></row><row><cell>APG</cell><cell cols="2">15.7836 90</cell><cell>5.77</cell><cell>46.4970</cell><cell cols="3">90 16.52 22.6277 90</cell><cell>8.36</cell></row><row><cell>ADM</cell><cell cols="8">53.3470 281 5.72 159.8644 284 16.52 77.0864 282 8.33</cell></row><row><cell>LADM</cell><cell cols="3">9.6701 110 5.77</cell><cell>22.1467</cell><cell cols="3">64 16.52 12.4520 99</cell><cell>8.36</cell></row><row><cell>LADMAP</cell><cell>3.6964</cell><cell>22</cell><cell>5.72</cell><cell>10.9438</cell><cell cols="2">22 16.52 5.3114</cell><cell>22</cell><cell>8.33</cell></row><row><cell cols="2">LADMAP(A) 2.1348</cell><cell>22</cell><cell>5.72</cell><cell>6.1098</cell><cell cols="2">22 16.52 3.0202</cell><cell>22</cell><cell>8.33</cell></row><row><cell cols="2">4.2 On Real World Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we switch to bold capital letters in order to emphasize that the variables are matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>As in<ref type="bibr" target="#b19">[20]</ref>, we can also introduce a parameter γ and update λ asλ k+1 = λ k +γβ[A(x k+1 )+B(y k+1 )-c].We choose not to do so in this paper in order not to make the exposition of LADMAP too complex. The readers can refer to Supplementary Material for full details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The current PROPACK can only output a given number of singular values and vectors. So one has to predict the number of singular values that are greater than a threshold<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. See step 3 of Algorithm 2. Recently, we have modified PROPACK so that it can output the singular values that are greater than a threshold and their corresponding singular vectors. See<ref type="bibr" target="#b9">[10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>When forming N k explicitly, X T XZ k can be computed as ((X T (XU k ))Σ k )V T k , whose complexity is still O(rn 2 ), while X T E k+1 could also be accelerated as E k+1 is a column-sparse matrix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Please see Supplementary Material for the detail of solving LRR by APG.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We use the Matlab code provided online by the authors of<ref type="bibr" target="#b11">[12]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Note that the second criterion differs from that in<ref type="bibr" target="#b17">(18)</ref>. However, this does not harm the convergence of LADMAP because<ref type="bibr" target="#b17">(18)</ref> is always checked when updating β k+1 (see<ref type="bibr" target="#b10">(11)</ref>).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Dr. Xiaoming Yuan for pointing us to <ref type="bibr" target="#b19">[20]</ref>. This work is partially supported by the grants of the NSFC-Guangdong Joint Fund (No. U0935004) and the NSFC Fund (No. 60873181, 61173103). R. Liu also thanks the support from CSC.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 3</head><p>Proof By Proposition 2 (1), {(x k , y k , λ k )} is bounded, hence has an accumulation point, say (x kj , y kj , λ kj ) → (x ∞ , y ∞ , λ ∞ ). We accomplish the proof in two steps.</p><p>This shows that any accumulation point of {(x k , y k )} is a feasible solution.</p><p>By letting k = k j -1 in Proposition 1 and the definition of subgradient, we have</p><p>Let j → +∞, by observing Proposition 2 (2), we have</p><p>where we have used the fact that both (x ∞ , y ∞ ) and (x * , y * ) are feasible solutions. So we conclude that (x ∞ , y ∞ ) is an optimal solution to (1).</p><p>Again, let k = k j -1 in Proposition 1 and by the definition of subgradient, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We next prove that the whole sequence</head><p>As (x ∞ , y ∞ , λ ∞ ) can be an arbitrary accumulation point of {(x k , y k , λ k )}, we may conclude that {(x k , y k , λ k )} converges to a KKT point of problem <ref type="bibr" target="#b0">(1)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning</title>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to compressive sampling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alternating direction method with Gaussian back substitution for separable convex programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alternating direction method with self-adaptive penalty parameters for monotone variational inequality</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="337" to="356" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lanczos bidiagonalization with partial reorthogonalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<idno>DAIMI PB-357</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Aarhus University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Some software packages for partial SVD computation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1108.1548</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1009.5055</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">UIUC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task feature learning via efficient l 2,1 norm minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust low-rank subspace segmentation with semidefinite guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshop</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recovering low-rank and sparse components of matrices from incomplete and noisy observations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An accelerated proximal gradient algorithm for nuclear norm regularized least sequares problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific J. Optimization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="640" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3D montion segmentation algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapirao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Linearized augmented Lagrangian and alternating direction methods for nuclear norm minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alternating direction algorithms for l 1 problems in compressive sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysis and generalizations of the linearized Bregman method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
