<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Synthetic Data to Train Neural Networks is Model-Based Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tuan</forename><forename type="middle">Anh</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>OX1 3PJ</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atılım</forename><surname>Günes</surname></persName>
							<email>gunes@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>OX1 3PJ</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Zinkov</surname></persName>
							<email>zinkov@iu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics and Computing</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<addrLine>919 E 10th Street</addrLine>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><surname>Wood</surname></persName>
							<email>fwood@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>OX1 3PJ</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Synthetic Data to Train Neural Networks is Model-Based Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57119C69EDAE9200BC218A2E023F5820</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural networks are powerful regressors <ref type="bibr" target="#b0">[1]</ref>. Training a neural network for regression means finding values for its free parameters using supervised learning techniques. This generally requires a large amount of labeled training data. Generally the harder the task, the larger the neural network, and the more training data required.</p><p>When labeled training data are scarce, one must either generate and use synthetic data to train, or resort to unsupervised generative modeling and generally slow test-time inference since it must be run afresh for new data. The deep learning community has reported remarkable results taking the former approach, either in the limited form of data augmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, where a dataset is artificially enlarged using label-preserving transformations, or training models solely on synthetic data, such as the groundbreaking work on text recognition in the wild <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, which was achieved by training a neural network to recognize text using synthetically generated realistic renders. Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref> addressed recognition of house numbers in Google Street View images in a supervised fashion, also solving reCaptcha <ref type="bibr" target="#b7">[8]</ref> images using synthetic data to train a recognition network from image to latent text. That the authors were Google employees meant that they had access to the true reCaptcha generative model and thus could generate millions of labeled instances for use in a standard supervised-learning pipeline. More recently, Stark et al. <ref type="bibr" target="#b8">[9]</ref> also used synthetic data for Captcha-solving and Wang et al. <ref type="bibr" target="#b9">[10]</ref> for font identification.</p><p>A contribution of this paper is to point out that this kind of use of synthetic data to train a neural network under a standard loss is, in fact, equivalent to training an artifact to do amortized approximate inference, in the sense of Gershman and Goodman <ref type="bibr" target="#b10">[11]</ref>, for the generative model corresponding to the synthetic data generator. This relationship forms the basis of our recent work on inference compilation for probabilistic programming <ref type="bibr" target="#b11">[12]</ref> and is also noted by both Paige and Wood <ref type="bibr" target="#b12">[13]</ref> and Papamakarios and Murray <ref type="bibr" target="#b13">[14]</ref>, where approximate inference guided by neural proposals is the goal rather than training neural networks using synthetic data. A consequence of this is that there is no need to ever reuse training data, as "infinite" labeled training data can be generated at training time from the generative model. Another contribution we make is a suggestion for how to take advantage of this framework by running a neural network more than once at test time to compute task-specific uncertainties of interest.</p><p>These contributions can also be seen as a reminder and guidance to the neural network community as it continues to move towards tackling unsupervised inference and problems in which labeled training data are difficult or impossible to obtain. Towards this end, we examine experimental findings that highlight problems that are likely to arise when using synthetic data to train neural networks. We discuss these problems in terms of the brittleness demonstrated to exist for deep neural networks, for example by Szegedy et al. <ref type="bibr" target="#b14">[15]</ref>, who showed that perceptually indistinguishable variations in neural network input can lead to profound changes in output. We also discuss model misspecification in the Bayesian sense <ref type="bibr" target="#b15">[16]</ref>.</p><p>The paper structure is as follows. In Section II, we develop a probabilistic synthetic data generative model and suggest a single, flexible neural network architecture for Captcha-breaking. In Section III, we train each such model independently using training data derived from running the synthetic data generator with parameters set to produce the corresponding style. These neural networks are shown to produce extremely good breaking performance, both in terms of accuracy and speed, well beyond standard computer vision pipeline results and comparable to recent deep learning results. We then discuss and demonstrate the brittleness of these regressors. We demonstrate improved robustness by focusing on and improving the generative model. In Section IV, we illustrate the connection of the demonstrated brittleness with Bayesian model mismatch. We end by explaining how the learned neural network can be used to perform sample-based approximate inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CAPTCHA-BREAKING</head><p>Assuming no access to the true Captcha <ref type="bibr" target="#b20">[21]</ref> generating system and a paucity of labeled training data, how does one go about breaking Captchas? A hint appears in the probabilistic programming community's approach to procedural graphics <ref type="bibr" target="#b21">[22]</ref> where a generative model for Captchas is proposed and then general purpose Markov chain Monte Carlo (MCMC) Bayesian inference is used to computationally inefficiently invert the said model. We will make the argument that this is, effectively, the same as generating synthetic training data in the manner of Jaderberg et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to train a neural network that regresses to the latent Captcha variables. In either case, developing a flexible, well-calibrated synthetic training data generator is our first concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generating synthetic training data</head><p>Our synthetic data generative model for Captcha specifies joint densities p s (x, y), parameterized by style s, that describe how to generate both the latent random variable x and the corresponding Captcha image y. Referring to the first row of Table <ref type="table" target="#tab_0">I</ref>, style s pertains to different schemes (e.g., Baidu, eBay, Wikipedia, Facebook) involving distinct character ranges, fonts, kerning, deformations, and noise. Note that in the following equations we omit the style subscript while keeping in mind that there is a separate unique model for each style. The latent structured random variable x = {L, 1:K , i 1:L } includes L, the number of letters, 1:K , a multidimensional structured parameter set controlling Captcha-rendering parameters such as kerning and various style-specific deformations, and i i:L , letter identities. Given these, we use a custom stochastic Captcha renderer R to generate each Captcha image y, this renderer and its fidelity being the primary component of the synthetic data generation effort. The corresponding per-style synthetic data generator corresponds to the model</p><formula xml:id="formula_0">x ∼ p(x) (1) y|x ∼ R(x) , (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where p(x) is a style-specific prior distribution over the latent variables including the character identities. For each different style shown in Table <ref type="table" target="#tab_0">I</ref>, we use different settings of the prior parameters to drive the Captcha renderer. In particular, the model places style-specific uniform distributions over different intervals for L, 1:K , and i 1:L . This is the mechanism for generating synthetic training data {(x (n) , y (n) )}. Note that p(y|x) cannot be evaluated for a given y, rather only sampled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural network architecture</head><p>Our Captcha-breaking neural network is designed taking into account architectures that have been shown to perform well on image inputs and variable-length output sequences <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Specifically, we choose a combination of convolutional neural networks (CNNs) and recurrent neural networks.</p><p>The core of our neural architecture (Figure <ref type="figure" target="#fig_0">1</ref>) is a long shortterm memory (LSTM) network <ref type="bibr" target="#b24">[25]</ref>, the output of which at each time step is passed through output layers corresponding one-to-one to the components of the latent variable x in the generative model (i.e., number of letters L, rendering parameters 1:K , and letter identities i 1:L ) that constitute the inputs to the Captcha renderer. Since the latent variable x has T = 1 + K + L components, where K is stylespecific and L is instance-specific, the LSTM is run for T time steps, and we represent by x 1:T the components of the latent x at each time step. The output layers are fullyconnected layers followed by a softmax function, distinct for each latent variable, that parameterize a discrete probability distribution. Since the LSTM has a fixed-dimensional output, these output layers allow us to match the dimensions of the discrete distributions for the corresponding latent variables.</p><p>A CNN is used to embed the Captcha image y into a fixeddimensional embedding vector CNN(y). At each time step, the LSTM input is constructed as the concatenation of the image embedding CNN(y), the value of the latent variable x t-1 of the previous time step, and a label vector {0, 1} D corresponding to each x t . During training, all x 1:T are provided to the network in a way similar to that used by Reed and de Freitas <ref type="bibr" target="#b25">[26]</ref>, using the actual values that generated the synthetic image y. At test time, the values of x t are sampled from the corresponding discrete probability distribution.</p><p>We denote the combined set of parameters of the overall architecture θ and its forward propagation function η, so given an input y, the output of the softmax layer at time step t corresponding to x t is η θ,t (y). In the running example of Figure <ref type="figure" target="#fig_0">1</ref>, x 1 = L, x 2:(2+K-1) = 1:K , and x (2+K):(2+K+L-1) = i 1:L . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss</head><p>By design, the softmax outputs determine the parameters for the discrete probability distributions of the Captcha generator parameters. The loss we minimize during training is the negative sum of the log of the softmax outputs</p><formula xml:id="formula_2">L(θ) = 1 N N n=1 - T t=1 log [η θ,t (y (n) )] x (n) t ,<label>(3)</label></formula><p>where we use the notation [z] i to denote the ith element of z. This is a standard loss used in training neural networks for classification. The connection with Bayesian modeling in which we interpret softmax outputs as probabilities of discrete random variables in a joint importance sampling proposal distribution is explored in more detail in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>We wrote synthetic data generative models for seven different Captcha styles, covering the types frequently found in the Captcha breaking literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>. For each of these, we trained a neural architecture consisting of (1) a CNN with six convolutions (3×3, with successively 64, 64, 64, 128, 128, 128 filters), max-pooling (2×2, step size 2) after the second, fifth, and sixth convolutions, and two final fully-connected layers of 1024 units; (2) a stack of two LSTMs of 512 hidden units each; and (3) fully-connected layers of appropriate dimension mapping the LSTM output to the corresponding softmax dimension of each latent variable. ReLU activations were used after the convolutions and the fully-connected layers overall.</p><p>We empirically verified that supplying the image embedding CNN(y) to the LSTM at every time step makes the training progress faster in our setup where we train the CNN from scratch together with the rest of the components, compared with the alternative of using CNN(y) only once and pretraining CNN weights on an image recognition database as in Vinyals et al. <ref type="bibr" target="#b22">[23]</ref> and Karpathy and Fei-Fei <ref type="bibr" target="#b23">[24]</ref>.</p><p>The networks were implemented in Torch <ref type="bibr" target="#b26">[27]</ref> and trained with Adam <ref type="bibr" target="#b27">[28]</ref> optimization, with initial learning rate α = 0.0001, hyperparameters β 1 = 0.9, β 2 = 0.999, using minibatches of size 128. The generative models were implemented in the Anglican probabilistic programming language <ref type="bibr" target="#b28">[29]</ref>. The two are coupled in our inference compilation <ref type="bibr" target="#b11">[12]</ref> framework. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initial results</head><p>As can be seen in Table <ref type="table" target="#tab_0">I</ref>, this architecture, and our method for training it using synthetic data, outperforms nearly all state-of-the-art Captcha breakers in terms of both accuracy and recognition times with the exception of Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref>, which used data drawn from the true reCaptcha generator. The row labeled "our method" shows breaking results and speeds for our neural network trained using synthetic data to decode unlabeled Captchas from the same Captcha generator. The Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref> and Stark et al. <ref type="bibr" target="#b8">[9]</ref> rows show the most directly comparable results, namely, using deep neural networks to break unlabeled Captchas training on synthetic data. The additional rows show breaking results for more traditional segment-and-classify computer vision image processing pipelines. These, in contrast to the others, do not have access to the true Captcha generator but instead report test results on real-world Captchas gathered in the wild. If robust, &gt; 90% accuracies would seem to confirm that Captcha, from a computer security perspective <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, is indeed broken.</p><p>While the capabilities of deep neural networks are impressive, it should be noted that these kinds of results, on occasion, can be somewhat misleading <ref type="bibr" target="#b14">[15]</ref>. In particular, one should note the assumption that, up to this point in this paper and in the referenced results from the deep learning literature, the training procedure of the Captcha-breaking network has access to data from the true generative process. Indeed, samples from the true generative process are superior even to hand-labeled training instances gathered in the wild. Any simulated data, required when we do not have access to the true generative model, must come from an approximation to the true generative process, a model per se. Whether or not networks trained using such approximate data are robust in the sense of working well on real data in the wild becomes the real question. To put it another way, is Captcha really broken if we do not have access to the true generative model-or a legion of human labelers and a pile of cash?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness of results</head><p>So, what happens to these state-of-the-art models if the test data is subtly different to the generated synthetic data? Or, what happens if you attempt to transfer learning from one Captcha style to another? Our exploration of these questions forms the inspiration and basis for the rest of the paper.</p><p>To start, we tried to use our trained models on real Captchas from Wikipedia and Facebook, which we identified as two major web services that still make use of textual Captchas,<ref type="foot" target="#foot_1">2</ref> collecting and hand-labeling test sets of 500 images each. We found that the trained Wikipedia and Facebook models achieving &gt; 90% recognition with synthetic data yielded practically zero breaking rates with real data. We then tried using a model trained on one Captcha style to break another style and found that it nearly always failed as well. We found that this was only partially caused by the non-overlapping latent variable domains (e.g., the distinct character ranges) for renderers of different styles. For instance, one might expect the reCaptcha breaker to work on the visually similar Yahoo Captchas, but we found that this was not the case.</p><p>To investigate, we performed experiments where we constructed test Captchas that the trained networks cannot recognize despite being perceptually indistinguishable from Captchas from the original generative model. We found that we could more-or-less arbitrarily degrade test performance by shifting the test data in either of two ways away from the original synthetic data (Figure <ref type="figure">2</ref>, left). In the first (Figure <ref type="figure">2</ref>, middle), we corrupted the image by subtle additive noise which shifts each Captcha a small, imperceptible Euclidean distance from its original position. This causes our Captcha breaking networks to exhibit the kind of brittleness well known to be a problem for deep neural network classifiers <ref type="bibr" target="#b14">[15]</ref>. In the second (Figure <ref type="figure">2</ref>, right), by changing the generative model of the test data relative to the training data, even in ways that are arguably below human ability to perceive, we were also able to cause test performance to degrade. This is the kind of model misspecification that has been discussed in the Bayesian inference literature <ref type="bibr" target="#b15">[16]</ref>.</p><p>Inspired by the success of Jaderberg et al. <ref type="bibr" target="#b3">[4]</ref>, we attacked these problems by improving our synthetic training Fig. <ref type="figure">2</ref>. Synthetic data from the Wikipedia generative model (left) are recognized correctly whereas even perceptually subtle changes such as adding per-pixel white noise with σ = 5 (middle) and kerning modified by just one pixel (right) result in severely degraded recognition rates. The overall recognition rates for the test groups from which these samples are taken are 93.6% (left), 24.0% (middle) and 65.2% (right). Note that the middle and right columns do get recognized correctly with the robust Wikipedia model. data generation. In particular, we developed a substantially more flexible generative model using the elastic displacement fields introduced by Simard et al. <ref type="bibr" target="#b1">[2]</ref>, effectively forcing the neural network to generalize over a greater variation than that exhibited by ground-truth labeled test data from the wild. These improved generative models have been observed to be robust to the subtle modifications that we report in Figure <ref type="figure">2</ref>. The results we obtained are encouraging, achieving 81% and 42% recognition rates on real Wikipedia and Facebook Captchas respectively. In both cases our robust results, arrived at by improving the quality of the synthetic data generator, have performance comparable (in the case of Wikipedia, superior) to traditional vision pipelines, and are significantly higher than the 1% recognition threshold suggested to deem a deployed Captcha system broken <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONNECTIONS TO MODEL-BASED BAYESIAN REASONING</head><p>In order to explore some of the factors that cause the brittleness of the neural network performance that we have just reported, we draw a connection between Bayesian model mismatch and out-of-sample generalization failure of neural network and other regressors when tested on data that is different to that used for training.</p><p>As a prerequisite to this, we review importance sampling <ref type="bibr" target="#b31">[32]</ref>, the approximate probabilistic inference algorithm that most naturally corresponds to the kind of inference our trained neural networks allow us to do. Given a joint distribution p(x, y) and a user-specified proposal distribution q(x|y), importance sampling allows us to approximate the posterior distribution p(x|y) and expectations of arbitrary functions f under it</p><formula xml:id="formula_3">p(x|y) ≈ M m=1 W m δ(x -x (m) )<label>(4)</label></formula><formula xml:id="formula_4">E p(x|y) [f ] ≈ M m=1 W m f (x (m) ) . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>This is done by generating M weighted samples</p><formula xml:id="formula_6">{(w m , x (m) )} M m=1 x (m) ∼ q(x|y) m = 1, . . . , M<label>(6)</label></formula><formula xml:id="formula_7">w m = p(x (m) , y)/q(x (m) |y) m = 1, . . . , M<label>(7)</label></formula><formula xml:id="formula_8">W m = w m / j w j m = 1, . . . , M .<label>(8)</label></formula><p>Note that importance sampling is generally inefficient unless the proposal distribution is well-matched to the target distribution in the sense that it "overlaps" the target, and is extremely efficient if it matches exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bayesian model misspecification</head><p>We illustrate the effects of mismatch between synthetic and real data in terms of Bayesian model misspecification using a simpler experiment (Figure <ref type="figure" target="#fig_1">3</ref>), highlighting conceptually what we believe to be happening. Let π(x, y) be the true data generating distribution and p(x, y) a model, where</p><formula xml:id="formula_9">π(x) = N (x|μ π , Σ π ) (9) π(y|x) = N (y|x, Σ) (10) p(x) = N (x|μ p , Σ p ) (11) p(y|x) = N (y|x, Σ) . (<label>12</label></formula><formula xml:id="formula_10">)</formula><p>We will use the mismatch between the distributions π(x) and p(x) as an illustrative proxy to the mismatch of the joint distributions π(x, y) and p(x, y). The marginal p(x) of the model distribution p(x, y) is shown in Figure <ref type="figure" target="#fig_1">3</ref> as a thin blue dashed ellipse which covers 99% of its probability mass. We draw a data point y from this model by first drawing x from p(x) and then drawing y from π(y|x) where Σ p = 2I, μ p = [0, 0] T and Σ = I.</p><p>The marginal π(x) of the true data generating distribution π(x, y) is shown in Figure <ref type="figure" target="#fig_1">3</ref> as a black solid ellipse. A typical data point y is drawn by first drawing x from π(x) and then drawing y from π(y|x) where Σ π = I and μ π is [0, 0] T , [5, 0] T and [8, 0] T from left to right.</p><p>Such a model has a posterior</p><formula xml:id="formula_11">p(x|y) = N (x|μ post , Σ post )<label>(13)</label></formula><formula xml:id="formula_12">Σ post = (Σ -1 p + Σ -1 ) -1 (<label>14</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">μ post = Σ post (Σ -1 p μ p + Σ -1 y) ,<label>(15)</label></formula><p>which is shown in Figure <ref type="figure" target="#fig_1">3</ref> as a thick blue dashed ellipse. Using a procedure similar to the one described in Section II, we generate training data {(x (n) , y (n) )} from the model p(x, y) and use it to train a neural network mapping from y to importance sampling proposal parameters (μ q , Σ q ) := η θ (y). The resulting proposals generated from such a proposal distribution q(x|η θ (y)) := N (x|μ q , Σ q ) are shown in Figure <ref type="figure" target="#fig_1">3</ref> as magenta dash-dotted ellipses. Remember that μ q and Σ q are functions of y computed by the trained neural network regressor.</p><p>If we then draw M = 1000 samples from this proposal distribution by repeatedly running the trained neural network forward and weight the resulting samples according to the importance sampling scheme in the beginning of Section IV, we arrive at approximations to the model-based posterior mean and covariance: Now consider the three scenarios in Figure <ref type="figure" target="#fig_1">3</ref>, in which the difference between the true data generating distribution, illustrated by its marginal π(x), and the model p(x) is progressively increased from left to right. As the true data generating distribution π(x) moves further away from our model p(x) we see that we get, for a fixed computational budget of M = 1000 samples, progressively worse estimates p(x|y) of p(x|y) (Figure <ref type="figure" target="#fig_1">3</ref>, middle and right). What is happening here is that the neural network, at training time, learns to invert the model p(x, y) from samples drawn from it. In Figure <ref type="figure" target="#fig_1">3</ref> (left), when the model overlaps the true data generative process, the neural network sees examples of x and y pairs that are representative of the true data generating mechanism and then, given sufficient capacity in terms of neural architecture and training time (remembering that we have access in this setting to infinite training data), can almost certainly learn a mapping that solves the task of predicting x given y. If the model is slightly misspecified then the number of training examples in the domain of the true model might be small and as such we might not expect good generalization performance. When there is high model misspecification (Figure <ref type="figure" target="#fig_1">3</ref>, right) the neural network will simply never see training examples that look like the true data, and, as such, will produce mostly spurious regression results leading to unhelpful proposal distributions.</p><formula xml:id="formula_15">μM ≈ E p(x|y) [x]<label>(16</label></formula><p>This experiment graphically illustrates the kinds of problems that can arise from model misspecification. What it indicates is that if we are going to use synthetic data to train a neural network regressor we should ensure that our synthetic data generator is ideally as close as possible to the true data generation process and that mismatch from the true data in terms of broadness (e.g., the Gaussian example in Figure <ref type="figure" target="#fig_1">3</ref> (left), in which μ π and μ p match but Σ π and Σ p do not) is more tolerable and in fact preferable to a perceptually indistinguishably miscalibrated model (e.g. the phenomenon illustrated in Figure <ref type="figure">2</ref> and described in Section III-B). We conjecture that the latter is what caused the brittleness we discovered in our trained neural networks and illustrate in Figure <ref type="figure">2</ref>.</p><p>This intuition guided our decision to broaden our synthetic data generator by adding the displacement fields of Simard et al. <ref type="bibr" target="#b1">[2]</ref> in Section III-B, leading to significant improvements to robustness evidenced by the improved real-data results we obtained. This, we believe, accounts for the fact that our Captcha generator is not likely to capture all details of the true generative model such as subtle font differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference</head><p>A corollary to the Bayesian inference interpretation of training a neural network on synthetic data is that the resulting neural network can be used for approximate inference in the probabilistic model p(x, y) corresponding to the synthetic training data generator.</p><p>Let the importance sampling proposal distribution be factorized as q(x|y) = T t=1 q t (x t |x 1:t-1 , y). If we consider the individual time-dependent softmax layers of the Captchasolving neural network to be probabilities of a proposal distribution q t (x t |x 1:t-1 , y), we can adopt an alternative way of writing our loss in (3) as</p><formula xml:id="formula_16">L(θ) = 1 N N n=1 - T t=1 log [η θ,t (y (n) )] x (n) t = 1 N N n=1 - T t=1 log q t (x (n) t |x (n) 1:t-1 , y) = 1 N N n=1 -log T t=1 q t (x (n) t |x (n) 1:t-1 , y) = 1 N N n=1 -log q(x (n) |η θ (y (n) )) . (<label>18</label></formula><formula xml:id="formula_17">)</formula><p>The loss in <ref type="bibr" target="#b17">(18)</ref> can be viewed as a Monte Carlo approximation of an expectation over a function under the joint distribution p(x, y) of the synthetic data, which, following Paige and Wood <ref type="bibr" target="#b12">[13]</ref>, can be shown to be the Kullback-Leibler divergence between the proposal and the posterior averaged over all possible datasets </p><p>Hence, minimizing <ref type="bibr" target="#b17">(18)</ref> is also known as importance sampling proposal adaptation. Running a neural network trained using synthetic data and this common loss on an input y actually produces efficient proposal distribution parameters η θ (y). By running the neural network M times given the same input and subsequently weighting the sampled x values according to <ref type="bibr" target="#b6">(7)</ref>, we obtain an approximate posterior distribution (Figure <ref type="figure" target="#fig_4">4</ref>). We note that, in the case of Captchas, we must use a likelihood based on approximate Bayesian computation (ABC) <ref type="bibr" target="#b32">[33]</ref> instead of the intractable p(y|x) in order to calculate the weight in <ref type="bibr" target="#b6">(7)</ref>.</p><p>Accounting for uncertainty is a principal benefit of modelbased inference and is particularly useful when there is actual ambiguity in y as in Figure <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>What is remarkable about the natural scene text recognition results of Jaderberg et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> is that they show generalization from synthetic data, to the degree that one could argue that their result is actually a generative modeling triumph. Our results showing improved robustness of Wikipedia-and Facebook-style Captcha-breaking stem likewise from focusing on the synthetic data generative model. In addition to being usefully prescriptive, our point that training neural networks using synthetic data is equivalent to performing proposal adaptation for importance sampling inference in the synthetic data generative model sets an empirical cornerstone for future theory that quantifies and bounds the impact of model mismatch on neural network and approximate inference performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Neural network architecture mapping the Captcha image y to the latent variables x of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of model mismatch. Left: The model encompasses the true data distribution; Middle: the model partially matches the true data distribution; Right: the model is completely mismatched to the true data distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc>ΣM ≈ E p(x|y) x -E p(x|y) [x] x -E p(x|y) [x] T . (17)The distribution p(x|y) := N (x|μ M , ΣM ) is shown in Figure 3 as a magenta dashed ellipse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>p(x,y) [-log q(x|η θ (y))] = Y X p(x, y)(-log q(x|η θ (y))) dxdy = Y p(y) X p(x|y) log p(x|y) q(x|η θ (y))) dxdy + const. = E p(y) [D KL (p(x|y) || q(x|η θ (y)))] + const.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Posteriors of real Facebook and Wikipedia Captchas. Conditioning on each Captcha, we show an approximate posterior produced by a set of weighted importance sampling particles {(wm, x (m) )} M =100 m=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SYNTHETIC</head><label>I</label><figDesc>CAPTCHA BREAKING RESULTS. RR: RECOGNITION RATE, BT: BREAKING TIME.</figDesc><table><row><cell>Style</cell><cell>Baidu (2011)</cell><cell>Baidu (2013)</cell><cell>eBay</cell><cell>Yahoo</cell><cell>reCaptcha</cell><cell>Wikipedia</cell><cell>Facebook</cell></row><row><cell>Our method</cell><cell>RR 99.8%</cell><cell>99.9%</cell><cell>99.2%</cell><cell>98.4%</cell><cell>96.4%</cell><cell>93.6%</cell><cell>91.0%</cell></row><row><cell></cell><cell>BT 72 ms</cell><cell>67 ms</cell><cell>122 ms</cell><cell>106 ms</cell><cell>78 ms</cell><cell>90 ms</cell><cell>90 ms</cell></row><row><cell>Bursztein et al. [17]</cell><cell>RR 38.68%</cell><cell>55.22%</cell><cell>51.39%</cell><cell>5.33%</cell><cell>22.67%</cell><cell>28.29%</cell><cell></cell></row><row><cell></cell><cell>BT 3.94 s</cell><cell>1.9 s</cell><cell>2.31 s</cell><cell>7.95 s</cell><cell>4.59 s</cell><cell></cell><cell></cell></row><row><cell>Starostenko et al. [18]</cell><cell>RR</cell><cell></cell><cell></cell><cell>91.5%</cell><cell>54.6%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BT</cell><cell></cell><cell></cell><cell></cell><cell>&lt; 0.5 s</cell><cell></cell><cell></cell></row><row><cell>Gao et al. [19]</cell><cell>RR 34%</cell><cell></cell><cell></cell><cell>55%</cell><cell>34%</cell><cell></cell><cell></cell></row><row><cell>Gao et al. [20]</cell><cell>RR</cell><cell>51%</cell><cell></cell><cell>36%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BT</cell><cell>7.58 s</cell><cell></cell><cell>14.72 s</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Goodfellow et al. [7]</cell><cell>RR</cell><cell></cell><cell></cell><cell></cell><cell>99.8%</cell><cell></cell><cell></cell></row><row><cell>Stark et al. [9]</cell><cell>RR</cell><cell></cell><cell></cell><cell></cell><cell>90%</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://probprog.github.io/inference-compilation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Facebook Captchas appear as a measure for preventing flood-posting and when links to particular Facebook pages are followed. Wikipedia Captchas appear on the account creation page. We note that textual reCaptchas, as of version 2.0, have been replaced with tasks such as image recognition<ref type="bibr" target="#b30">[31]</ref>, making them unlikely to encounter and collect.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Tuan Anh Le is supported by EPSRC DTA and Google (project code DF6700) studentships. Atılım Günes ¸Baydin and Frank Wood are supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668. Robert Zinkov is supported under DARPA grant FA8750-14-2-0007.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Document Analysis and Recognition</title>
		<meeting>the Seventh International Conference on Document Analysis and Recognition<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
	<note>ser. ICDAR &apos;03</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic Data for Text Localisation in Natural Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6082</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">reCAPTCHA: Human-based character recognition via web security measures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5895</biblScope>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Captcha recognition with active deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazırbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR Workshop on New Challenges in Neural Computation</title>
		<meeting><address><addrLine>Aachen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepfont: Identify your font from an image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 36th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inference compilation and universal probabilistic programming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Fort Lauderdale, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 20-22, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inference networks for sequential Monte Carlo in graphical models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast -free inference of simulation models with Bayesian conditional density estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06376</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Philosophy and the practice of Bayesian statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shalizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="38" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The end is nigh: generic solving of text-based CAPTCHAs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bursztein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aigrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moscicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th USENIX Workshop on Offensive Technologies</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breaking text-based CAPTCHAs with variable word and character orientation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Starostenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cruz-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Uceda-Ponga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alarcon-Aquino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1101" to="1112" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The robustness of &quot;connecting characters together&quot; CAPTCHAs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="369" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The robustness of hollow CAPTCHAs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security</title>
		<meeting>the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1075" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CAPTCHA: Using hard AI problems for security</title>
		<author>
			<persName><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on the Theory and Applications of Cryptographic Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="294" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximate Bayesian image interpretation using generative probabilistic graphics programs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Perov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural programmerinterpreters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new approach to probabilistic programming inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1024" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text-based CAPTCHA strengths and weaknesses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bursztein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Computer and communications security</title>
		<meeting>the 18th ACM Conference on Computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">I am robot: (deep) learning to break semantic image captchas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sivakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Keromytis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="388" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A tutorial on particle filtering and smoothing: Fifteen years later</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Nonlinear Filtering</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation (ABC) gives exact results under the assumption of model error</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Applications in Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
