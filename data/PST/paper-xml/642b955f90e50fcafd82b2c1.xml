<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-03">3 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
							<email>xionghl@shanghaitech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">United Imaging Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linlin</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Huashan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>wangqian2@shanghaitech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">United Imaging Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">United Imaging Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-03">3 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.01097v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT's help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. We are sharing it with the broader community to invite feedback and suggestions to improve its healthcare-focused capabilities: https://github.com/xionghonglin/DoctorGLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large Language Models (LLMs) are highly advanced artificial intelligence systems that have undergone extensive training on vast amounts of text data. By utilizing deep learning techniques, these models are able to generate responses that resemble human-like speech, making them incredibly useful in a variety of tasks, such as language translation, question answering, and text generation. OpenAI's GPT series, among other LLMs, has exhibited remarkable results, and has the potential to revolutionize various industries, including marketing, education, and customer service. LLMs are highly sought after for their ability to process and understand large amounts of data, which makes them wellsuited to solve complex problems.</p><p>Despite their remarkable performance in natural lan-guage processing, large language models like ChatGPT and GPT-4 have not been designed specifically for the medical domain. As a result, using LLMs for medical purposes may lead to suboptimal precision in diagnoses, drug recommendations, and other medical advice, potentially causing harm to patients. Another limitation of large language models like ChatGPT and GPT-4 is that they are typically trained in English, which restricts their ability to comprehend and respond to other languages. This can create a barrier for individuals who do not speak English as their first language and limit the accessibility of medical advice to a wider audience.</p><p>In order to overcome these limitations and better integrate LLMs into the lives of most ordinary people, it's crucial to develop medical-tailored LLMs that can be trained in multiple languages. This will not only improve the accuracy of medical advice provided by these models but also make it more accessible to a wider audience.</p><p>In order to improve the precision and accuracy of medical advice provided by language models in the medical domain, a database of medical dialogues in Chinese has been compiled. This database contains information from a large number of patients, including their symptoms, recommended medications, and the necessary medical tests. The database has been created to provide language models with extensive medical knowledge and to enable them to generate more accurate and personalized responses to medical queries. By incorporating this knowledge, the hope is to improve the ability of language models to diagnose illnesses and provide better recommendations to patients, ultimately improving the quality of healthcare.</p><p>To optimize our medical language model for both Chinese and English languages and, more importantly, explore a feasible pipeline of customized medical LLMs, we finetuned it based on ChatGLM, a pre-trained language model with 6 billion parameters. This model is unique in that it is bilingual, offering proficiency in both English and Chinese. Furthermore, the GLM model has a unique scaling property that allows for INT4 quantization enabling effective inference on a single RTX 3060 (12G). This scaling property is a major breakthrough in the field of healthcare language modeling, as it allows for more efficient and cost-effective computation on affordable GPUs, making it easier for hospitals to deploy their medical dialogue models based on their inhouse data. Also, we use low-rank adaptation that facilitates fine-tuning on an A100 80G GPU. This allows for faster inference times, making it easier for researchers and developers to utilize large-scale language models for a variety of applications.</p><p>At present, the general public often assumes that large language models (LLMs) are monopolized by technology giants due to the substantial computational costs associated with ChatGPT. However, in this paper, we demonstrate that a specialized Chinese dialogue language model focused on the medical domain can be trained for less than 100 USD. We accomplish this by utilizing parameter-efficient tuning and quantization techniques, enabling the development of an LLM-based system that can be customized for specific tasks. The main contributions of this paper are summarized below:</p><p>? We present the first attempt at training a non-English healthcare LLM.</p><p>? We develop a comprehensive pipeline for training dialogue models, applicable across different languages and adaptable to any specific clinical department. The source code is made available on GitHub.</p><p>? We demonstrate that the costs of training and deploying a personalized LLM are affordable, thus encouraging hospitals to train their own LLMs based on inhouse data with ease. Recent advances in Transformer architecture <ref type="bibr" target="#b16">[11]</ref> and computing power have enabled the training of large language models with billions of parameters, leading to a significant improvement in their ability to summarize, translate, predict and generate human-like text <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b14">9]</ref>. In pre-ChatGPT era, several healthcare language models have been developed based on general-purpose model weight and training schemes. BioBERT <ref type="bibr" target="#b8">[7]</ref> and PubMedBERT <ref type="bibr" target="#b6">[5]</ref> are examples of BERT <ref type="bibr">[3]</ref> models trained on PubMed for biomedical data, while ClinicalBERT <ref type="bibr" target="#b1">[1]</ref> was further trained on the MIMIC dataset and outperformed its predecessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Large Language Models in Healthcare</head><note type="other">ChatGPT Translation</note><p>After ChatGPT showed the potential of 100B-scale model, researches expand healthcare language model to a much larger scale and give very promising results. Med-PaLM <ref type="bibr" target="#b14">[9]</ref> was developed in late 2022 using curated biomedical corpora and human feedback, and showed promising results, including a 67.6% accuracy on the MedQA exam. ChatGPT, which was not given supplementary medical training, passed all three parts of the USMLE and achieved over 50% accuracy across all exams and surpassed 60% accuracy in the majority of them <ref type="bibr" target="#b7">[6]</ref>. ChatCAD <ref type="bibr" target="#b17">[12]</ref> combined medical image analysis models with ChatGPT and offered an interactive computer-aided diagnosis. Chat-Doctor <ref type="bibr" target="#b18">[13]</ref> is a medical chat model fine-tuned on LLaMA model using clinical QA that is synthesised by ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset with ChatGPT's Help</head><p>It is worth noting that there exists a lot of high-quality datasets released in English. To utilize the available resources, we have translated ChatDoctor <ref type="bibr" target="#b18">[13]</ref> dataset to enhance the Chinese language proficiency of the DoctorGLM.</p><p>The medical-targeted LLM requires professional training data, which asks high demands for English-Chinese translation. ChatGPT is capable of professional clinical text translation, but this would incur an overhead of tens of thousands of dollars for a large-scale dataset, which is unacceptable to most researchers. Here, we take a simple and lowcost approach to large-scale translation by leveraging the capabilities of ChatGPT.</p><p>Translation of the dataset is generally in two steps as shown in Figure <ref type="figure" target="#fig_1">2</ref>. X = {x 1 , x 2 , ..., x N } is initially selected from the ChatDoctor dataset, where x n is the raw En-glish text, and corresponding high-quality translation Y = {y 1 , y 2 , ..., y N } is obtained through ChatGPT API. Then, a BART-based pre-trained model <ref type="bibr" target="#b15">[10]</ref> is fine-tuned solely on paired X and Y without any additional datasets. In this way, the language model can distill the expert-level knowledge from ChatGPT and the refined small model can act as an acceptable alternative to LLMs<ref type="foot" target="#foot_0">1</ref> . We have translated ChatDoctor<ref type="foot" target="#foot_1">2</ref> to use in our training.</p><p>To develop conversational models of high quality on a limited academic budget, ChatDoctor <ref type="bibr" target="#b18">[13]</ref> utilized a strategy where each message from the disease database was entered as an individual prompt into the GPT3.5-turbo model to generate instruction data automatically. The prompts provided to the ChatGPT API contained the gold standard of diseases, symptoms, and drugs, resulting in a dataset that preserves the conversational fluency of ChatGPT while also achieving higher diagnostic accuracy than ChatGPT alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prompt Designer</head><p>Large language models have achieved remarkable performance in conversational tasks. However, their outputs may be unreliable and deceptive. This issue also exists with ChatGLM, which is utilized in DoctorGLM. To address this problem and enhance the reliability of DoctorGLM's outputs, we use a prompt designer module that pre-processes the user's input.</p><p>The prompt designer module extracts relevant keywords such as the name of the disease or symptoms from the user's input. The module then utilizes the name of the most likely disease as a label and generates a brief description based on a professional disease knowledge library. This library comprises a comprehensive collection of detailed documents about various diseases. In particular, we have 3231 disease documents in detail, all of which are sourced from the Merck Manual of Diagnosis and Therapy.</p><p>The prompt designer's output includes information about the disease's symptoms, diagnosis, treatment options, and preventive measures. By providing a professionally generated prompt, the prompt designer expands the expertise and reliability of DoctorGLM for a particular disease. Additionally, it serves as a reliable source of information for users. The generated prompt is integrated into the large language model, along with the original input, to improve the accuracy and reliability of DoctorGLM's responses. By using this approach, we can enhance the overall performance of DoctorGLM and provide reliable and trustworthy information to users. This system can also be used with other large language models that have decent in-context learning ability, e.g., ChatGPT, LLaMA and ChatGLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training of DoctorGLM</head><p>We utilized the ChatGLM-6B model <ref type="bibr">[4,</ref><ref type="bibr" target="#b19">14]</ref> in developing our DoctorGLM. This open bilingual language model is based on the General Language Model (GLM) framework and has 6.2 billion parameters. ChatGLM-6B is optimized for Chinese QA and dialogue, and its technology is similar to ChatGPT. The model was trained on approximately 1 trillion tokens of Chinese and English corpus, with additional supervised fine-tuning, feedback bootstrap, and reinforcement learning using human feedback. Despite having only 6.2 billion parameters, ChatGLM-6B generates answers that are aligned with human preference. Furthermore, we use low-rank adaptation (LoRA) to finetune ChatGLM with only 7 million trainable parameters.</p><p>The fine-tuning process using all Chinese medical dialogue dataset was conducted using an A100 GPU for a duration of 8 hours. The hyper-parameters employed in the training process were as follows: the batch size of 4, a learning rate of 2e-5 with lion optimizer, a total of 1 epochs, a maximum sequence length of 512 tokens, a maximum target length of 100 tokens. with no warmup and weight decay. The low-rank adaption is applied to q, v and rank is set to 8 with alpha set to 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Compare to General Purpose Models</head><p>Here we demonstrate some some QA with DoctorGLM in Table <ref type="table" target="#tab_1">2</ref>, 3 and 4. We compared to our base model ChatGLM-6B and ChatGPT (gpt-3.5-turbo). DoctorGLM here have no additional prompt and filter. This model is trained based on CMD., with data from five departments. We also ask a doctor to mark the error and</p><p>In general, DoctorGLM tend to be too arbitrary while general models like ChatGPT are more conservative. For example, in Q2 (see Table <ref type="table">3</ref>), ChatGLM suggested to use Ribavirin Granules,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generation Diversity</head><p>Top-p and temperature are techniques used in text generation models to control the diversity and quality of the generated output. Top-p (also known as nucleus sampling or softmax sampling) is a method used to select the most likely words based on their probabilities in the model's output distribution. It works by selecting the smallest set of words whose cumulative probability exceeds a certain threshold. This allows for more diverse output than simply selecting the most probable word at each step. Temperature, on the other hand, is a scalar parameter that controls the randomness of the generated output. A higher temperature value results in more diverse and creative output, while a lower value leads to more conservative and predictable output. The temperature parameter controls the softmax function used to calculate the probabilities of the next word in the generated sequence. The results are presented on Table. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cost</head><p>Training a LLM from scratch with web-scale data can be a costly endeavor, but fine-tuning can be a more economical approach. DoctorGLM's training process can handle approximately 80,000 single question and answer pairs per hour per GPU. Assuming that three epochs are necessary, and the cloud computing server of an A100 GPU is approximately 5 USD per hour, the total training time required is 3.75 hours, which translates to a cost of approximately 18.75 USD for fine-tuning a DoctorGLM on 100,000 QA pairs.</p><p>On the other hand, the inference process for DoctorGLM requires only about 13 GB of GPU memory and can be performed on a consumer-level GPU such as an RTX 3090. This results in a total cost (inference PC) of approximately 1500 USD.</p><p>It's worth noting that these costs may vary depending on the current price of GPUs and your location, but they remain relatively affordable for many research institution and hospitals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Technical Limitations</head><p>This work is in a very early stage and contains numerous mistakes, making it unsuitable for any commercial or clinical use. One of the reasons we have published our work is to invite the broader community to help improve this healthcare-focused language model, with the aim of making it more accessible, affordable, and convenient for a larger audience. Below are some critical technical issues we encountered during this project:</p><p>1. DoctorGLM experiences a loss in capability during logistic training, and it occasionally repeats itself (see Table <ref type="table" target="#tab_1">2</ref> A4 (DoctorGLM)). We suspect that fine-tuning typically incurs a higher alignment cost compared to reinforcement learning with human feedback (RLHF).</p><p>2. Generating a response takes approximately 15 to 50 seconds, depending on token length, which is significantly slower than interacting with ChatGPT via the web API. This delay is partly due to the chatbot's typing indicator.</p><p>3. We are currently facing difficulties in quantizing this model. While ChatGLM runs satisfactorily on INT-4 (using about 6G), the trained LoRA of DoctorGLM appears to have some issues. As a result, we are currently unable to deploy our model on more affordable GPUs, such as the RTX 3060 and RTX 2080.</p><p>4. We have noticed that the model's performance declines with prolonged training, but we currently lack a strategy for determining when to stop training. It appears that cross-entropy is an overly rigid constraint when fine-tuning LLMs.</p><p>Top-p = 0.6</p><p>Top-p = 0.7 Top-p = 0.8 T = 0.90</p><formula xml:id="formula_0">?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ? ? ?38.5?C? ? ? ? ? ? ? ? ????????38.5?C???? ?????????????? ?????????????? ?????????????? ???????(F) ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ?38.5? ? ?????????????? ?39???????????? ??????40??????? ?(C) ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ??????(B) T = 0.95 ?????????????? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????(F) ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ? ? ? ? ? ?1? ? ? ? ? ? ? ?????????????? ?????????????? ? ? ? ?2? ? ? ? ? ? ? ? ? ?????????????? ?3????????????? ?????????????? ???4??????????? ?????????????? ?????????????? ? ? ? ? ? ? ?5? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ?(A) ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ??????(A) T = 0.99 ?????????????? ?????????????? ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????(A) ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ? ? ? ?1? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ?????????????? ???????(B) ?????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?38???????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ?????????????? ?????????????? ?????????????? ?????????????? ??(C)</formula><p>Table <ref type="table">5</ref>. The impact of different hyperparameter settings on the generated results, where T refers to temperature. Temperature and top-p are used to control the diversity and fluency of the generated text. We ask doctor to give a score (ABCF, from good to bad).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of DoctorGLM fine-tuning and inference pipeline.</figDesc><graphic url="image-1.png" coords="2,55.06,72.00,485.11,208.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. The implementation of large-scale translation. A tiny and high-quality dataset is built through ChatGPT. The collected dataset serves as a fine-tuning set for a pre-trained language model, enabling it to perform specialized machine translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the used datasets. Chat means multi-round QA. Syn. means whether the data is generated by other language models. CMD. means Chinese medical dialogue.</figDesc><table><row><cell>Dataset</cell><cell>Department</cell><cell cols="4">Language Q&amp;A Chat Number Syn.</cell><cell>Size</cell></row><row><cell></cell><cell>Surgical</cell><cell></cell><cell></cell><cell>116K</cell><cell></cell><cell>52MB</cell></row><row><cell></cell><cell>Obstetrics and Gynecology</cell><cell></cell><cell></cell><cell>229K</cell><cell></cell><cell>78MB</cell></row><row><cell>CMD.</cell><cell>Pediatrics</cell><cell>CN</cell><cell>?</cell><cell>117K</cell><cell>?</cell><cell>47MB</cell></row><row><cell></cell><cell>Internal Medicine</cell><cell></cell><cell></cell><cell>307K</cell><cell></cell><cell>102MB</cell></row><row><cell></cell><cell>Andriatria</cell><cell></cell><cell></cell><cell>113K</cell><cell></cell><cell>44MB</cell></row><row><cell>MedDialog</cell><cell>Multiple</cell><cell>CN&amp;EN</cell><cell></cell><cell>3.4M</cell><cell>?</cell><cell>1.5GB</cell></row><row><cell>ChatDoctor</cell><cell>Multiple</cell><cell>EN</cell><cell>?</cell><cell>5.4K</cell><cell></cell><cell>2.9MB</cell></row><row><cell>HearlthcareMagic</cell><cell>Multiple</cell><cell>EN</cell><cell>?</cell><cell>200K</cell><cell>?</cell><cell>216MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Doctor's comments are marked in blue. Factual errors are marked in red. Improper diagnosis are marked in green.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at: https://huggingface.co/zhaozh/medical chat-en-zh</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>ChatDoctor: Google drive</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">????(mri)??????????????</forename><surname>??????????????</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">?????5 d?1???? Table 3. Doctor&apos;s comments are marked in blue. Factual errors are marked in red</title>
		<imprint/>
	</monogr>
	<note>Improper diagnosis are marked in green</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Per-formance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. medRxiv</title>
		<author>
			<persName><forename type="first">Tiffany</forename><forename type="middle">H</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Cheatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arielle</forename><surname>Medinilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chat-Gpt</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Czarina</forename><surname>Sillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorie</forename><forename type="middle">De</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Elepano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Madriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimel</forename><surname>Aggabao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giezel</forename><surname>Diaz-Candido</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<pubPlace>Katherine Lee</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">??????????????????????????</forename><surname>????????</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">????????????????????????????</forename><surname>???????????</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Doctor&apos;s comments are marked in blue. Factual errors are marked in red. Improper diagnosis are marked in green</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Tatoeba Translation Challenge -Realistic data sets for low resource and multilingual MT</title>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="1174" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Chatcad: Interactive computer-aided diagnosis on medical image using large language models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07257</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yunxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ruilong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14070</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GLM-130b: An open bilingual pre-trained model</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
