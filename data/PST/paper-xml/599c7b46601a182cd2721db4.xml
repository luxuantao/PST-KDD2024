<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lip Reading in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lip Reading in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89F831BD78565BC716C4CCBA5934CE06</idno>
					<idno type="DOI">10.1007/978-3-319-54184-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our aim is to recognise the words being spoken by a talking face, given only the video but not the audio. Existing works in this area have focussed on trying to recognise a small number of utterances in controlled environments (e.g. digits and alphabets), partially due to the shortage of suitable datasets.</p><p>We make two novel contributions: first, we develop a pipeline for fully automated large-scale data collection from TV broadcasts. With this we have generated a dataset with over a million word instances, spoken by over a thousand different people; second, we develop CNN architectures that are able to effectively learn and recognize hundreds of words from this large-scale dataset.</p><p>We also demonstrate a recognition performance that exceeds the state of the art on a standard public benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lip-reading, the ability to understand speech using only visual information, is a very attractive skill. It has clear applications in speech transcription for cases where audio is not available, such as for archival silent films or (less ethically) off-mike exchanges between politicians and celebrities (the visual equivalent of open-mike mistakes). It is also complementary to the audio understanding of speech, and indeed can adversely affect perception if audio and lip motion are not consistent (as evidenced by the McGurk <ref type="bibr" target="#b22">[23]</ref> effect). For such reasons, lipreading has been the subject of a vast research effort over the last few decades. It has also been the subject of excellent comedy sketches, e.g. Seinfeld "The Lip Reader", and its ambiguity and challenge can be exploited to replace/overdub actual speech, e.g. in the YouTube channel "Bad Lip Reading".</p><p>Our objective in this work is a scalable approach to large lexicon speaker independent lip-reading. Furthermore, we aim to recognize words from continuous speech, where words are not segmented, and there may be co-articulation of the lips from preceding and subsequent words.</p><p>In lip-reading there is a fundamental limitation on performance due to homophemes. These are sets of words that sound different, but involve identical movements of the speaker's lips. Thus they cannot be distinguished using visual information alone. For example, in English the phonemes 'p' 'b' and 'm' are visually identical, and consequently the words mark, park and bark, are homophemes (as are pat, bat and mat) and so cannot be distinguished by lip-reading. This problem has been well studied and there are lists of ambiguous phonemes and words available <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. It is worth noting that the converse problem also applies: for example 'm' and 'n' are easily confused as audio, but are visually distinct. We take account of such homopheme ambiguity in assessing the performance of our methods.</p><p>Apart from this limitation, lip-reading is a challenging problem in any case due to intra-class variations (such as accents, speed of speaking, mumbling), and adversarial imaging conditions (such as poor lighting, strong shadows, motion, resolution, foreshortening, etc.).</p><p>The usual approach to inference for temporal sequences is to employ sequence models such as Hidden Markov Models or Recurrent Neural Networks (e.g. LSTMs). For lip-reading such models can be employed for predicting individual characters or phonemes. In contrast, we investigate using Convolutional Neural Networks (CNNs) for directly recognizing individual words from a sequence of lip movements.</p><p>Clearly, visual registration is an important element to consider in the design of the networks. Typically, the imaged head will move in the video, either due to actual movement of the head or due to camera motion. One approach would be to tightly register the mouth region (including lips, teeth and tongue, that all contribute to word recognition), but another is to develop networks that are tolerant to some degree of motion jitter. We take the latter approach, and do not enforce tight registration.</p><p>We make contributions in two areas: first, we develop a pipeline for automated large scale data collection, including visual and temporal alignment. With this we are able to obtain training data for hundreds of distinct words, thousands of instances for each word, and over a thousand speakers (Sect. 2); second, we develop CNN architectures for classifying multi-frame time series of lips. In particular we propose and compare different input and temporal fusion architectures, and discuss their pros and cons (Sect. 3). We analyse the performance and ambiguity of the resulting classifications in Sect. <ref type="bibr" target="#b3">4</ref>.</p><p>As discussed in the related work below, in these three aspects: speaker independence, learning from continuous speech, and lexicon (vocabulary) size, we go far beyond the current state of the art. We also exceed the state of the art in terms of performance, as is also shown in Sect. 4 by comparisons on the standard OuluVS benchmark dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Research on lip reading (a.k.a. visual speech recognition) has a long history. A thorough survey of shallow (i.e. not deep learning) methods is given in the recent review <ref type="bibr" target="#b44">[45]</ref>, and will not repeated in detail here. Many of the existing works in this field have followed similar pipelines which first extract spatiotemporal features around the lips (either motion-based, geometric-feature based or both), and then align these features with respect to a canonical template. For example, Pei et al. <ref type="bibr" target="#b27">[28]</ref>, which holds state-of-the-art on many datasets, extracts the patch trajectory as a spatiao-temporal feature, and then aligns these features to reference motion patterns.</p><p>A number of recent papers have used deep learning methods to tackle problems related to lip reading. Koller et al. <ref type="bibr" target="#b15">[16]</ref> train an image classifier CNN to discriminate visemes (mouth shapes, visual equivalent of phonemes) on a sign language dataset where the signers mouth words. Similar CNN methods have been performed by <ref type="bibr" target="#b24">[25]</ref> to predict phonemes in spoken Japanese. In the context of word recognition, <ref type="bibr" target="#b32">[33]</ref> has used deep bottleneck features (DBF) to encode shallow input features such as LDA and GIF <ref type="bibr" target="#b35">[36]</ref>. Similarly <ref type="bibr" target="#b28">[29]</ref> uses DBF to encode the image for every frame, and trains a LSTM classifier to generate a word-level classification.</p><p>One of the major obstacle to progress in this field has been the lack of suitable datasets <ref type="bibr" target="#b44">[45]</ref>. Table <ref type="table" target="#tab_0">1</ref> gives a summary of existing datasets. The amount of available data is far from sufficient to train scalable and representative models that will be able to generalise beyond the controlled environments and the very limited domains (e.g. digits and the alphabet). Word classification with large lexicons has not been attempted in lip reading, but <ref type="bibr" target="#b10">[11]</ref> has tackled a similar problem in the context of text spotting. Their work shows that it is feasible to train a general and scalable word recognition model for a large pre-defined dictionary, as a multi-class classification problem. We take a similar approach.</p><p>Of relevance to the architectures and methods developed in this paper are ConvNets for action recognition that learn from multiple-frame image sequences such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref>, particularly the ways in which they capture spatio-temporal information in the image sequence using temporal pooling layers and 3D convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Building the Dataset</head><p>This section describes our multi-stage pipeline for automatically collecting and processing a very large-scale visual speech recognition dataset, starting from British television programs. Using this pipeline we have been able to extract 1000s of hours of spoken text covering an extensive vocabulary of 1000s of different words, with over 1M word instances, and over 1000 different speakers. The key ideas are to: (i) obtain a temporal alignment of the spoken audio with a text transcription (broadcast as subtitles with the program). This in turn provides the time alignment between the visual face sequence and the words spoken; (ii) obtain a spatio-temporal alignment of the lower face for the frames corresponding to the word sequence; and, (iii) determine that the face is speaking the words (i.e. that the words are not being spoken by another person in the shot). The pipeline is summarised in Fig. <ref type="figure" target="#fig_1">2</ref> and the individual stages are discussed in detail in the following paragraphs. Stage 1. Selecting Program Types. We require programs that have a changing set of talking heads, so choose news and current affairs, rather than dramas with a fixed cast. Table <ref type="table" target="#tab_1">2</ref> lists the programs. There is a significant variation of format across the programs -from the regular news where a single speaker is talking directly at the camera, to panel debate where the speakers look at each other and often shifts their attention. There are a few people who appear repeatedly in the videos (e.g. news presenter in BBC News or the host in the others), but the large majority of participants change every episode (Fig. <ref type="figure" target="#fig_0">1</ref>).  Stage 2. Subtitle Processing and Alignment. We require the alignment between the audio and the subtitle in order to get a timestamp for every word that is being spoken in the videos. The BBC transmits subtitles as bitmaps rather than text, therefore subtitle text is extracted from the broadcast video using standard OCR methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. The subtitles are not time-aligned, and also not verbatim as they are generated live. The Penn Phonetics Lab Forced Aligner <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref> (based on the open-source HTK toolbox <ref type="bibr" target="#b39">[40]</ref>) is used to force-align the subtitle to the audio signal. The aligner uses the Viterbi algorithm to compute the maximum likelihood alignment between the audio (modelled by PLP features <ref type="bibr" target="#b29">[30]</ref>) and the text. This method of obtaining the alignment has significant performance benefits over regular speech recognition methods that do not use prior knowledge of what is being said. The alignment result, however, is not perfect due to: (1) the method often misses words that are spoken too quickly; (2) the subtitles are not verbatim;</p><p>(3) the acoustic model is only trained to recognise American English. The noisy labels are filtered by double-checking against the commercial IBM Watson Speech to Text service. In this case, the only remaining label noise is where an interview is dubbed in the news, which is rare.</p><p>Stage 3. Shot Boundary Detection, Face Detection, and Tracking. The shot boundaries are determined to find the within-shot frames for which face tracking is to be run. This is done by comparing color histograms across consecutive frames <ref type="bibr" target="#b19">[20]</ref>. The HOG-based face detection method of <ref type="bibr" target="#b14">[15]</ref> is performed on every frame of the video (Fig. <ref type="figure" target="#fig_3">4</ref> left). As with most face detection methods, this results in many false positives and some missed detections. In a similar manner to <ref type="bibr" target="#b5">[6]</ref>, all face detections of the same person are grouped across frames using a KLT tracker <ref type="bibr" target="#b33">[34]</ref> (Fig. <ref type="figure" target="#fig_3">4 middle</ref>). If the track overlaps with face detections on the majority of frames, it is assumed to be correctly tracking the face. (2) for speaker/non-speaker classification. Facial landmarks are determined in every frame of the face track using the method of <ref type="bibr" target="#b13">[14]</ref> (Fig. <ref type="figure" target="#fig_3">4</ref> right). To identify who is speaking, we assume that a person speaking will have lip movements that fall within a particular frequency range that is different to that arising from tracking noise. The 'openness' of the mouth is measured on every frame using the distance between the top and the bottom lip, normalised with respect to the size of the face in the video. For a speaking face, the openness signal contains the actual lip motion as well as the tracking noise, whereas for a non-speaking face (e.g. reaction shot, etc.), the only observed movement is the noise. A simple method of taking the Fourier transform of the mouth 'openness' temporal signal is performed to separate the lip movements that fall into different frequencies bins. A linear SVM classifier is trained on the frequency spectrum to make the distinction between a face that is speaker from a face that is not.  <ref type="table" target="#tab_2">3</ref>. Note that we leave a week's gap between the test set and the rest in case any news footage is repeated. The lexicon is obtained by selecting the 500 most frequently occurring words between 5 and 10 characters in length (Fig. <ref type="figure">6</ref> gives the word duration statistics). This word length is chosen such that the speech duration does not exceed the fixed one-second bracket that is used in the recognition architecture, whilst shorter words are not included because there are too many ambiguities due to homophemes (e.g. 'bad', 'bat', 'pat', 'mat', etc. are all visually identical), and sentence-level context would be needed to disambiguate these. These 500 words occur at least 800 times in the training set, and at least 40 times in each of the validation and test sets. For each of the occurrences, the one-second clip is taken, and the face is cropped with the mouth centered using the registration found in Stage 4. The words are not isolated, as is the case in other lip-reading datasets; as a result, there may be co-articulation of the lips from preceding and subsequent words. The test set is manually checked for errors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network Architecture and Training</head><p>The task for the network is to predict which words are being spoken, given a video of a talking face. The input format to the network is a sequence of mouth regions, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. Previous attempts at visual speech recognition have relied on very precise localisation of the facial landmarks (the mouth in particular); our aim is learn from from more noisy data, and tolerate some localisation irregularities both in position and in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We cast the problem as one of multi-way classification, and so base our architecture on ones designed for image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>. In particular, we build on the VGG-M model <ref type="bibr" target="#b2">[3]</ref> since this has a good classification performance, but is much faster to train and experiment on than deeper models, such as VGG-16 <ref type="bibr" target="#b31">[32]</ref>. We develop and compare four models that differ principally in how they 'ingest' the T input frames (where here T = 25 for a 1 s interval). These variations take inspiration from previous work on human action classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Apart from these differences, the architectures share the configuration of VGG-M, and this allows us to directly compare the performance across different input designs.</p><p>We next describe the four architectures, summarized in Fig. <ref type="figure" target="#fig_6">7</ref>, followed by a discussion of their differences. Their performance is compared in Sect. 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Convolution with Early Fusion (EF-3).</head><p>This architecture is inspired by the work of <ref type="bibr" target="#b11">[12]</ref> on human action recognition using 3D ConvNets. The general structure resembles that of an ordinary CNN used for image classification, but instead of taking H×W×3 input, it takes H×W×T×3 input. The convolutional and pooling filters operate and move along all three dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Convolution with Multiple Towers (MT-3).</head><p>The model shares its basic design principles with the architecture of EF-3, however there is no explicit timedomain connectivity between frames before conv2. There are T = 25 towers with common conv1 layers (with shared weights), each of which takes an input frame. Here, the activations at pool1 are concatenated along a new dimension, and the 3D convolutions from conv2 are performed in the same manner as <ref type="bibr" target="#b11">[12]</ref> and EF-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Fusion (EF).</head><p>The network ingests a T-channel image, where each of the channels encode an individual frame in greyscale. The layer structure for the subsequent layers is identical to that of the regular VGG-M network. This method is related to the Early Fusion model in <ref type="bibr" target="#b12">[13]</ref>, which takes colour images and uses a T × 3-channel convolutional filter at conv1. We did experiment with 25 × 3-channel colour input, but found that the increased number of parameters at conv1 made training difficult due to overfitting (resulting in validation performance that is around 5% weaker; not quoted in Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Towers (MT).</head><p>There are T = 25 towers with common conv1 layers (with shared weights), each of which takes an input frame. The activations from the towers are concatenated channel-wise after pool1, producing an output activation with 1200 channels. The subsequent 1 × 1 convolution is performed to reduce this dimension, to keep the number of parameters at conv2 at a managable level. The rest of the network is the same as the regular VGG-M.</p><p>Discussion. There are two basic divisions of the architectures: between early fusion and multiple towers, and between 2D and 3D convolutions. We will discuss these in turn. The early fusion architectures, EF-3 and EF, share similarities with previous work on human action recognition using ConvNets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42]</ref> in the way that they assume registration between frames. The models perform timedomain operations beginning from the first layer to precisely capture local motion direction and speed <ref type="bibr" target="#b12">[13]</ref>. For these methods to capture useful information, good registration of details between frames is critical. However, we are not imposing strict registration, and in any case it goes slightly against the signal (lip motion and mouth region deformation) that we are trying to capture.</p><p>In contrast, the multiple towers architectures, MT-3 and MT, both delay all time-domain registrations (and operations) until after the first set of convolutional and pooling layers. This gives tolerance against minor registration errors (the receptive field size at conv2 is 11 pixels). Note, the common conv1 layers of the multiple towers ensures that the same filter weights are used for all frames, whereas in the early fusion architecture EF it is possible to learn different weights for each frame. The experimental results show that these registrationtolerant models gives a modest improvement over their counterparts, and the performance improvement is likely to be more significant where the tracking quality is less ideal.</p><p>The reason for including 3D convolutions (the architectures EF-3 and MT-3) is that intuitively a 3D convolution (that can have small spatial and temporal kernel size) should be able to match well a spatio-temporal feature, such as a particular lip shape over a particular sub-sequence. In contrast the 2D convolutions extend over the entire temporal range, and thus might be thought to waste parameters or require redundancy when trying to respond to such spatiotemporal features. Despite this intuition, the experimental results show that the 2D convolutions are superior to their 3D counterparts.</p><p>One other design choice is the size of the input images. This was chosen as 112 × 112 pixels, which is smaller than that typically used in image classification networks. The reason is that the size of the cropped mouth images are rarely larger than 112 × 112 pixels, and this smaller choice means that smaller filters can be used at conv1 (than those used in VGG-M) without sacrificing receptive fields, but at a gain in avoiding unnecessary parameters being learnt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Data Augmentation. Data augmentation often helps to improve validation performance by reducing overfitting in ConvNet image classification tasks <ref type="bibr" target="#b17">[18]</ref>. We apply the augmentation techniques used on the ImageNet classification task by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> (e.g. random cropping, flipping, colour shift), with a consistent transformation applied to all frames of a single clip. To further augment the training data, we make random shifts in time by up to 0.2 s, which improves the top-1 validation error by 3.5% compared to the standard ImageNet augmentation methods. It was not feasible to scale in the time-domain as this results in artifacts being shown due to the relatively low video refresh rate of 25 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details.</head><p>Our implementation is based on the MATLAB toolbox MatCon-vNet <ref type="bibr" target="#b36">[37]</ref> and trained on a NVIDIA Titan X GPU with 12GB memory. The network is trained using SGD with momentum 0.9 and batch normalisation <ref type="bibr" target="#b9">[10]</ref>, but without dropout. The training was stopped after 20 epochs, or when the validation error did not improve for 3 epochs, whichever is sooner. The learning rate of 10 -2 to 10 -4 was used, decreasing on log scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we evaluate and compare the several proposed architectures, and discuss the challenges arising from the visual ambiguities between words. We then compare to the state of the art on a public benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Architectures</head><p>Evaluation Protocol. The models are evaluated on the independent test set (Sect. 2). We report top-1 and top-10 accuracies, as well as recall against rank curves. Here, the 'Recall@K' is the proportion of times that the correct class is found in the top-K predictions for the word. We also report the character-level edit distance <ref type="bibr" target="#b16">[17]</ref>, which is the minimum number of character-level operations required to convert the predicted string to the ground truth. This metric imposes smaller penalties where the predicted string is similar to the ground truth (e.g. 'concerned' and 'concerns' have an edit distance of 2) and larger penalties where the words are very different (e.g. 'concerned' and 'company' have an edit distance of 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>As discussed in Sect. 3.1, the MT-3 and MT variants have the advantage of being more tolerant to registration errors compared to their early fusion counterparts. The results in Table <ref type="table" target="#tab_3">4</ref> and Fig. <ref type="figure" target="#fig_7">8</ref> confirm this, where we see a modest (3.2% on average for top-1 ) but consistent improvement in performance across the experiments. The performance of 3D ConvNets fall short of the 2D architectures by an average of around 14%.</p><p>The recall curves in Fig. <ref type="figure" target="#fig_7">8</ref> rise sharply for all models at low-K; the top-10 figure for the EF and MT models being over 85%, despite the modest top-1 figure of around 60%. This is a result of ambiguities in lip reading, which we will discuss next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OuluVS1 OuluVS2</head><p>Top-1 Top-1 <ref type="bibr" target="#b28">[29]</ref> 81.8% - <ref type="bibr" target="#b43">[44]</ref> 85.6% 73.5% <ref type="bibr" target="#b27">[28]</ref> 89.7% -MT 91.4% 93.2%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Confusions</head><p>Here, we examine the classification results, in particular, the scenarios in which the network fails to correctly classify the spoken word. Table <ref type="table" target="#tab_5">5</ref> shows the most common confusions between words in the test set. This is generated by taking the largest off-diagonal values in the word confusion matrix. This result confirms our prior knowledge about the challenges in visual speech recognition -almost all of the top confusions are either (i) a plural of the original word (e.g. 'report' and 'reports') which is ambiguous because one word is a subset of the other, and the words are not isolated in our dataset so this can be due to co-articulation; or (ii) a known homopheme visual ambiguity (explained in Sect. 1) where the words cannot be distinguished using visual information alone (e.g. 'billion' and 'million', 'worse' and 'worst').  Therefore, we generate a second test set where we eliminate these two types of known ambiguities. We first group the words according to the aforementioned criteria (e.g. 'billion', 'million' and 'millions' would form a single group), and keep only the most frequently occuring word in the training set for each group, eliminating the ambiguous words for that group. This process produces a new balanced test set containing a lexicon of 333 word-classes.</p><p>The network is finetuned on this new vocabulary for 1 epoch, before being reevaluated. The results reported in Table <ref type="table" target="#tab_3">4</ref> and Fig. <ref type="figure" target="#fig_7">8</ref> that are labelled '333-word' are evaluated on this vocabulary. The top-10 performance increases from 90.4% (for the 500 word-class test set) to 92.3% (for the 333 word-class test set). This is an improvement, but still not perfect. The reason is that even excluding the known homopheme and plural ambiguities does not remove all confusion. Table <ref type="table" target="#tab_5">5</ref> shows the common errors remaining, and these are phonetically understandable. For example, some of the most common confusions, e.g. 'claims' which is phonetically (K L EY M Z) and 'games' (G EY M Z), 'probably' (P R AA B AH B L IY) and 'problem' (P R AA B L AH M), actually share most of the phonemes.</p><p>Apart from these difficulties, the failure cases are typically for extreme samples. For example, due to strong international accents, or poor quality/low bandwidth location reports and Skype interviews, where there are motion compression artifacts or frames dropped from the transmission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualisation of Salient Mouth Shapes</head><p>Our aim here is to visualize the frames of the temporal sequence that are most discriminative for the word. Simonyan et al. <ref type="bibr" target="#b30">[31]</ref> have shown that it is possible to infer the localization of visual objects in an image as a saliency map for a network trained to classify images. We adapt this method to find the salient temporal information in a time-sequence. The method approximates the relation between the class score S and the input image I (represented as a vector) as S(I) = w T I + b. The vector w is the same size as the input image, and the magnitude of its elements signify the influence of the corresponding elements of the image on the class score. Hence the magnitude of w determines a saliency map on the image. The vector w can be obtained as w = ∂Sc ∂I I0 and this derivative is obtained by back-prop from the class score S 0 (I 0 ) to the image.</p><p>The resulting salient regions are shown in Fig. <ref type="figure" target="#fig_8">9</ref>. For example, the most distinctive mouth shape for 'office' (AO F AH S) is the 'AH' with the mouth open and 'F' with the top teeth biting the bottom lip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State of the Art</head><p>It is worth noting that the top-1 classification accuracy of 65%, shown in Table <ref type="table" target="#tab_3">4</ref>, is comparable to that of many of the recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref> performed on lexicon sizes that are orders of magnitude smaller (Table <ref type="table" target="#tab_0">1</ref>). OuluVS. We evaluate our method on the OuluVS datasets. OuluVS1 <ref type="bibr" target="#b42">[43]</ref> consists of 20 subjects uttering 10 phrases (e.g. 'thank you', 'hello', etc.), and has been widely used in previous works. OuluVS2 <ref type="bibr" target="#b0">[1]</ref> (short phrases) consists of 52 subjects uttering the same phrases as <ref type="bibr" target="#b42">[43]</ref>. Here, we assess on a speakerindependent experiment, where some of the subjects are reserved for testing.</p><p>To apply our method on this dataset, we pre-train the model on the BBC data, and fine-tune the fully-connected layers. Training from scratch on OuluVS underperforms as the size of this dataset is insufficient to train a deep network. If the phrase is shorter than 25 frames, we simply repeat the first and the last frames to fill the 1-second clip. If the clip is longer, we take a random crop.</p><p>As can be seen in Table <ref type="table" target="#tab_3">4</ref> our method achieves a strong performance, and sets the new state-of-the-art. Note that, without retraining the convolutional part of the network, we achieve these strong results on videos that are very different to ours in terms of lighting, background, camera perspective, etc. (Fig. <ref type="figure" target="#fig_9">10</ref>), which shows that our model generalises well across different formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Extensions</head><p>We have shown that CNN architectures can be used to classify temporal sequences with excellent results. On the 333-word test set, we achieve top-1 accuracy of 65.4%, which exceeds state-of-the-art <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43]</ref> on multiple datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> that have lexicon sizes that are orders of magnitude smaller, and a top-10 accuracy of 92.3%. We also demonstrate a recognition performance that exceeds the state of the art on a standard public benchmark dataset, OuluVS.</p><p>Next steps include extending to lip reading of profile views, and combining the CNNs pre-trained using this approach with LSTMs trained with a language model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>, in order to recognize sentences rather than individual words. Of course, the visual only speech recognition method developed here can also be combined with audio only speech recognition to both their benefits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A sample of speakers in our dataset.</figDesc><graphic coords="4,42.81,97.82,338.71,169.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline to generate the text and visually aligned dataset. Timings are for a one-hour video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Subtitles on BBC TV. Left: 'Question Time', Right: 'BBC News at One'.</figDesc><graphic coords="5,56.97,221.78,338.77,93.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left: face detections; Middle: KLT features and the tracked bounding box (in yellow); Right: facial landmarks. (Color figure online)</figDesc><graphic coords="6,43.29,122.39,337.36,127.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Stage 5 .</head><label>5</label><figDesc>Compiling the Training and Test Data. The training, validation and test sets are disjoint in time. The dates of videos corresponding to each set is shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. One-second clips that contain the word 'about'. Top: male speaker, bottom: female speaker.</figDesc><graphic coords="7,57.48,54.62,337.36,117.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. CNN architectures. Left: VGG-M architecture that is used as a base. Right: EF-3: 3D convolution with early fusion; MT-3: 3D convolution with multiple towers; EF: early fusion; MT: multiple towers.</figDesc><graphic coords="8,142.35,393.29,223.84,155.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Recall vs rank curves for the word classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Salient visual features of sequences 'office' and 'water' are highlighted in red. (Color figure online)</figDesc><graphic coords="13,98.46,294.05,255.16,122.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Original video frames for 'hello' on OuluVS. Compare this to the our original input frames in Fig. 3.</figDesc><graphic coords="14,43.29,131.90,337.33,54.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Existing lip reading datasets. I for Isolated (one word, letter or digit per recording); C for Continuous recording. The reported performance is on speakerindependent experiments. ( * For GRID<ref type="bibr" target="#b3">[4]</ref>, there are 51 classes in total, but the first word in a phrase is restricted to 4, the second word 4, etc. 8.5 is the average number of possible classes at each position in the phrase.)</figDesc><table><row><cell>Name</cell><cell cols="2">Env. Output</cell><cell cols="4">I/C # class # subj. Best perf.</cell></row><row><cell cols="3">AVICAR [19] In-car Digits</cell><cell>C</cell><cell>10</cell><cell>100</cell><cell>37.9% [7]</cell></row><row><cell cols="2">AVLetter [22] Lab</cell><cell cols="2">Alphabet I</cell><cell>26</cell><cell>10</cell><cell>43.5% [43]</cell></row><row><cell cols="2">CUAVE [27] Lab</cell><cell>Digits</cell><cell>I</cell><cell>10</cell><cell>36</cell><cell>83.0% [26]</cell></row><row><cell>GRID [4]</cell><cell>Lab</cell><cell>Words</cell><cell>C</cell><cell>8.5  *</cell><cell>34</cell><cell>79.6% [39]</cell></row><row><cell cols="2">OuluVS1 [43] Lab</cell><cell cols="2">Phrases I</cell><cell>10</cell><cell>20</cell><cell>89.7% [28]</cell></row><row><cell cols="2">OuluVS2 [1] Lab</cell><cell cols="2">Phrases I</cell><cell>10</cell><cell>52</cell><cell>73.5% [44]</cell></row><row><cell cols="2">OuluVS2 [1] Lab</cell><cell>Digits</cell><cell>C</cell><cell>10</cell><cell>52</cell><cell>-</cell></row><row><cell>BBC TV</cell><cell>TV</cell><cell>Words</cell><cell>C</cell><cell cols="3">333/500 1000+ -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Video statistics. The yield is the proportion of useful face appearance relative to the total length of video. A useful face appearance is one that appears continuously for at least 5 s, with the face being that of the speaker.</figDesc><table><row><cell>Channel</cell><cell>Series name</cell><cell>Description</cell><cell cols="2"># vid. Length Yield</cell></row><row><cell cols="2">BBC 1 HD News at 1</cell><cell>Regular news</cell><cell>1242</cell><cell>30 mins 39.9%</cell></row><row><cell cols="2">BBC 1 HD News at 6</cell><cell>Regular news</cell><cell>1254</cell><cell>30 mins 33.9%</cell></row><row><cell cols="2">BBC 1 HD News at 10</cell><cell>Regular news</cell><cell>1301</cell><cell>30 mins 32.9%</cell></row><row><cell cols="2">BBC 1 HD Breakfast</cell><cell>Regular news</cell><cell>395</cell><cell>Varied 39.2%</cell></row><row><cell cols="2">BBC 1 HD Newsnight</cell><cell cols="2">Current affairs debate 734</cell><cell>35 mins 40.0%</cell></row><row><cell cols="2">BBC 2 HD World news</cell><cell>Regular news</cell><cell>376</cell><cell>30 mins 31.9%</cell></row><row><cell cols="4">BBC 2 HD Question time Current affairs debate 353</cell><cell>60 mins 48.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Set</cell><cell>Dates</cell><cell cols="2"># class #/class</cell></row><row><cell cols="3">Train 01/01/2010-28/02/2015 500</cell><cell>800+</cell></row><row><cell>Val</cell><cell cols="2">01/03/2015-25/07/2015 500</cell><cell>50</cell></row><row><cell cols="3">Test 01/08/2015-31/03/2016 500</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Word classification results. Left: on the BBC data for the four different architectures. ED is the edit distance. Right: on OuluVS1 and OuluVS2 (short phrases, frontal view).</figDesc><table><row><cell>Net</cell><cell>500-class</cell><cell>333-class</cell></row><row><cell></cell><cell cols="2">Top-1 Top-10 ED Top-1 Top-10</cell></row><row><cell cols="3">EF-3 43.9% 81.0% 3.13 55.7% 87.9%</cell></row><row><cell cols="3">MT-3 46.2% 82.4% 2.97 56.8% 88.7%</cell></row><row><cell>EF</cell><cell cols="2">57.0% 88.8% 2.32 63.2% 91.8%</cell></row><row><cell cols="2">MT 61</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.1% 90.4% 2.06 65.4% 92.3%</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Most frequently confused word pairs.</figDesc><table><row><cell></cell><cell cols="2">500-class</cell><cell></cell><cell>333-class</cell><cell></cell></row><row><cell cols="2">0.32 BENEFITS</cell><cell>BENEFIT</cell><cell>0.30</cell><cell cols="2">BORDER IMPORTANT</cell></row><row><cell cols="3">0.31 QUESTIONS QUESTION</cell><cell cols="2">0.29 PROBABLY</cell><cell>PROBLEM</cell></row><row><cell>0.31</cell><cell>REPORT</cell><cell>REPORTS</cell><cell>0.27</cell><cell>TAKING</cell><cell>TAKEN</cell></row><row><cell>0.31</cell><cell cols="2">BORDER IMPORTANT</cell><cell cols="2">0.25 PERSONAL</cell><cell>PERSON</cell></row><row><cell>0.31</cell><cell cols="2">AMERICA AMERICAN</cell><cell>0.23</cell><cell>CLAIMS</cell><cell>GAMES</cell></row><row><cell>0.29</cell><cell>GROUND</cell><cell>AROUND</cell><cell>0.22</cell><cell>AROUND</cell><cell>GROUND</cell></row><row><cell>0.28</cell><cell>RUSSIAN</cell><cell>RUSSIA</cell><cell>0.21</cell><cell>TONIGHT</cell><cell>NIGHT</cell></row><row><cell>0.28</cell><cell>FIGHT</cell><cell>FIGHTING</cell><cell>0.21</cell><cell cols="2">PROBLEM PROBABLY</cell></row><row><cell>0.26</cell><cell>FAMILY</cell><cell>FAMILIES</cell><cell>0.19</cell><cell>SEVERAL</cell><cell>SEVEN</cell></row><row><cell cols="2">0.26 AMERICAN</cell><cell>AMERICA</cell><cell cols="2">0.19 CHALLENGE</cell><cell>CHANGE</cell></row><row><cell>0.26</cell><cell>BENEFIT</cell><cell>BENEFITS</cell><cell>0.18</cell><cell>PRICES</cell><cell>PERSON</cell></row><row><cell cols="3">0.25 ELECTIONS ELECTION</cell><cell>0.18</cell><cell>WARNING</cell><cell>MORNING</cell></row><row><cell>0.24</cell><cell>WANTS</cell><cell>WANTED</cell><cell>0.18</cell><cell cols="2">CAPITAL HAPPENED</cell></row><row><cell>0.24</cell><cell cols="2">HAPPEN HAPPENED</cell><cell>0.18</cell><cell>OTHER</cell><cell>ANOTHER</cell></row><row><cell>0.24</cell><cell>FORCE</cell><cell>FORCES</cell><cell>0.17</cell><cell>AHEAD</cell><cell>AGAIN</cell></row><row><cell cols="2">0.23 HAPPENED</cell><cell>HAPPEN</cell><cell>0.16</cell><cell>WORKERS</cell><cell>WORDS</cell></row><row><cell>0.23</cell><cell>SERIOUS</cell><cell>SERIES</cell><cell>0.16</cell><cell>MEDIA</cell><cell>MEETING</cell></row><row><cell>0.23</cell><cell>TROOPS</cell><cell>GROUPS</cell><cell>0.16</cell><cell>UNITED</cell><cell>NIGHT</cell></row><row><cell cols="3">0.22 QUESTION QUESTIONS</cell><cell>0.16</cell><cell>NEVER</cell><cell>SEVEN</cell></row><row><cell cols="3">0.21 PROBLEM PROBABLY</cell><cell>0.15</cell><cell>WORLD</cell><cell>WORDS</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Funding for this research is provided by the EPSRC Programme Grant Seebibyte EP/M013774/1. We are very grateful to Rob Cooper and Matt Haynes at BBC Research for help in obtaining the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OuluVS2: a multi-view audiovisual database for non-rigid mouth motion analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Anina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning sign language by watching TV (using weakly aligned subtitles)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hello! My name is.. Buffy&quot; -automatic naming of characters in TV video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification and feature extraction by simplexization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rationale for phoneme-viseme mapping and feature selection in visual speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speechreading by Humans and Machines</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hennecke</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="505" to="515" />
		</imprint>
	</monogr>
	<note type="report_type">Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning, NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dlib-ml: a machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning of mouth shapes for sign language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A new algorithm for the alignment of phonetic sequences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kondrak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="288" to="295" />
		</imprint>
	</monogr>
	<note>Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
		<respStmt>
			<orgName>NIPS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">AVICAR: audio-visual speech corpus in a car environment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goudeseune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<editor>INTERSPEECH. Citeseer</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reliable transition detection in videos: a survey and practitioner&apos;s guide. Images Graph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="469" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Confusability of phonemes grouped according to their viseme classes in noisy environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Australian International Conference on Speech Science &amp; Technical</title>
		<meeting>Australian International Conference on Speech Science &amp; Technical</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="265" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extraction of visual features for lipreading</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bangham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML 2011)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="1149" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CUAVE: a new audio-visual database for multimodal human-computer interface research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurbuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gowdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2002 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised random forest manifold alignment for lipreading</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2304" to="2308" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Content-based tools for editing audio stories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthouzoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep bottleneck features and highperformance lipreading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osuga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iribe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayamizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="575" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>Vancouver, BC, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<title level="m">Learning spatiotemporal features with 3D convolutional networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GIF-LR: GA-based informative feature for lipreading</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ukai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayamizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno>CoRR abs/1412.4564</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show and tell: a neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.08188</idno>
		<title level="m">Lipreading with long short-term memory</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The 1994 HTK large vocabulary speech recognition system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leggetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995. 1995. 1995</date>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond short snippets: deep networks for video classification</title>
		<author>
			<persName><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A compact representation of visual speech data using latent variables</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
