<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Wikipedia for cross-lingual and multilingual information retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-03-08">8 March 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">P</forename><surname>Sorg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institut AIFB</orgName>
								<orgName type="department" key="dep2">KIT (Campus Süd)</orgName>
								<address>
									<postCode>76128</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
							<email>cimiano@cit-ec.uni-bielefeld.de</email>
							<affiliation key="aff1">
								<orgName type="department">Cognitive Interaction Technology Excellence Center (CITEC)</orgName>
								<orgName type="laboratory">Semantic Computing Group</orgName>
								<orgName type="institution">University of Bielefeld</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefed</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Wikipedia for cross-lingual and multilingual information retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-03-08">8 March 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">7FEA6A73F1BFC504F24279EBB677FBE0</idno>
					<idno type="DOI">10.1016/j.datak.2012.02.003</idno>
					<note type="submission">Received 15 July 2010 Received in revised form 30 January 2012 Accepted 23 February 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Cross-Lingual Information Retrieval Concept-based Information Retrieval Social Web Wikipedia</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article we show how Wikipedia as a multilingual knowledge resource can be exploited for Cross-Language and Multilingual Information Retrieval (CLIR/MLIR). We describe an approach we call Cross-Language Explicit Semantic Analysis (CL-ESA) which indexes documents with respect to explicit interlingual concepts. These concepts are considered as interlingual and universal and in our case correspond either to Wikipedia articles or categories. Each concept is associated to a text signature in each language which can be used to estimate language-specific term distributions for each concept. This knowledge can then be used to calculate the strength of association between a term and a concept which is used to map documents into the concept space. With CL-ESA we are thus moving from a Bag-Of-Words model to a Bag-Of-Concepts model that allows language-independent document representations in the vector space spanned by interlingual and universal concepts. We show how different vector-based retrieval models and term weighting strategies can be used in conjunction with CL-ESA and experimentally analyze the performance of the different choices. We evaluate the approach on a mate retrieval task on two datasets: JRC-Acquis and Multext. We show that in the MLIR settings, CL-ESA benefits from a certain level of abstraction in the sense that using categories instead of articles as in the original ESA model delivers better results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The activity of searching for information has become a central activity in our lives. We use web search engines daily to find information satisfying our information needs. Web search engines in essence apply Information Retrieval (IR) techniques to the Web and thus suffer from the vocabulary mismatch problem that every IR system is faced with. In short, the vocabulary mismatch problem can be described as the problem that highly relevant documents might potentially be judged as irrelevant due to a low textual overlap between query and document. An extreme case of the vocabulary mismatch problem arises in settings where relevant documents are written in other languages than the one of the query.</p><p>Supporting the task of retrieving information across languages is an important task in a multi-cultural and multilingual society as we live in. While the lingua franca of the World Wide Web has been English for a long time, this situation has changed dramatically since a few years. In fact, the percentage of internet users from countries with official languages other than English is growing at a much higher rate compared to the percentage of English-speaking users 1 <ref type="bibr" target="#b0">[1]</ref>. Many users from countries with official languages other than English are however able to understand the English language in addition to their own one. For these bilingual users content in at least two languages is thus relevant. This clearly motivates the need for efficient and effective retrieval techniques that cross the boundaries of languages.</p><p>As already mentioned, the task of retrieving relevant documents in a language different from the query language is inherently affected by the so-called vocabulary mismatch problem. One way to overcome the language barrier is to factor in background knowledge into IR models by relating terms in different languages to concepts or units of thought. This allows to calculate the match between documents and queries at a conceptual level abstracting from the realization of these concepts in a specific language. This requires the availability of resources meeting the following two requirements:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Topical coverage</head><p>Resources need to cover many relevant concepts of interest to the users of the IR system in question. Resources have to contain broad-coverage definitions of concepts. Given a specific retrieval task, these concepts need to span all topics of interest of the given document collection. For general retrieval systems, resources need to cover a sufficient amount of general knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Language independence</head><p>Resources need to contain knowledge about how these concepts are realized across languages. The concepts need to be universal, having the same meaning when interpreted in each language.</p><p>The so-called Web 2.0 is a term describing the transition of the WWW from a Web in which a small number of people publish information and the majority merely consumes this information into a Web where the masses actively contribute content, in particular using social media sites such as Flickr, Facebook, Wikipedia, Wiktionary, etc. These resources have typically a wide coverage as they are used by many people and thus cover many topics that are of interest to a broad mass of people. In our context, Wikipedia is a particularly interesting resource due to the fact that i) it spans a broad coverage of topics and ii) it contains knowledge about how concepts are realized in different languages.</p><p>Therefore, Wikipedia shows a high potential for being successfully employed in the task of Cross-Lingual IR (CLIR). This is the issue we systematically explore in this article. Many CLIR techniques are based on bi-lingual dictionaries. For the purposes of retrieval, Wikipedia can indeed be seen as a large-coverage multilingual resource. A further interesting characteristic of Wikipedia is that it is up-to date in contrast to many lexical resources which are not constantly developed further.</p><p>As a specific contribution of this paper we show the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Wikipedia as interlingual concept space</head><p>We show how Wikipedia can be used to define an interlingual or universal concept space. Each concept is associated to a text signature in different languages which is the content of a set of Wikipedia articles. The article set relevant for a given concept is extracted by exploiting the structure of Wikipedia, i.e. the assignment of articles to categories, the subcategory relation and the cross-language relations linking equivalent articles across languages. Further, we examine whether concepts are better defined on the basis of Wikipedia articles or rather categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Bag-of-concepts model</head><p>We show how documents can be represented as vectors in this interlingual concept space. Here we follow the approach of Explicit Semantic Analysis (ESA) <ref type="bibr" target="#b1">[2]</ref>. Therefore we call our approach Cross-lingual Explicit Semantic Analysis (CL-ESA) as it basically extends ESA to cross-lingual retrieval scenarios. <ref type="foot" target="#foot_0">2</ref> The underlying model of ESA is the Bag-of-Concepts model. In contrast to the widely used Bag-of-Words model which is based on terms as smallest unit of meaning, the Bag-of-Concepts model abstracts from single terms and represents documents by concept vectors. As concepts are assumed to be universal and abstract from specific languages, the corresponding Bag-of-Concepts vectors can be regarded as language-independent such that vectors from documents/ queries in different languages are mapped into the same space and become thus directly comparable. In contrast, using the Bag-of-Words model, the dimensions of vectors differ between languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Vector-based retrieval</head><p>The Bag-of-Concepts model allows the application of vector-based retrieval frameworks that are well-known from monolingual IR to CLIR settings. We present the generalization of different IR retrieval models from the Bag-of-Words to the Bag-of-Concepts model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.">Mate-retrieval experiments</head><p>We carry out experiments on a mate retrieval task examining the performance of different ways of mapping documents to concept vectors (pruning strategies in particular) as well as different retrieval models. We also show how the category structure of Wikipedia can be successfully factored into the CL-ESA approach. These experiments also reveal the difference between crosslingual and multilingual IR in respect to optimal design choices in the CL-ESA model.</p><p>The remainder of the article is structured as follows: In Section 2 we motivate concept based retrieval models and define the structure of Wikipedia as used in our approach. Then, in Section 3, we introduce CL-ESA. In Section 4, different design choices of the CL-ESA model and different concept space definitions are presented. Section 5 presents experimental settings and results of our experiments. In Section 6 we discuss related work. Last, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Many established and widely applied approaches to IR are based on vector space representations of documents. Vector-based retrieval models evolved from the Boolean retrieval model-which only takes into account whether a term occurs in a given document or not-by adding term weights quantifying the importance of a term for a given document. These weights are usually based on term frequencies in documents and on the distribution of terms in the corpus. Vector-based retrieval models have been applied very successfully and have comparable performance to alternative approaches such as retrieval models based on language models. Typically, they are based on the Bag-of-Words (BoW) model. In the BoW model, a dictionary consisting of all the words found in the document collection in question essentially defines the dimensions of the document vectors used to represent documents and queries. In the BoW model, the order of the words in the original document is not represented (hence the name "bag").</p><p>The problems we encounter when extending this model for cross-lingual scenarios are apparent. As terms are languagespecific, the dimensions of term vectors from documents in different languages are in general orthogonal. Thus, any similarity measure defined on vector spaces is zero in the general case. <ref type="foot" target="#foot_2">3</ref> In this paper we provide an alternative to the BoW model and show how Bag-of-Concepts (BoC) models can be successfully applied in CLIR scenarios. We also show how standard vectorbased retrieval models can be combined with such BoC models.</p><p>Further, we present two retrieval scenarios, cross-lingual and multilingual IR. We define both retrieval tasks and highlight the differences. In Section 5 we also present experiments corroborating that multilingual retrieval is inherently more difficult than cross-lingual retrieval.</p><p>Finally, we describe the structure of the Wikipedia database exploited in our approach. We introduce the relevant terminology and describe how we exploit Wikipedia as a multilingual knowledge resource in our approach. In particular, we discuss different approaches how concept spaces as well as text signatures for concepts can be defined by exploiting the structure in Wikipedia as multilingual knowledge resource. As Wikipedia is user-generated, we also discuss the impact of problems with user-generated content on the cross-lingual retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Cross-lingual vs. multilingual IR</head><p>Two different retrieval problems have been considered in the context of retrieval systems bridging between languages: Crosslanguage Information Retrieval (CLIR) and Multi-lingual Information Retrieval (MLIR). While CLIR is concerned with retrieval for given language pairs (i.e. all the documents are given in a specific language and need to be retrieved to queries in another language), MLIR is concerned with retrieval from a document collection where documents in multiple languages co-exist and need to be retrieved to a query in any language. MLIR is thus inherently more difficult than CLIR.</p><p>There are two main problems that make MLIR very challenging. On the one hand, we face what we could call a language-bias problem which manifests itself in two facets. The language bias type I captures the intuition that, given a query in a certain language, the retrieval system is in principle biased to return documents in that specific language. The language bias type II captures the fact that retrieval systems based on standard term weighting models are intrinsically biased to rank documents in the most prominent language of the collection relatively high. Both problems arise in particular in connection with document models used in monolingual IR, which are language-specific.</p><p>The application of Machine Translation (MT) is a common technique to solve the vocabulary mismatch introduced by the cross-lingual scenario. By translating documents or queries, the resulting term vectors are based on the same dictionary to define the dimensions and become thus directly comparable <ref type="bibr" target="#b3">[4]</ref>. However, the MT based approach has two main problems. First, translation errors are propagated to the retrieval step. Wrong translations will probably result in retrieval of non-relevant documents. Second, while this approach works relatively well for CLIR, Nie <ref type="bibr" target="#b4">[5]</ref> argues that it is less optimal in MLIR settings. According to Nie, the main problem, given that multiple languages are represented in the document collection, lies in finding an optimal combination of independent retrieval processes for the different languages. The default strategy for MT-based MLIR is to translate the query into all languages in the corpus and create a separate index for each language. This implies that, in order to process a query, the results of separate retrieval steps operating on each of the language-specific indices need to be combined. The final relevance score for a document is thus obtained by aggregation of the language-specific results. As the absolute score values are not comparable across languages in general, normalization and weighting are necessary steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Motivation for concept based vector space models</head><p>Instead of BoW representations, we can also opt for concept-based representations which abstract from specific terms and index documents with respect to concepts as atomic units of meaning. As concept-based representations abstract from surface representation, they are inherently suitable for multilingual scenarios. As vector-based similarity measures in the interlingual concept space can be used for retrieval, the problem of score aggregation-which is introduced by retrieval models based on Machine Translation-can be circumvented.</p><p>However, for retrieval models based on concept spaces, the language bias problem still remains. In our experiments in Section 5 we show that the proposed model overcomes both language bias problems as defined above. We also present variants of the retrieval model that optimize performance for the specific scenario, i.e. CLIR or MLIR. Finally, we compare the suggested model to alternative concept-based retrieval approaches, showing that our model indeed outperforms other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Wikipedia database</head><p>For our retrieval framework we consider a fixed set of supported languages L = {l 1 ,…, l n }. As background knowledge for the multilingual retrieval model we use Wikipedia databases W l with l ∈ L. W l consists of a set of articles A W l ð Þand a set of categories C W l ð Þ in language l. In this paper we will use a sample subset of English and German articles and categories to visualize the proposed retrieval model, which is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. Articles are presented in the lower level and categories in the upper level. In Wikipedia, there are links that assign articles to categories as well as links between categories defining sub-category relationships.</p><p>We will refer to both as category links, which are illustrated by the vertical arrows in Fig. <ref type="figure" target="#fig_0">1</ref>. Cross-language links are another feature of Wikipedia exploited in our approach. They connect equivalent articles or categories across languages and are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> by horizontal connections. The concept-based document representations proposed in this paper exploit different features of the Wikipedia database. The most simple model is only based on articles and cross-language links between articles. Other models are based on categories and subtrees of categories. When describing these models in Section 4, we will again refer to Fig. <ref type="figure" target="#fig_0">1</ref> that visualizes the structure of Wikipedia used to define each of the different concept spaces.</p><p>The Wikipedia databases contain user-generated content. All links, category and cross-language links in particular, are manually created and therefore not always consistent across languages. We identified three problems that are most relevant in our context. First, there is the problem of missing or false language links. As an example, the English article "Container ship" is not linked to its Spanish equivalent "Buque portacontenedores". Second, categories in one language might contain articles that are not part of the equivalent category in another language. A similar problem is given by the differences in sub-category relationships. An example in the Wikipedia dataset used in the experiments is the category "Natural scientists" which is a sub-category of "Natural sciences". In the German Wikipedia however, the equivalent articles "Naturwissenschaftler" and "Naturwissenschaft" have no such sub-category relation. <ref type="foot" target="#foot_3">4</ref>While manually created links might be erroneous, it has been shown that the high number of Wikipedia editors as a collective succeeds very well in maintaining information quality at high standards <ref type="bibr" target="#b5">[6]</ref>. As our approach relies on statistical term distributions across a high number of articles, we assume that these "errors"-actually structural differences across languages-will only marginally affect the retrieval performance. While we do not proof this hypothesis, the results of our experiments based on different snapshots of Wikipedia are in line with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross-lingual explicit semantic analysis</head><p>In this paper we are concerned with the question of how user-generated content from Web 2.0 sites-Wikipedia in particularcan be exploited for cross-lingual and multilingual IR. Thereby, we follow the approach of Explicit Semantic Analysis (ESA)introduced by Gabrilovich and Markovitch <ref type="bibr" target="#b1">[2]</ref>. They propose indexing of documents with respect to externally defined concept spaces. In their model, the function computing the association strength between documents and concepts is essentially based on the word overlap between the document in question and the textual signature of the concept in the corresponding language. As a possible knowledge source for concept definitions they propose using Wikipedia, with concepts corresponding to articles. We will also rely on Wikipedia to define the universal concept space, but not limit ourselves to articles as potential concepts, also considering the category structure of Wikipedia to define concepts.</p><p>One question raised in this paper is how this concept indexing can be extended to multilingual scenarios. We propose to use an extended version of ESA that indexes documents with respect to interlingual concepts. Based on the words appearing in documents, a degree of association to every concept is calculated relying on the concept's text signature. In Wikipedia, cross-language links identify identical or similar articles in Wikipedias of different languages which can thus be exploited to define an interlingual concept spaces along with language specific textual descriptions for each concept.</p><p>In this section, we first introduce the ESA model and present a cross-lingual extension of ESA. We call this extension CL-ESA. This model was also independently introduced in <ref type="bibr" target="#b2">[3]</ref>. Finally we describe how CL-ESA can be applied to retrieval scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Explicit semantic analysis</head><p>Explicit Semantic Analysis (ESA) <ref type="bibr" target="#b1">[2]</ref> indexes a given document d with respect to a set of explicitly given external concepts. Gabrilovich and Markovitch <ref type="bibr" target="#b1">[2]</ref> have outlined the general theory behind ESA and in particular described its instantiation for the case of using Wikipedia articles as external concepts. We will basically build on this instantiation of ESA, which we describe in more detail in the following. As we rely on Wikipedia as multilingual knowledge resource to define universal concepts, we will introduce ESA and its cross-lingual extension CL-ESA using notations specific to Wikipedia, e.g. articles, categories and cross language links. However, ESA is a general model and has been applied to other resources, e.g. Wiktionary <ref type="foot" target="#foot_4">5</ref> and the Open Directory Project<ref type="foot" target="#foot_5">6</ref>  <ref type="bibr" target="#b6">[7]</ref>. Any dataset containing multilingual concept descriptions could potentially be used as background knowledge for CL-ESA. As a running text example to illustrate different aspects of CL-ESA and its application to retrieval, we use the title of a document from the Multext dataset<ref type="foot" target="#foot_6">7</ref> (introduced in Section 5), translated into English and German: English: The transport of bicycles on trains German: Beförderung von Fahrrädern mit dem zug</p><p>In our examples, the concepts used for indexing this text will be defined by the articles and categories presented in Fig. <ref type="figure" target="#fig_0">1</ref>. The structural features presented in this figure, i.e. category links and language links, will also be used to define the interlingual concepts used in the examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Definition of concepts</head><p>All concept models described below build on an explicitly defined finite set of concepts C = {c 1 ,…, c m } with respect to which documents will be indexed across languages.</p><p>Further, for all models we assume the existence of a text signature τ l of concepts which defines the textual description of concepts for the given language l. As we extract concepts from Wikipedia, this is defined by the content of a set of articles of Wikipedia W l in language l:</p><formula xml:id="formula_0">τ l : C→2 A W l ð Þ</formula><p>By considering the textual signature of concepts, we can estimate the language-specific term distribution for each concept. Using term weighting techniques, this information is then used to index documents in each language with respect to concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Original ESA model</head><p>ESA maps a document d in language l to a high-dimensional real-valued vector space spanned by concepts C. This mapping function Φ l : D→R m is defined as follows:</p><formula xml:id="formula_1">Φ l d ð Þ :¼ ϕ l d; c 1 ð Þ; …; ϕ l d; c m ð Þ ð Þ T<label>ð1Þ</label></formula><p>The value ϕ l (d, c i ) in the ESA vector of d expresses the strength of association between a document d and concept c i . This value is computed based on term distributions in d and in the text signature τ l (c i ) of concept c i .</p><p>In the original model, it is based on a TFIDF function on the text signature of concepts applied to all words w of document d:</p><formula xml:id="formula_2">ϕ l d; c ð Þ:¼ ∑ w∈d TFIDF τ l c ð Þ w ð Þ</formula><p>The TFIDF function is based on the Bag-of-Words model, with each dimension of term vectors corresponding to a unique term. For a given document d in corpus D, the values of each dimension represent the weight of a term t in d. This weight is typically computed by taking into account the distribution of the term in the document and corpus. For instance, TFIDF is a widely used function counting the occurrences of a term in a document weighted by the inverse number of documents in the corpus which have at least one occurrence of this term:</p><formula xml:id="formula_3">TFIDF d t ð Þ ¼ TF d t ð Þ d j j log D j j DF t ð Þ</formula><p>with TF d (t) as the term frequency of t in d, |d| the number of tokens in d, |D| the number of documents and DF(t) the document frequency of t, defined by the number of documents in D containing term t.</p><p>For our running example the association strength to the concept Bicycle is visualized in Fig. <ref type="figure" target="#fig_1">2</ref>. For illustration purposes, we indicate the term frequency vector defined by TF τ en (Bicycle) (t) instead of TFIDF values in our examples. In this case the association strength is computed by summing all values in this frequency vector that correspond to terms occurring in the sample text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-lingual ESA</head><p>In order to be able to use Wikipedia as multilingual background knowledge, we build on the notion of interlingual articles IA W ð Þ and interlingual categories IC W ð Þ. The definition of interlingual articles is based on cross-language links in Wikipedia. These directed links connect equivalent articles across languages and define the relation LANGLINKpA W ð ÞÂA W ð Þ with (a 1 , a 2 ) ∈ LANGLINK if article a 1 is linked to a 2 via a cross-language link. The symmetric, reflexive and transitive closure of the LANGLINK relation can be used to define the equivalence relation ≡ LL . The set of interlingual articles is then defined by the set of equivalence classes of all articles in different languages based on ≡ LL :</p><formula xml:id="formula_4">IA W ð Þ :¼ A W ð Þ=≡ LL</formula><p>Interlingual articles can be represented by any article in the according equivalence class and therefore have representations in different languages.</p><p>A bicycle, also known as a bike, pushbike or cycle, is a pedal-driven, human-powered, single-track vehicle, having two wheels attached to a frame, one behind the other. A person who rides a bicycleis called a cyclist or a bicyclist. …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicycle</head><p>The transport of bicycles on trains Interlingual articles are thus defined by sets of equivalent articles in different languages. The function ARTLS l maps interlingual articles to their Wikipedia articles in a specific language l:</p><formula xml:id="formula_5">ARTLS l : IA W ð Þ→2 A W l ð Þ</formula><p>In Fig. <ref type="figure" target="#fig_2">3</ref> the interlingual article [Train] is visualized. It is defined by the equivalence class containing the English article Train and all articles that are connected to this article via cross-language link (and therefore defined as equivalent). In our example there are four articles that are fully conntected across all languages, i.e. Zug (German), Train (English), Train (French) and Tren (Spanish). However, the German article Bahn (Verkehr) is also part of this equivalence class as it also links to Train.</p><p>To apply ESA in a cross-lingual setting, the concept space is based on interlingual articles:</p><formula xml:id="formula_6">C :¼ IA W ð Þ.</formula><p>We define this extension to ESA as cross-lingual ESA (CL-ESA). In this case the text signature of concepts is defined as:</p><formula xml:id="formula_7">τ CLESA;l c ð Þ :¼ ARTLS l c ð Þ</formula><p>This text signature is used to compute the association strength between documents and concepts. If interlingual articles map to several articles in a specific language, the text signature is defined as the concatenation of these articles. As the concept representation is based on interlingual concepts, documents of different languages are mapped to the same concept space.</p><p>In Section 4 we present ESA models based on interlingual categories instead of interlingual articles. As categories in Wikipedia are also connected via cross-language links, the definition of interlingual categories IC W ð Þ is analogous to the notion of interlingual articles using the same equivalence relation. The function CATS l maps interlingual categories to their Wikipedia categories in each language:</p><formula xml:id="formula_8">CATS l : IC W ð Þ→2 C W l ð Þ</formula><p>Applied to CL-ESA, the concept space is then defined by the interlingual categories:</p><formula xml:id="formula_9">C :¼ IC W ð Þ.</formula><p>As an example, the interlingual category [Rail transport] is visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. By analogy to the definition of interlingual articles, the presented equivalence class </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beförderung von Fahrrädern mit dem Zug</head><p>The transport of bicycles on trains of categories contains four fully connected categories and an additional Spanish category Ferrocarril that links to the English category Rail transport.</p><p>Ideally, each interlingual article (category) contains exactly one article (category) in each language. However, this can not be assumed in the general case as is also shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Statistics about the used Wikipedia databases, e.g. the average number of articles (categories) in each interlingual article (category), will be presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CL-ESA applied to CLIR/MLIR</head><p>ESA has been applied to IR by mapping both queries and documents to the concept space and then ranking documents by the vector distance to the query using the cosine distance measure. As the representations delivered by CL-ESA are languageindependent, the framework can be naturally extended to multilingual retrieval settings without any further modification. Coming back to our running example, we interpret the English text as query, the German text as document. Fig. <ref type="figure" target="#fig_3">4</ref> visualizes the CL-ESA representation of both text snippets. For retrieval purposes, the vector-based similarity of the concept vector of the query and every document in the collection can be calculated and the documents can be ranked according to this similarity.</p><p>A common technique used in all ESA implementations known to us is pruning of ESA vectors. This has two main reasons: Firstly, it allows more efficient implementations of the retrieval step due to more compact vector representations. This is also highly relevant for reducing the storage size of inverted concept indexes that are required for real-time retrieval. Second, pruning of the vectors also reduces noise in concept representations. Experiments using different pruning functions will be presented in Section 5. An example for pruning is given in Figure <ref type="figure" target="#fig_4">5</ref>. Setting selected values of the CL-ESA vector to zero raises the sparseness of the vector and allows more efficient implementations with regard to memory and storage consumption.</p><p>The final ranking function for a document d in language l d given a query q in language l q is then defined as:</p><formula xml:id="formula_10">REL q; d ð Þ¼REL Π Φ l q q ð Þ ; Π Φ l d d ð Þ<label>ð2Þ</label></formula><p>First, the query and the document are mapped to the concept space using the language-specific CL-ESA functions Φ l q and Φ l d (compare Fig. <ref type="figure" target="#fig_3">4</ref>). Both resulting concept vectors are then pruned using the functionΠ (compare Fig. <ref type="figure" target="#fig_4">5</ref>). Finally, the similarity of both pruned vectors is computed using the function REL. The output can be used to compute a relevance score between query and document.</p><p>When instantiating the CL-ESA model, we are faced with many design choices. Particular design choices are: How is the concept space defined? How does this definition exploit the structure of Wikipedia? Which function should be used to compute the association strength? What is the best strategy for pruning ESA vectors? How is relevance between query and document vectors modeled? In the next section, we present various alternatives for the above mentioned design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Design choices for CL-ESA</head><p>The CL-ESA model applied to CLIR/MLIR as presented in Section 3 allows different choices for functions and parameters. These choices essentially determine the concept space, how concept vectors are constructed and how the relevance of a document given a query is computed in the space of concepts (see Eq. ( <ref type="formula" target="#formula_10">2</ref>)). Wikipedia also allows variations in the definition of the interlingual concept space, e.g. based on articles vs. categories. In the following we present the design choices of our generalized CL-ESA model, which can be summarized as follows:</p><p>• Dimension Projection Function Π: Defines the pruning of the CL-ESA vector to reduce the number of non-zero dimensions.</p><p>When implementing ESA, processing and storing of concept vectors using all dimensions with association strength greater than 0 implies high computation and storage costs. Therefore, we prune the vectors to obtain sparser representations. • Association Strength Function ϕ l : Quantifies the degree of association between document d in language l and concept c. This is based on the text signature τ l of concept c and the text of document d.  • Concept Space: The concept space essentially defines a set of interlingual concepts as well as their textual signatures in all relevant languages. Interlingual concept spaces can be defined both on the basis of Wikipedia articles and categories. We also present an approach that is based on the category taxonomy and uses the sub-category relation in Wikipedia to define concepts.</p><p>In this section, we will discuss particular implementations of the above functions and design choices for the concept space that can be used to instantiate the proposed generic retrieval framework based on CL-ESA. In Section 5 we will also provide experimental evaluation of particular instantiations applied to CLIR and MLIR scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dimension projection</head><p>The dimension projection function is used to prune CL-ESA vectors and thus reduces the number of non-zero entries in each concept vector, thus speeding up the computation of any relevance function. We present different design choices for the realization of the dimension projection function Π that have been considered in previous literature but never been analyzed nor compared systematically.</p><p>Let d → i denote the i-th dimension of the ESA vector of d representing association strength of d and concept c i . The function α d defines an order on the indices of the dimensions according to descending values such that ∀i; j : ibj→d</p><formula xml:id="formula_11">→ α i ð Þ ≥d → α j ð Þ , e.g. d → α 10 ð Þ is the 10-th highest value of d → .</formula><p>We consider the following variants for the dimension projection function:  </p><formula xml:id="formula_12">→ j ≥t Ã d → α 1 ð Þ , t ∈ [0..1],</formula><formula xml:id="formula_13">→ α i-l ð Þ -d → α i ð Þ ≥t Ã d → α 1 ð Þ , t ∈ [0..1]</formula><p>A relevant question is certainly how to set the parameters m and t. We address this in the experiments by first fixing a reasonable value for m in Π abs m . In order to be able to compare the different approaches, we choose the parameter t in such a way that the number of non-zero dimensions of the projected ESA vectors of all documents in the datasets amounts to m on average. The parameter l was set to 100 as in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Association strength</head><p>For each language, we rely on a function ϕ l (d, c) to calculate the value for the dimension corresponding to concept c, representing the association score between d and this concept. These functions are based on the term representation of d and the text signature of c in language l.</p><p>Let </p><formula xml:id="formula_14">ϕ TFICF;l d; c ð Þ:¼ ∑ w∈d TF d w ð ÞRTF τ l c ð Þ w ð ÞICF w ð Þ</formula><p>• TFICF*: A modified TFICF version ignoring how often the terms occur in document d:</p><formula xml:id="formula_15">ϕ TFICF Ã ;l d; c ð Þ¼ ∑ w∈d RTF τ l c ð Þ w ð ÞICF w ð Þ</formula><p>• TF: An association function only based on term frequencies (ignoring inverse document frequencies):</p><formula xml:id="formula_16">ϕ TF;l d; c ð Þ¼ ∑ w∈d TF d w ð ÞRTF τ l c ð Þ w ð Þ</formula><p>• The BM25 ranking function as defined by Robertson et al. <ref type="bibr" target="#b9">[10]</ref> with parameters set to the following standard value: k 1 = 2, b = 0.75:</p><formula xml:id="formula_17">ϕ BM25;l d; c ð Þ¼ ∑ w∈d TF τ l c ð Þ w ð Þ k 1 þ 1 ð Þ k 1 1-b ð Þþb τ l c ð Þ j j ∑ c ′ ∈C τ l c ′ ð Þ j j C j j 0 @ 1 A þ TF τ l c ð Þ w ð Þ ICF BM25 w ð Þ with ICF BM25 w ð Þ ¼ log C j j-CF w ð Þ þ 0:5 CF w ð Þ þ 0:5</formula><p>• The Cosine similarity between the TF and TFICF vectors:</p><formula xml:id="formula_18">d → ¼ TF d w 1 ð Þ; TF d w 2 ð Þ; … ð Þ T c → l ¼ TFICF τ l c ð Þ w 1 ð Þ; TFICF τ l c ð Þ w 2 ð Þ; … T ϕ cos d; c ð Þ¼ b d → ; c → l &gt; ∥ d → ∥∥ c → l ∥</formula><p>Note that we have also experimented with versions of the above where the TF τ l (c) instead of RTF τ l (c) values were used, yielding in all cases worse results with a performance degradation of about 75% in all cases. For this reason, we do not present the results with the TF τ l (c) versions of the above functions in detail.</p><p>For MLIR settings we also performed experiments putting more weight on the ICF factor:</p><p>• TFICF 2 : TFICF as presented above with quadratic ICF factor: • TFICF 3 : TFICF with cubic ICF factor:</p><formula xml:id="formula_19">ϕ TFICF 2 ;l d; c ð Þ:¼ ∑ w∈d TF d w ð ÞRTF τ l c ð Þ w ð ÞICF w ð Þ<label>2</label></formula><formula xml:id="formula_20">ϕ TFICF 3 ;l d; c ð Þ:¼ ∑ w∈d TF d w ð ÞRTF τ l c ð Þ w ð ÞICF w ð Þ 3</formula><p>The quadratic and cubic ICF factor gives higher weight to seldom words and lower weights to frequent words. Our experiments indeed confirm that considering ICF to the power of 2 and 3 is beneficial in MLIR but not in CLIR settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relevance function</head><p>The relevance function REL(q, d) defines the score of a document d ∈ D in language l d for a given query q in language l q and is used to rank the documents in the retrieval process. In this multilingual setting, the function is defined on pruned CL-ESA vectors q <ref type="bibr">Section 3)</ref>. By analogy to the Bag-of-Words model, term statistics used in retrieval models can be generalized to the Bag-of-Concepts model. The "term frequency" of concept c in document d is defined as</p><formula xml:id="formula_21">→ :¼ Π Φ lq q ð Þ of query q and d → :¼ Π Φ l d d ð Þ À Á of document d (see</formula><formula xml:id="formula_22">TF d c ð Þ :¼ d → c</formula><p>, the "document frequency" DF(c) is the number of documents in D with d → c &gt; 0. Based on this analogy, standard relevance functions defined for text retrieval can be applied to the Bag-of-Concepts model.</p><p>• The Cosine similarity of query and document vectors (used by all ESA implementations known to us):</p><formula xml:id="formula_23">REL cos q; d ð Þ¼ b q → ; d → &gt; ∥ q → ∥∥ d → ∥</formula><p>• TFIDF: The TFIDF function transferred to the Bag-of-Concepts model:</p><formula xml:id="formula_24">REL TFIDF q; d ð Þ¼∑ c∈C TF q c ð ÞRTF d c ð ÞIDF c ð Þ ¼ ∑ c∈C q → c d → c ∑ c ′ ∈C d → c′ log D j j DF c ð Þ</formula><p>• KL-Divergence: Many recent text retrieval systems use relevance functions based on the theory of language modeling. In order to be able to apply these approaches to our setting, we define the conditional probability of an article given a document as follows:</p><formula xml:id="formula_25">P c ð jdÞ :¼ d → c ∑ c ′ ∈C d → c′</formula><p>This definition of the conditional probability originates from the Bag-of-Words model and is inspired by Zhai and Lafferty <ref type="bibr" target="#b10">[11]</ref>, where it is also described how these probabilities can be used to define a ranking function based on the Kullback-Leibler divergence <ref type="bibr" target="#b11">[12]</ref>, which measures the difference between the query and the document model. Transferred to our model this results in the following retrieval function:</p><formula xml:id="formula_26">REL KL q; d ð Þ¼-D KL q ð jdÞ≅-∑ c∈C P c ð jqÞlogP c ð jdÞ</formula><p>• LM: An alternative approach is to use the conditional probability P(q|d) as relevance function. This distribution can be converted using the conditional distributions of documents given concepts, Bayes law and the a priori probability of concepts</p><formula xml:id="formula_27">P c ð Þ ¼ DF c ð Þ D j j : REL LM q; d ð Þ¼P q ð jdÞ ¼ ∑ c∈C P q ð jcÞP c ð jdÞ ≅ ∑ c∈C P c ð jqÞ P c ð Þ P c ð jdÞ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Concept spaces</head><p>Besides the design choices and alternatives discussed above, a crucial question is how to define the concept space with respect to which documents will be indexed. In the original ESA model applied to Wikipedia, concepts correspond to single articles. For CL-ESA, we extended concept definitions by introducing interlingual concepts having language-specific text signatures (see Section 3). These text signatures are defined by articles.</p><p>In the following, we propose two new approaches to define concepts based on Wikipedia: Cat-ESA and Tree-ESA. They are novel extensions of CL-ESA inspired by the model introduced by Liberman et al. <ref type="bibr" target="#b12">[13]</ref>. However, their model has not been applied to IR nor CLIR/MLIR so far. Cat-ESA relies on categories of Wikipedia to define concepts. In this model, only links assigning articles to categories are considered, while relations between categories are not used. Finally, Tree-ESA uses sub-category relations to propagate textual descriptions of concepts along the category hierarchy. Fig. <ref type="figure" target="#fig_7">6</ref> contains examples for concept vectors based on different definitions of concept space. Again, articles and categories presented in Fig. <ref type="figure" target="#fig_0">1</ref> are used in this example. The ESA model defines concepts by articles, e.g. Automobile. Using Cat-ESA, concepts correspond to categories which abstract from single articles. The text of each article in the signature is also used to compute the association strength, but as the text signature consists of more articles compared to using single articles as concepts, the single article has a smaller overall weight. Finally, Tree-ESA exploits the sub-category structure of Wikipedia and considers all articles along the category tree to define the text signatures.</p><p>The intuition behind these concept models is that they become more and more language-independent the more concept descriptions abstract from single articles. Therefore, indexing documents with respect to Wikipedia categories instead of with respect to Wikipedia articles might be a good choice for cross-lingual retrieval. Missing language links between articles or existing language links between articles describing different concepts may have a significant influence on the performance of CL-ESA. When using Cat-ESA with many articles in each category, these problems will surely have a smaller impact. In Tree-ESA, descriptions of categories are even bigger as they also contain subcategories. Our hypothesis is that the category-based representations used in Cat-ESA and Tree-ESA are better candidates for MLIR document models as the category structure is more stable across languages compared to the structure of articles. Our results indeed support this conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Category explicit semantic analysis</head><p>Category ESA (Cat-ESA) is based on a set of categories Γ ={γ 1 ,…, γ o }. We define the function MEMBERS : Γ → 2 C that maps category γ to all articles assigned to the category.</p><p>Instantiated for Wikipedia as in our case, the categories Γ correspond to interlingual categories IC W ð Þ as defined in Section 3. The category membership function MEMBERS is then essentially defined by category links. These links are part of Wikipedia and assign articles to categories in a specific language l: CATLINK l A W l ð ÞÂC W l ð Þ. Using equivalence classes of articles and categories as defined above, these links can be generalized to interlingual articles and categories. More details about mining these links from Wikipedia will be presented in Section 5. As articles may contain more than one category link, the sets of interlingual articles of categories may not be disjoint.</p><p>In contrast to CL-ESA, the concept space of Cat-ESA is then spanned by Γ and not by C. The text signature τ of category γ is defined as the union of text signatures of all interlingual articles that are linked to one of the categories in the interlingual category.</p><formula xml:id="formula_28">τ Cat-ESA γ; l ð Þ :¼ ∪ c∈MEMBERS γ ð Þ τ CL-ESA c; l ð Þ</formula><p>When computing term statistics, this union is equivalent to the concatenation of the content of the articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Tree category explicit semantic analysis</head><p>For Tree Category ESA (Tree-ESA), we assume that the categories as described for Cat-ESA are part of a tree structure with a single root category γ r . This is inspired by the category structure found in Wikipedia. To some extend the categories and category links in Wikipedia form a tree structure that is rooted in a dedicated category, the main topic classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>en</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multext JRC-ACQUIS</head><p>The given tree structure on categories defines the sub-category relation SUB p Γ × Γ. Based on this sub-category relation we define the function TREE : Γ → 2 Γ that maps a category γ to the set ofcategories that build the subtree rooted in γ:</p><formula xml:id="formula_29">TREE γ ð Þ :¼ γ∪ ∪ γ ′ j γ;γ ′ ð Þ∈SUB f g TREE γ ′</formula><p>As all categories are part of a tree structure without cycles, this recursion stops at a leaf category node, i.e.</p><formula xml:id="formula_30">TREE γ ð Þ :¼ γ if γ is a leaf</formula><p>The association strength of document d to category γ is then not only based on concepts in γ but also on the concepts of all subcategories. This results in the following definition of the text signature function τ:</p><formula xml:id="formula_31">τ Tree-ESA γ; l ð Þ :¼ ∪ γ ′ ∈TREE γ ð Þ τ Cat-ESA γ ′ ; l</formula><p>As the category link graph in Wikipedia is not a well-formed tree and contains cycles, we use a breadth-first algorithm that explores the graph starting from the root category. All links introducing cycles are ignored in the exploration, thus resulting in a tree-shaped category structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Summary of concept spaces</head><p>We introduced different variants of CL-ESA that are based on different concept spaces. These models differ in the definition of the text signature of concepts. In our experiments we will compare the different alternatives for i) defining concepts spaces, ii) computing the concept vectors and iii) calculating the relevance of a document given a query in conceptual space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In order to evaluate the presented multilingual retrieval model based on concept representations defined by Wikipedia, we present two different sets of experiments. Each set of experiments emphasizes different aspects and evaluates specific design choices:</p><p>• ESA Model Variants: Evaluation of variation in the CL-ESA model on a CLIR scenario. The goal is to identify best choices and parameter settings. • Concept Spaces: Evaluation of CL-ESA based on different concept spaces, using articles, categories and trees of categories. The goal is to identify appropriate concept spaces for CLIR and for MLIR.</p><p>For all experiments we build on Wikipedia databases in English, German, French and Spanish as background knowledge. Experiments were performed on two established parallel corpora: Multext 8 and JRC-Acquis. 9 In the following, we present more details about these datasets and also introduce the mate-retrieval task together with the appropriate evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Wikipedia</head><p>For the experiments analysing ESA implementation variants we used Wikipedia database dumps 10 from June 2008 in the languages English, German and French. As we rely on the language links as a basis to define interlingual concepts, we select only such articles that are linked across all three languages. Altogether, we used 166,484 articles in every language. 8 http://aune.lpl.univ-aix.fr/projects/MULTEXT/. 9 http://langtech.jrc.it/JRC-Acquis.html. 10   For the experiments on concept spaces, we used Wikipedia database dumps from September 2009 in the languages English, German, French and Spanish. In order to extract interlingual articles as described in Section 3, we filtered out articles having less than 500 characters. Then, we selected interlingual articles (categories) having associated articles (categories) in three or more languages. This resulted in 358,519 interlingual articles and 35,628 interlingual categories; 94% of the interlingual articles and 95% of the interlingual categories are thereby linked to exactly one article (category) in each language. This shows that the Wikipedia databases are highly consistent across languages and shows the potential of Wikipedia to be used as multilingual knowledge resource and in particular to define universal (in our case called interlingual) concepts.</p><p>In each Wikipedia language version, articles are assigned to categories by category links. We use these links to define the relation CATLINK l p ARTLS l ×CATS l in Wikipedia W l . For Cat-ESA and Tree-ESA, we need to generalize the language-specific links to links between interlingual articles and categories, defining the relation CATLINKpIA W ð ÞÂIC W ð Þ. Therefore, we define the support σ of a category link (α, γ) ∈ CATLINK as follows:</p><formula xml:id="formula_32">σ α; γ ð Þ:¼ j a; c ð Þ∈CATLINK l f j l∈L; a∈ARTLS l α ð Þ; c∈CATS l γ ð Þgj</formula><p>We use a support threshold of 2 to select interlingual category links: (α, γ) ∈ CATLINK if σ(α, γ) ≥ 2. This results in 605,672 links between articles and categories. We apply the same definition of support and the same threshold for links between interlingual categories, resulting in 58,837 links.</p><p>As root for the category tree used in Tree-ESA, we selected the interlingual category associated with the top category in the different language-specific Wikipedias; 86% of the interlingual categories extracted before are connected to this root by category links and are therefore part of the category tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Parallel corpora</head><p>As datasets for the evaluation we use two parallel corpora: Multext, 11 consisting of 3152 question/answer pairs from the Official Journal of European Community (JOC) and JRCAcquis, 12 consisting of 7745 legislative documents of the European Union. Both corpora contain manual translations of each document in English, German, French and Spanish. Both corpora were prepared using common IR-like preprocessing steps including elimination of stopwords, special characters and extremely short terms (length b 3) and stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methodology of experiments and measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Mate retrieval</head><p>The mate retrieval task assumes the availability of a parallel corpus that can be used to evaluate cross-language information retrieval approaches. It essentially consists in the task of, given a certain document as query, retrieving the parallel document -its so called "mate" -as best match. Per definition, the single mate in each language is thus the only relevant document for each language. Usually mate retrieval is used to evaluate CLIR approaches in which the language of the retrieval collection is homogeneous.</p><p>We also extended Mate Retrieval to MLIR by i) considering for each query the translations into four languages and ii) adding the translated versions of all documents in each language to the corpus, including all mate documents of the queries. In our case there exist thus four relevant documents for each query, i.e. the four equivalent articles or "mates". This is clearly an MLIR setting as the retrieval corpus contains documents in all languages and the four mates need to be retrieved to yield a recall of 100%. Retrieving only the mate in the query language will give us a recall of 25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Evaluation measures</head><p>We evaluate the retrieval performance with respect to a gold standard in which the mate(s) for each document are given. If d is the mate of d in some language, then relevant (d, d ) = 1. We use standard information retrieval to evaluate this performance. In particular, given a set of queries Q and the above mentioned relevance judgements, we use the following evaluation measures: 11 http://aune.lpl.univ-aix.fr/projects/MULTEXT/. 12 http://langtech.jrc.it/JRC-Acquis.html. Multext JRC-ACQUIS en-de en-fr de-fr en-de en-fr de-fr Fig. <ref type="figure">9</ref>. Variation of the association strength function ϕ l using the projection function Π abs 10,000 and cosine retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.1.">Recall at cutoff rank k (R@k).</head><p>Recall defines the number of relevant documents that are retrieved in relation to the total number of relevant documents. Recall at a specified cutoff rank k is defined by only considering the top k results.</p><p>In the CLIR setting, R@k defines the number of queries for which the mate document was found in the top k results. In the MLIR setting, it measures how many of all translations have been found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.2.">Mean Reciprocal Rank (MRR).</head><p>Applied to the CLIR setting MRR measures the average position of the mate documents. In contrast to R@k, MRR also takes into account the position of the mate document, resulting in higher values the higher the position of the mate in the ranked results list is. MRR is defined as follows:</p><formula xml:id="formula_33">MRR ¼ 1 Q j j ∑ q∈Q 1 min k d q;k relevant d q;k ; q ¼ 1 o n</formula><p>with d q, k as the retrieved document given query q at rank k and relevant(d, q) as the binary relevance function of document d given query q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.3.">Mean Average Precision (MAP).</head><p>MAP is another standard measure used in IR that is also sensitive to the rank of relevant documents. It averages precision measured at the rank of each relevant document. As in the case of MRR, this measure is inverse proportional to the position of the mate in the ranked retrieval list. MAP is defined as follows:</p><formula xml:id="formula_34">MAP ¼ 1 Q j j ∑ q∈Q ∑</formula><p>n k¼1 P@k⋅relevant d q;k ; q d∈D relevant d; q ð Þ¼1 j g f j with P@k as Precision at cutoff rank k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of ESA model variants</head><p>As described in Section 4, the ESA model allows different design choices. To find best parameters for CLIR, we performed experiments testing the influence on retrieval performance of different variants. Our experiments have been carried out in an iterative and greedy fashion in the sense that we start from the original ESA model as a baseline, then iteratively varying different parameters  .51 .46 6,11  .33 .31 5,11  and always fixing the best configuration before studying the next parameter. At the end of our experiments we will thus be able to assess the combined impact of the best choices on the performance of the ESA model. In summary the contributions of the experiments on ESA model variants are the following:</p><p>1. We identify best choices for the degrees of freedom of the ESA model on a CLIR scenario. 2. We show that the ESA model is sensitive to parameter settings, heavily influencing the retrieval results.</p><p>3. We show that the settings chosen in the original ESA model are reasonable, but can still be optimized for CLIR and MLIR settings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Experimental settings</head><p>Results of the evaluation of variants of the ESA model are presented in Figs. 7 to 10. As the observed effects were constant across measures, we only present recall at cutoff rank 1 (R@1). For experiments on the Multext corpus we used all documents (2,783) as queries to search in all documents in the other languages. The results for language pairs were averaged for both retrieval directions (e.g. using English documents as queries to search in the German documents and vice versa). For the JRC-Acquis dataset we randomly selected 3000 parallel documents as queries (to yield similar settings as in the Multext scenario) and the results were again averaged for language pairs. This task is harder compared to the experiments on the Multext corpus as the search space consists of 15,464 documents and is thus bigger by a factor of approximately 5. This explains the generally lower results on the JRC-Acquis dataset.</p><p>To prove the significance of the improvement of our best settings (projection function Π abs 10000 , association strength function TFICF*, cosine retrieval model) we carry out paired t-tests (confidence level 0.01) comparing the best settings pairwise with all other results for all language pairs on both datasets. Results where the differences are not significant with respect to all other variants at a confidence level of 0.01 are marked with "X" in Figs. 7 to 10.</p><p>In the following we discuss the results of the different variations of the CL-ESA model:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Projection function</head><p>We first used different values for the parameter m in the projection function Π abs m . The results in Fig. <ref type="figure">7</ref> showed that m = 10,000 is a good choice for both datasets.</p><p>On the basis of this result, we investigated different projection functions. In order to be able to compare them, we set the different threshold values t such that the projected ESA vectors had an average number of approx. 10,000 non-zero dimensions. An exception is the function sliding window (orig.) where we used the parameters described in <ref type="bibr" target="#b7">[8]</ref>: t = 0.05 and l = 100. Using an absolute number of non-zero dimensions yielded the best results (see Fig. <ref type="figure" target="#fig_8">8</ref>), the difference being indeed significant with respect to all other variants. Thus, we conclude that neither the settings of the original ESA approach (sliding window) nor in the model of Gurevych et al. (fixed threshold) are ideal in our experimental settings. For the remaining experiments we thus use the absolute dimension projection function with 10,000 articles (Π abs 10,000 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Association Strength</head><p>The results in Fig. <ref type="figure">9</ref> show that the functions TFICF (used in the original ESA model) and TFICF* perform much better compared to the other functions. The better performance of TFICF*, which ignores the term frequencies in the queries, was indeed significant w.r.t. all other alternatives for all language pairs considered on both datasets. We thus conclude that the settings in the original ESA model are reasonable, but, surprisingly, can be improved by ignoring the term frequency of the words in the document to be indexed. The low results using the TF function show that ICF is an important factor in the association strength function. Surprisingly, the normalization of the TFICF values (= cosine function) reduces the retrieval performance substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Retrieval model</head><p>Experiments with different retrieval models lead to the result that the cosine function, which is used by all ESA implementations known to us, constitutes indeed a reasonable choice. All other models perform worse (the difference being again significant for all language pairs on both datasets), which can be seen at the charts in Fig. <ref type="figure" target="#fig_10">10</ref>, especially on the JRC-Acquis dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5.">Discussion</head><p>Our results show on the one hand that ESA is indeed quite sensitive to certain parameters, in particular the association strength function and the retrieval model. Different choices for these parameters can have a large impact on the performance of the approach. For example, using TF c instead of RTF c (which is length normalized) values in the association strength function decreases performance by about 75%. Unexpectedly, abstracting from the number of times that a word appears in the query document (using TFICF*) improves upon the standard TFICF measure (which takes them into account) by 17% (Multext dataset, English/French) to 117% (JRC-Acquis dataset, German/French). We have in particular shown that all the settings that are ideal in our experiments result in statistically significant improvements of the retrieval performance compared to all other settings (with the exception of the number of dimensions taken into account).</p><p>On the other hand, while we can confirm by our experiments that the settings in the original ESA model (Π window 0.05,100</p><p>, TFICF, cosine) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> are reasonable, it is also the case that with the settings which according to our experiments are ideal on both datasets (Π abs 10,000 , TFICF*, cosine) we achieve a relative improvement in TOP-1 accuracy between 62% (from .51 to .83, Multext dataset, English/French) and 237% (from .09 to .31, JRC-Acquis dataset, English/German), which shows again that the settings can have a substantial effect on the ESA model and that ESA shows the potential to be further optimized and yield even better results on the various tasks it has been applied to.</p><p>Finally, all experiments including the German datasets have worse results compared to the English/French experiments. This is likely due to the frequency of specific German compounds in the datasets, leading to a vocabulary mismatch between documents and Wikipedia articles. However, an examination of this remains for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Concept spaces for multilingual scenarios</head><p>While CL-ESA model variants were evaluated on a CLIR scenario, we also tested its performance on multilingual settings. This is an inherently more difficult problem as documents of different languages compete in the retrieval process. We particularly focus on different concept spaces that are used by CL-ESA, Cat-ESA and Tree-ESA.</p><p>In MLIR settings, avoiding the language biases introduced in Section 2 is a challenging problem. This is the main reason why MLIR is inherently more difficult than CLIR, where the collection in which relevant documents are searched for is monolingual (but the language differs from the language of the query).</p><p>One of the goals in the following experiments is to show that cross-lingual ESA is not strongly affected by language bias. In summary, the contributions of the experiments on multilingual scenarios are the following:</p><p>1. We present and analyze the performance of different CL-ESA models on a MLIR task.</p><p>2. We show that CL-ESA using concept spaces based on Wikipedia categories improves performance on MLIR settings compared to concept spaces based on articles. Our results show that there is indeed a gain in MAP between 18% (JRC-Acquis dataset, Cat-ESA) to 39% (Multext dataset, Tree-ESA). 3. We show that CL-ESA applied to MLIR is not strongly affected by language bias, considering both facets defined by type I and II. 4. We argue that CLIR and MLIR are quite different problems for which different retrieval approaches need to be applied. In particular, we show how the performance in CLIR and MLIR tasks varies substantially for the same parametric choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Experimental settings</head><p>To define queries for our experiments, we randomly selected 50 documents each from the Multext and from the JRC-Acquis dataset. We then used all translations of these documents as queries, resulting in 200 multilingual queries for each dataset. As evaluation measures we used Mean Average Precision (MAP) and Recall at cut-off level of 10 (R@10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Document models</head><p>The results on the multilingual mate retrieval using the different CL-ESA abstractions and retrieval models are presented in Table <ref type="table" target="#tab_4">1</ref>, which shows R@10 and MAP values for the different configurations under analysis. The results clearly show that the models relying on categories as concepts instead of articles are indeed superior, with MAP improvements of 70% (Multext) and 32% (JRC-Acquis) to the BoW model baseline and 39% (Multext) and 18% (JRC-Acquis) to CL-ESA.</p><p>The interlingual document representation given by Cat-ESA and Tree-ESA is therefore more suited for MLIR than any of the other models considered. Our hypothesis that category-based concept models smooth the structural and content-based differences of Wikipedia databases in different languages is a possible explanation for this result. Interestingly, however, we did not observe any significant difference between Cat-ESA and Tree-ESA.</p><p>When considering the different retrieval models, the results vary significantly. For all category-based models we achieve a high performance gain when using TFICF 2 instead of TFICF on both datasets (MAP improvements between 11% for Tree-ESA on the JRC-Acquis dataset to 27% for Tree-ESA on the Multext dataset). About the same improvement is observed again when using TFICF 3 instead of TFICF 2 (MAP improvements between 10% for Cat-ESA on the JRC-Acquis dataset to 11% for Cat-ESA on the Multext dataset). All these improvements using the different retrieval models have been tested to be statistically significant at confidence level .001 using a paired t-test. Giving more weight to non-frequent terms is therefore beneficial for multilingual mate retrieval. A possible explanation is that reducing the weight of frequent terms also reduces the language bias, as this bias might be induced by the different distributions of these terms in each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Language Bias</head><p>The fact that the R@10 values are close to .50 for Cat-ESA and Tree-ESA shows that to a query in a certain language the system is retrieving about two mates in the top ten results on average. This shows that Cat-ESA and Tree-ESA can deal with the language bias reasonably well. The fact that the performance is comparable across languages and not biased to any specific languages can be appreciated in Fig. <ref type="figure" target="#fig_0">11</ref>, which plots MAP results for English, German, French and Spanish queries on the Multext dataset using Tree-ESA with TFICF 3 . The X-axis consists of the 50 queries, each having a translation into all languages. On the Y-axis the difference of MAP to the average MAP of all queries in each language is plotted. This difference in MAP for each query language is presented as an (additive) single bar. The results show that the performance of our approach does not vary across languages, as differences to average MAP are mostly consistent when using the same query in English, German, French or Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4.">CLIR vs. MLIR</head><p>In Section 2 we introduced MLIR as an inherently more difficult problem than CLIR. To support this claim we have additionally performed CLIR experiments on the Multext dataset. In this case the target document collection consists only of documents in one language. Experiments using Cat-ESA with the TFICF 2 model on English documents resulted in high MAP values ranging from .72 to .85 using German, French and Spanish queries. This shows that our model is also applicable to CLIR but also that CLIR is indeed a much easier problem.</p><p>The results for CLIR presented above are based on an optimal selection of parameters. Interestingly, we appreciate that the best performing configuration is different from the best performing configuration for MLIR. To illustrate this effect we set the relative performance for both best performing configurations on the Multext dataset, on the one hand for CLIR and on the other hand for MLIR, to 100% in Fig. <ref type="figure" target="#fig_1">12</ref>. The performance of all other configurations is shown relatively to the best configuration. The results in Fig. <ref type="figure" target="#fig_1">12</ref> show that while using cubic ICF values always improves results for MLIR, this is definitely not the case for CLIR. Using Cat-ESA and Tree-ESA, we observe a significant drop in MAP compared to the quadratic ICF model. This effect is also confirmed using Tree-ESA for CLIR on the JRC-Acquis dataset. This shows that model and parameter selection has to be adjusted to the specific settings, i.e. CLIR or MLIR. As these are different problems, good approaches to solve one setting might not be optimal on the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Methods for CLIR/MLIR</head><p>Among the most prominent approaches to CLIR are translation-based techniques on the one hand and concept-based on the other. Translation-based techniques come in two different modes: either the query is translated into the target language, typically by using bilingual dictionaries (see <ref type="bibr" target="#b13">[14]</ref>), or the documents are translated into the query language by using some full-fledged translation system. Systems translating queries have been recently shown to be very successful in CLEF campaign evaluations (see <ref type="bibr" target="#b14">[15]</ref>) on different datasets.</p><p>The second class of approaches are concept-based as motivated in Section 1. We can distinguish between implicit and explicit concept models. Implicit models need a training corpus from which to extract relevant concepts. If such concepts should be valid for different languages, the training has to be performed on a language-aligned document collection. Prominent instantiations of the implicit concept model are Latent Semantic Indexing (LSI) <ref type="bibr" target="#b15">[16]</ref> or Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b16">[17]</ref>. Both approaches perform dimension reduction on the term-document matrix of a corpus. The reduced dimensions correspond to implicit concepts that are used to index new documents. Both LSI and LDA have been applied to cross-lingual retrieval tasks (see <ref type="bibr" target="#b17">[18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">External vs. intrinsic concept definitions</head><p>Cimiano et al. <ref type="bibr" target="#b18">[19]</ref> have compared latent models of concepts (LSI and LDA in particular) to CL-ESA using the same mate retrieval scenario as presented in this paper. The results show that CL-ESA achieves comparable results to intrinsic models that are trained on the dataset, using a test/train split. However, training of intrinsic models on Wikipedia results in low retrieval performance. CL-ESA is therefore able to exploit Wikipedia as background knowledge more effectively. Thus, it can be claimed that CL-ESA clearly outperforms LSI/LDA unless the latter are trained on the document collection and not on Wikipedia as a generic, domain-independent resource. The availability of aligned corpora is a serious restriction, so that CL-ESA is clearly the preferred model here as it delivers reasonable results requiring no data aligned across languages besides Wikipedia. A further crucial advantage is its excellent scalability: CL-ESA does not require comprehensive computations with nonlinear space/time behavior, which is, however, given when building both the LSI and the LDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Using background knowledge for IR</head><p>Gonzalo et al. <ref type="bibr" target="#b19">[20]</ref> present an approach to improve IR using background knowledge. They use WordNet as language resource to map terms to synsets. Similarly to (CL-) ESA, term vectors are therefore mapped to vectors in the space of synsets. They achieve improvements of up to 29% in retrieval performance for monolingual IR.</p><p>Schönhofen et al. <ref type="bibr" target="#b20">[21]</ref> use Wikipedia as knowledge source for CLIR. Queries are translated using bi-lingual dictionaries that are extended by titles of Wikipedia articles in different languages. They also exploit the Wikipedia hyperlink structure for query term disambiguation after translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Explicit semantic analysis</head><p>We have mentioned already different approaches for folding in "semantics" (meaning very different things depending on the approach in question) into IR tasks (see Section 1). We have examined in particular the ESA model in this paper, which has gained substantial attention in recent years <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref> since it was originally published in 2007 <ref type="bibr" target="#b1">[2]</ref> and partially already (not under this name) in 2005 <ref type="bibr" target="#b24">[25]</ref>. The original application of the ESA model was the computation of semantic relatedness between words. In fact, Gabrilovich and Markovitch showed that the ESA model outperforms Bag-of-Word and Latent Semantic Indexing approaches on this task. Similar to and inspired by ESA, Hassan and Mihalcea presented an approach that further improves the performance of computing semantic relatedness between word pairs <ref type="bibr" target="#b25">[26]</ref>. ESA has been also exploited in text classification approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> where it has been already shown that an appropriate dimension selection function has a significant influence on the performance of the ESA model.</p><p>ESA has been also applied with reasonable success to IR settings <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>, in particular to CLIR settings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Theoretical insights into the ESA model are presented by Gottron et al. <ref type="bibr" target="#b30">[31]</ref>. They developed a general probablistic model for term weights that explains some of the phenomena observed in previous work related to ESA.</p><p>Knoth et al. presented an approach to use cross-lingual ESA for cross-lingual link discovery <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented an approach called Cross-Language Explicit Semantic Analysis (CL-ESA) that is able to exploit Wikipedias in different languages for cross-lingual and multilingual Information Retrieval. This model abstracts from the Bag-of-Words model that uses term vectors to represent documents by mapping documents to concept vectors. We showed how statistical measures and retrieval models typically applied to the Bag-of-Words model can be generalized to the Bag-of-Concepts model. As concepts areassumed to exist universally across languages, the proposed models can be directly applied to cross-lingual or multilingual scenarios.</p><p>We presented and experimentally analyzed different strategies for exploiting the Wikipedia structure to define concept spaces. In particular, we have introduced, in addition to the standard approach of using articles as concepts, the two novel approaches Cat-ESA and Tree-ESA. The results of our experiments in a mate retrieval task involving two datasets (JRC-Acquis and Multext) have indeed shown that CL-ESA delivers better approaches when using more abstract concepts and larger text signatures, i.e. Cat-ESA and Tree-ESA. Further, we have also shown that CL-ESA is quite sensitive to certain parameters. We have presented various alternatives for the different design choices that CL-ESA leaves open and experimentally compared these design choices, in particular retrieval models, term weighting strategies and pruning functions. We have in particular shown that while the settings for the original ESA model deliver quite good results, the parameters can still be optimized for CLIR / MLIR settings. We also have shown that the optimal settings even depend on whether we are considering a CLIR or MLIR task.</p><p>Finally, as a further contribution to the community, we have released a reference implementation of CL-ESA that is available as open source. <ref type="foot" target="#foot_8">13</ref></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Wikipedia articles, categories and link structure exploited for document representations in concept spaces (CL-ESA, Cat-ESA and Tree-Esa).</figDesc><graphic coords="4,112.61,185.11,311.47,107.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Association strength between the example text and the concept Bicycle, described by the corresponding Wikipedia article.</figDesc><graphic coords="6,157.97,535.45,132.19,136.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Examples for interlingual articles and interlingual categories. The connecting arrows represent cross-language links in Wikipedia. Interlingual articles/categories correspond to equivalence classes of articles which are based on the symmetric, reflexive and transitive closure of the cross-language link relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. The basic principle of CL-ESA: Queries and documents of different languages are mapped to the interlingual concept space using language specific concept descriptions. Relevance of documents to queries is then measured by using similarity measures defined in the concept space.</figDesc><graphic coords="7,75.89,586.57,397.15,76.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example for pruning of ESA vectors. Dimensions having the lowest association strength values are set to zero.</figDesc><graphic coords="8,151.97,51.07,235.51,85.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>thus restricting it to those values above a certain fraction of the highest-valued dimension. • Sliding Window: This function moves a window of fixed size l over the sorted values until the difference of the first and last value in the window falls below a threshold t. The projection cuts off all values after this position. This function was used in the original ESA model [8]. Formally, the projected vector Π window t, l (Φ(d)) is defined by restricting d → to the first i dimensions according to the order α d for which the following condition holds: d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>|C| denote the number of concepts, |τ l (c)| the number of tokens in the text signature τ l (c) of concept c. Let further TF d (w) (TF τ l (c) (w)) denote the term frequency of w in document d (text signature τ l (c)) and RTF τ l (c) (w) = TF τ l (c) (w)/|τ l (c)| represent relative term frequency. CF(w) is the number of concepts containing term w in their text signature. The inverse concept frequency is then defined as ICF w ð Þ ¼ log C j j CF w ð Þ . We can use the following functions here: • TFICF: The most widely used version of the TFIDF function applied to concepts:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ESA vectors based on different definitions of concept space. The original ESA model is based on articles. Concepts in Cat-ESA are defined by categories. The textual description of each category is based on the articles of the category. For Tree-ESA sub category relations are additionally used to define the textual descriptions.</figDesc><graphic coords="10,43.91,450.25,399.67,187.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Variation of the projection function Π using the TFICF* association function and cosine retrieval model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Variation of the retrieval model using Π abs 10,000 and TFICF*.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Differences in MAP to average MAP in each language for each topic. Retrieval using Tree-ESA with TFICF 3 model on the Multext dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>Results of mate retrieval experiments using queries and documents of all languages. Statistically significant differences according to paired t-test at confidence level .001 are marked with the ID of the compared result.</figDesc><table><row><cell>Multext</cell><cell>JRCAcquis</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Independent from our research the CL-ESA approach was developed and published by Potthast et al. in parallel<ref type="bibr" target="#b2">[3]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>P. Sorg, P. Cimiano / Data &amp; Knowledge Engineering 74 (2012) 26-45</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Words which are the same across languages are an exception here, in particular named entities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This structural error is present in the Wikipedia dump of September 2009 that was used in our experiments. In the current online version of Wikipedia, the missing category link was added. This is an example of how Wikipedia is constantly improved and errors are detected and solved by the community.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://www.wiktionary.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://www.dmoz.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Document ID: FXAC93006DEC.0012.02.00.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>TOP-1 Accuracy X X X X X X X X X X X X X X X X X X X X X X X X X X Fig.7. Variation of m in Π abs m using the TFICF* association function and cosine retrieval model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8"><p>http://code.google.com/p/research-esa/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the German Research Foundation (DFG) under the Multipla project (grant 38457858) and by the European Commission under the Monnet Project (grant FP7-ICT-4-248458).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The new Babel: language barriers on the world wide web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Large</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Universal Language</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipedia-based explicit semantic analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 20th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Wikipedia-based multilingual retrieval model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th European Conference on Information Retrieval, ECIR</title>
		<meeting>the 30th European Conference on Information Retrieval, ECIR<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A comparative study of query and document translation for cross-language information retrieval, Machine Translation and the Information Soup</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a unified approach to CLIR and multilingual IR</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cross-language Retrieval Workshop at the international Conference on Research and Development in Information Retrieval</title>
		<meeting>the Cross-language Retrieval Workshop at the international Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Internet encyclopaedias go head to head</title>
		<author>
			<persName><forename type="first">J</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="issue">7070</biblScope>
			<biblScope unit="page" from="900" to="901" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Using wikipedia and wiktionary in domain-specific information retrieval, Working Notes for the CLEF 2008 Workshop</title>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Feature generation for textual information retrieval using world knowledge</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Israel Institute of Technology, Haifa</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-lingual information retrieval with explicit semantic analysis, Working Notes for the CLEF 2008 Workshop</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 17th International Conference on Research and Development in Information Retrieval, SIGIR<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Information and Knowledge Management, CIKM, ACM</title>
		<meeting>the 10th International Conference on Information and Knowledge Management, CIKM, ACM<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measures of distributional similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual Meeting of the Association for Computational Linguistics, ACL, ACL</title>
		<meeting>the 37th annual Meeting of the Association for Computational Linguistics, ACL, ACL<address><addrLine>College Park, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compact hierarchical explicit semantic representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on User-Contributed Knowledge and Artificial Intelligence at the International Joint Conference on Artificial Intelligence</title>
		<meeting>the Workshop on User-Contributed Knowledge and Artificial Intelligence at the International Joint Conference on Artificial Intelligence<address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dictionary-based techniques for cross-language information retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="547" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
		<title level="m">CLEF 2009 Ad-Hoc TEL task: combining different retrieval models and addressing the multilinguality, Working Notes of the Annual CLEF Meeting</title>
		<meeting><address><addrLine>Corfu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic cross-language information retrieval using latent semantic indexing, Cross-Language Information Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explicit versus latent concept models for cross-Language information retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Joint Conference on Artificial Intelligence</title>
		<meeting>the 21st International Joint Conference on Artificial Intelligence<address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Indexing with WordNet synsets can improve Text Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cigarrán</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM Computing Research Repository</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cross-Language retrieval with wikipedia, Advances in Multilingual and Multimodal Information Retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schönhofen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benczúr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bíró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Csalogáy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Concept-based feature generation and selection for information retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Egozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 23 rd AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>23 rd AAAI Conference on Artificial Intelligence, AAAI<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1132" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What to be?-electronic career guidance based on semantic relatedness</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual Meeting of the Association for Computational Linguistics, ACL, ACL, Prague</title>
		<meeting>the 45th annual Meeting of the Association for Computational Linguistics, ACL, ACL, Prague</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1032" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An experimental comparison of explicit semantic analysis implementations for cross-language retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems, NLDB</title>
		<meeting>the 14th International Conference on Applications of Natural Language to Information Systems, NLDB<address><addrLine>Saarbrüken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="36" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature generation for text categorization using world knowledge</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Concerence on Artificial Intelligence (IJCAI)</title>
		<meeting>the 19th International Joint Concerence on Artificial Intelligence (IJCAI)<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1048" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic relatedness using salient semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text categorization with knowledge transfer from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23 rd AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the 23 rd AAAI Conference on Artificial Intelligence, AAAI<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="842" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-language plagiarism detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-9114-</idno>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross lingual text classification by mining multilingual topics from wikipedia</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM International Conference on Web Search and Data Mining, WSDM &apos;11</title>
		<meeting>the fourth ACM International Conference on Web Search and Data Mining, WSDM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Concept-based information retrieval using explicit semantic analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Egozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Insights into Explicit Semantic Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gottron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM &apos;11</title>
		<meeting>the 20th ACM international conference on Information and knowledge management, CIKM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1961" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Philipp Sorg received his PhD at the Karlsruhe Institute of Technology (KIT), Germany. He had a researcher position at the Institute of Applied Informatics and Formal Description Methods (AIFB). the Cognitive Interaction Technology Excellence Cluster</title>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zilka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Workshop on Cross Lingual Information Access: Computational Linguistics and the Information Need of Multilingual Societies (CLIA) at IJC-NLP 2011</title>
		<imprint>
			<date type="published" when="2008">2011. 2008-2009</date>
		</imprint>
	</monogr>
	<note>Philipp is mainly interested in topics related to knowledge representation and text processing, including text mining, natural language processing, computational semantics, information extraction and retrieval, question answering, ontology learning. Semantic Web, multilinguality, etc. He is editorial board member of the Semantic Web Journal</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
