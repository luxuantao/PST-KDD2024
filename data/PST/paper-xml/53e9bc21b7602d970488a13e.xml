<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated identification of animal species in camera trap images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-09-04">4 September 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyuan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangping</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Kays</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Smithsonian Tropical Research Institute (STRI)</orgName>
								<address>
									<settlement>Balboa</settlement>
									<country>Ancon Panama, Republic of Panama</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">North Carolina Museum of Natural Sciences</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Fisheries, Wildlife &amp; Conservation Program</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">A</forename><surname>Jansen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Smithsonian Tropical Research Institute (STRI)</orgName>
								<address>
									<settlement>Balboa</settlement>
									<country>Ancon Panama, Republic of Panama</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Environmental Sciences</orgName>
								<orgName type="institution">Wageningen University</orgName>
								<address>
									<settlement>Wageningen</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianjiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated identification of animal species in camera trap images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-09-04">4 September 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">C48BDD6ED2962F4FFB848CD5AB1E2899</idno>
					<note type="submission">Received: 1 February 2013 Accepted: 21 August 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Species identification</term>
					<term>SIFT</term>
					<term>cLBP</term>
					<term>Feature learning</term>
					<term>Max pooling</term>
					<term>Weighted sparse coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image sensors are increasingly being used in biodiversity monitoring, with each study generating many thousands or millions of pictures. Efficiently identifying the species captured by each image is a critical challenge for the advancement of this field. Here, we present an automated species identification method for wildlife pictures captured by remote camera traps. Our process starts with images that are cropped out of the background. We then use improved sparse coding spatial pyramid matching (ScSPM), which extracts dense SIFT descriptor and cell-structured LBP (cLBP) as the local features, that generates global feature via weighted sparse coding and max pooling using multi-scale pyramid kernel, and classifies the images by a linear support vector machine algorithm. Weighted sparse coding is used to enforce both sparsity and locality of encoding in feature space. We tested the method on a dataset with over 7,000 camera trap images of 18 species from two different field cites, and achieved an average classification accuracy of 82%. Our analysis demonstrates that the combination of SIFT and cLBP can serve as a useful technique for animal species recognition in real, complex scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monitoring biodiversity, especially the effects of climate and land-use change on wild populations, is a critical challenge for our society <ref type="bibr" target="#b0">[1]</ref>. Sensor networks are a promising approach for collecting the spatio-temporal data at scales needed to address this challenge <ref type="bibr" target="#b1">[2]</ref>, especially visual sensors that record images of animals that move across their field of view (i.e. camera traps <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>). However, processing the large volumes of images that such studies generate to identify the species of animals recorded remains a challenge.</p><p>At present, all camera-based studies of wildlife use a manual approach where researchers examine each photograph to identify the species in the frame. For studies collecting many tens or hundreds of thousands of photographs, this is a daunting task <ref type="bibr" target="#b4">[5]</ref>.</p><p>Computer-assisted species recognition on camera-trap images could make this work flow more efficient, and reduce, if not remove, the amount of manual work *Correspondence: jwang63@illinois.edu 2 Beckman Institute, University of Illinois at Urbana-Champaign, Urbana, IL, USA Full list of author information is available at the end of the article involved in the process. However, in comparison with the typical video from surveillance of building and street views, camera trap of animals amidst vegetation are more difficult to incorporate into image analysis routines because of low frame rates, background clutter, poor illumination, serious occlusion, and complex pose of the animals.</p><p>Inspired by recent object recognition works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> in the computer vision community, we improved sparse coding spatial pyramid matching (ScSPM) method for species recognition on images collected by camera traps. During the local feature extraction, we combined dense scaleinvariant feature transform (SIFT) <ref type="bibr" target="#b7">[8]</ref> of features with cell structured local binary patterns (cLBP) <ref type="bibr" target="#b8">[9]</ref> to represent the object of interest. We apply weighted sparse coding for dictionary learning, and thus enforce both sparsity and locality, since locality may be more important than sparsity, as suggested by Wang et al. <ref type="bibr" target="#b6">[7]</ref>. Then we used linear SVM to classify image of species.</p><p>We tested our method with images collected by camera traps that were deployed in two different environments, tropical rainforest and temperate forest, that represent a wide variety of backgrounds and conditions. From this http://jivp.eurasipjournals.com/content/2013/1/52 collection, we selected sequences and species to keep the data balanced. Then, we manually cropped animals from all the frames to generate a dataset with 7, 196 images over 18 different vertebrate species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most related works are camera-based studies of wildlife that use image analysis to identify individual animals of select species with unique coat patterns (e.g., spots or stripes). Bolger et al. <ref type="bibr" target="#b9">[10]</ref> applied software to help identify individual animals based on coat patterns for subsequent photographic mark-recapture analysis. The data they used was image based, which is a cost-effective, non-invasive way to study population. The method they used was the SIFT key points extraction and matching. Thus, they only focused on individual animal identification for these strongly marked texture species.</p><p>Identifying species from remote camera images remains a major challenge that has not been addressed. In the community of computer vision, there exist a lot of methods to recognize general object. One of the most successful ones is Yang's work <ref type="bibr" target="#b5">[6]</ref>, in which ScSPM is applied. Spatial pyramid matching (SPM) with max pooling <ref type="bibr" target="#b10">[11]</ref> can not only model the spatial layout of local image features, but also achieve translation invariance of animal body. As being easy and simple to construct, the SPM kernel turns out to be highly effective in practice <ref type="bibr" target="#b11">[12]</ref>. Sparse coding has been successfully applied to model local features, and to construct overcomplete dictionary that can sparsely represent the local features. Sparse coding can yield better results than vector quantization and hard assignment <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and methods</head><p>Our pattern extraction and classification program is based on the ScSPM <ref type="bibr" target="#b5">[6]</ref>, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The algorithm first extracts local feature descriptor densely. We combine two kinds of local descriptors: SIFT and cLBP. In order to sparsely represent local features, the dictionary is learned via weighted sparse coding, for each kind of descriptor feature. Similar local features can generate similar codes after sparse coding on the dictionary, which is essential for recognition because it retains discriminative information while suppressing the noise. Finally, max pooling using SPM is used to construct the global image feature that converts an image or a bounding box to a single vector. We then apply linear multi-class SVMs to classify the global feature to one category of species, assuming SVMs are trained beforehand using training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local feature extraction</head><p>The camera-trap images contain rich noise and clutter. This requires us to develop a both discriminant and invariant local feature to describe local image patches. Dense SIFT feature, also known as dense histogram of oriented gradients, is successfully used in some recognition work. SIFT descriptor is invariant to moderate scaling and shifting change of edges and linear illuminance variation in image patch; however, it fails when nonlinear illuminance change occurs. cLBP, in contrast, is the perfect local texture descriptor that is invariant to moderate nonlinear illuminance variation. In the area of computer vision, for human detection <ref type="bibr" target="#b12">[13]</ref>, HOG and cLBP features are concatenated to obtain the final feature. But the simple concatenation would potentially cause the following problem: the feature space becomes more complex and more difficult to classify. We thus used the procedure of Zhang et al. <ref type="bibr" target="#b13">[14]</ref> to extract HOG and cLBP, and concatenate responses only after coding them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>The SIFT descriptor is similar to the HOG. Both are histograms of oriented gradients. The SIFT descriptor is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. After calculating the gradient map for each image, SIFT creates oriented gradient histograms for 4 × 4 grid regions, instead of 2 × 2 as in HOG. The full 128 dimensional SIFT descriptor is created by concatenating the 16 histograms in 16 × 16 image patch.</p><p>cLBP is a very good texture descriptor that extracts histogram of the LBP patterns from local cells, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In order to filter out noises, LBP is modified into a uniform LBP pattern <ref type="bibr" target="#b14">[15]</ref>. We use the notation LBP u n , r to denote LBP feature that takes n sample point with radius r, and the number of 0-to-1 transitions is no more than u. The pattern that satisfies this constraint is called uniform pattern <ref type="bibr" target="#b14">[15]</ref>. For example, the pattern 0010010 is a nonuniform pattern for LBP 2 , and is a uniform pattern for LBP 4 because LBP 4 allows four 0-to-1 transitions. In our approach, we set u = 2, n = 8, and r = 1. In this setting, the dimension of LBP is 59.</p><p>The rationale for combination of SIFT and cLBP is that at pixel level, the oriented gradient has been assigned to 8 bins in SIFT, while in uniform LBP 2  8 , 1 the number of bins is 59. At cell level, 16 cells are used in SIFT while only 1 cell is used in cLBP. So SIFT is very accurate at the cell level but invariant at the pixel level, while the opposite holds for cLBP. The combination of the two solves the trade-off between discrimination and invariance, at both the pixel and the cell level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dictionary learning and weighted sparse coding</head><p>The goal of dictionary learning is to capture high-level information, that is, to select some items to describe the distribution of the input space. We get a local image feature set X by randomly sampling in feature space. Then X approximates the distribution of the input space. But X contains a huge number of signals, which make it impossible to use X directly in coding. Dictionary learning aims to generate a compact dictionary that can sparsely represent the incoming signal with minimum error.</p><p>Let X be in a D-dimensional features space, i.e.</p><formula xml:id="formula_0">X = [ x 1 , • • • , x N ] ∈ R D×N . The dictionary is V = [v 1 , • • • , v K ] ∈ R D×K with K atoms.</formula><p>The traditional dictionary leaning and sparse coding method formulate the problem as follows: min</p><formula xml:id="formula_1">V ,U X -V U 2 + λ U 1 s.t. v k ≤ 1, ∀k = 1, 2, • • • , K,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">U = [u 1 , • • • , u N ] ∈ R K×N is the matrix of sparse codes.</formula><p>Inspired by the work of Wang et al. <ref type="bibr" target="#b6">[7]</ref> in which encoding of features is based on the locality in the feature space, we adapt the original sparse coding to the weighted sparse coding as follows to enforce both sparsity and locality: min</p><formula xml:id="formula_3">V ,U X -V U 2 + λ W U 1 s.t. v k ≤ 1, ∀k = 1, 2, • • • , K, (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where W is a diagonal weighting matrix whose elements are computed as</p><formula xml:id="formula_5">W i (k, k) = X i -V k 2 , k = 1, 2, • • • , K.</formula><p>(</p><p>Many algorithms have been proposed to solve this dictionary learning problem, e.g., <ref type="bibr" target="#b15">[16]</ref>. V is well known as a codebook and can be trained and fixed in the testing phase. Recently, there has been a lot of work on supervised dictionary learning (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>) to adapt the dictionary for classification purpose, but it is often computationally expensive and cannot handle large multi-class problem well. Thus, our work employs unsupervised dictionary learning using weighted sparse coding, as in Equation <ref type="formula" target="#formula_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linear SPM and multi-scale max pooling</head><p>Spatial pyramid matching is an extension of Bag of Words (BoW) method, and it models the spatial layout of local image features at multiple scales. Figure <ref type="figure" target="#fig_0">1</ref>     whole structure of ScSPM. Let U be the matrix of sparse codes of applying Equation 2 to a descriptor set X, assuming the codebook V is pre-computed. The pooled features from various locations and scales are then concatenated to form a spatial pyramid representation of the image. In each pyramid, a max pooling function is applied on the absolute sparse codes:</p><formula xml:id="formula_7">z j = max{|u j1 |, |u j2 |, • • • , |u jM |}<label>(4)</label></formula><p>where z j is the jth element of z, u ji is the matrix element at jth row and ith column of U. Max pooling is beneficial for translation invariance because the maximum response will be filtered out if it is a small translation. Let image I i be represented by z i , a simple linear SPM kernel is defined by <ref type="bibr" target="#b5">[6]</ref> </p><formula xml:id="formula_8">κ(z i , z j ) = z T i z j (5)</formula><p>With linear SPM kernel, we can directly use linear SVM, for which the training cost is O(n) in computation, and the testing cost for each image depends on the dimension of feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-class linear SVM</head><p>Let {(z i , y i )} n i=1 , y i ∈ Y = {1, 2, • • • , L} be the training data. We stick to the implementation in Yang et al. <ref type="bibr" target="#b5">[6]</ref>, and use one-against-all strategy to train L binary linear SVMs that each solve the following unconstrained convex optimization problem:  where y c i = 1 if y i = c, otherwise y c i = -1, and l(w c ; y c i , z i ) is the hinge loss function. The standard hinge loss function is not differentiable everywhere, but here we can use quadratic hinge loss as below instead to make use of gradient-based optimization methods, e.g., LBFGS <ref type="bibr" target="#b5">[6]</ref>.</p><formula xml:id="formula_9">l(w c ; y c i , z i ) = [ max(0, 1 -w T c z • y c i )] 2</formula><p>4 Experimental results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data set</head><p>We used images of wildlife captured with motionsensitive camera traps (Reconyx RC55, PC800 and HC500, Holmen, WI, USA), which generate sequences of 3.1 Megapixel JPEG images at about 1 frame/s upon triggering by an infrared motion sensor. Color images are captured during the day and gray-scale images are captured at night using and an infrared flash, which is invisible to most animals. We used images from tropical rain forest (Barro Colorado Island, Panama) and temperate forest and heathland (Hoge Veluwe National Park, the Netherlands). Expert zoologists identified the animals in the images. We did not edit the data set for ease of identification, so it includes many of the typical challenges faced by camera trapping data, including cases where the animal is too small or is occluded by vegetation. http://jivp.eurasipjournals.com/content/2013/1/52</p><p>Figure <ref type="figure">4</ref> The cropped sample images. Each row contains a species. From top to bottom, they are the agouti, collared peccary, paca, red brocket deer, white-nosed coati, spiny rat, and ocelot. Each sample image has their own scale, aspect ratio, and pose.</p><p>http://jivp.eurasipjournals.com/content/2013/1/52 In total, we got 10, 598 sequences over 57 species. The numbers of sequences of each species were unbalanced. As shown in Table <ref type="table" target="#tab_0">1</ref>, 40 out of 57 species have less than 50 sequences. we exclude these species and remain top 18 species. In order to build a balanced test data set, we chose up to 100 sequences from each species. Where the available number of sequences for a species was less than 100, we choose all of the sequences for that species. After such operation, 1, 739 sequences for 18 species remained. Table <ref type="table" target="#tab_0">1</ref> lists the number of remained sequences and frames for each species.</p><p>The camera trapped sequences are of low frame rate (1 frame/s) and short length (about 10 frames/sequence). Two typical image sequences are shown in Figure <ref type="figure" target="#fig_3">3</ref>. The first two rows show consecutive frames of the agouti, in which the leaves dangled in the wind. The second two rows are continual frames of the collared peccary. If the peccary suddenly moved close to the camera, the illumination changes a lot because it cut out much of the light. The common motion detection method cannot handle this case very well. In order to get clear data, we manually cropped all the animals from the sequences. Since most of them are empty frames, in which the cameras are activated by motion from background, only 7, 196 animal images are kept. Table <ref type="table" target="#tab_2">2</ref> lists the details of the proposed dataset. During the progress of cropping, we kept the original animal size, color, and aspect ratio. Figure <ref type="figure">4</ref> shows the cropped samples for seven species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and result</head><p>We developed a species recognition algorithm based on ScSPM, implemented as follows. The images were all converted into gray scale and both the SIFT descriptor and the cLBP descriptor were then extracted from 16×16 pixel patches. All the patches of each image were densely sampled on a grid with stepsize of 4 pixels. Both SIFT and cLBP were normalized to be unit norm with dimensions 128 and 59, respectively. For the dictionary learning process, we extracted SIFT and cLBP from 20, 000 patches that are randomly sampled on training set. Dictionaries were trained for SIFT and cLBP separately, with the same dictionary size K = 1, 024.</p><p>Following the standard benchmark procedures, we repeated the experimental process by 10 runs to obtain reliable results. In each run, we randomly selected 70% of the images of each species for training and kept the remaining 30% for testing. We report our final results as a confusion matrix.</p><p>We first test our approach on all 18 species, and the classification result is shown in Table <ref type="table">3</ref>. In real world scenarios, it is not necessary to distinguish species across the two-place datasets. Thus, we also test our method on the two datasets (Panama and Netherlands) separately. The classification results are shown in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>. Since the SIFT and cLBP can describe the texture at different level, we did the experiment using SIFT, cLBP, and the combination of SIFT and cLBP, respectively, to show how the combination improved the performance. The SIFT feature is good at extracting the silhouette of an animal, while cLBP is powerful in describing the skin texture of animals. Thus, it is reasonable to combine SIFT and cLBP. As we can see in Table <ref type="table" target="#tab_5">6</ref>, SIFT feature is more discriminative than cLBP, and the performance is boosted much by combining them.</p><p>In Table <ref type="table">3</ref>, we can see that the overall accuracy is about 82%. Wood mouse is correctly recognized 100%, which is surprising, considering that none biometric features are used. For over one third of the 18 species, this experiment obtained classification accuracy over 90%, such as paca, ocelot, red deer, and wild boar. As expected, red brocket deer is easily misclassified as white-tailed deer because they are of the same ontology and have the similar appearance. In order to better classify the two species like these, biometric features, such as spots on the fur and shape of antlers, play a key role in species recognition. However, automatically identifying biometric features is a challenging task, to our best knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that object recognition techniques from computer vision science can be effectively used to recognize and identify wild mammals on sequences of photographs taken by camera traps in nature, which are The combination of SIFT and cLBP improves performance a lot. http://jivp.eurasipjournals.com/content/2013/1/52 notorious for high levels of noise and clutter. Although some species are of the same ontology, the proposed method can detect imperceptible differences between them. The combination of SIFT and cLBP as descriptors of local images features significantly improved the recognition performance, which is abundant in texture description at multiple scales.</p><p>In the future work, some biometric features that are important for species analysis will be included in the local features, such as color, spots, and size of the body. Since the original sequences captured with motion-sensitive camera traps have motion information, we will develop an automatic animal segmentation algorithm in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 The architecture of ScSPM algorithm. The densely extracted local features are pooled across different spatial locations over different spatial scales.</figDesc><graphic coords="2,198.97,493.46,196.90,207.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 The procedure of extracting local features. First, calculate the gradients and LBP patterns on raw pixel patch. Second, create histogram features for SIFT and LBP, respectively .</figDesc><graphic coords="3,127.93,589.37,311.14,111.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Two sequences of agouti and collared peccary captured in day and night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,64.42,97.58,467.62,281.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 The numbers of sequences for each species Common name Number of Number of Remained total seq remained seq frames</head><label>1</label><figDesc></figDesc><table><row><cell>Agouti</cell><cell>5,423</cell><cell>100</cell><cell>950</cell></row><row><cell>Collared peccary</cell><cell>904</cell><cell>100</cell><cell>901</cell></row><row><cell>Paca</cell><cell>298</cell><cell>100</cell><cell>1,196</cell></row><row><cell>Red brocket deer</cell><cell>588</cell><cell>100</cell><cell>982</cell></row><row><cell>White nosed coati</cell><cell>203</cell><cell>100</cell><cell>1,313</cell></row><row><cell>Spiny rat</cell><cell>130</cell><cell>100</cell><cell>712</cell></row><row><cell>Ocelot</cell><cell>345</cell><cell>100</cell><cell>548</cell></row><row><cell>Mouflon</cell><cell>216</cell><cell>100</cell><cell>2,365</cell></row><row><cell>Red deer</cell><cell>462</cell><cell>100</cell><cell>2,830</cell></row><row><cell>Wild boar</cell><cell>240</cell><cell>100</cell><cell>1,883</cell></row><row><cell>Wood mouse</cell><cell>264</cell><cell>100</cell><cell>1,350</cell></row><row><cell>Red squirrel</cell><cell>160</cell><cell>99</cell><cell>645</cell></row><row><cell>Great tinamou</cell><cell>130</cell><cell>99</cell><cell>1,194</cell></row><row><cell>Roe deer</cell><cell>620</cell><cell>99</cell><cell>1,271</cell></row><row><cell>Common opossum</cell><cell>93</cell><cell>93</cell><cell>916</cell></row><row><cell>White-tailed deer</cell><cell>93</cell><cell>92</cell><cell>2,226</cell></row><row><cell>European hare</cell><cell>87</cell><cell>87</cell><cell>700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 The numbers of sequences for each species</head><label>1</label><figDesc></figDesc><table><row><cell>(Continued)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Song thrush</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Wood pigeon</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Carrion crow</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Common buzzard</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Robinson's mouse opossum</cell><cell>2</cell><cell>0</cell><cell>0</cell></row><row><cell>Bank Vole</cell><cell>2</cell><cell>0</cell><cell>0</cell></row><row><cell>Black eared opossum</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Crab eating racoon</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Lizard</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Armadillo</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>European rabbit</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Chaffinch</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Goshawk</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>B l u e t i t</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>European robin</cell><cell>1</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 The 18 terrestrial species, captured by camera traps in Panama and the Netherlands Common name Latin name Pictures (n) Site</head><label>2</label><figDesc></figDesc><table><row><cell>Agouti</cell><cell>Dasyprocta punctata</cell><cell>518</cell><cell>Panama</cell></row><row><cell>Paca</cell><cell>Cuniculus paca</cell><cell>285</cell><cell>Panama</cell></row><row><cell>Collared peccary</cell><cell>DTayassu tajacu</cell><cell>263</cell><cell>Panama</cell></row><row><cell>Red brocket deer</cell><cell>Mazama americana</cell><cell>297</cell><cell>Panama</cell></row><row><cell>White-nosed coati</cell><cell>Nasua narica</cell><cell>325</cell><cell>Panama</cell></row><row><cell>Spiny rat</cell><cell>Proechimys semispinosus</cell><cell>175</cell><cell>Panama</cell></row><row><cell>Ocelot</cell><cell>Leopardus pardalis</cell><cell>184</cell><cell>Panama</cell></row><row><cell>Red-tailed squirrel</cell><cell>Sciurus granatensis</cell><cell>143</cell><cell>Panama</cell></row><row><cell cols="2">Common opossum Didelphis marsupialis</cell><cell>264</cell><cell>Panama</cell></row><row><cell>Great tinamou</cell><cell>Tinamus major</cell><cell>350</cell><cell>Panama</cell></row><row><cell>White-tailed deer</cell><cell>Odocoileus virginianus</cell><cell>1,091</cell><cell>Panama</cell></row><row><cell>Mouflon</cell><cell>Apodemus sylvaticus</cell><cell>896</cell><cell>Holland</cell></row><row><cell>Red deer</cell><cell>Cervus elaphus</cell><cell>802</cell><cell>Holland</cell></row><row><cell>Roe deer</cell><cell>Capreolus capreolus</cell><cell>362</cell><cell>Holland</cell></row><row><cell>Wild boar</cell><cell>Sus scrofa</cell><cell>487</cell><cell>Holland</cell></row><row><cell>Red fox</cell><cell>Vulpes vulpes</cell><cell>120</cell><cell>Holland</cell></row><row><cell>European hare</cell><cell>Lepus europaeus</cell><cell>176</cell><cell>Holland</cell></row><row><cell>Wood mouse</cell><cell>Apodemus sylvaticus</cell><cell>455</cell><cell>Holland</cell></row></table><note><p>Images were used to test the recognition algorithm.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 The confusion matrix of species recognition on Panama data</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Agouti</cell><cell>Collared</cell><cell>Paca</cell><cell>Red brocket</cell><cell>White-nosed</cell><cell>Spiny rat</cell><cell>Ocelot</cell><cell>Red</cell><cell>Common</cell><cell>Great</cell><cell>White-tailed</cell></row><row><cell></cell><cell></cell><cell>peccary</cell><cell></cell><cell>deer</cell><cell>coati</cell><cell></cell><cell></cell><cell>squirrel</cell><cell>opossum</cell><cell>tinamou</cell><cell>deer</cell></row><row><cell>Agouti</cell><cell>90.1</cell><cell>1.2</cell><cell>0.6</cell><cell>0.2</cell><cell>0.8</cell><cell>0.6</cell><cell>0.0</cell><cell>0.6</cell><cell>1.3</cell><cell>2.9</cell><cell>1.7</cell></row><row><cell>Collared peccary</cell><cell>8.7</cell><cell>79.5</cell><cell>0.6</cell><cell>1.4</cell><cell>1.3</cell><cell>0.0</cell><cell>1.1</cell><cell>0.0</cell><cell>1.9</cell><cell>1.3</cell><cell>4.2</cell></row><row><cell>Paca</cell><cell>0.0</cell><cell>0.3</cell><cell>91.3</cell><cell>1.6</cell><cell>0.1</cell><cell>0.6</cell><cell>0.8</cell><cell>0.0</cell><cell>2.1</cell><cell>0.8</cell><cell>2.3</cell></row><row><cell>Red brocket deer</cell><cell>0.6</cell><cell>0.2</cell><cell>0.3</cell><cell>59.7</cell><cell>0.9</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.4</cell><cell>0.6</cell><cell>37.2</cell></row><row><cell>White-nosed coati</cell><cell>3.2</cell><cell>0.2</cell><cell>0.0</cell><cell>0.1</cell><cell>89.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.7</cell><cell>0.9</cell><cell>1.2</cell><cell>4.4</cell></row><row><cell>Spiny rat</cell><cell>2.2</cell><cell>0.6</cell><cell>1.1</cell><cell>0.0</cell><cell>0.0</cell><cell>84.3</cell><cell>0.0</cell><cell>0.4</cell><cell>8.1</cell><cell>2.0</cell><cell>1.3</cell></row><row><cell>Ocelot</cell><cell>0.0</cell><cell>0.2</cell><cell>3.4</cell><cell>0.2</cell><cell>0.4</cell><cell>0.2</cell><cell>91.4</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>4.1</cell></row><row><cell>Red squirrel</cell><cell>18.4</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>3.7</cell><cell>1.6</cell><cell>0.0</cell><cell>71.2</cell><cell>0.0</cell><cell>3.0</cell><cell>2.1</cell></row><row><cell>Common opossum</cell><cell>2.0</cell><cell>1.6</cell><cell>5.1</cell><cell>0.5</cell><cell>0.3</cell><cell>3.6</cell><cell>0.0</cell><cell>0.1</cell><cell>83.1</cell><cell>0.8</cell><cell>2.9</cell></row><row><cell>Great tinamou</cell><cell>4.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>1.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.2</cell><cell>1.5</cell><cell>90.6</cell><cell>1.9</cell></row><row><cell>White-tailed deer</cell><cell>1.0</cell><cell>1.1</cell><cell>0.4</cell><cell>3.2</cell><cell>0.5</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.7</cell><cell>92.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 The confusion matrix of species recognition on Holland data Mouflon Red Roe Wild Red European Wood deer deer boar fox hare mouse</head><label>5</label><figDesc>For the 7 species, accuracy averaged 85.4% with standard deviation of 1.5%.</figDesc><table><row><cell>Mouflon</cell><cell>97.3</cell><cell>1.5</cell><cell>0.4</cell><cell>0.2 0.2</cell><cell>0.4</cell><cell>0.0</cell></row><row><cell>Red deer</cell><cell>2.7</cell><cell cols="2">93.6 2.0</cell><cell>1.4 0.0</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Roe deer</cell><cell>6.3</cell><cell cols="3">9.6 81.5 0.7 0.5</cell><cell>1.3</cell><cell>0.1</cell></row><row><cell>Wild boar</cell><cell>0.5</cell><cell>1.0</cell><cell cols="2">0.1 98.0 0.0</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>Red fox</cell><cell>14.2</cell><cell>7.8</cell><cell>7.5</cell><cell>6.7 53.3</cell><cell>10.6</cell><cell>0.0</cell></row><row><cell>European hare</cell><cell>8.9</cell><cell>3.0</cell><cell>7.4</cell><cell>2.8 2.8</cell><cell>73.8</cell><cell>1.3</cell></row><row><cell>Wood mouse</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0 0.0</cell><cell>0.0</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 Performance of different procedures for recognition of local image features</head><label>6</label><figDesc></figDesc><table><row><cell>Feature</cell><cell>Average accuracy (%)</cell><cell>Standard deviation (%)</cell></row><row><cell>SIFT</cell><cell>78.9</cell><cell>0.7</cell></row><row><cell>cLBP</cell><cell>74.5</cell><cell>1.1</cell></row><row><cell>SIFT + cLBP</cell><cell>82.0</cell><cell>0.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>For the 18 species, accuracy averaged 82% with standard deviation of 0.9%.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>For the 11 species, accuracy averaged 83.8% with standard deviation of 1.2%.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Science Foundation Grant DBI 10-62351. Field data were collected with support from the National Science Foundation (NSF-DEB 0717071 to R.W.K.) and the Netherlands Organization for Scientific Research (863-07-008 to P.A.J.). XY and TW would like to acknowledge support by the National Natural Science Foundation of China Grant 61073094.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>http://jivp.eurasipjournals.com/content/2013/1/52 Wood mouse 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Grand Challenges in Environmental Sciences</title>
		<meeting><address><addrLine>Washingthon, DC</addrLine></address></meeting>
		<imprint>
			<publisher>National Academies Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wireless sensor networks for ecology</title>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arzberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="561" to="572" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monitoring wild animal communities with arrays of motion sensitive camera traps</title>
		<author>
			<persName><surname>Kays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tilak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kranstauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rowcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fountain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Res Rev Wireless Sensor Netw</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel morphometry-based protocol of automated video-image analysis for species recognition and activity rhythms monitoring in deep-sea fauna</title>
		<author>
			<persName><surname>Aguzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramirez-E</forename><surname>Iwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Llorda</surname></persName>
		</author>
		<author>
			<persName><surname>Menesatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8438" to="8455" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data acquisition and management software for camera trap data: a case study from the TEAM Network</title>
		<author>
			<persName><surname>Fegraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahumada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><surname>Youn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Inform</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="353" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Miami</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intell, IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computer-assisted system for photographic mark-recapture analysis</title>
		<author>
			<persName><surname>Bolger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object recognition with features inspired by visual cortex</title>
		<author>
			<persName><forename type="first">L</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="994" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-04">27 September -4 October, 2009</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosted local structured HOG-LBP for object localization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Colorado Springs, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-25">20-25 June 2011</date>
			<biblScope unit="page" from="1393" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">801</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Task-driven dictionary learning</title>
		<author>
			<persName><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intell, IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="804" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning the sparse representation for classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1186/1687-5281-2013-52</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated identification of animal species in camera trap images</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2013">2013 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
