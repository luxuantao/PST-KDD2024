<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bioinspired Programming of Memory Devices for Implementing an Inference Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member IEEE</roleName><forename type="first">Damien</forename><surname>Querlioz</surname></persName>
							<email>damien.querlioz@u-psud.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite ´Paris-Sud</orgName>
								<orgName type="institution" key="instit2">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Bichler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite ´Paris-Sud</orgName>
								<orgName type="institution" key="instit2">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrien</forename><surname>Francis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite ´Paris-Sud</orgName>
								<orgName type="institution" key="instit2">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Gamrat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite ´Paris-Sud</orgName>
								<orgName type="institution" key="instit2">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bioinspired Programming of Memory Devices for Implementing an Inference Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">145A51FD359D27D3C1024A0B0E6FE305</idno>
					<idno type="DOI">10.1109/JPROC.2015.2437616</idno>
					<note type="submission">received November 4, 2014; revised April 15, 2015; accepted May 6, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Inference</term>
					<term>memory devices</term>
					<term>neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emerging memory structures have several characteristics that are suitable for neuromorphic implementations. This paper connects the behavior of these ''new'' memory devices to achieving inference engines and provides a connection between the behavior of different devices and the learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Modern electronics applications for smart sensors, medical electronics, and the future Internet of Things require electronic circuits capable of handling large volumes of noisy and incomplete real-life data. Tasks such as recognition or mining, broadly qualified as ''cognitive,'' are necessary. While such assignments are not hard for humans, because of their natural capability to perform inference, performing them with computers necessitates advanced machine learning algorithms. In contrast to more traditional programming, these algorithms require minimal computing but significant memory access <ref type="bibr" target="#b0">[1]</ref>. For this reason, cognitive algorithms are severely affected by the separation of computing and memory, known as the Von Neumann bottleneck, and need big computers with a large power budget. For example, a recent widely publicized demonstration by Google of a ''deep'' network uses no complicated equations but accesses one billion parameters from memory <ref type="bibr" target="#b1">[2]</ref> (see also Fig. <ref type="figure" target="#fig_0">1</ref>). Due to the inefficiency of cognitive tasks running on computers, smart devices currently must rely on the cloud and its megawatt data centers.</p><p>Emerging nonvolatile memory devices (e.g., resistive memory <ref type="bibr" target="#b2">[3]</ref>, memristor <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, magnetic memory <ref type="bibr" target="#b5">[6]</ref>) may be a major advancement for this situation. As they are quite compact and can be embedded in complementary metal-oxide-semiconductor (CMOS) circuitry, they offer an opportunity to fuse computing and memory, with the additional benefit of nonvolatility. Cognitive systems, the parameters of which would be stored in nonvolatile memory at the core of CMOS, would provide instant on/ off and offer the possibility of highly reduced power consumption for the smart objects of the Internet of Things. However, designing such systems requires a complete reinvention of electronics around cognitive tasks and inference problems, in stark contrast with the Von Neumann paradigm. A current idea is to try to use the human brain directly as an inspiration for a new type of architecture. Indeed, brains excel at inference and do not differentiate between computing and memory: memory is embedded at the core of computation.</p><p>Although new memory devices are particularly appealing for microelectronics, they also pose many challenges. They often suffer from high device variability, noise, and partly nonrepeatable behavior <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, very much like the synapses and ion channels that the brain uses for computing <ref type="bibr" target="#b8">[9]</ref>. Despite this, the brain achieves exceptional inference capability.</p><p>For these reasons, in recent years, the exploitation of emerging memory devices in brain-inspired systems has stimulated a considerable interest. They are typically proposed to be used as synapses between silicon neurons <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b28">[29]</ref>. Such efforts fit particularly well with recent progress on the implementation of large neuroinspired systems <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>, realizing a vision pioneered by the research of Carver Mead in the late 1980s <ref type="bibr" target="#b34">[35]</ref>.</p><p>Building on these ideas, here we show an example of how simple stochastic programming of memory devices can lead to a system capable of performing complex inference. We differentiate between two main approaches: one that exploits multilevel memory effects, and one that relies on binary memory devices programmed in a stochastic way, possibly employing device redundancy. We provide many examples, exploring a range of inference tasks and device physics phenomena. We introduce a theoretical analysis for the systems functionality, based on <ref type="bibr" target="#b35">[36]</ref>. We explain in detail how the device physics relates to the inference capability, and how this in turn relates to sophisticated machine learning ideas and to Bayesian inference. Finally, we explore the intrinsic robustness of these approaches with regards to device issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STOCHASTIC PROGRAMMING OF AN INFERENCE ENGINE A. General Ideas</head><p>In brains, the capability for inference naturally emerges from learning <ref type="bibr" target="#b37">[38]</ref>. Though the underlying mechanism is far from understood, some models of how learning occurs at the synaptic and neuronal level provide inspiration for novel electronics paradigms. It is widely accepted that long-term memory is stored in synapses, which are the connections between the neurons, which are the active computation units of the brain. Synapses not only transmit information from one neuron to another, but also adjust their strength (usually called ''synaptic weight'') in response to experience. This adaptation, known as synaptic plasticity, is thought to be the most important phenomenon for long-term learning.</p><p>A canonical model of synaptic plasticity, known as spike-timing-dependent plasticity (STDP), was identified in the late 1990s <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>. It was proposed to provide an intuitive interpretation of experimental observations. Neurons communicate by asynchronous spikes, which are transmitted by synapses from ''presynaptic neurons'' to ''postsynaptic neurons.'' The assumption underlying STDP is that synapses tend to reinforce causal links. That is, when the presynaptic neuron spikes just before the postsynaptic neuron spikes, the synapse between the two becomes stronger. Therefore, if the presynaptic neuron spikes again, the synapse will allow the postsynaptic neuron to spike faster. In contrast, when the postsynaptic neuron spikes just before the presynaptic neuron, the synapse becomes weaker. This is summarized in the conventional STDP curve of Fig. <ref type="figure" target="#fig_1">2</ref>(a). In addition to this conventional curve, many variations of STDP have been observed experimentally (e.g., symmetric, asymmetric, voltage dependent, etc.) <ref type="bibr" target="#b39">[40]</ref>. One should be careful to note that neuroscientists consider STDP a simplified model; it does not account for all that has been observed regarding synaptic plasticity and should not be seen as the only learning process the brain possesses <ref type="bibr" target="#b40">[41]</ref>. However, research in neuroscience has shown that STDP is sufficient to achieve some impressive forms of learning <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>, and we therefore use it as a major source of inspiration.</p><p>It is insightful to compare ideas coming from neuroscience with the algorithms used in the field of machine learning for processing big data. Synaptic plasticity, and in particular STDP, shares features with learning rules used by various machine learning paradigms. For example, STDP depends only on the activity of the presynaptic and postsynaptic neurons. This locality feature is similar to the learning rule of restricted Boltzmann machines, the first system used to demonstrate deep unsupervised learning <ref type="bibr" target="#b44">[45]</ref>. Some learning rules, however, are fundamentally nonlocal. This is the case for backpropagation, traditionally used in artificial neural networks <ref type="bibr" target="#b45">[46]</ref> and in the popular convolutional neural networks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Other machine learning paradigms such as support vector machines use fundamentally different principles than STDP and neurons.</p><p>In this work, we program memory devices to implement a simplified version of STDP. Several proposals exist for implementing traditional STDP with purely CMOS circuits <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b50">[51]</ref>. Numerous other works propose implementing it directly when programming memory devices <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b54">[55]</ref>. In contrast with purely CMOS solutions, this brings a higher synaptic density, offering the possibility of massive connectivity between neurons as in the human brain. Even more importantly, the memory devices offer nonvolatility to the synapses, preserving their state without time degradation, and thus offer instant on/off operation to the system. In these proposals, the memory devices are hybridized with CMOS neurons, which do not require nonvolatility. This idea is made feasible by the tremendous progress on implementing neurons with CMOS <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b55">[56]</ref>, and on the very largescale integration of such neurons <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Most proposals for implementing STDP with memory devices possess a relatively high complexity. They usually employ multiple programming pulses <ref type="bibr" target="#b51">[52]</ref> or complex analog programming waveforms <ref type="bibr" target="#b10">[11]</ref>. In this work, we abandon biological realism for a highly simplified version of STDP <ref type="bibr" target="#b35">[36]</ref>, shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>). The simplification results from two major ideas. First, we ignore the analog time dependence of STDP, allowing only two possibilities: increasing or decreasing synaptic weight. Second, we decide that the actual synaptic weight change specifically when a POST spike occurs. At this point, if the presynaptic neuron was active recently, the synaptic weight is increased (synaptic potentiation). In any other case, it is decreased (synaptic depression).</p><p>This strategy, which is not mandatory and loses part of the richness of STDP, provides two main benefits for implementation. First, we need only use simple square voltage pulses. Second, the STDP learning rule is applied during clearly defined ''programming'' phases, which are distinct from system ''operation.'' We can thus design a system such that nothing else can happen during the programming phase, making circuit design significantly less complex. We show in Section III how this works in practice, and that this extremely simplified version of STDP can allow a system to learn sophisticated inference.</p><p>The precise implementation of STDP, and the exact behavior, depends on the chosen device technology. Different emerging memory devices have unique physical and qualitative behaviors, as illustrated in Fig. <ref type="figure">3</ref>. While such disparities do not have many fundamental consequences when the devices are used as binary memories, they become very apparent, however, when the devices are used as plastic synapses. In this context, a complete understanding of their operating principles and electrical characteristics is necessary.</p><p>Nevertheless, our approach adapts to many kinds of memory devices, which can have multilevel as well as binary memory capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation With a Multilevel Memory</head><p>1) ''Cumulative'' Memristive Device: We first consider the case of memristive devices. Many emerging memory technologies have some form of multilevel capability, and this was a core idea of the original memristor proposal <ref type="bibr" target="#b3">[4]</ref> illustrated in Fig. <ref type="figure">3(a)</ref>. Devices with multilevel behaviors similar to the memristor idea function based on different physical phenomena: ionics <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b60">[61]</ref>, ferroelectrocity <ref type="bibr" target="#b19">[20]</ref> and magnetic domain wall motion <ref type="bibr" target="#b61">[62]</ref>. In these devices, it is possible to increase or decrease the conductance of the device by applying short programming voltage or current pulses. The change is cumulative: if one repeats a programming pulse to a device, its conductance will change further, as is seen in the measurements of Fig. <ref type="figure">3</ref>(a) (reproduced from <ref type="bibr" target="#b4">[5]</ref>). This behavior is reminiscent of synaptic plasticity, and it is thus natural to compare memristive devices with synapses.</p><p>The way these memristive devices are used is straightforward. The conductance of the device encodes the ''strength'' of the synapse, or synaptic weight. Applying a brief positive voltage pulse that is higher than a programming threshold ðV Tþ Þ increases the conductance of the device. Applying a brief negative voltage pulse that is lower than a programming threshold ðV TÀ Þ decreases the conductance of the device. To obtain a simplified STDP analog, we need only apply cleverly designed programming voltage pulses, as illustrated in Fig. <ref type="figure" target="#fig_2">4(c</ref>). When the postsynaptic neuron fires, it applies a waveform to its synapses, while the presynaptic neurons that were active recently simultaneously apply a simple voltage pulse. Synapses presenting only the postsynaptic waveform have their conductance reduced. Synapses that experience both the postsynaptic waveform and the presynaptic pulse have their conductance increased.</p><p>This implementation (analogous to [63, Fig. <ref type="figure">3</ref>]) clearly separates phases in which the device is used for transmitting information, from phases in which it is programmed and learning occurs. It is only possible if, when STDP programming occurs, the system does not need the synapses to transmit information. This is typically the case in the winner-takes-all-type systems studied in Section III. It is also possible to avoid this separation between transmission and programming by making input pulses very long. This is illustrated in <ref type="bibr" target="#b62">[63,</ref><ref type="bibr">Fig. 2]</ref>, and in various other proposals such as <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b52">[53]</ref>, and <ref type="bibr" target="#b54">[55]</ref>. Such a scheme, however, makes the design of the synapses driving circuitry harder <ref type="bibr" target="#b63">[64]</ref> and leads to a significant energy consumption due to the input pulses.</p><p>2) Phase-Change Memory (PCM): In a PCM [Fig. <ref type="figure">3(b)</ref>], a small volume of material can switch between a highresistance amorphous state and a low-resistance crystalline state <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b64">[65]</ref>. A characteristic of PCM when compared with more conventional memristive devices is its unipolar nature: positive and negative voltages are equivalent. While the transition from the amorphous to the crystalline state exhibits the desirable cumulative behavior found in memristive devices, as is seen in the measurements of Fig. <ref type="figure">3</ref>(b), the reverse transition from the crystalline to the amorphous state is not cumulative <ref type="bibr" target="#b57">[58]</ref>. The programming current determines the final state, and if one repeats the same programming pulse twice, the second Fig. <ref type="figure">3</ref>. Different nanodevices and their behavior. (a) Cumulative memristive device (measurements from <ref type="bibr" target="#b4">[5]</ref> and model from <ref type="bibr" target="#b56">[57]</ref>). (b) Phase-change memory (measurements and model from <ref type="bibr" target="#b57">[58]</ref>). (c) Conductive bridge memory (measurements and model from <ref type="bibr" target="#b58">[59]</ref>). (d) Spin transfer torque magnetic tunnel junction (basic cell of STT-MRAM) (measurements and model from <ref type="bibr" target="#b59">[60]</ref>).</p><p>pulse has no effect. This is a serious issue for implementing a functional synapse with PCM.</p><p>A possibility to regain cumulativity in both directions is to associate two PCMs in the ''2-PCM'' structure described in <ref type="bibr" target="#b64">[65]</ref>. Additionally, it should be noted that PCMs suffer from a drift issue that makes low-resistive states partially unstable. This does not, however, seem to forbid neuromorphic applications <ref type="bibr" target="#b65">[66]</ref>.</p><p>C. Adaptation to Other Device Physics: Stochastic Synapse 1) Conductive Bridge Memory (CBRAM): In most CMOScompatible CBRAMs <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b66">[67]</ref>, both directions of programming are noncumulative [Fig. <ref type="figure">3(c)</ref>]. It is possible to provide multilevel memory by carefully choosing the programming current, but applying the same programming pulse multiple times has no further effect (some academic technologies may exhibit cumulative effects or more complex behaviors).</p><p>Therefore, we have proposed an alternate method for using CBRAMs <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b66">[67]</ref>. We chose to treat the CBRAMs as binary devices, and to replace the progressivity of multilevel operation with a progressivity resulting from probabilistic programming. The idea is simple: when STDP occurs, the memory device only has a probability of changing its conductance. When learning occurs, with traditional STDP, all synapses change their conductance by a small amount. Here, however, a fraction of synapses whose weight should slightly increase change their weight entirely, while a fraction of synapses whose weight should slightly decrease change their weight entirely, but most synapses' states do not change. Stochastic STDP is illustrated in Fig. <ref type="figure" target="#fig_1">2(c</ref>). We compare this form of STDP to traditional STDP in Section III and show that it can be quite powerful. In theoretical works, the idea of using stochastic synapses instead of deterministic ones (in a broad sense) has also been proposed with supervised neural networks <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>.</p><p>An important question is how to actually implement this probabilistic behavior. One obvious solution is to use pseudorandom number generators to achieve stochastic programming. A more ideal option is to have some kind of intrinsic probabilistic effects inherent to the devices. Both options are possible with CBRAM, but the second option may prove more difficult to control. Additionally, it has not been proven experimentally that the intrinsic probabilistic effects in CBRAM are sufficiently random <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b66">[67]</ref>.</p><p>2) Spin Transfer Torque Magnetic Tunnel Junction (STT-MTJ): One final remarkable case is the STT-MTJ [Fig. <ref type="figure">3(d)</ref>], the basic cell of spin transfer torque magnetic memory (STT-MRAM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>. STT-MTJs may be the ideal technology for the stochastic STDP approach. Unlike the previously mentioned technologies, this device is truly binary: it has only two memory states, the highresistive antiparallel (AP) state and the low-resistive parallel (P) state. A striking feature is that switching is fundamentally stochastic, as observed in the measurements of Fig. <ref type="figure">3(d)</ref>. This effect is also seen in CBRAMs and other kinds of memory when using weak programming pulses <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>. It is, however, better understood and controlled in STT-MTJs than in CBRAMs, as it emerges from the basic MTJ physics <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Furthermore, STT-MTJ switching can be modeled with comprehensive analytical equations <ref type="bibr" target="#b59">[60]</ref>, which we use in this work.</p><p>While this stochastic switching is deleterious for applications such as memory cells, and necessitates the use of long programming pulses to ensure reliable switching <ref type="bibr" target="#b73">[74]</ref>, it can fortuitously be used to generate true random numbers, as their quality has already been verified with regards to true random number generator standards <ref type="bibr" target="#b74">[75]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXAMPLE OF INFERENCES MADE WITH STDP-TYPE PROGRAMMING</head><p>A. Problem of Classification 1) Basic Principle: We now describe how STDP-type programming can allow a system to classify images, i.e., to infer in which class an image belongs. In fact, the system not only performs the inference, but also identifies and creates the different categories by which the images can be categorized. These categories constitute the values of a ''latent variable'' of the data in the inference language.</p><p>The canonical data set for machine learning is the MNIST handwritten numbers data set <ref type="bibr" target="#b45">[46]</ref>, and its is thus a natural first test for our learning approaches. It consists of 60 000 handwritten digits dedicated to training a system, and 10 000 digits for testing it.</p><p>To realize our system, we lay out the memory devices in a conventional memory crossbar structure, as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>(a), and use CMOS circuits for the input and output neurons. Each input neuron is connected to each output neuron by a memory device. This structure is feasible for devices with strong nonlinearity that limit sneak paths <ref type="bibr" target="#b75">[76]</ref>. For other devices, access devices (diode or transistor depending on the technology) might be necessary (as in PCM <ref type="bibr" target="#b76">[77]</ref> or CBRAM <ref type="bibr" target="#b58">[59]</ref>), but the basic principle remains the same.</p><p>It is straightforward to understand how such a system may learn. We present the handwritten digits converted to spikes as the input to the system, each input neuron corresponding to one pixel of the image. Alternative possibilities to encode the pixel values as spikes are described in <ref type="bibr" target="#b62">[63]</ref>, and do not significantly affect systemlevel performance. These input spikes give rise to currents as they are applied to the memory devices, the magnitude of these currents being directly determined by the conductances of the devices. These currents I out are received by the output neurons, which act as leaky integrate-and-fire neurons <ref type="bibr" target="#b29">[30]</ref>. That is, they have an internal state variable X that evolves according to a firstorder differential equation</p><formula xml:id="formula_0">dX dt þ gX ¼ I out<label>(1)</label></formula><p>where is an integration time constant associated with the neuron, and g is a real constant the value of which approaches one. When the variable X of one of the output neuron reaches a given threshold, this output neuron spikes and X is reset to zero during a ''refractory period.'' Many efficient options (analog or digital) exist to implement leaky integrate-and-fire neurons with CMOS <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b55">[56]</ref>. When the output neuron spikes, it also inhibits the other output neurons during an inhibition period. This can be efficiently implemented by a nearest neighbor scheme such as the diffuser network used in <ref type="bibr" target="#b48">[49]</ref>.</p><p>After an output neuron has spiked, the STDP step also occurs [Fig. <ref type="figure" target="#fig_2">4(b)</ref> and<ref type="figure">(c)]</ref>. It affects all the synapses connected to the output neuron that spiked. In STDP, the synapses that were active recently become stronger; that is, the synapses change such that the output neurons will spike faster in response to a similar input. All other synapses connected to the output neuron become weaker; that is, if an input substantially differs from the one which caused the output neuron to spike, the neuron will be less likely to activate, and thus leave more time for the other neurons to activate. The output neurons thus become specialized to a particular type of input. We show in the next sections that this approach works impressively well in practice, for different types of problems.</p><p>Interestingly, this process is entirely unsupervised: every output neuron progressively becomes specialized to a class of patterns that continuously return. These categories that implicitly separate the input presented constitute a latent variable, using the language of inference. Through the process of learning, our system is able to identify the values of latent variable behind the inputs, and to classify the input among them.</p><p>2) Performance on the MNIST Data Set: This process is observed when we present the MNIST data set to the system. This example is taken from <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b62">[63]</ref>, and is based on system level simulation <ref type="bibr" target="#b77">[78]</ref>. This kind of simulations, based on a specialized simulator written in the C++ programming language, includes comprehensive physical models of the memory devices, but simulates their peripheral circuits functionally. This allows simulations to be considerably faster than circuit-level simulations, and to simulate realistic full-scale applications. The detailed physical models of the memory devices allow us to understand how device properties translate in terms of learning the applications. In particular, it is possible to include all imperfections seen on real devices and to evaluate the resilience of the applications. On the other hand, some effects due to peripheral circuits (such as neuron variability) need to be simulated in a highly abstracted way.</p><p>As memory devices, we are inspired from a cumulative memristive device <ref type="bibr" target="#b4">[5]</ref>. The device model, fitted from experiments, is presented in <ref type="bibr" target="#b56">[57]</ref> and based on realistic parameter values following <ref type="bibr" target="#b4">[5]</ref>. This model includes precisely the dependence of conductance change to the state of the memristive devices. It allows including effects of device mismatch, as explored in Section V. Discussion of the required precision of the devices, and of how controlled the multilevel behavior of the device needs to be appears in <ref type="bibr" target="#b62">[63]</ref>.</p><p>We first consider a simple system with ten output neurons, according to the basic architecture of Fig. <ref type="figure" target="#fig_3">5(a)</ref>. The final postlearning weights of each synapse connected to an output neuron are plotted as a 2-D image in Fig. <ref type="figure" target="#fig_3">5(b)</ref>. We can see that each output neuron has specialized in a type of digit, and has learned its distinctive features: the loop of the digit two, the bars of the digit eight, etc. This suggests that the system has played its role as an inference engine: it has recognized the different digits, which constitute the latent variable behind images of handwritten digits.</p><p>If we identify the correspondence between digits and output neurons, we can translate the performance of the system on the test data set as a recognition rate. It reaches 60%, which is not an impressive recognition rate by machine learning standards, although it is significantly better than random choice (10%). This results from the fact that there are numerous ways to handwrite the same digit. The true latent variable behind the MNIST data set thus represents the various typical handwrittings of all digits. As can be seen in Fig. <ref type="figure" target="#fig_3">5</ref>(b), the system learned two handwritings of the digit seven, yet no handwriting of the digit four. Thus, we need more than ten output neurons for successful digit recognition.</p><p>Therefore, we performed simulations with various numbers of output neurons. At the end of learning, using a limited set of the data set, we automatically identify the digit to which each output neuron corresponds. This step may be performed using simple counting or by another spiking neural network, as described in <ref type="bibr" target="#b78">[79]</ref>. We then tested the system on the MNIST test data set. This allows us to plot recognition rate as function of the number of output neurons, as shown in Fig. <ref type="figure" target="#fig_3">5(c</ref>). For 300 output neurons, the recognition rate reaches 93.5%. This is far from the best results obtained for the data set, which used millions of adjustable parameters and an augmented data set, and where the recognition rate can reach 99.7% <ref type="bibr" target="#b79">[80]</ref>. However, our unsupervised results compare well to supervised neural network with backpropagation and the same number of adjustable parameters, which have a recognition rate of 95% <ref type="bibr" target="#b45">[46]</ref>. The supervised approach requires knowing which digit corresponds to every image of the training data set, while the unsupervised approach requires knowledge of only a small subset. This is an important feature in the era of big data, where we have access to massive amounts of unlabeled data.</p><p>A big advantage of the MNIST benchmark is that it provides a natural metric to compare potential devices as artificial synapses. In particular, it is of special interest to compare cumulative memristive synapses with stochastic synapses such as stochastically programmed CBRAM or the intrinsically stochastic STT-MTJs. In Fig. <ref type="figure" target="#fig_4">6</ref>, we show our results for a stochastically programmed CBRAM instead of the cumulative memristive devices. The device model, described in <ref type="bibr" target="#b66">[67]</ref>, is fitted on experiments. In particular, it includes the experimental intrinsic cycle-tocycle dispersion on device conductance: each time a device is programmed to the on of off state, its conductance is varied.</p><p>Fig. <ref type="figure" target="#fig_4">6</ref>(a) shows the final states of the memory devices for eight of the output neurons. This appears similar as Fig. <ref type="figure" target="#fig_3">5(b</ref>), but with binary (black and white) values instead of a color spectrum representing multilevel weights. The system has 50 output neurons, and would have a recognition rate of 82% with cumulative memristors. If we replace every cumulative memristive device by just one stochastic device, the recognition rate is a little better than 60%. However, it is possible to obtain equivalent recognition by introducing redundancy. Fig. <ref type="figure" target="#fig_4">6(b)</ref> shows the recognition rate as a function of the number of stochastic devices replacing one cumulative memristive device. We see that if we replace a cumulative memristive device by five stochastic devices, a recognition of 77.2% is achieved, and with seven we reach 78.0%. As we show in Section III-B, the required level of redundancy is extremely problem dependent and can be much smaller for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection Within Dynamic Data</head><p>Beyond image classification, the time-dependent nature of STDP makes it particularly appropriate to learn inference on dynamic data. First, we take an example of video processing <ref type="bibr" target="#b76">[77]</ref>. We used a video acquired from a bioinspired dynamic vision sensor <ref type="bibr" target="#b80">[81]</ref>, which naturally produces spikes, analogous to our retina. The video shows vehicles moving in front of a camera on a freeway in Pasadena, CA, USA, and is freely available online <ref type="bibr" target="#b81">[82]</ref>. A 2-D presentation of the spikes, integrated over 30 ms, is shown in the top picture of Fig. <ref type="figure" target="#fig_5">7</ref>. Here, the latent variable behind the video represents the vehicles passing on the six lanes. If our system learns to recognize them, it naturally becomes a proper vehicle counter.</p><p>To learn this task, we use the same system as for the MNIST case. Each input neuron is connected to one pixel of the camera. Only the actual values for the neurons' threshold, refractory and inhibitory periods need to be changed for this new problem. The results are impressive: in the case of cumulative memristive devices <ref type="bibr" target="#b64">[65]</ref> or of 2-PCM structure <ref type="bibr" target="#b76">[77]</ref>, the system becomes a vehicle counter with the detection rate higher than 95% on all lines except the two outer lanes where very few cars are passing. Less than ten false positives occur during the 80-s video. Cumulative memristive devices were modeled with the same model used as for MNIST recognition <ref type="bibr" target="#b56">[57]</ref>, and 2-PCM structures with a variation of this model fitted on experiments <ref type="bibr" target="#b57">[58]</ref>. Including the drift effects seen in the high-resistance states of PCM did not affect the result. As a comparison, the best result on the same data set, using a neural network with double precision analog weights obtains a detection rate of 98.1% and nine false positives <ref type="bibr" target="#b76">[77]</ref>.</p><p>The results with stochastic devices are even more interesting. Fig. <ref type="figure" target="#fig_5">7</ref> shows the results with STT-MTJs, where we replaced every cumulative memristive device by only one binary STT-MTJ. STT-MTJs are modeled with a comprehensive physics-based analytical model which reproduces full magnetic simulations <ref type="bibr" target="#b59">[60]</ref>. This models allows including various device imperfections (device mismatch, dependence to temperature) in a physical way. In terms of the detection rate and the number of false positives, in simulations employing the STT-MTJ analytical model, we still obtain recognition rates higher than 95% on the four inward lanes, and less than ten false positives <ref type="bibr" target="#b82">[83]</ref>. Unlike the MNIST case, redundancy was not necessary to reproduce the performance obtained with a cumulative memristive device with stochastic binary devices.</p><p>As the car detection constitutes a practical task, it is insightful to estimate the energy that needs to be used by the system to program the memory devices. These estimates solely concern the programming energy at the  device level (peripheral circuitry is ignored). With a conservative PCM technology (used in a 2-PCM structure), the energy consumption during learning has been estimated to 110 W. This power consumption value scales with the technology node and could be as low as 100 nW on advanced PCM <ref type="bibr" target="#b64">[65]</ref>. With stochastic CBRAM, energy consumption has been estimated to 74 W <ref type="bibr" target="#b58">[59]</ref>. With STT-MTJs representative of a 45-nm technology, energy consumption has been estimated to 4 W using long programming pulses with very low voltages <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b82">[83]</ref>, and can be as low as 180 nW using shorter programming pulses with higher voltages (0.46 V) <ref type="bibr" target="#b70">[71]</ref>. These values support the potential of our bioinspired programming approach for low energy computation. They are much smaller than the power consumption of processors capable of machine learning. However, designing the full system is necessary to evaluate the final global power consumption. Once the car detection task has been learned, we may choose to either continue STDP so that the system can adapt to changes in the inputs, or to deactivate STDP to save programming power.</p><p>The system can also be used to process auditory data. In this case, we implement a collection of filters inspired by the cochlea <ref type="bibr" target="#b83">[84]</ref>, and the result of each filter is connected to one input neuron. We then present inputs consisting of audio noise, in which repeated patterns have been inserted. After learning, every output neuron becomes sensitive to one of the repeated patterns. The final sensitivity is similar to that demonstrated by humans given the same problem. The details of this task are given in <ref type="bibr" target="#b58">[59]</ref>.</p><p>This task can also be implemented either with cumulative memristive devices or stochastically programmed ones. However, to achieve similar sensitivity with both implementations, each cumulative memristive device needs to be replaced by three stochastically programmed devices.</p><p>These three examples illustrate that, in many situations, stochastically programmed binary devices can function equivalently to cumulative memristive devices. However, the quantity of redundancy to introduce in the binary case varies greatly. No redundancy is needed for car counting; one cumulative device needs to be replaced by three stochastic devices for auditory pattern detection, and by five for roughly equivalent MNIST character recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL ANALYSIS OF THE INFERENCE ENGINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Link With Bayesian Inference</head><p>We have seen that simplified STDP-type programming of memory devices can allow the system to learn sophisticated inference, in very different situations. In the field of computational neuroscience, a very powerful theory exists that can explain this. In several works, Maass has studied how systems of spiking neurons can be used to perform Bayesian computation, in the presence of high noise and stochasticity <ref type="bibr" target="#b84">[85]</ref>. The most relevant work for our discussion investigates how a specific form of STDP can lead to a form of optimal Bayesian inference <ref type="bibr" target="#b35">[36]</ref>, by approximating the powerful machine learning algorithm of expectation-maximization.</p><p>Nessler et al. <ref type="bibr" target="#b35">[36]</ref> use a simplified version of STDP, which has the same graph as the STDP that we proposed for memory devices [Fig. <ref type="figure" target="#fig_1">2(b)</ref>]. However, the exact impact of STDP steps on the weight of the synapses differs from what we have considered previously. We can introduce w the synaptic weight, w þ the weight increase when a presynaptic spike preceded a postsynaptic spike, and w À the weight decrease in other situations. Nessler et al. considered</p><formula xml:id="formula_1">w þ ¼ C expðÀwÞ À 1 w À ¼ 1 (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where C is a real constant greater than 1.</p><p>The exact comparison with the behavior of physical memory devices will be discussed in Section IV-B. Based on (2), it is straightforward to evaluate analytically which values the synaptic weights approach at the end of a learning process. We introduce</p><formula xml:id="formula_3">pðPREjPOSTÞ ¼ p t PRE 2 ½t POST À t STDP ; t POST jt POST ð Þ<label>(3)</label></formula><p>the probability that when an output neuron spiked, a synapse spiked in the STDP window t STDP preceding the spike. Then, we can show that the final weight w 1 that this particular synapse will approach during the learning process is</p><formula xml:id="formula_4">w 1 ¼ log pðPREjPOSTÞ þ log C:<label>(4)</label></formula><p>This simple result has deep implications. Nessler et al. have shown that this allows the system to perform an approximation of expectation-maximization, an extremely powerful machine learning algorithm <ref type="bibr" target="#b85">[86]</ref>. The process is illustrated in Fig. <ref type="figure" target="#fig_6">8</ref>. The two phases in our system directly correspond to the ones of the expectation-maximization algorithm. The operating phase where synapses transmit information from input to output until an output neuron spikes correspond to expectation steps, the execution of inference. The STDP programming steps correspond to maximization steps, the optimization of the latent variable.</p><p>This theoretical study provides insight into how our inference engine learns and performs inference. However, the physical synapses that we have studied do not correspond to <ref type="bibr" target="#b1">(2)</ref>. We now consider the details regarding the importance of this difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Link Between Device Physics and Learning</head><p>1) Cumulative Memristive Synapses: We first consider the case of cumulative memristive devices. We use a simple model of the conductance increase and decrease <ref type="bibr" target="#b56">[57]</ref>, which fits the measurements of the classical cumulative memristors of <ref type="bibr" target="#b4">[5]</ref>, and can also be used to model the 2-PCM structure <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b64">[65]</ref>. For the sake of simplicity, we use normalized units w ¼ G=G MAX , and assume that minimum and maximum conductances are 0 and 1. We identify normalized conductance with a synaptic weight w. The device model of <ref type="bibr" target="#b56">[57]</ref> then simplifies to</p><formula xml:id="formula_5">w þ ¼ þ expðÀ þ wÞ w À ¼ À exp À À ð1 À wÞ ð Þ :<label>(5)</label></formula><p>þ and À represent by how much the conductance of the memristive device changes when a programming pulse is applied. Smaller values lead to more analog behavior.</p><p>þ and À model the dependency of this conductance change with the state of the memristive device. values of the order of 3.0 can model the devices of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and <ref type="bibr" target="#b64">[65]</ref>.</p><p>Additionally, the weight is bounded by a minimum (0) and maximum (1) conductance.</p><p>Under these conditions, we can show that the final weight of this particular synapse approaches</p><formula xml:id="formula_6">w 1 ¼ À þ þ À þ 1 þ þ À log pðPREjPOSTÞ 1 À pðPREjPOSTÞ þ 1 þ þ À log þ À (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>with w 1 being additionally bounded between 0 and 1. The derivation appears in Appendix A.</p><p>In the case where þ and À are equal, this simplifies to</p><formula xml:id="formula_8">w 1 ¼ 1 2 þ 1 2 log pðPREjPOSTÞ 1 À pðPREjPOSTÞ þ 1 2 log þ À : (7)</formula><p>This equation is reminiscent of ( <ref type="formula" target="#formula_4">4</ref>), but a significant difference is that w 1 appears to approach infinity when pðPREjPOSTÞ approaches 1. However, since the weight of a physical device is bounded between 0 and 1, this divergence does not actually occur. When considering and putting practical values into (6), it becomes in fact remarkably similar to (4). This is shown clearly in Fig. <ref type="figure" target="#fig_8">9(b)</ref>, where ( <ref type="formula" target="#formula_6">6</ref>) is plotted for different values of þ = À , and the value for is taken from real devices <ref type="bibr" target="#b4">[5]</ref>. This suggests that our inference engine with cumulative memristive devices may work by an approximation of expectation-maximization.</p><p>Interestingly, the curves corresponding to differing values of þ = À (2.0, 1.0, and 0.5) are qualitatively similar. This is in agreement with the fact that when simulating the problems of Section III, the value of þ = À is not a sensitive parameter. For example, on the car counting task, and with cumulative memristive devices, the best recognition rate on the four inward lanes (99%) is obtained with þ = À ¼ 2:0. With þ = À ¼ 1:0, the recognition rate on the four inward lanes is only slightly reduced (97%). This result has important implications when dealing with nanodevices. The parameters associated with learning do not need to be too fine tuned for the system to be able to learn tasks.</p><p>Additionally, we note that only the ratio þ = À appears in <ref type="bibr" target="#b5">(6)</ref>, not the actual value. This is also consistent with the research in Section III. This does not mean, however, that the actual values are entirely insignificant, as they directly affect the speed of learning.</p><p>Finally, we should note that the value has considerable impact on the shape of the w 1 curves. To illustrate this, we simulated a network with only one output neuron, to which we presented static photographs of faces, and repeated the simulation for devices with different values. The resulting synaptic weights, organized as a 2-D picture, as well as the corresponding w 1 curves appear in Fig. <ref type="figure" target="#fig_7">10</ref>. We can see that with a value of 3.0 (which is close to what is observed in the devices of <ref type="bibr" target="#b4">[5]</ref>, or the 2-PCM structure <ref type="bibr" target="#b64">[65]</ref>), the final weights are very analog, and approach the mean of all the presented faces. By contrast, a value of 1.0 produces a more binary map, amplifying what is distinctive about a face. A value of 0 leads to an entirely binary map separating pixels where pðPREjPOSTÞ is lower and greater than 0.5. This corresponds well to what would be expected from w 1 as a function of pðPREjPOSTÞ curves. This means that, depending on the device, very different kinds of learning can thus be envisioned.</p><p>In summary, we have observed a remarkable insensitivity to relative steps of potentiation and depression, as well as to the actual value of these steps ( values). We have observed that the devices with different dependences of steps with actual values of the conductance ( values) can have different learning characteristics.</p><p>2) Stochastic Synapses: We now consider the case of stochastic programming, which we introduced in particular for CBRAM and STT-MTJs. </p><formula xml:id="formula_9">3, þ = À ¼ 1, middle: ¼ 1, þ = À ¼ 1, bottom:</formula><p>¼ 0. Graph: Final weight as a function of p(PRE|POST) in these three situations. We introduce p þ , the probability for a synapse to switch from low conductance (''0'') to high conductance (''1'') when a presynaptic spike occurred before the postsynaptic spike p À , its probability to switch from high conductance to low conductance in the other situations, and ¼ p þ =p À . At the end of the learning process, we show in Appendix B that the probability of a synapse to be in the high-conductance state is</p><formula xml:id="formula_10">w 1 ¼ pðPREjPOSTÞ 1 þ pðPREjPOSTÞð À 1Þ:<label>(8)</label></formula><p>If p þ and p À are equal ð ¼ 1Þ, w 1 reduces to pðPREjPOSTÞ. As can be seen in Fig. <ref type="figure" target="#fig_8">9</ref>(c), the shape of the w 1 as a function of pðPREjPOSTÞ appears relatively different from those of ( <ref type="formula" target="#formula_4">4</ref>), but retains some of its distinctive features. It can be thus expected that learning with stochastic synapses also performs an approximation of expectation-maximization in an extremely stochastic form. This correspondence is also discussed in more detail in a recent work <ref type="bibr" target="#b96">[97]</ref>.</p><p>When redundancy between stochastic synapses is introduced, w 1 not only represents the probability of an individual device to be 1, but also a mean value of the weight of the equivalent synapse formed by the ensemble of the stochastic synapses. It is thus natural that the system approximates expectation-maximization better, as was seen with the MNIST classification task [Fig. <ref type="figure" target="#fig_4">6(b)</ref>].</p><p>Finally, it is insightful to compare the w 1 curves for values ranging from 0.5 to 2. Once again, the curves are qualitatively relatively similar. This result is consistent with our practical observation that the choice of is not extremely sensitive when solving the actual tasks of Section III, although more sensitive than þ = À in the case of cumulative memristive devices. For example, when solving the vehicle counting task with an value of 1:0 ðp þ ¼ p À ¼ 0:1Þ, the detection rate is 97.3%. With an value of 2:0 ðp þ ¼ 2p À ¼ 0:1Þ, the detection rate is reduced significantly, but remains high (83.0%). Once again, this is an essential feature for being able to use a system with real devices, where mean switching probability might not be tuned with an arbitrary precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ROBUSTNESS OF THE INFERENCE ENGINE TO NOISE AND TO DEVICE IMPERFECTIONS</head><p>As mentioned in the Introduction, device nonidealities may be a fundamental obstacle to the approach that we are proposing. In particular, new memory devicesVlike almost all nanodevicesVsuffer from a high level of device variation. Monte Carlo simulations allow us to evaluate and understand the robustness of our inference engine with respect to device nonidealities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robustness to Device Variability</head><p>To study the impact of device variations on the inference engine, we perform Monte Carlo simulations where every device in the system is different, as is routinely done in modern CMOS circuit design. We first consider the case of cumulative memristive devices, and take the example of MNIST recognition. We use a system with only 50 outputs, which without variability has a recognition rate (as defined in Section III) of 82%. Recognition rates when variability is introduced are plotted in Fig. <ref type="figure" target="#fig_0">11</ref>. We introduced variations on the initial states of the devices, on their minimum and maximum conductance, and on the parameters of ( <ref type="formula" target="#formula_5">5</ref>). These parameters model by how much the conductance of a memristive device changes when it is programmed. The variability is expressed in relative standard dispersion =.</p><p>We can see that variations of 10% on all parameters have no impact on the recognition rate, and variations of 25% reduce the recognition rate only by a few percent. Extreme variations of 100% reduce the recognition rate to 70%, but is still higher than random recognition (10%), and it is incredible that the system remains functional with so much variability of the device properties. It should be noted that, in this situation, approximately one third of the devices have an value of zero in at least one direction, and are thus practically incapable of learning. Additionally, devices where the minimum and maximum conductances appear inverted due to device variations are also considered as devices incapable of learning.</p><p>A similar robustness was observed in the task of car counting. With variations of 25%, the detection rate is identical to that seen in the case of no variations. We call Fig. <ref type="figure" target="#fig_0">11</ref>. Impact of device variations, on the problem of MNIST recognition with cumulative memristive devices, with 50 output neurons. Recognition rate as a function of relative standard deviations on device parameters. Triangles: only the initial conductance. Squares: only the þ and À parameters of (5). Circles: initial conductance, þ and À , minimum and maximum conductance. Each simulation was repeated ten times. The error bar indicates one standard deviation.</p><p>such robustness to device variability with respect to traditional uses of memory near-immunity. More in-depth discussions of these results appear in <ref type="bibr" target="#b62">[63]</ref>.</p><p>We also performed Monte Carlo simulations for the case of stochastic devices. In this implementation, device variability affects minimum and maximum conductance. If the stochastic effects directly emerge from the device physics, like we proposed in the case of STT-MTJs, the probability to switch p þ and p À (as introduced in Section -IV) will also be variable, possibly dramatically. This is illustrated in Fig. <ref type="figure" target="#fig_9">12(a)</ref>, which plots histograms of the low (P) and high (AP) resistance states values, and the corresponding switching probability, when synaptic variability is introduced. It is observed that due to STT-MTJ device physics, switching probability is more disperse than the states' resistance. The synaptic variability introduced corresponds to relative standard deviation (one-sigma) on resistance of the P state and tunnel magnetoresistance TMR [TMR ¼ ðR AP À R P Þ=R P ]. The fact that resistance of the P state and TMR are varied independently is inspired by experiments. A typical value for the synaptic variability in experimental demonstrators is 5% <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>.</p><p>Once again we observed spectacular robustness to device variations. For example, let us consider the case of car counting with STT-MTJs. As observed in Fig. <ref type="figure" target="#fig_9">12(b)</ref>, with device variability of 10% of the STT-MTJs (higher than what is seen in experiments), which directly translates to variability of 61% on the p þ and p À parameters, the same detection rate and number of false positives are observed as with no variability <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Roots of the Robustness to Device Variability</head><p>The extreme robustness of our inference engine to device variation is impressive and an understanding of its fundamental origin is instructive for nanoelectronic design. We see two basic elements for this robustness: the unsupervised nature of learning and the diversity of synapses that can approximate the expectation-maximization algorithm.</p><p>First, the fact that the system learns in an unsupervised way is an important asset to tolerate variations. When initialized, the neurons are not specialized and respond more readily to the patterns they are naturally capable of learning. For example, we can consider a specific input pattern. If some synapses associated with input neurons fundamental to this pattern do not work, then the output neurons of these synapses will likely learn another input pattern. In that sense, a reasonable device variability is not deeply troublesome for the system. It may even be considered as a feature that precipitates in the beginning of the learning process.</p><p>A second component of robustness to variability can be gathered from the theoretical analysis of Section IV. For the case of cumulative devices, we have seen that the curve of w 1 as a function of pðPREjPOSTÞ depends only on the ratio of þ and À , and that its shape does not qualitatively depend dramatically on this ratio. Similarly, in the stochastic synapses case, the curve of w 1 depends only on the ratio of p þ and p À and its shape does not exhibit significant qualitative dependence on this ratio. This suggests that variable synapses will still manage to perform their task even if they learn through completely different manners. This also suggests that the analysis of Section IV can be an effective way to assess if a particular technology will give rise to a robust inference engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robustness to Other Issues</head><p>First, we should mention the one effect to which our inference engine is not robust: the performance of the system is strongly affected by variability between the neurons <ref type="bibr" target="#b62">[63]</ref>. The neurons with lower threshold are activated frequently and prevent the neurons with higher thresholds from learning patterns. Neuron variability is an issue if the neurons are implemented with analog circuits Switching probability is 10% in all cases for SV ¼ 0. From top to bottom, synaptic variability SV is 5%, 10%, and 25% of relative standard deviation (one-sigma) on resistance of the P state and TMR. (b) Detection rate and proportion of false positives as a function of synaptic variability for the task of car counting with STT-MTJs. <ref type="bibr" target="#b29">[30]</ref>. We proposed a solution in <ref type="bibr" target="#b62">[63]</ref>, which is to implement a form of ''homeostasis'' in the neurons (Fig. <ref type="figure" target="#fig_10">13</ref>). Homeostasis is a bioinspired paradigm that ensures that, over long periods, each output neuron spikes a similar number of times. Circuit implementations of homeostasis can cause significant overhead. In digital neuromorphic circuits, homeostasis implementation would be straightforward. It is more challenging for analog neuromorphic systems, since homeostasis requires particularly long time constants. Both pure analog and mixed analog/digital solutions have been proposed <ref type="bibr" target="#b88">[89]</ref>. In terms of inference, homeostasis implements a prior belief that all latent variable values have a significant chance of occurring. It is particularly interesting to note that synaptic variability (which is the primary concern, as it emerges from nanodevices' variability) is naturally tolerated, while neuronal variability requires a correction mechanism. It also stresses the necessity to consider all aspects of a system when studying its robustness.</p><p>We can also investigate the robustness to noise in the inputs. It is impressive. For example, in the car counting task, for both cumulative memristive devices and stochastic synapses, we can add up to 10% of purely random spikes in the inputs without affecting the detection rate and the number of false positives. This is caused by the neuronal behavior of the outputs, which naturally acts as a filter for input noise.</p><p>Another significant effect in a real system are the transient and potentially local variations of the temperature. Such variations affect all memory devices, but STT-MTJs are among the devices most vulnerable to this effect: since their memory switching is thermally activated, their switching probability depends heavily on the temperature. Therefore, we performed simulations, for the task of vehicle detection, with exacerbated temperature effects: each STDP step, the temperature of all STT-MTJs is chosen randomly. We observed that with the temperature chosen randomly with a Gaussian law of standard deviation of 30 K, the detection rate and the number of false positives are not affected. Temperature fluctuations act as a form of transient device variations, and the roots of the system's robustness are the same as for static device variations.</p><p>Another potential issue is the read disturb effect (the fact that read operation may affect the state of the memory devices). In our scheme, devices frequently transmit spikes. Therefore, even if a read operation has an extremely small effect on the memory state of the device, this effect may accumulate and become significant. Simulations in the case of cumulative memristive devices used for MNIST recognition are presented in <ref type="bibr" target="#b62">[63]</ref> and reproduced in Fig. <ref type="figure" target="#fig_11">14</ref>. The robustness to the read disturb effect is astonishing: if we assume that read disturb affects the device conductance by 1% of the effect of STDP operations, the recognition rate is barely affected. When the read operation has 10% of the effect of STDP operations, the recognition rate moves from 82% to a very reasonable 78%. This suggests that the system naturally corrects the drift associated with the read disturb. This comes from a very specific feature of our inference engine, which never stops learning. When the system has specialized and is stable, STDP is not deactivated. This allows the system to autocorrect for effects such as the read disturb, or for a long-term evolution of the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MEMORY IMPLICATIONS</head><p>In this section, we summarize the core ideas that underlie this work, discuss research approaches that share some  This research is based on several fundamental ideas. First, we try to embed memory at the very core of computing, analogous to the brain structure, in order to avoid the von Neumann bottleneck. This vision, which has been previously investigated in several contexts, finds specific appeal today. The emergence of compact, CMOS-compatible, nonvolatile memory offers an ideal technological basis for this concept. Simultaneously, emerging cognitive applications require frequent memory access and suffer from the von Neumann bottleneck. Novel general-purpose or application-specific processor ideas merging computing and memory are therefore being proposed. The level of integration between computing and memory can be comprehensive, in accordance to the original ''logic-in-memory'' concept <ref type="bibr" target="#b89">[90]</ref> approach, or to novel ideas <ref type="bibr" target="#b90">[91]</ref>- <ref type="bibr" target="#b92">[93]</ref>, while some designs keep a stricter separation of computing and memory at a local level <ref type="bibr" target="#b93">[94]</ref>. Fusing computing and memory is also at the core of many neuromorphic structures, at the present day usually employing SRAM as memory <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p><p>In addition to integrating memory and computing, the research reported here incorporates a second idea: to program memory following a bioinspired approach similar to synaptic learning rules. This idea can already be implemented with SRAM <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, but has significant advantages when the memory is based on nanodevices. Programming memory nanodevices in a conventional digital and deterministic way can be wasteful in terms of energy. Bioinspired programming using learning rules, by contrast, can map more closely to device physics, and exploit the complex intrinsic behaviors seen such as multilevel memory and stochasticity. A drawback is that nanodevices used in this fashion exhibit nonidealities such as device variations. We have tried to show in this work that bioinspired architectures can feature an intrinsic resilience to these nonidealities, and are therefore able to benefit from the intrinsic features of memory devices.</p><p>Other research explores this idea of bioinspired programming. Most of this research attempts to precisely map bioinspired programming to biological models, with the aim of providing a direct correspondence between neuroscience and nanoelectronics research. Here, we suggest that highly abstracted bioinspired programming may already achieve useful features. The balance between biological abstraction and biomimetism, however, is an important open research question. It is not solely determined by purely engineering questions, but also by the prospectives of the researchers.</p><p>Another research approach that emphasizes memory at the core of computing paradigm is the concept of ''memcomputing'' <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>. Memcomputing goes further on the path of fusing computing and memory, with memory devices used as the active computing elements instead of transistors. It has been shown that non-Turing-type memcomputing machines could solve some nondeterministic polynomial (NP)-complete problems in polynomial time, though also requiring a polynomial growth of memory <ref type="bibr" target="#b95">[96]</ref>. Memcomputing therefore allows higher computational power than our proposed approach. However, it does not appear to feature intrinsic resilience to memory device imperfections. Nonetheless, it constitutes a powerful concept to which approaches integrating computing and memory should be confronted.</p><p>This work leaves many open questions, at the technological level, but also at the computational level. We have shown that bioinspired programming may be connected to machine learning techniques. Nevertheless, the tasks presented in this work are still relatively simple textbook applications. More interdisciplinary research is needed to push the idea further and investigate the possibility of state-of-the-art machine learning. For this, we need to convince machine learning specialists to perform the needed specific research based on an understanding of the benefits but also of the limitations of nanodevices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we have explored a bioinspired methodology for programming nanodevices, which naturally implements an inference engine. The approach is fundamentally different from those of traditional electronics. Its source of inspiration is the spike-timing-dependent plasticity model of synaptic learning in neurosciences. The system puts memory at the very core of computing, with the physics of the memory devices playing an active role in the process of learning and inference. The system is capable of learning using an unsupervised paradigm. More precisely, it can identify the latent variable underlying the input to which it is exposed, and at the same time infer its classification among the latent variable values. The system is incredibly tolerant to device variations and read disturb issues.</p><p>We have shown that it may be adapted to various emergent memory device technologies. The most natural implementation uses cumulative memristive or PCM devices, in an analog manner. Another implementation can use stochastic devices such as STT-MTJs. In many situations, stochastic and analog devices are equivalent. However, for most problems, this requires adding redundancy in the stochastic version.</p><p>We have explored several applications for the inference engine: image classification and detection of patterns in video and auditory data. These applications are impressive, especially given that they use precisely the same system. However, they are currently far from production and need to be associated with other circuits capable of exploiting the inference results. The methodology introduced in this work provides a new paradigm for nanoelectronics. Bioinspired programming of memory devices could identify latent variables and perform inference, which is the biggest challenge for processing big data. A simpler system can then interpret the inference result.</p><p>We have already connected the inference engine with Bayesian inference and the algorithm of expectationmaximization. By advancing this comparison further, it should be possible to develop significantly more complex and comprehensive inference engines, while retaining the vision of this work: using the bioinspired idea of fusing computing and memory, we intend to put inference at the core of nanoelectronics. h APPENDIX A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DERIVATION OF THE EXPRESSION OF w 1 IN THE CASE OF CUMULATIVE MEMRISTIVE SYNAPSES</head><p>This appendix derives (6) from Section IV. At the end of learning, if a synapse has reached a stable state, it experiences as many depression events as potentiation events. With the notations of Section IV, this reads w þ pðPREjPOSTÞ ¼ w À pðPREjPOSTÞ <ref type="bibr" target="#b8">(9)</ref> where we have introduced pðPREjPOSTÞ ¼ 1À pðPREjPOSTÞ. By introducing the expressions of w þ and w À from (5), this becomes</p><formula xml:id="formula_11">þ expðÀ þ w 1 ÞpðPREjPOSTÞ ¼ À exp À À ð1 À w 1 Þ ð Þ1 À pðPREjPOSTÞ ð Þ :<label>(10)</label></formula><p>Therefore, we have</p><formula xml:id="formula_12">exp ð þ þ À Þw 1 À À ð Þ ¼ þ À pðPREjPOSTÞ 1 À pðPREjPOSTÞ ð Þ<label>(11)</label></formula><p>which leads to (6)</p><formula xml:id="formula_13">w 1 ¼ À þ þ À þ 1 þ þ À log pðPREjPOSTÞ 1 À pðPREjPOSTÞ þ 1 þ þ À log þ À :<label>(12)</label></formula><p>APPENDIX B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DERIVATION OF THE EXPRESSION OF w 1 IN THE CASE OF STOCHASTIC SYNAPSES</head><p>This appendix derives (8) from Section IV. At the end of learning, if a synapse has reached a stable state, it experiences as many depression events as potentiation events. With the notations of Section IV, and by introducing pðState ¼ 1Þ and pðState ¼ 0Þ, the probabilities of the synapse to be in the 1 and 0 states, this reads p þ Á pðPREjPOSTÞ Á pðState ¼ 0Þ</p><p>¼ p À Á pðPREjPOSTÞ Á pðState ¼ 1Þ: <ref type="bibr" target="#b12">(13)</ref> With the notations of Section IV, pðState ¼ 1Þ ¼ w 1 and pðState ¼ 0Þ ¼ 1 À w 1 . If we introduce ¼ p þ =p À , (13) becomes</p><formula xml:id="formula_14">pðPREjPOSTÞð1 À w 1 Þ ¼ 1 À pðPREjPOSTÞ ð Þ w 1<label>(14)</label></formula><p>which leads to (8)</p><formula xml:id="formula_15">w 1 ¼ pðPREjPOSTÞ 1 þ pðPREjPOSTÞð À 1Þ :<label>(15)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Implementation of cognitive algorithms such as neural network using a Von Neumann architecture suffers heavily from the separation of computing and memory: the Von Neumann bottleneck. (b) A neuron only performs basic operations, which depend on a high number of memory access.</figDesc><graphic coords="2,51.71,520.53,231.98,160.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Variations of the STDP curve. (a) Original measurement from biology [37] and the traditional model. (b) Simplified STDP used in our inference engine. (c) Stochastic version of the simplified STDP.</figDesc><graphic coords="3,73.31,287.85,180.14,412.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Simplified crossbar architecture of the inference engine, and its equivalent neural network. (b) Representation of the two phases of the system: operation and STDP. (c) Voltage pulses used to implement STDP with cumulative memristive devices.</figDesc><graphic coords="5,47.75,76.65,231.98,488.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results on MNIST recognition with cumulative memristive devices. (a) System topology for processing MNIST data. (b) Representation of the final conductance of memristive devices, at the end of learning, with ten output neurons. Each image is a 2-D representation of the devices connected to one output neuron. (c) MNIST recognition rate as a function of the number of output neurons. Error bar is one standard deviation.</figDesc><graphic coords="7,55.31,422.73,216.14,237.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Representation of the final conductance of stochastic synapses, at the end of MNIST learning. Each image is a 2-D representation of the devices connected to one output neuron. Black: high-resistance state, white: low-resistance state. (b) Recognition rate as a function of device redundancy (the number of devices connecting each input to each output), in a situation with 50 output neurons. Horizontal line: recognition rate with cumulative memristive devices in the same situation.</figDesc><graphic coords="8,62.39,76.65,210.14,200.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Simulation of the car detection task using STT-MTJs. Top: part of the stimulus, obtained by averaging the input spike-based video over 30 ms. Bottom: final weights of the stochastic STT-MTJs. Each image corresponds to one output neuron. The outputs neurons are classified by the lane to which they specialize.</figDesc><graphic coords="8,308.39,76.17,204.14,229.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Equivalence between the inference engine and the expectationmaximization algorithm.</figDesc><graphic coords="10,51.71,76.77,231.74,201.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Top left picture: mean image of the faces presented. Right pictures: representation of the final weights in a system with one neuron, to which photographs of faces were presented, as obtained in simulation, and as expected from theory. With cumulative memristive devices and top: ¼3, þ = À ¼ 1, middle: ¼ 1, þ = À ¼ 1, bottom:¼ 0. Graph: Final weight as a function of p(PRE|POST) in these three situations.</figDesc><graphic coords="11,290.87,489.81,231.74,171.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) Final weight as a function of p(PRE|POST) in Nessler theory [see (4)], compared with cumulative memristive devices [see (7)] and final probability of a stochastic synapse being in the state 1 [see (8)]. (b) Final weight as a function of p(PRE|POST) in with cumulative memristive devices for different þ and À values. (c) Final probability of a stochastic synapse being in the state 1 as a function of p(PRE|POST) for different values.</figDesc><graphic coords="11,107.87,75.93,353.66,284.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (a) Histograms representing the values of P and AP states resistance of 2000 STT-MTJs (left subfigures) and switching probabilities (right subfigures) when synaptic variability (SV) is introduced. Switching probability is 10% in all cases for SV ¼ 0. From top to bottom, synaptic variability SV is 5%, 10%, and 25% of relative standard deviation (one-sigma) on resistance of the P state and TMR. (b) Detection rate and proportion of false positives as a function of synaptic variability for the task of car counting with STT-MTJs.</figDesc><graphic coords="13,49.31,324.09,228.14,326.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Impact of homeostasis using the MNIST recognition task, with cumulative memristive devices, where 50 output neurons were used. Recognition rate as a function of variability on neurons' threshold (expressed in relative standard deviation) with and without homeostatic mechanisms. The results are described in detail in<ref type="bibr" target="#b62">[63]</ref>.</figDesc><graphic coords="14,64.79,521.85,204.62,158.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Impact of the READ disturb on the task of MNIST recognition with cumulative memristive devices, where 50 output neurons were used. Recognition rate as a function of the READ disturb parameter , as defined in the figure. ¼ 0 signifies no READ disturb.</figDesc><graphic coords="14,307.56,542.61,205.74,147.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,51.71,345.57,231.98,325.10" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Querlioz et al.: Bioinspired Programming of Memory Devices for Implementing an Inference Engine Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1401</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Querlioz et al.: Bioinspired Programming of Memory Devices for Implementing an Inference Engine Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1403</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Querlioz et al.: Bioinspired Programming of Memory Devices for Implementing an Inference Engine</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1405</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1407</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Querlioz et al.: Bioinspired Programming of Memory Devices for Implementing an Inference Engine Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1409</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>Vol. 103, No. 8, August 2015 | Proceedings of the IEEE 1411</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank C. Bennett, P. Bessie ´re, L. Calvet, D. Chabi, D. Colliaux, B. De Salvo, J. Droulez, J. S. Friedman, J. Grollier, J.-O. Klein, J. Larroque, N. Locatelli, E. Mazer, A. Mizrahi, M. Suri, S. Tiwari, D. Vodenicarevic, and W. S. Zhao.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by ANR COGNISPIN under Grant ANR-13-JS03-0004-01 and FP7 ICT BAMBI under Grant FP7-ICT-2013-C, and the CNRS/MI DEFI NANO program. D. Querlioz and A. F. Vincent are with the Institut d'Electronique Fondamentale,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABOUT THE AUTHORS</head><p>Adrien Francis Vincent (Student Member, IEEE) received the M.S. degree from the E ´cole Normale Supe ´rieure de Cachan, Cachan, France, in 2013. He is currently working toward the Ph.D. degree at the Universite ´Paris-Sud, Orsay, France, studying the integration of spintronic nanodevices in neuromorphic architectures.</p><p>Christian Gamrat received a degree in electrical engineering from the Universite ´Joseph Fourier, Grenoble, France, in 1979 and a degree in information processing from the E ´cole Nationale Supe ´rieure d'E ´lectronique et de Radioe ´lectricite</p><p>´, Grenoble, France, in 1993.</p><p>In 1981, he started his career at CEA/DSM, Grenoble, France, on the design of high-speed data acquisition systems for solid-state and nuclear physics experiments, became involved in the study and design of neural networks computing machines in 1987, and led the team for the MIND-1024 neurocomputer project in 1989. In 1994, he joined the Parallel Computing Architecture Lab, CEA, near Paris, France, where he finalized the development of the SYMPHONIE embedded massively parallel computer for use on board military fighter aircraft. In 1997, he started activity on hardware-reconfigurable computing, and in 2003, he initiated research on novel computing architectures aimed at nanotechnologies. He is currently a Senior Expert in the field of advanced computing architectures and nanocomputing, and he leads the Nanocomputing group at CEA LIST, Gif-sur-Yvette, France.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convergence of recognition, mining, synthesis workloads and its implications</title>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="790" to="807" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="8595" to="8598" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of candidate device technologies for storage-class memory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Burr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="449" to="464" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The missing memristor found</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Strukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nanoscale memristor device as synapse in neuromorphic systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nano Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1297" to="1301" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spin-transfer torque switching in magnetic tunnel junctions and spin-transfer torque random access memory</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Diao</surname></persName>
		</author>
		<idno>ID. 165209</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phys., Condensed Matter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2007-04">Apr. 2007</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the stochastic nature of resistive switching in metal oxide RRAM: Physical modeling, Monte Carlo simulation, experimental characterization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single-shot time-resolved measurements of nanosecond-scale spin-transfer induced switching: Stochastic versus deterministic aspects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devolder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">57206</biblScope>
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variability, compensation and homeostasis in neuron and network function</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Goaillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="563" to="574" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memristive model of amoeba learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Pershin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">La</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Di</forename><surname>Ventra</surname></persName>
		</author>
		<idno>ID. 021926</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting memristance in adaptive asynchronous spiking neuromorphic nanotechnology systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Programmable resistance switching in nanoscale two-terminal devices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nano Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-organized computation with unreliable, memristive nanodevices</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Snider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page">365202</biblScope>
			<date type="published" when="2007-09">Sep. 2007</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analog memory and spiketiming-dependent plasticity characteristics of a nanoscale titanium oxide bilayer resistive switching device</title>
		<author>
			<persName><forename type="first">K</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">254023</biblScope>
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The brain of a new machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Versace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simulation of a memristor-based spiking neural network immune to device variations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1775" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Phase change memory as synapse for ultra-dense neuromorphic systems: Application to complex visual pattern extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An electronic synapse device based on metal oxide resistive switching memory for neuromorphic computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeyasingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2729" to="2737" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Material memristive device circuits with synaptic plasticity: Learning and memory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Erokhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioNanoScience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="24" to="30" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">&apos;A ferroelectric memristor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chanthbouala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Mater</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="860" to="864" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integration of nanoscale memristor synapses in neuromorphic computing architectures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Prodromakis</surname></persName>
		</author>
		<idno>ID. 384010</idno>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">38</biblScope>
			<date type="published" when="2013-09">Sep. 2013</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spin-based neuron model with domain-wall magnets as synapse</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="843" to="853" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Defect-tolerant nanoelectronic pattern classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Likharev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Circuit Theory Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="264" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hebbian learning in spiking neural networks with nanocrystalline silicon TFTs and memristive synapses</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1066" to="1073" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust learning approach for neuro-inspired nanoscale crossbar architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Emerging Technol. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neuromorphic function learning with carbon nanotube based synapses</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gacem</surname></persName>
		</author>
		<idno>ID. 384013</idno>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">38</biblScope>
			<date type="published" when="2013-09">Sep. 2013</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Design and modeling of a neuro-inspired learning circuit using nanotube-based memory devices</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2172" to="2181" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scalable Neuromorphic Hardware With 1-bit Stochastic Nano-synapses</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kaveheihighly</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1309.6419" />
		<imprint>
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Plasticity in memristive devices for spiking neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabghi</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2015.00051</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neuromorphic silicon neuron circuits</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2011.00073</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neuromorphic Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neurogrid: A mixedanalog-digital multichip system for large-scale neural simulations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="716" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A million spiking-neuron integrated circuit with a scalable communication network and interface</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Merolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6197</biblScope>
			<biblScope unit="page" from="668" to="673" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The SpiNNaker project</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Plana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="652" to="665" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A wafer-scale neuromorphic hardware system for large-scale neural modeling,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="1947" to="1950" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<title level="m">Analog VLSI and Neural Systems 1st ed</title>
		<meeting><address><addrLine>Reading, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1989-01">Jan., 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003037</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synaptic modification by correlated activity: Hebb&apos;s postulate revisited</title>
		<author>
			<persName><forename type="first">G.-Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="166" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A tutorial introduction to Bayesian models of cognitive development</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lubke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frotscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sakmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5297</biblScope>
			<biblScope unit="page" from="213" to="215" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Synaptic plasticity: Taming the beast</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1178" to="1183" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dendritic excitability and synaptic plasticity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Sjo ¨stro ¨m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Rancz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ha ¨usser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiol. Rev</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="769" to="840" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning real-world stimuli in a neural network with spike-driven synaptic dynamics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2881" to="2912" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features through spike timing dependent plasticity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.0030031</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guyonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0001377</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cirezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Joint Conf</title>
		<meeting>22nd Int. Joint Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image net classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
			<date type="published" when="2012">2012</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning in silicon: Timing is everything</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="281" to="1185" />
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A VLSI array of low-power spiking neurons and bistable synapses with spike-timing dependent plasticity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="221" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spike-based synaptic plasticity in silicon: Design, implementation, application, challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azghadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iannella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Sarawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="717" to="737" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spike-timing-dependent learning in memristive nanodevices,&apos;&apos; in Prof</title>
		<author>
			<persName><forename type="first">G</forename><surname>Snider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Nanoscale Architect</title>
		<imprint>
			<biblScope unit="page" from="85" to="92" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A memristive nanoparticle/ organic hybrid synapstor for neuroinspired computing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alibart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Funct. Mater</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="609" to="616" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Implementation of biologically plausible spiking neural network models on the memristor crossbar-based CMOS/nano circuits</title>
		<author>
			<persName><forename type="first">A</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ayatollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Raissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Circuit Theory Design</title>
		<meeting>Eur. Conf. Circuit Theory Design</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Excitatory and inhibitory memristive synapses for spiking neural networks,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lecerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="1616" to="1619" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Silicon-neuron design: A dynamical systems approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst., Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1034" to="1043" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning with memristive devices: How should we model their behavior?&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Nanoscale Architect</title>
		<imprint>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Physical aspects of low power synapses based on phase change memory devices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suri</surname></persName>
		</author>
		<idno>ID. 054904</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Phys</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bio-inspired stochastic computing using binary CBRAM synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2402" to="2409" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Analytical macrospin modeling of the stochastic switching time of spin-transfer torque devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="170" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pattern classification by memristive crossbar circuits using ex situ and in situ training</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alibart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zamanidoost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Strukov</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms3072</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Vertical-currentinduced domain-wall motion in MgO-based magnetic tunnel junctions with low current densities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chanthbouala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Immunity to device variations in a spiking neural network with memristive nanodevices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="295" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Silicon neuron dedicated to memristive spiking neural networks,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lecerf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="1568" to="1571" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual pattern extraction using energy-efficient &apos;&apos;2-PCM synapse&apos;&apos; neuromorphic architecture</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2206" to="2214" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Impact of PCM resistance-drift in neuromorphic systems and drift-mitigation strategy,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Nanoscale Architect</title>
		<imprint>
			<biblScope unit="page" from="140" to="145" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">CBRAM devices as binary synapses for low-power stochastic neuromorphic systems: Auditory (cochlea) and visual (retina) cognitive processing applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suri</surname></persName>
		</author>
		<idno type="DOI">10.1109/IEDM.2012.6479017</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Convergence of stochastic learning in perceptrons with binary synapses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">61907</biblScope>
			<date type="published" when="2005-06">Jun. 2005</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Functional abilities of a stochastic logic neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sawada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="443" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Spintronics for low-power computing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.7873/DATE.2014.316</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Autom. Test Eur. Conf. Exhibit</title>
		<meeting>Design Autom. Test Eur. Conf. Exhibit</meeting>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spin-transfer torque magnetic memory as a stochastic memristive synapse for neuromorphic systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="174" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stochastic memristive devices for computing and neuromorphic applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanoscale</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5872" to="5878" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stochastic learning in oxide binary synaptic device for neuromorphic computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2013.00186</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Electrical modeling of stochastic spin transfer torque writing in magnetic tunnel junctions for memory and logic applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Magn</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4375" to="4378" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Spin dice: A scalable truly random number generator based on spin tronics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fukushima</surname></persName>
		</author>
		<idno>ID. 083001</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Exp</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014-08">Aug. 2014</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Engineering nonlinearity intomemristors for passive crossbar applications</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Yang</surname></persName>
		</author>
		<idno>ID. 113501</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Lett</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Extraction of temporally correlated features from dynamic vision sensors with spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Bourgoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="339" to="348" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Design exploration methodology for memristor-based spiking neuromorphic architectures with the Xnet event-driven simulator,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roclin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Nanoscale Architect</title>
		<imprint>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bioinspired networks with nanoscale memristive devices that combine the unsupervised and supervised learning approaches,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">D</forename><surname>Querlioz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Nanoscale Architect</title>
		<imprint>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1003.0358" />
		<title level="m">Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition</title>
		<imprint>
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A 128Â 128 120 dB 15 s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">&apos;AER data files</title>
		<author>
			<persName><forename type="first">L</forename><surname>Longinotti</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/p/jaer/wiki/AERdata/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Spin-transfer torque magnetic memory as a stochastic memristive synapse,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="1074" to="1077" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">AER EAR: A matched silicon cochlea pair with address event representation interface</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="59" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Noise as a resource for computation and learning in networks of spiking neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="860" to="880" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A statistical study of magnetic tunnel junctions for high-density spin torque transfer MRAM (STT-MRAM),&apos;&apos; in</title>
		<author>
			<persName><forename type="first">R</forename><surname>Beach</surname></persName>
		</author>
		<idno type="DOI">10.1109/IEDM.2008.4796679</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Switching distributions and write reliability of perpendicular spin torque MRAM</title>
		<author>
			<persName><forename type="first">D</forename><surname>Worledge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
			<biblScope unit="page" from="12" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Implementing homeostatic plasticity in VLSI networks of spiking neurons</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nikolayeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Electron. Circuits Syst</title>
		<meeting>IEEE Int. Conf. Electron. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="682" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A logic-in-memory computer</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="1970-01">Jan. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fabrication of a nonvolatile full adder based on logic-in-memory architecture using magnetic tunnel junctions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Exp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
	<note>Art. ID. 091301</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Synchronous non-volatile logic gate design based on resistive switching memories</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Spintronic devices as key elements for energy-efficient neuroinspired architectures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Locatelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Autom. Test Eur. Conf. Exhibit</title>
		<meeting>Design Autom. Test Eur. Conf. Exhibit</meeting>
		<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
			<biblScope unit="page" from="994" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Nonvolatile logic-in-memory array processor in 90 nm MTJ/MOS achieving 75% leakage reduction using cycle-based power gating</title>
		<author>
			<persName><forename type="first">M</forename><surname>Natsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Solid-State Circuits Conf. Dig. Tech. Papers</title>
		<imprint>
			<biblScope unit="page" from="194" to="195" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Neuromorphic, digital, quantum computation with memory circuit elements</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pershin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Di</forename><surname>Ventra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Universal memcomputing machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Traversa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Di</forename><surname>Ventra</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2391182</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A compound memristive synapse model for statistical learning through STDP in spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2014.00412</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
