<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic selection of classifiers-A comprehensive review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alceu</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pontifícia Universidade Católica do Paraná (PUCPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universidade Estadual de Ponta Grossa (UEPG)</orgName>
								<address>
									<settlement>Ponta Grossa</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Sabourin</surname></persName>
							<email>robert.sabourin@etsmtl.ca</email>
							<affiliation key="aff2">
								<orgName type="laboratory">École de technologie supérieure (ÉTS)</orgName>
								<orgName type="institution">Université du Québec</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
							<email>lesoliveira@inf.ufpr.br</email>
							<affiliation key="aff3">
								<orgName type="institution">Universidade Federal do Paraná (UFPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Post-graduate Program in Informatics (PPGIa)</orgName>
								<orgName type="institution">Catholic University of Parana Rua Imaculada Conceição</orgName>
								<address>
									<postCode>1155, 80215-901</postCode>
									<region>Curitiba (PR)</region>
									<country>Ponti-fical, Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic selection of classifiers-A comprehensive review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE10711AD1E8E16D70118DECAC9A2FCE</idno>
					<idno type="DOI">10.1016/j.patcog.2014.05.003</idno>
					<note type="submission">Received 28 August 2013 Received in revised form 3 May 2014 Accepted 7 May 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Ensemble of classifiers Dynamic selection of classifiers Data complexity</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a literature review of multiple classifier systems based on the dynamic selection of classifiers. First, it briefly reviews some basic concepts and definitions related to such a classification approach and then it presents the state of the art organized according to a proposed taxonomy. In addition, a two-step analysis is applied to the results of the main methods reported in the literature, considering different classification problems. The first step is based on statistical analyses of the significance of these results. The idea is to figure out the problems for which a significant contribution can be observed in terms of classification performance by using a dynamic selection approach. The second step, based on data complexity measures, is used to investigate whether or not a relation exists between the possible performance contribution and the complexity of the classification problem. From this comprehensive study, we observed that, for some classification problems, the performance contribution of the dynamic selection approach is statistically significant when compared to that of a single-based classifier. In addition, we found evidence of a relation between the observed performance contribution and the complexity of the classification problem. These observations allow us to suggest, from the classification problem complexity, that further work should be done to predict whether or not to use a dynamic selection approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Classification is a fundamental task in Pattern Recognition, which is the main reason why the past few decades have seen a vast number of research projects devoted to classification methods applied to different fields of the human activity. Although the methods available in the literature may differ in many respects, the latest research results lead to a common conclusion; creating a monolithic classifier to cover all the variability inherent to most pattern recognition problems is somewhat unfeasible.</p><p>With this in mind, many researchers have focused on Multiple Classifier Systems (MCSs), and consequently, many new solutions have been dedicated to each of the three possible MCS phases: (a) generation, (b) selection, and (c) integration, which are represented in Fig. <ref type="figure" target="#fig_0">1</ref>. In the first phase, a pool of classifiers is generated; in the second phase, one or a subset of these classifiers is selected, while in the last phase, a final decision is made based on the prediction(s) of the selected classifier(s). It is worth noting that such a representation is not unique, since the selection and integration phases may be facultative. For instance, one may find MCS where the whole pool of classifiers is used without any selection or systems where just one classifier is selected from the pool, making the integration phase unnecessary.</p><p>In a nutshell, recent contributions with respect to the first phase indicate that the most promising direction is to generate a pool of accurate and diverse classifiers. The authors in <ref type="bibr" target="#b0">[1]</ref> state that a necessary and sufficient condition for an ensemble of classifiers to be more accurate than any of its individual members is for the classifiers to be accurate and diverse. Dietterich <ref type="bibr" target="#b1">[2]</ref> explains that an accurate classifier has an error rate lower than the random guessing on new samples, while two classifiers are diverse if they make different errors on new samples. The rationale behind this is that the individual accurate classifiers in the pool may compete each other by making different and perhaps complementary errors. As for the selection phase, interesting results have been obtained by selecting specific classifiers for each test pattern, which characterizes a dynamic selection of classifiers, instead of using the same classifier for all of them (static selection). Moreover, additional contributions have been observed when ensembles are selected instead of just one single classifier. In such a case, Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr the outputs of the selected classifiers must be combined and the third phase of the MCS is necessary. The main contributions for this phase have been comprised of different strategies combining the classifiers and the assumption that the best integration choice is usually problem depended.</p><p>The focus of this paper is on the second phase of an MCS, particularly, the approaches based on dynamic selection (DS) of classifiers or ensembles of such classifiers. Despite the large number of DS methods available in the literature, there is no comprehensive study available to those wishing to explore the advantages of using such an approach. In addition, due to the high computational cost usually observed in the DS solutions, its application is often criticized. In fact, the decision as to whether or not to use DS is still an open question.</p><p>In this scenario, we have three research questions, namely:</p><p>1. Are the performance results of the DS methods reported in the literature significantly better than those obtained by a singlebased classifier approach? 2. Is there any relation between the classification complexity and the observed DS performance for a given problem? 3. Can we predict whether or not DS should be used for a given classification problem?</p><p>To answer these questions, we have reviewed several works on dynamic selection and performed a thorough statistical analysis of the results reported in the literature for different classification problems.</p><p>The motivation for investigating the possible existence of a relation between the DS contribution and the complexity of a classification problem is inspired by previous works in which the data complexity is used to better define the classifier models. An interesting work in this vein is presented in <ref type="bibr" target="#b2">[3]</ref>, in which the authors use geometrical characteristics of data to determine the classifier models. Two other interesting studies are presented in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, where the authors characterize the behavior of a specific classifier approach considering problems with different complexities.</p><p>With this in mind, our contribution is two-fold that (a) presents a comprehensive review of the main DS methods available in the literature, providing a taxonomy for them and (b) performs a further analysis of the DS results reported in the literature to determine when to apply DS.</p><p>This paper is organized as follows. After this brief introduction, Section 2 presents the main basic concepts and definitions related to the dynamic selection of classifiers. Section 3 presents the state of the art of DS methods and describes the suggested taxonomy. The algorithms of some key examples of each category are presented based on the same notation to facilitate comprehension. Section 4 presents further analysis of the DS results available in the literature, in a bid to answer our research questions. Finally, Section 5 presents the conclusions and further works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Basic concepts and definitions</head><p>This section presents the main concepts related to MCS and DS approaches, which represent the necessary background for the comprehension of the different works available in the literature.</p><p>The first concepts are related to the generation phase of the MCS. As described earlier, this first phase is responsible for the generation of a pool of base classifiers, considering a given strategy, to create diverse and accurate experts. A pool may be composed of homogeneous classifiers (same base classifiers) or heterogeneous classifiers (different base classifiers). In both cases, some diversity is expected. The idea is to generate classifiers that make different mistakes, and consequently, show some degree of complementarity. A comprehensive study of different diversity measures may be found in the work of Kuncheva and Whitaker <ref type="bibr" target="#b5">[6]</ref>. The schemes to provide diversity are categorized in <ref type="bibr" target="#b6">[7]</ref> as implicit, when there is no use of diversity measures during the generation process, or as explicit, in opposite cases.</p><p>In homogeneous pools, diversity is achieved by varying the information used to construct their elements, such as changing the initial parameters, using different subsets of training data (Bagging <ref type="bibr" target="#b7">[8]</ref>, Boosting <ref type="bibr" target="#b8">[9]</ref>), or using different feature subspaces (Random Subspace Selection <ref type="bibr" target="#b9">[10]</ref>). On the other hand, the basic idea behind heterogeneous pools is to obtain experts that differ in terms of the properties and concepts on which they are based.</p><p>Regarding the selection phase of an MCS, the main concepts are related to the type of selection and the notion of classifier competence. The type of selection may be static or dynamic, as explained earlier. The rationale behind the preference for dynamic over static selection is to select the most locally accurate classifiers for each unknown pattern. Both static and dynamic schemes may be devoted to classifier selection, providing a single classifier, or to ensemble selection, selecting a subset of classifiers from the pool.</p><p>Usually, the selection is done by estimating the competence of the classifiers available in the pool on local regions of the feature space. To that end, a partitioning process is commonly used during the training or testing phases of the MCS. In this process, the feature space is divided into different partitions, and the most capable classifiers for each of them are determined. In static selection methods, the partitioning is usually based on clustering or evolutionary algorithms, and it is executed during the training phase. This means that the classifier competence is always determined during the training phase of the system. Although it is possible to apply similar strategies for dynamic selection methods, what is mostly commonly seen with this approach is the use of a partitioning scheme based on the NN-rule to define the neighborhood of the unknown pattern in the feature space during the testing phase. In this case, the competence of each classifier is defined on a local region on the entire feature space represented by the training or validation dataset.</p><p>Regarding the competence measures, the literature reports several of them, which consider the classifiers either individually or in groups. This is the basis of the DS taxonomy proposed in the next section. It is worth noting that, basically, the individual-based measures most often take into account the classifier accuracy. However, the measures are conducted in different ways. For instance, one may find measures based on pure accuracy (overall local accuracy or local class accuracy) <ref type="bibr" target="#b10">[11]</ref>, ranking of classifiers <ref type="bibr" target="#b11">[12]</ref>, probabilistic information <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, classifier behavior calculated on output profiles <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, and oracle information <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Moreover, we may find measures that consider interactions among classifiers, such as diversity <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, ambiguity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref> or other grouping approaches <ref type="bibr" target="#b24">[25]</ref>.</p><p>The third phase of an MCS consists in applying the selected classifiers to recognize a given testing pattern. In cases where all classifiers are used (without selection) or when an ensemble is selected, a fusion strategy is necessary. For the integration of the classifier outputs, there are different schemes available in the literature. Complete details regarding the combination methods and their taxonomy are available in Jain et al. <ref type="bibr" target="#b25">[26]</ref> and in Kittler et al. <ref type="bibr" target="#b26">[27]</ref>. With respect to the experimental protocols used to evaluate a DS approach, one may usually find a comparison of the proposed method against the single best (SB) classifier, a combination of all classifiers in the pool (CC), the static selection (SS) using the same pool, and other DS approaches. In fact, it suggests that the minimum requirement for a DS method is to surpass the SB, CC, and any SS in the same pool. Moreover, the concept of oracle performance is usually present in the evaluation of the proposed methods. This means that the proposed method is compared against the upper limit in terms of performance of the pool of classifiers. The oracle performance is estimated by considering that if at least one classifier can correctly classify a particular test sample, then the pool can also do so as well.</p><p>Finally, since in the next section, we present the algorithms of some key works available in the literature, for the sake of comprehension, we have adopted the same notation. To that end, let Ω ¼ fω 1 ; ω 2 ; …; ω L g denote the set of classes of a hypothetical pattern recognition problem, while Tr, Va, and Te represent training, validation, and testing datasets, respectively. Moreover, let C ¼ fc 1 ; c 2 ; …; c M g be a pool composed of M diverse classifiers, and EoC ¼ fEoC 1 ; EoC 2 ; …; EoC N g be a pool of N diverse ensembles of classifiers. The unknown sample, or the testing sample, is referred to as t. In addition, let Ψ be the region of the feature space used to compute the competence of the base classifiers.</p><p>The next section presents the proposed taxonomy, the key works that represent each category, and some general statistics related to their performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">State of the art</head><p>It is worth noting that a categorization of the existing methods is not a trivial task since they present a large overlapped region. So, in order to better present the state of the art, we first outline a taxonomy in the context of MCS, which is inspired by the taxonomy of ensembles <ref type="bibr" target="#b6">[7]</ref>, and then we review the literature following the diagram depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. However, here, the focus is DS, and the main criterion is the source of information used to evaluate the competence of the base classifiers in the pool. The measures of competence are organized into two groups: individual-based and group-based. The former presents the measures wherein somehow, the individual performance of each classifier is the main source of information. This category was subdivided into other five subcategories, as follows: ranking, accuracy, probabilistic, behavior, and oracle-based. The latter is composed of the measures that consider the interaction among the elements in the pool. This category was subdivided into three subcategories, as follows: diversity, ambiguity, and data handling-based.</p><p>The next subsections describe each category by reviewing the most important methods available in the literature. However, before proceeding it is necessary to clarify some points. First, the proposed categorization contemplates only the DS methods where the competence of each base classifier, or its contribution inside an ensemble, is used to decide whether or not it will be selected. Second, even knowing the importance of selection mechanisms based on dynamic weighting of scores or mixture of experts <ref type="bibr" target="#b27">[28]</ref><ref type="bibr">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, they were not described here since they are dedicated to the use of a specific base classifier (multilayer perceptron neural networks). Third, it is known that the best strategy for calculating competence of the base classifiers in a DS method is to use a validation set. However, since we follow the original description of each algorithm can be observed in some cases that the training set is used instead. In addition, most of algorithms originally defined to select one classifier may be modified to select an ensemble by just applying an additional threshold on the proposed competence measure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Individual-based measures</head><p>In this category, the classifiers are selected based on their individual competence on the whole feature space represented by the training or validation set, or on part of it referred to as a local region. As described earlier, the local region may be defined in advance, during the training phase, by using partitioning techniques, or during the testing phase, by using the NN-rule to define the k-nearest-neighbors of the unknown pattern in the feature space also represented by the training or validation datasets. Basically, while the main source of information in this category is related to classifier accuracy, its subcategories however differ in their representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Ranking-based measures</head><p>The methods in this subcategory exploit a rank of the classifiers available in the pool. An interesting approach was proposed in 1993 by Sabourin et al. in <ref type="bibr" target="#b11">[12]</ref>, referred in this paper as the DSC-Rank (see Algorithm 1). It may be considered as one of the pioneers in DS. In their work, the ranking was done by estimating three parameters related to the correctness of the classifiers in the pool. The mutual information of these three parameters with correctness was estimated using part of their training dataset. Let X be a set of classifier parameters, like those suggested by the authors for their k-nearest neighbors base classifiers, the distance to the winner, the distance to the first non-winner, and the distance ratio of the first non-winner to the winner. In addition, let S be the classifier "success" variable, defined as S ¼ δðt; </p><p>The rationale behind the calculation of IðS; XÞ is to estimate the uncertainty in the decision that is resolved by observing each classifier parameter. After determining the most informative classifier parameters, the authors defined what they called a meta-pattern space (MP), represented by a subset of training samples, where for each element it is kept the values of the classifier parameters. During the classification step, the parameter values of the classifiers associated with the nearest neighbor of the test pattern in the meta-pattern space are ranked. The authors have considered a single parameter decision based on the largest parameter value. The classifier with the best ranking position is selected. Thus, it selects just one classifier and the partitioning process is done during the training phase, when MP is created.</p><formula xml:id="formula_1">Algorithm 1. DSC-Rank method.</formula><p>Input the pool of classifiers C; the set of classifier parameters X;</p><p>the datasets Tr and Te; Output c n t , the most promising classifier for each testing sample t in Te; Compute S ¼ ðt; oÞ as the classifiers "success" variable using the training samples in Tr; Compute IðS; XÞ as the mutual information between X and S using the training samples in Tr;</p><p>Determine X 0 as the most informative classifier parameters based on IðS; XÞ; Create the meta-pattern space MP, as a subset of Tr with the corresponding values of the parameters in X 0 ; for each testing sample t in Te do Apply NN-rule to find ψ as the nearest neighbor of the unknown sample t in MP; Rank the classifiers based on the parameter values associated to ψ; Select c n t as the classifier in the best ranking position; Use c n t to classify t; end for</p><p>The second example of this category is the DS-MR proposed by Woods et al. in <ref type="bibr" target="#b10">[11]</ref>. In fact, it is a simplification of the original DSC-Rank method. Different from the original, the modified method ranks the classifiers based on local class accuracy calculated as the number of correct classified samples on the k-nearest neighbors of the unknown pattern in the training set. During classification, a local region of the feature space near the test pattern is defined, the rank is constructed, and the best classifier is selected. Another difference from the original ranking approach is that the partitioning process used to define the local region in estimating the classifier competence is done during the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Accuracy-based measures</head><p>Here, the main characteristic is the estimation of the classifier accuracy, overall or local, as a simple percentage of corrected classified samples. The two variations of the DS-LA method proposed in <ref type="bibr" target="#b10">[11]</ref>  In the second one, they calculate the local class accuracy (LCA), as shown in Algorithm 3. The LCA is estimated for each base classifier as the percentage of correct classifications within the local region, but considering only those examples where the classifier has given the same class as the one it gives for the unknown pattern. In both versions of the DS-LA method, OLA and LCA-based, the partitioning of the feature space is defined based on the k-nearest neighbors of the unknown pattern in the training dataset during the testing phase. Moreover, only one classifier is selected from the pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Probabilistic-based measures</head><p>More than just estimating the classifier accuracy based on a simple percentage of corrected classified samples, the methods in this subcategory use some probabilistic representation. Two interesting schemes, named A Priori and A Posteriori selection, were proposed in <ref type="bibr" target="#b12">[13]</ref>. Both schemes select a single classifier from the pool based on a local region defined by the k-nearest neighbors of the test pattern in the training set during the testing phase.</p><p>In the A Priori method, a classifier is selected based on its accuracy within the local region, without considering the class assigned to the unknown pattern. This measure of classifier accuracy is calculated as the class posterior probability of the classifier c j on the neighborhood Ψ of the unknown sample t.</p><p>As we can see in Eq. ( <ref type="formula" target="#formula_2">4</ref>), the class posterior probability is weighted by δ i , which represents the Euclidian distance between the sample ψ i and the unknown pattern t.</p><p>Similarly, in the A Posteriori method, local accuracies are estimated using the class posterior probabilities and the distances of the samples in the defined local region (neighborhood of size K).</p><p>However, Eq. <ref type="bibr" target="#b4">(5)</ref> shows that in this measure the class ω l assigned by the classifier c j to the unknown sample t is taken into account. Both methods are presented in Algorithm 4, where the Threshold value suggested by the authors was 0.1.</p><formula xml:id="formula_2">pðcorrect j Þ ¼ ∑ K i ¼ 1 P j ðω l jψ i A ω l Þ Á δ i ∑ K i ¼ 1 δ i<label>ð4Þ</label></formula><formula xml:id="formula_3">pðcorrect j |c j ðtÞ ¼ ω l Þ ¼ ∑ ψ i A ω l P j ðω l jψ i Þ Á δ i ∑ K i ¼ 1 P j ðω l jψ i Þ Á δ i<label>ð5Þ</label></formula><formula xml:id="formula_4">δ i ¼ 1 Euclidian Distanceðψ i ; tÞ<label>ð6Þ</label></formula><formula xml:id="formula_5">Algorithm 4. A Priori/A Posteriori method.</formula><p>Input the pool of classifiers C; the datasets Tr and Te; the neighborhood size K; Output c n t , the most promising classifier for each unknown sample t in Te; for each testing sample t in Te do Find Ψ as the K nearest neighbors of the sample t in Tr;</p><p>for each classifier c j in C do Compute pðcorrect j Þ on Ψ by using Eq. ( <ref type="formula" target="#formula_2">4</ref>) or Eq. ( <ref type="formula" target="#formula_3">5</ref>);  <ref type="bibr" target="#b13">[14]</ref> proposed two interesting classifier competence measures. The first one (DES-M1) is an interesting accuracy-based approach, where the competence of each classifier for a given unknown pattern is computed based on a potential function model which is able to estimate the classifier capability of doing the correct classification. The competence of each classifier is computed considering the support it gives for the correct class of each validation sample. The second measure named DES-M2 is a probabilistic-based example, where the authors use the probability of correct classification of a probabilistic reference classifier (PRC). Both measures inspired by the work described in <ref type="bibr" target="#b30">[31]</ref> differ from the majority of the methods available in the literature, since the competence of each classifier is estimated using the whole validation set during the testing phase. The ideas presented in DES-M2 were extended and improved in <ref type="bibr" target="#b31">[32]</ref>, where the main contribution is the modeling scheme based on a unified model representing the whole vector of class supports. Different variants of the proposed method were evaluated considering the selection of classifiers and ensembles. The best results were achieved by the system named DES-CS, which selects an ensemble of classifiers, considers continuous-valued outputs and weighted class supports.</p><formula xml:id="formula_6">if ð pðcorrect j Þ Z 0:5Þ then CS ¼ CS [ c j ; end if end for pðcorrect m Þ ¼ max j ð pðcorrect j ÞÞ; c m ¼ arg max j ð</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Behavior-based measures</head><p>The methods in this subcategory are based on some kind of behavior analysis using classifier predictions as information sources. Inspired by the Behavior-Knowledge Space (BKS) proposed by <ref type="bibr">Huang et al. in 1995 [33]</ref>, Giacinto et al., in <ref type="bibr" target="#b14">[15]</ref>, propose a dynamic classifier selection based on multiple classifier behavior (MCB), named DS-MCB (see <ref type="bibr">Algorithm 5)</ref>. They estimate the MCB using a similarity function to measure the degree of similarity of the output profiles of all base classifiers. First, a local region Ψ is defined as the k-nearest neighbors of the unknown pattern in the training set. Then, the similarity function is used as a filter to preselect from Ψ, the samples for which the classifiers present similar behavior to that observed for the unknown sample t. The remaining samples are used to select the most accurate classifier by using local classifier accuracy (OLA). Finally, if the selected classifier is significantly better than the others in the pool based on a defined threshold value, then it is used to classify the unknown sample. Otherwise, all classifiers are combined using the majority voting rule (MVR).  The core of this algorithm is the vector named MCB (Multiple Classifier Behavior) which can be defined as</p><formula xml:id="formula_7">MCB ψ ¼ fC 1 ðψ Þ; C 2 ðψ Þ; …; C M ðψ Þg.</formula><p>It contains the class labels assigned to the sample ψ by the M classifiers in the pool. The measure of similarity Sim can be defined as</p><formula xml:id="formula_8">Simðψ 1 ; ψ 2 Þ ¼ 1 M ∑ M i ¼ 1 T i ðψ 1 ; ψ 2 Þ ð<label>7Þ</label></formula><p>where</p><formula xml:id="formula_9">T i ðψ 1 ; ψ 2 Þ ¼ 1 if C i ðψ 1 Þ ¼ C i ðψ 2 Þ 0 if C i ðψ 1 Þ a C i ðψ 2 Þ (<label>ð8Þ</label></formula><p>Another interesting method in this category is the DSA-C proposed by Cavalin et al. in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b16">[17]</ref>. Different from the previously described works, the DSA-C method may select one or a subset of ensembles. To that end, first they computed the output profiles of each available ensemble using a validation set. Different approaches for estimating the similarity between output profiles were used, while the selection of ensembles was done by choosing those with output profiles most similar to the output profile estimated for the testing pattern.</p><p>Nabiha et al. <ref type="bibr" target="#b15">[16]</ref> proposed the dynamic selection of ensembles (DECS-LA) by calculating the reliability of each base classifier over a validation set during the testing phase. The reliability of each classifier is derived from its confusion matrix obtained over the validation set. In the selection step, each base classifier is evaluated considering its accuracy on a local region close to the unknown pattern combined with its reliability, which may be considered as a kind of probability that the local behavior is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Oracle-based measures</head><p>To some extent, the methods here use the concept of the oracle, i.e., the one who may provide wise counsel. In the linear random oracle proposed by Kuncheva <ref type="bibr" target="#b17">[18]</ref>, each classifier in the pool has a subset with two sub-classifiers and an oracle. The oracle in their work is a random linear function that is responsible for deciding which of two possible sub-classifiers will be used for an unknown pattern. After consulting the oracle of each base classifier, the subclassifiers selected are combined. Another key work in this category is the k-nearest-oracles (KNORA) method proposed in <ref type="bibr" target="#b18">[19]</ref>. The oracles are represented by the k-nearest neighbors of the unknown pattern in a validation set, where the classifiers that correctly classify each sample are known. This validation set is a kind of meta-space (sVa in Algorithms 6 and 7). It is used as the source of "oracles". By finding the k-nearest-neighbors of the test pattern t in sVa, we can select the classifiers that correctly recognize these neighbors to classify t. Thus, the oracles may suggest the classifiers that must be used to recognize the unknown pattern. The authors evaluated different schemes to select the classifiers suggested by the oracles. The most promising strategies were Knora-Eliminate (KNE) and Knora-Union (KNU). The former selects only those classifiers which are able to correctly recognize the entire neighborhood of the testing pattern (see Algorithm 6), while the later selects all classifiers that are able to correctly recognize at least one sample in the neighborhood. As we can see in Algorithm 7, the Knora-Union strategy considers that a classifier can participate in the ensemble more than once if it correctly classifies more than one neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group-based measures</head><p>The methods in this category combine the accuracy of the base classifiers with some information related to the interaction among them, such as diversity, ambiguity or complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Diversity-based measures</head><p>In 2003, Shin et al. in <ref type="bibr" target="#b19">[20]</ref> used a clustering process based on the coefficients of their set of base logistic regression classifiers to create clusters of classifiers. Two clusters of classifiers were selected on the local region of the feature space close to the unknown pattern, one based on accuracy and the other on diversity. The definition of the local regions was done based on the NN-rule on the validation set during the testing phase. In fact, they modified the DS-LA approach proposed in <ref type="bibr" target="#b10">[11]</ref> by considering the selection of ensembles of classifiers both in terms of accuracy and error diversity.</p><p>Santana et al., in <ref type="bibr" target="#b20">[21]</ref>, combined accuracy and diversity to build ensembles. The classifiers were sorted in decreasing order of accuracy and in increasing order of diversity. Two variations were presented. In the DS-KNN, accuracy and diversity are calculated in the local region defined by the k-nearest neighbors of the unknown pattern in the validation set, while in the DS-Cluster, the partitioning process is done during the training phase, when a clustering process is used to divide the validation set into clusters where the most promising classifiers will be associated. The diversity is calculated in a pairwise fashion using the double fault diversity measure, the idea being to select the classifiers more diverse among those more accurate (see <ref type="bibr">Algorithm 8)</ref>  <ref type="bibr" target="#b21">[22]</ref> considered the use of the diversity measure in the approach based on the randomized reference model proposed in <ref type="bibr" target="#b13">[14]</ref>, the proposed DES-CD method, first selects the most accurate classifier to start the ensemble, and then other classifiers are added to the ensemble as they improve the ensemble diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Ambiguity-based measures</head><p>The methods in this category, which are different from diversity, use consensus. One of the pioneers of DS, Srihari et al. propose a classifier selection strategy in 1994 <ref type="bibr" target="#b22">[23]</ref>, based on the consensus of the classifiers on the top choices.</p><p>In the same vein, the authors in <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b23">[24]</ref> describe the A and the DSA methods, respectively. Both methods select the ensemble of classifiers from a population of highly accurate ensembles with the lowest ambiguity among its members. The algorithm of the DSA method is presented in Algorithm 9. With the use of consensus, the authors observed an increase in the generalization performance since the level of confidence of classification had increased. For each test pattern, they selected, from a pool of diverse ensembles, the one showing the highest consensus in terms of the outputs provided by its members. To this end, the ambiguity of the ith classifier of the ensemble EoC j for the sample ψ was determined as</p><formula xml:id="formula_10">a i ðψ Þ ¼ 0 if c i ðψ Þ ¼ EoC j ðψ Þ 1 otherwise<label>ð9Þ</label></formula><p>while the ambiguity A of the ensemble EoC j , considering the neighborhood Ψ, was calculated as denoted in Eq. ( <ref type="formula" target="#formula_11">10</ref>), in which jΨ j and jEoC j j are the cardinalities of these sets. As one may see in Eq. ( <ref type="formula" target="#formula_10">9</ref>) each classifier output is compared with the ensemble output, which represents the combined decision of their classifiers. In <ref type="bibr" target="#b16">[17]</ref>, the DSA method was improved through dynamic multistage organization and the use of contextual information. The authors organized the classification dynamically in different layers, according to the test patterns. In addition, they expanded the concept of consensus by considering additional information related to all classes involved instead of considering just the outputs of the most voted and the second most voted ones, in selecting the ensembles.</p><formula xml:id="formula_11">A ¼ 1 jΨ j Á jEoC j j ∑ i A EoC j ∑ ψ A Ψ a i ðψ Þ ð<label>10Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Data handling-based measure</head><p>Some authors used different pieces of group information. That is the case of the method proposed in <ref type="bibr" target="#b24">[25]</ref>, in which the authors use an adaptive classifier ensemble selection based on the group method of data handling theory (GMDH). A multivariate analysis theory for complex systems modeling firstly described in <ref type="bibr" target="#b35">[36]</ref>. Their dynamic ensemble selection algorithm, named GDES, selects the ensemble with the optimal complexity for each test pattern from the initial pool of classifiers, also given the combination weights among the classifiers (see <ref type="bibr">Algorithm 10)</ref>. To that end, the algorithm deals with the pool of classifiers and a local region of the training set related to the k-nearest neighbors of the test pattern. for each sample ψ i in Ψ do</p><formula xml:id="formula_12">O i ¼ c i ðψ i Þ</formula><p>, the output of the ith classifier for the sample</p><formula xml:id="formula_13">ψ i ;</formula><p>end for end for Compute MOCðO i Þ as the model with optimal complexity by using Ψ, L and the GMDH theory <ref type="bibr" target="#b35">[36]</ref>;</p><p>Select the ensemble EoC n t and the weights for each classifier using MOCðo i Þ; Use the ensemble EoC n t to classify t; end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Summary</head><p>Table <ref type="table" target="#tab_7">1</ref> presents the main DS methods available in the literature and discussed in this paper. The first six columns lay out all the overall features of each method. In the table, we can find any given category based on the proposed taxonomy; the type of selection in terms of whether a single classifier or an ensemble of classifiers is selected; the kind of pool created (Homogeneous or Heterogeneous) and the number of classifiers in it. In addition, we can find out when and what kind of partitioning process is used; for instance, during the training phase based on clustering or other scheme, or during the testing phase based on the NN-Rule. The last five columns cover the experiments used to evaluate each method. They provide information related to the quantity and size of the datasets used in the experiments, as well as, the number of wins, ties and losses of the DS method against the SB, CC, SS, and other related methods. These numbers take into account the experiments reported in the original papers, as well as those done by other researchers.</p><p>The rationale behind the computing of the number of wins, ties and losses between these approaches is that using such information allows us to compare them even if they were originally evaluated through different experimental protocols. Based on these numbers, Fig. <ref type="figure" target="#fig_7">3</ref> let us compare the performance of each DS method with respect to SB, while Fig. <ref type="figure" target="#fig_8">4</ref> shows a similar comparison of DS against the CC results. In both cases, the data are not normalized in order to show the most cited methods, and consequently, the most evaluated ones.</p><p>Although such information may provide us with some insight on each specific method, the main objective of our study is to evaluate the DS approach in general. In that respect, Fig. <ref type="figure" target="#fig_2">5</ref> shows the overall performance of DS in terms of percentage of wins, ties, and losses when compared against the usual alternative approaches. The last bar (General) represents all these alternatives (SB, CC and SS) together. The DS approach has shown better results in 59%, 56% and 68% of the cases when compared to SB, CC, and SS, respectively.</p><p>These general statistics have proven to be positive for the DS approach. However, they do not give us any clue about the significance of the results or about when such an approach must be used. To accomplish that, a deeper analysis is done in the next section where a two-step methodology is executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A further analysis</head><p>A preliminary analysis of the last section based on several experiments available in the literature suggests that most often, DS will win when compared to the usual alternative approaches. However, it is worth emphasizing that the results also show that the "no free lunch" theorem <ref type="bibr" target="#b40">[41]</ref> holds for such analyses in the sense that DS is not universally better. From all the experiments considered, it is clear that the only way one classification approach can outperform another is if it is specialized to the problem at hand.</p><p>To answer our research questions, a deeper analysis is necessary. To that end, a two-step methodology is executed in this section. Basically, the idea is to understand how significant the DS performance contribution is when compared to its alternative approaches, and we try to reveal how such a contribution could be related to the problem complexity. In the first step, different non-parametric statistic tests are used to evaluate the significance of the DS results when compared to SB, CC and SS, while in the second one, a set of complexity measures is used to describe the difficulty of a classification problem and relate it to the observed DS performance.</p><p>The experimental protocol considers the comparison of DS, SB, CC and SS approaches based on the computed wins, ties and losses in two sets of experiments. The first set (S1) is composed of all experiments in Table 1, while the second set (S2) is composed of experiments on 12 datasets that represent the intersection among all the studied works. Set S2 was constructed based on a careful search for experiments based on the same datasets divided into similar partitions for training and testing. We successfully found 12 datasets that appear in different works. Table <ref type="table" target="#tab_10">2</ref> presents these datasets and their main features. The following works were the sources of experimental results for S2 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Significance of the results</head><p>In the first step of the methodology used, two non-parametric tests followed by a post hoc test are executed. The objective is to answer our first research question related to the significance of the DS results when compared to a single-based classifier.</p><p>The first non-parametric statistic is the simple and well-known sign test <ref type="bibr" target="#b42">[43]</ref>. It was calculated on the computed wins, ties and losses reported in Table <ref type="table" target="#tab_7">1</ref>, i.e., the set of experiments S1. Let us consider the comparison of DS and SB. The number of wins, ties and losses is 467, 45 and 273, respectively, amounting to 785 experiments. First, we must add the ties to both wins and losses. Thus, we have 512 wins and 318 losses. In this case, the null hypothesis (H 0 ) is that the DS and SB approaches are equally successful. To reject the null hypothesis, and show that DS performance is significantly better than SB, DS must satisfy Eq. ( <ref type="formula" target="#formula_14">11</ref>), in which n is the total number of experiments, n w is the computed DS wins, and Z α represents the z statistic at significance level α.</p><formula xml:id="formula_14">n w Z n 2 þ Z α ffiffiffi n p 2<label>ð11Þ</label></formula><p>If we consider α¼0.05, then Z α ¼ 1:645. With this setup, the null hypothesis is rejected since 512 4 416. The final conclusion is that with a significance level of α¼0.05, DS performs better than the SB approach. A similar evaluation may be done by considering the comparison of DS against CC and SS. The computed DS wins, ties and losses are (403, 23, 308) and (84, 0, 39), against CC and SS, respectively. In both cases, the null hypothesis is rejected. When compared against CC, the results are ð426 4389:2Þ, while for SS, they are ð84 4 70:6Þ. In addition to this simple non-parametric test, we performed a more comprehensive evaluation using the experiments on set S2. </p><p>[13] 3S (3,0,0) (0,0,3) NA (4,1,4) <ref type="bibr" target="#b36">[37]</ref> 2S (13,0,5) NA (6,0,12) (0,0,18) <ref type="bibr" target="#b20">[21]</ref> 2S    </p><formula xml:id="formula_19">[35] A Ambiguity ES Hom(100) Training/Opt - 3S NA<label>(18,0,0) NA (16,0,2) (18,0,0)</label></formula><formula xml:id="formula_20">NN-Rule - 6S/1L<label>(2,0,1) (3,0,0) (3,0,0) [19] KNORA Oracle ES Hom(10,100) Testing/</label></formula><p>Testing/allVs -6S (12,0,0) (12,0,0) NA (</p><p>[32] DES-CS Probabilistic ES Hom(50)/Het <ref type="bibr" target="#b9">(10)</ref> Training/allVs -22S (39,0,5) (37,0,7) NA (242,0, For each dataset, we found 10 experiments in which the SB, CC and DS approaches were compared. Except for the SH dataset for which only nine experiments were found. Unfortunately, not enough results were found to allow a comparison against the SS approach. With the set of experiments for each dataset on hand, we reformulated our null-hypothesis (H 0 ) to state that the three classification approaches, SB, CC and DS, perform equally well. In other words, there is no significant difference between their results.</p><p>Friedman's test (F t ) <ref type="bibr" target="#b42">[43]</ref> was performed and the results are shown in Table <ref type="table" target="#tab_11">3</ref>. Since we are comparing three approaches, the degree of freedom is 2. The level of significance (α) was defined as 0.05, while the corresponding critical value (p α ) is 5.99. In the same table, beyond the average rank of each classification approach, it is possible to find out whether or not the nullhypotheses is rejected.</p><p>As we may see, at a level of significance of α¼0.05, there is enough evidence to conclude that, for the majority of the datasets, there is a significant difference in the accuracy among the three classification approaches, except for the three datasets (TE, SA and FE), where F t o p α , and consequently, the null-hypothesis was not rejected.</p><p>Friedman's test only shows whether or not there is a significant difference between the three approaches, but it does not show  where the differences may be. To that end, we have performed a post hoc Nemenyi test <ref type="bibr" target="#b42">[43]</ref>, which compares the three approaches (SB, CC and DS) in a pairwise fashion. Fig. <ref type="figure" target="#fig_10">6</ref> shows a graphical representation of post hoc Nemenyi test results of the compared approaches for each dataset with the ranks given in Table <ref type="table" target="#tab_11">3</ref>. The numbers above the main line represent the average ranks, while CD is the critical difference for statistical significance. The methods with no significant difference are connected by lines. The CD is 1.10 for the SH dataset, and 1.05 for all other datasets. The performance of two classifiers is significantly different if the corresponding average ranks differ by at least the critical difference.</p><p>As already detected with Friedman's test for the FE, SA and TE datasets, there is no significant difference between the three approaches. On the other hand, for four datasets (SH, LR, ND and NL), we may observe a significant and positive difference between DS and SB; positive in the sense that DS is "better" than SB. For the same datasets, DS did not present a significant difference when compared to the CC approach. However, it is worth noting that we are not considering any other possible parameter of comparison between DS and CC, such as a reduction in terms of number of classifiers, for instance.</p><p>For another set of datasets (IS, WC, PD, LD and W), we observed no significant difference between DS and SB, but it could be seen that DS is significantly better than CC in those cases. This would suggest that the pools generated were composed of many weak classifiers.</p><p>Fig. <ref type="figure" target="#fig_11">7</ref> was obtained by plotting the differences between the DS and SB ranks used for the post hoc Nemenyi test. In this case, which is different from the usual graphical representation adopted in Fig. <ref type="figure" target="#fig_10">6</ref>, the numbers above the main line represent the difference between ranks. Thus, high numbers mean a more significant difference between DS and SB. The objective is to show a graphical representation of the impact of the DS for each dataset when compared with the corresponding SB approach. As can be seen, the best impact was observed for the dataset NL (1.95) and the worst case was the dataset FE (0.2). In addition, this figure shows that different base classifiers were used in the experiments.   From the results observed, we may confirm that, in general, there exists a significant difference between the performance of DS, SB, CC and SS. The results of the sign test have shown that. In addition, Friedman's test showed that in 9 from 12 datasets there is a significant difference between DS, SB and CC. A deeper pairwise analysis using the post hoc Nemenyi test, showed that in four of the remaining nine datasets, there was a significant performance contribution using the DS approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification difficulty</head><p>This section describes the second step of our methodology. The objective is to answer our second research question as to whether or not there is a relation between the classification complexity and the observed DS performance.</p><p>A first attempt to empirically characterize the DS approach as an interesting alternative to deal with complex problems was carried out in <ref type="bibr" target="#b16">[17]</ref>. With this in mind, the authors divided the large digit dataset available on NIST-SD19 into subsets of different sizes to create five scenarios, varying from few samples for training (5000) to a large training set (180,000). In their experiments, two monolithic classifiers based on SVM and MLP were compared against their DS method. They observed that when enough data is available, the trained monolithic classifiers perform better than the proposed DS method. Thus, they suggested that the DS approach is more suitable when enough data is not available to represent the whole variability of the learned pattern.</p><p>Inspired by that observation, we decided to go further by investigating evidence of a clear correlation between the performance of DS methods and the classification difficulty. We then implemented a set of complexity measures for classification problems <ref type="bibr" target="#b2">[3]</ref>, composed of two measures of overlap between single feature values (F1 and F2), two measures of separability of classes (N2 and N3) and one measure related to the dimensionality of the dataset (T2). The measures used are described below based on their generalization to problems with multiple classes, as in <ref type="bibr" target="#b3">[4]</ref>.</p><p>1. Fisher's Discriminant Ratio (F 1): this well-known measure of class overlapping is calculated over each single feature dimension as denoted in Eq. ( <ref type="formula" target="#formula_25">12</ref>), where M is the number of classes and μ is the overall mean, while n i , μ i and s j i are the number of samples, the mean and the jth sample of the class i, respectively. In this generalization of F1, Ψ is the Euclidian distance.</p><p>A high F1 value indicates the presence of discriminating features and hence a classification problem easier.</p><formula xml:id="formula_25">F1 ¼ ∑ M i ¼ 1 n i Á δðμ; μ i Þ ∑ M i ¼ 1 ∑ n i i ¼ 1 δðs i j ; μ i Þ<label>ð12Þ</label></formula><p>2. Volume of Overlap Region (F 2): this measure conducts a pairwise calculation of the overlap between the conditional distribution of classes. As can be observed in Eq. ( <ref type="formula" target="#formula_26">13</ref>), the overlap, considering two classes, c i and c j , and a T-dimension feature space, is calculated by finding the minimum and maximum values of each feature f k for both classes. The ratio between the range of the feature values for each class is normalized by the length of the total range considering both  <ref type="table" target="#tab_11">3</ref>. For each dataset, the numbers on the main line represent the average ranks and the CD is the critical difference for statistical significance. Methods with no significant difference are connected by additional lines. classes. The overlap region is estimated as the product of the normalized ratio obtained for all features. The generalization for multiple classes considers the sum of the overlapped regions calculated for each pair of classes. A small overlap (F2) value suggests a classification problem easier.</p><formula xml:id="formula_26">F2 ¼ ∑ ðc i c j Þ ∏ T k ¼ 1 min½maxðf k ; c i Þ; maxðf k ; c j Þ À max½minðf k ; c i Þ; minðf k ; c j Þ max½maxðf k ; c i Þ; maxðf k ; c j ÞÀ min½minðf k ; c i Þ; minðf k ; c j Þ<label>ð13Þ</label></formula><p>3. Non-parametric Separability of Classes (N 2, N 3): the first measure, referred to as N2 in the literature, compares the intraclass dispersion with the interclass separability, as denoted in Eq. ( <ref type="formula" target="#formula_27">14</ref>). For this purpose, let η intra 1 ðs i Þ and η inter 1 ðs i Þ denote the intra-and inter-class nearest neighbors of the sample s i , while Ψ represents the Euclidian distance. As can be observed, N2 calculates the ratio between the intra-and the inter-class dispersions. A small N2 value suggests high separability, and consequently, a classification problem easier. The second measure of separability (N3) is related to the estimated error rate of the 1-NN rule by using the leaving-one-out scheme.</p><formula xml:id="formula_27">N2 ¼ ∑ N i δðη intra 1 ðs i Þ; s i Þ ∑ N i δðη inter 1 ðs i Þ; s i Þ<label>ð14Þ</label></formula><p>4. Density per Dimension (T 2): this measure describes the density of spatial distributions of samples by computing the average number of instances per dimension. Referred to as T2 in the literature, it is calculated as shown in Eq. ( <ref type="formula" target="#formula_28">15</ref>), where N and T are the number of samples and features, respectively, of a classification problem. Similar to F1, a high T2 value suggests a classification problem easier.</p><formula xml:id="formula_28">T2 ¼ N T<label>ð15Þ</label></formula><p>After implementing the previously described complexity measures, they were applied to the datasets described in Table <ref type="table" target="#tab_10">2</ref>. The value of each measure for these datasets is shown in Table <ref type="table" target="#tab_12">4</ref>.</p><p>As can be seen, the same problem may be taken as difficult with respect to one measure, but easy with respect to another. The reason is that the different measures consider different aspects of the classification problems. However, some interesting analysis may be done when they are combined. For instance, let us consider the measure values related to the Wine (W), Liver-Disorders (LD) and Texture (TE) datasets. Based on the class overlapping calculated over each feature dimension (measure F1), W should be considered as an easy problem. Its value for this measure is the second higher. However, the values of measures N3 and T2 for the same dataset show the contrary. Based on N3, the error rate of the NN classifier is high for W (close to 23%), while the number of samples per dimension is very low (around 13). Despite the fact that we have a small overlap of the feature range values among classes shown by the value F1, the dataset W has few samples, making it more difficult to be learned. For the LD dataset, all measures seem to agree on its complexity: it is in fact the most complex problem among those in Table <ref type="table" target="#tab_12">4</ref>. On the other hand, TE is a very easy problem.</p><p>Although some interesting assumptions may be made based on these results, our question here is whether or not there is a relation between the observed DS performance and the classification difficulty.</p><p>In this perspective, except for the F2 measure, we carried out an analysis in which the complexity measures were combined in a pairwise fashion. The reason for excluding F2 was that it reflects the problem dimensionality, making it difficult to compare problems having overly different numbers of features. Fig. <ref type="figure" target="#fig_12">8</ref> plots the pairwise combinations F1 Â N2, F1 Â N3, F1 Â T2, N2 Â N3, N2 Â T2, and N3 Â T2. The datasets represented by red markers (SH, LR, ND and NL) in the plots are those for which we observed some significant contribution of the DS when compared to the corresponding SB approach. As can be seen, F1 Â N2, F1 Â N3 and F1 Â T2 showed some interesting results. The datasets mentioned show low F1 values (difficult), in addition to low values of N2 (easy) and N3 (easy). We see this in the plot corresponding to F1 Â N3. Moreover, in the plot corresponding to F1 Â T2, it can be seen that the datasets where DS did not show a significant contribution present a low T2 value (difficult), except for one outlier (FE dataset).</p><p>The significant DS contribution observed for the datasets SH, LR, ND and NL may be explained using F1, N3 and T2. For these datasets, F1 suggests a high difficulty related to the overlap among the ranges of the feature values per class (F1 r0:8). On the other hand, N3 and T2 suggest that they are easy problems. A low N3 means an easy problem for an NN classifier, since this measure represents the NN error rate using a leave-one-out strategy. A high T2 implies that there are more samples to deal with the problem variability. Thus, a low F1 combined with a low N3 and high T2 is an interesting observation when the goal is to use a DS approach.</p><p>As with SH, LR, ND and NL, the PD and LD datasets show a very low F1. However, they show a very high N3 and a very low T2. This means that they are difficult for the three measures. For this type of dataset, the DS approach seems unsuitable. The same assumption may be made for datasets with high F1 values.</p><p>Fig. <ref type="figure" target="#fig_13">9</ref> presents the evaluated datasets plotted in the space formed by the measures F1, N3 and T2. The SH, LR, ND and NL datasets are represented by red markers. It is possible to see that these datasets appear close to the origin in terms of the F1 and N3 axes, and usually present a high T2 value.</p><p>Thus, from this analysis, we may conclude that a relation exists between the data complexity and the observed contribution of the DS approach. In addition, we can also conclude that this relation is based on some intrinsic aspects of the classification problem, more than just the size of the problem (number of samples, classes and features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future works</head><p>In this paper, we have presented the state of the art of DS methods, proposing a taxonomy for them. In addition, we have revisited the main basic concepts related to DS and presented the algorithms of some key methods available in the literature. This review has shown that different selection schemes have been   <ref type="table" target="#tab_12">4</ref>. proposed, and basically differ in terms of the source of information used to evaluate the competence of the available classifiers in the pool for a given unknown sample.</p><p>As expected, the study performed does not allow us to point out the best DS method. On the contrary, it is seen that there is no evidence that one specific method may surpass all the others for any classification problem. However, it can be observed that simpler selection schemes (KNORA and DS-LCA) may provide similar, or sometimes even better, classification performances than the sophisticated ones (GDES, DSA and DSA-C).</p><p>Despite the importance of the literature review presented, the research questions addressed in this paper were related to when DS is applied. A further analysis of the reported results of the studied DS methods showed that for some classification problems, the DS contribution is statistically significant. In addition, it showed that there is some evidence of a relation between the DS performance contribution and the corresponding complexity of the classification problem. Thus, we may conclude that it is possible to predict when to use or not DS.</p><p>As observed, the DS has shown better results for classification problems presenting low F1 values, combined with high N3 and T2 values. Such an observed relation between the problem complexity and the DS contribution confirms the Ho et al. expectation <ref type="bibr" target="#b2">[3]</ref>. They suggested that complexity measures may be used as a guide for static or dynamic selection of classifiers. In our case, we suggest that they may be used to determine when to apply DS. However, as also cautioned in their work, the extrapolation of the observations must be done with extreme care. The reason for this is that the observations are based on a tiny set of classification problems. Further work can be done, considering these three complexity measures and a huge set of classification problems in order to model a meta-classifier dedicated to determining whether or not to use a DS approach for a specific classification problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The possible phases of a Multiple Classifier System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Proposed DS Taxonomy in the context of MCS.</figDesc><graphic coords="3,92.52,411.11,420.24,321.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 5 .</head><label>5</label><figDesc>DS-MCB method.Input the set of classes Ω; the pool of classifiers C; the datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 10 .</head><label>10</label><figDesc>GDES-based method. Input pool of classifiers C; the datasets Tr and Te, the neighborhood size K; the set of labels of the training samples L; Output EoC n t , an ensemble of classifiers for each testing sample t in Te; for each testing sample t in Te do Find Ψ as the K nearest neighbors of the test sample t on Tr; for each classifier c i in C do o i ¼ c i ðtÞ, the output of the ith classifier for the sample t;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Hom( 20 )</head><label>20</label><figDesc>¼ pool of 20 homogeneous classifiers, Het(10) ¼ pool of 10 heterogeneous classifiers, Hom (10,100) pools with 10 and 100 classifiers, CS¼ classifier selection, ES ¼ ensemble selection, Opt¼ optimization, allVs ¼ all validation samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of the main DS methods in terms of number of wins, ties and losses with respect to the single best (SB) classifier.</figDesc><graphic coords="10,82.70,58.64,420.24,250.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of the main DS methods in terms of number of wins, ties and losses with respect to the combination of all classifiers (CC) in the pool.</figDesc><graphic coords="10,82.70,338.46,420.24,264.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of the DS methods in terms of percentage of wins, ties and loses when compared to the single best classifiers (SB), the fusion of all classifiers (CC), static selection approach (SS), and any alternative solution (General).</figDesc><graphic coords="11,92.52,58.64,420.24,248.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Graphical representation of post hoc Nemenyi test results of compared methods for each dataset with ranks given in Table3. For each dataset, the numbers on the main line represent the average ranks and the CD is the critical difference for statistical significance. Methods with no significant difference are connected by additional lines.</figDesc><graphic coords="12,112.73,58.64,360.00,260.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. The differences between the DS and SB ranks. For each dataset, the numbers on the main line represent the rank difference between DS and SB approaches. The CD is the critical difference for statistical significance.</figDesc><graphic coords="12,38.21,362.00,240.24,130.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Pairwise combination of the complexity measures F1, N2, N3 and T2, considering the datasets presented in Table4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. 3D graphical representation of the datasets based on the measures F1, N3 and T2. The SH, LR, ND and NL are in red. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc><graphic coords="14,130.73,512.84,324.00,211.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>oÞ, where t is the true label of a given training sample, and o is the classifier output. The mutual information between S and X can be calculated as</figDesc><table><row><cell cols="4">IðS; XÞ ¼ HðS; XÞÀHðXÞ</cell><cell>ð 1Þ</cell></row><row><cell cols="4">where H(X) is estimated based on p(x), the probability mass</cell></row><row><cell cols="4">function of outcome x, as</cell></row><row><cell>HðXÞ ¼ À ∑</cell><cell cols="3">pðxÞ log ðpðxÞÞ</cell><cell>ð2Þ</cell></row><row><cell>xϵX</cell><cell></cell><cell></cell></row><row><cell cols="4">and the joint entropy between S and X is given by Eq. (3), where</cell></row><row><cell cols="4">pðl; xÞ is the probability of the classifier parameter x being related</cell></row><row><cell cols="3">to correctness.</cell></row><row><cell cols="2">HðS; XÞ ¼ À∑</cell><cell>∑</cell><cell>pðl; xÞ log ðpðl; xÞÞ</cell></row><row><cell cols="2">lϵS</cell><cell>xϵX</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The DS-LA was proposed byWoods et  al. with two different versions. The first calculates the overall local accuracy (OLA) of the base classifiers in the local region of the feature space close to the unknown pattern in the training dataset (see Algorithm 2). The OLA of each classifier is computed as the percentage of the correct recognition of the samples in the local region. Output c n t , the most promising classifier for each testing sample t in Te; for each testing sample t in Te do Submit t to all classifiers in C; if (all classifiers agree with the label of the sample t) then return the label of t; else for each classifier c i in C do ω j ¼c i (t), the predicted output of c i for the sample t; Find Ψ as the K nearest neighbors of the sample t in Tr that belongs to the class ω j ; Calculate LCAði; jÞ as the percentage of correct labeled samples of class ω j by the classifier c i on Ψ; end for Select the best classifier for t as c n t ¼ arg max i fLCAði; jÞg;</figDesc><table><row><cell>Use c n t to classify t;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>Use c n t to classify t;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note><p>are examples of this subcategory. Algorithm 2. DS-LA OLA-based method. Input the pool of classifiers C; the datasets Tr and Te; and the neighborhood size K; Output c n t , the most promising classifier for each testing sample t in Te; for each testing sample t in Te do Submit t to all classifiers in C; if (all classifiers agree with the label of the sample t) then return the label of t; else Find Ψ as the K nearest neighbors of the sample t in Tr; for each classifier c i in C do Calculate OLA i as the percentage of correct classification of c i on Ψ; end for Select the best classifier for t as c n t ¼ arg max i fOLA i g; Algorithm 3. DS-LA LCA-based method. Input the set of classes Ω; the pool of classifiers C; the datasets Tr and Te; and the neighborhood size K;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>pðcorrect j ÞÞ; selected ¼TRUE; for each classifier c j in CS do d ¼ pðcorrect m ÞÀ pðcorrect j Þ; if ððja mÞ and ðd oThresholdÞÞ then</figDesc><table><row><cell>selected ¼FALSE;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>if (selected ¼¼ TRUE) then</cell></row><row><cell>c n t ¼ c m ;</cell></row><row><cell>else</cell></row><row><cell>c n t ¼ a classifier randomly selected from CS, with</cell></row><row><cell>d o Threshold;</cell></row><row><cell>end if</cell></row><row><cell>Use the classifier c n t to classify t;</cell></row><row><cell>end for</cell></row><row><cell>Kurzynski et al.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Tr and Te; the neighborhood size K; Output c n t , the most promising classifier for each unknown sample t in Te; for each testing sample t in Te doCompute the vector MCB t as the class labels assigned to t by all classifiers in C;Calculate OLA i as the local classifier accuracy of c i on Ψ 0 ;</figDesc><table><row><cell>end for</cell></row><row><cell>Select the best classifier c n t ¼ arg max i fOLA i g; if (c n t is significantly better than the other classifiers on Ψ 0 )</cell></row><row><cell>then</cell></row><row><cell>Use the classifier c n t to classify t</cell></row><row><cell>else</cell></row><row><cell>Apply MVR using all classifiers in C to classify t;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note><p><p><p><p>Find Ψ as the K nearest neighbors of the test sample t in Tr; for each sample ψ j in Ψ do Compute MCB ψ j as the class label assigned to ψ j by all classifiers in C; Compute Sim as the similarity between MCB t and MCB ψ j ;</p>if (Sim 4 SimilarityThreshold) then</p>Ψ 0 ¼ Ψ 0 [ ψ j ;</p>end if end for for each classifier c i in C do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 6. KNORA-Eliminate (KNE) method. Input pool of classifiers C; meta-space sVa where for each sample is assign the classifiers that correctly recognize it; the testing set Te, and the neighborhood size K; Output EoC n Select the classifiers able to recognize the same amount of samples of c i to compose the ensemble EoC n Input pool of classifiers C; meta-space sVa where for each sample is assign the classifiers that correctly recognize it; the testing set Te, and the neighborhood size K; Output EoC n t , an ensemble of classifiers for each testing sample t in Te; for each testing sample t in Te do Find Ψ as the K nearest neighbors of the test sample t in sVa; for each sample ψ i in Ψ do for each classifier c i in C do if (c i correctly recognize ψ i ) then</figDesc><table><row><cell>EoC n t ¼ EoC n t [ c i ;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>Use the ensemble EoC n t to classify t;</cell></row><row><cell>end for</cell></row><row><cell>EoC n t ¼ EoC n t [ c i ;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>if (EoC n t ¼ ¼Ø) then</cell></row><row><cell>k ¼ k À 1;</cell></row><row><cell>else</cell></row><row><cell>break;</cell></row><row><cell>end if</cell></row><row><cell>end while</cell></row><row><cell>if (EoC n t ¼ ¼Ø) then</cell></row><row><cell>Find the classifier c i that correctly recognizes more samples</cell></row><row><cell>in Ψ;</cell></row><row><cell>t ;</cell></row><row><cell>end if</cell></row><row><cell>Use the ensemble EoC n t to classify t;</cell></row><row><cell>end for</cell></row><row><cell>Algorithm 7. KNORA-Union (KNU) method.</cell></row></table><note><p>t , an ensemble of classifiers for each testing sample t in Te; for each testing sample t in Te do k ¼K; while k 4 0 do Find Ψ as the k nearest neighbors of the test sample t in sVa; for each classifier c i in C do if (c i correctly recognizes all samples in Ψ) then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>.</head><label></label><figDesc>Algorithm 8. DS-KNN method.Find Ψ as the K nearest neighbors of the sample t in Va;for each classifier c i in C do Compute A i as the accuracy of c i on Ψ; end for for each classifier c i in C do for each classifier c j in C do if (ia j) then Compute D ij as the diversity between c i and c j on Ψ; Create R 2 as the rank of the classifiers in C by increasing order of diversity D; Based on R 1 , select the N 0 most accurate classifiers in C to compose the ensemble EoC; Based on R 2 , select the N″ (N″ oN 0 ) most diverse classifiers in EoC to compose EoC n</figDesc><table><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>Create R 1 as the rank of classifiers in C by decreasing order of</cell></row><row><cell>accuracy A;</cell></row><row><cell>t ;</cell></row><row><cell>Use the ensemble EoC n t to classify t;</cell></row><row><cell>end for</cell></row><row><cell>Lysiak et al.</cell></row></table><note><p>Input the pool of classifiers C; the datasets Va and Te; the neighborhood size K; the number of classifiers to be selected N 0 and N″; Output EoC n t , an ensemble of classifiers for each unknown sample t in Te; for each testing sample t in Te do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>for each testing sample t in Te do if (all N ensembles in EoC 0 agree on the label of t) then classify t; else for each EoC 0 i in EoC 0 do Compute A i as the ambiguity of the ensemble EoC 0</figDesc><table><row><cell>i by</cell></row><row><cell>using Eq. (10);</cell></row><row><cell>end for</cell></row><row><cell>Select the best ensemble for t as EoC n t ¼ arg min i fA i g;</cell></row><row><cell>Use the ensemble EoC n t to classify t;</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note><p>Algorithm 9. DSA method. Input the set of classes Ω; the pool of classifiers C; the datasets Va and Te; the neighborhood size K; Output EoC n t , an ensemble of classifiers for each unknown sample t in Te; EoC 0 ¼ OptimizationProcessðC; Va; ΩÞ; / n it generates a pool of N optimized ensembles n /</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc>Summary of the main features of the DS methods and the performance based on wins, ties, losses.</figDesc><table><row><cell>Ref Method</cell><cell>Category</cell><cell>Sel.</cell><cell cols="2">Pool type(size) Partitioning Phase/</cell><cell>Evaluated</cell><cell>Datasets S ¼small</cell><cell>SB</cell><cell>CC</cell><cell>SS</cell><cell>Other DS</cell></row><row><cell></cell><cell></cell><cell>type</cell><cell></cell><cell>Tech</cell><cell>in</cell><cell>L ¼Large</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[12] DSC-Rank</cell><cell>Ranking</cell><cell>CS</cell><cell>Het(2)</cell><cell>Training</cell><cell>-</cell><cell>1L</cell><cell>(6,0,0)</cell><cell>(4,0,2)</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[11]</cell><cell>3S</cell><cell>(1,0,2)</cell><cell>(2,0,1)</cell><cell>NA</cell><cell>(0,0,9)</cell></row><row><cell>[11] DS-LA</cell><cell>Accuracy</cell><cell>CS</cell><cell>Het(5)</cell><cell>Testing/NN-Rule</cell><cell>-</cell><cell>3S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(OLA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2</head><label>2</label><figDesc>Datasets used and their main features.</figDesc><table><row><cell>Dataset</cell><cell>Source</cell><cell cols="3"># Classes # Features # Samples</cell></row><row><cell>Wine (W)</cell><cell>UCI</cell><cell>3</cell><cell>13</cell><cell>178</cell></row><row><cell>Liver-Disorders (LD)</cell><cell>UCI</cell><cell>2</cell><cell>6</cell><cell>345</cell></row><row><cell cols="2">Wisconsin Breast Cancer (WC) UCI</cell><cell>2</cell><cell>30</cell><cell>569</cell></row><row><cell>Pima Diabetes (PD)</cell><cell>UCI</cell><cell>2</cell><cell>8</cell><cell>768</cell></row><row><cell>Image Segmentation (IS)</cell><cell>UCI</cell><cell>7</cell><cell>19</cell><cell>2310</cell></row><row><cell>Ship (SH)</cell><cell>Stalog</cell><cell>8</cell><cell>11</cell><cell>2545</cell></row><row><cell>Texture (TE)</cell><cell>Stalog</cell><cell>11</cell><cell>40</cell><cell>5500</cell></row><row><cell>Satimage (SA)</cell><cell>UCI</cell><cell>6</cell><cell>36</cell><cell>6435</cell></row><row><cell>Feltwell (FE)</cell><cell>Stalog</cell><cell>5</cell><cell>15</cell><cell>10,944</cell></row><row><cell>Letters (LR)</cell><cell>UCI</cell><cell>26</cell><cell>16</cell><cell>20,000</cell></row><row><cell>Nist Letter (NL)</cell><cell cols="2">NIST-SD19 26</cell><cell>132</cell><cell>67,192</cell></row><row><cell>Nist Digit (ND)</cell><cell cols="2">NIST-SD19 10</cell><cell>132</cell><cell>75,089</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3</head><label>3</label><figDesc>The results of Friedman's test (F t ) for each dataset, considering the degree of freedom¼2, the significance level α¼ 0.05 and the critical value p α ¼ 5:99. The average ranks for the Single Best Classifier (SB), the Combination of All Classifiers (CC) and the Dynamic Selection (DS).</figDesc><table><row><cell>Dataset</cell><cell cols="2">Average ranks</cell><cell></cell><cell>F t</cell><cell>Null-hypothesis</cell></row><row><cell></cell><cell>SB</cell><cell>CC</cell><cell>DS</cell><cell></cell><cell></cell></row><row><cell>W</cell><cell>2.050</cell><cell>2.900</cell><cell>1.050</cell><cell>17.15</cell><cell>Rejected</cell></row><row><cell>LD</cell><cell>2.100</cell><cell>2.750</cell><cell>1.150</cell><cell>12.95</cell><cell>Rejected</cell></row><row><cell>WC</cell><cell>1.750</cell><cell>2.900</cell><cell>1.350</cell><cell>12.95</cell><cell>Rejected</cell></row><row><cell>PD</cell><cell>1.900</cell><cell>2.900</cell><cell>1.200</cell><cell>24.33</cell><cell>Rejected</cell></row><row><cell>IS</cell><cell>1.750</cell><cell>2.800</cell><cell>1.450</cell><cell>10.05</cell><cell>Rejected</cell></row><row><cell>SH</cell><cell>2.889</cell><cell>1.667</cell><cell>1.444</cell><cell>10.88</cell><cell>Rejected</cell></row><row><cell>TE</cell><cell>2.300</cell><cell>2.200</cell><cell>1.500</cell><cell>3.80</cell><cell>Accepted</cell></row><row><cell>SA</cell><cell>2.200</cell><cell>2.100</cell><cell>1.700</cell><cell>0.67</cell><cell>Accepted</cell></row><row><cell>FE</cell><cell>1.800</cell><cell>2.600</cell><cell>1.600</cell><cell>5.60</cell><cell>Accepted</cell></row><row><cell>LR</cell><cell>2.700</cell><cell>2.100</cell><cell>1.200</cell><cell>11.40</cell><cell>Rejected</cell></row><row><cell>NL</cell><cell>3.000</cell><cell>1.950</cell><cell>1.050</cell><cell>19.05</cell><cell>Rejected</cell></row><row><cell>ND</cell><cell>3.000</cell><cell>1.800</cell><cell>1.200</cell><cell>16.80</cell><cell>Rejected</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4</head><label>4</label><figDesc>Results of the complexity measures for each dataset: ↑ means the higher easier, ↓ means the lower easier.</figDesc><table><row><cell>Dataset</cell><cell>F1↑</cell><cell>F2↓</cell><cell>N2↓</cell><cell>N3↓</cell><cell>T2↑</cell></row><row><cell>W</cell><cell>2.362</cell><cell>6.120E À 05</cell><cell>0.018</cell><cell>0.230</cell><cell>13.692</cell></row><row><cell>LD</cell><cell>0.017</cell><cell>7.320E À 02</cell><cell>0.853</cell><cell>0.377</cell><cell>57.500</cell></row><row><cell>WC</cell><cell>1.118</cell><cell>5.683E À 11</cell><cell>0.031</cell><cell>0.084</cell><cell>18.967</cell></row><row><cell>PD</cell><cell>0.032</cell><cell>2.515E À 01</cell><cell>0.838</cell><cell>0.320</cell><cell>96.000</cell></row><row><cell>IS</cell><cell>0.938</cell><cell>1.653E À 04</cell><cell>0.071</cell><cell>0.033</cell><cell>121.579</cell></row><row><cell>SH</cell><cell>0.706</cell><cell>6.687E À 02</cell><cell>0.293</cell><cell>0.095</cell><cell>231.364</cell></row><row><cell>TE</cell><cell>4.064</cell><cell>5.058E À 06</cell><cell>0.127</cell><cell>0.009</cell><cell>135.500</cell></row><row><cell>SA</cell><cell>2.060</cell><cell>3.754E À 04</cell><cell>0.215</cell><cell>0.091</cell><cell>178.750</cell></row><row><cell>FE</cell><cell>1.206</cell><cell>6.722E À 02</cell><cell>0.107</cell><cell>0.011</cell><cell>729.600</cell></row><row><cell>LR</cell><cell>0.479</cell><cell>2.162E þ00</cell><cell>0.228</cell><cell>0.038</cell><cell>1250.000</cell></row><row><cell>NL</cell><cell>0.642</cell><cell>9.080E À 30</cell><cell>0.535</cell><cell>0.068</cell><cell>509.030</cell></row><row><cell>ND</cell><cell>0.626</cell><cell>1.257E À 32</cell><cell>0.327</cell><cell>0.015</cell><cell>568.856</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: A.S. Britto Jr. et al., Dynamic selection of classifiers-A comprehensive review, Pattern Recognition (2014), http://dx.doi.org/10.1016/j.patcog.2014.05.003i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported by the Brazilian National Council for Scientific and Technological Development (CNPq) and by the Research Foundation of the Parana state (Fundação Araucária).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990-10">October (10. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1857</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Complexity measures of supervised classification problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of how training data complexity affects the nearest neighbor classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mollineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sotoca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="2007-07-03">July (3). 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data complexity measures and nearest neighbor classifiers: a practical analysis for meta-learning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D C</forename><surname>Cavalcanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Vale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 24th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1065" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date>May (2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of artificial-intelligence-based ensembles for intrusion detection: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Int. Soft Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boosting the margin: a new explanation for the effectiveness of voting methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th International Conference on Machine Learning</title>
		<meeting>14th International Conference on Machine Learning<address><addrLine>Nashville, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combination of multiple classifiers using local accuracy estimates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="410" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classifier combination for handprinted digit recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Document Analysis and Recognition</title>
		<meeting>the Second International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Methods for dynamic classifier selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 10th International Conference on Image Analysis and Processing</title>
		<meeting>10th International Conference on Image Analysis and Processing</meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="659" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On two measures of classifier competence for dynamic ensemble selection-experimental comparative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kurzynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woloszynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lysiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Symposium on Communications and Information Technologies (ISCIT)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1108" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic classifier selection based on multiple classifier behaviour</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1879" to="1881" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">New dynamic ensemble of classifiers selection approach based on confusion matrix for arabic handwritten recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nabiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nadir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Multimedia Computing and Systems (ICMCS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="308" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic selection approaches for multiple classifier systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="673" to="688" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifier ensembles with a random linear oracle</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="508" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From dynamic classifier selection to dynamic ensemble selection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H R</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1718" to="1731" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining both ensemble and dynamic classifier selection schemes for prediction of mobile internet subscribers</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="68" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dynamic classifier selection method to build ensembles using accuracy and diversity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G F</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M P</forename><surname>Canuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C P</forename><surname>De Souto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Brazilian Symposium on Neural Networks, SBRN &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic approach to the dynamic ensemble selection using measures of competence and diversity of base classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lysiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurzynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woloszynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid Artificial Intelligent Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Emilio</forename><surname>Corchado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marek</forename><surname>Kurzynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michal</forename><surname>Wozniak</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6679</biblScope>
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decision combination in multiple classifier systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dynamic overproduce-and-choose strategy for the selection of classifier ensembles</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2993" to="3009" />
			<date type="published" when="2008-10">October (10. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic classifier ensemble selection based on GMDH</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 International Joint Conference on Computational Sciences and Optimization</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptative mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bias/variance analyses of mixtures-of-experts architectures</title>
		<author>
			<persName><forename type="first">Jacobs</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="369" to="383" />
			<date type="published" when="1997-02">February (2. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<title level="m">Combining Pattern Classifiers, Methods and Algorithms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Method of collective recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rastrigin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Erenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energoizdat</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>in Russian</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A probabilistic model of classifier competence for dynamic ensemble selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Woloszynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2656" to="2668" />
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Semi-Supervised Learning for Visual Content Analysis and Understanding</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A method of combining multiple experts for the recognition of unconstrained handwritten numerals</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic selection of ensembles of classifiers using contextual information</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<idno>MCS&apos;10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Multiple Classifier Systems</title>
		<meeting>the 9th International Conference on Multiple Classifier Systems<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ambiguity-guided dynamic selection of ensemble of classifiers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Information Fusion</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Heuristic self-organization in problems of engineering cybernetics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ivakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for supervised remote sensing image classification based on dynamic classifier selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Smits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="801" to="813" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Switching between selection and fusion in combining classifiers: an experiment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Syst. Man Cybern. Part B</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="146" to="156" />
			<date type="published" when="2002-02">April (2. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic classifier ensemble model for customer classification with imbalanced class distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3668" to="3675" />
			<date type="published" when="2012-03">February (3. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic selection of generativediscriminative ensembles for off-line signature verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1326" to="1340" />
			<date type="published" when="2012-04">April (4. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The lack of a priori distinctions between learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996-07">October (7. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive selection of image classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Alberto</forename><surname>Bimbo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1310</biblScope>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">Evaluating Learning Algorithms: A Classification Perspective</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">degree in Industrial Informatics from the Centro Federal de Educação Tecnológica do Paraná (CEFET-PR, Brazil) in 1996, and Ph.D. degree in Computer Science from the Pontifícia Universidade Católica do Paraná (PUCPR, Brazil) in 2001</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>UEPG</publisher>
		</imprint>
		<respStmt>
			<orgName>Informatics Department of the Universidade Estadual de Ponta Grossa</orgName>
		</respStmt>
	</monogr>
	<note>In 1995, he also joined the Computer Science Department of the Pontifícia Universidade Católica do Paraná (PUCPR) and, in 2001, the Postgraduate Program in Informatics (PPGIa). His research interests are in the areas of document analysis and handwriting recognition</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">) where he was co-responsible for the implementation in 1995 of a master program and in 1998 a Ph.D. program in applied computer science. Since 1996, he is a senior member of the Centre for Pattern Recognition and Machine Intelligence (CENPARMI, Concordia University). Dr. Sabourin is the author (and co-author) of more than 260 scientific publications including journals and conference proceedings. He was a co-chair of the program committee of CIFED&apos;98 (Conférence Internationale Francophone sur l&apos;Écrit et Le Document</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He was nominated as a Conference cochair of ICDAR&apos;07 (9th International Conference on Document Analysis and Recognition) that has been</title>
		<meeting><address><addrLine>Curitiba, Brazil; Québec, Canada; Tokyo, Japan; Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Université du Québec ; Computer Science Department of the Pontifícia Universidade Católica do Paraná</orgName>
		</respStmt>
	</monogr>
	<note>) and IWFHR&apos;04 (9th International Workshop on Frontiers in Handwriting Recognition. His research interests are in the areas of handwriting recognition, signature verification. intelligent watermarking systems and bio-cryptography</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">degree in electrical engineering and industrial informatics from the Centro Federal de Educacao Tecnologica do Parana</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E S</forename><surname>Oliveira ; Curitiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Cefet-Pr)</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Curitiba</surname></persName>
		</author>
		<author>
			<persName><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His current interests include Pattern Recognition</title>
		<meeting><address><addrLine>Curitiba, PR, Brazil; Curitiba, PR, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 1998, and 2003. 2004. 2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science from Ecole de Technologie Superieure, Universite du Quebec ; Computer Science Department at Pontifical Catholic University of Parana ; Federal University of Parana</orgName>
		</respStmt>
	</monogr>
	<note>Computer Science from UnicenP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
