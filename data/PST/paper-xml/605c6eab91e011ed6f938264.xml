<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FASTMOE: A FAST MIXTURE-OF-EXPERT TRAINING SYSTEM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-24">24 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaao</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
							<email>zhaijidong@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ Beijing Academy of Artificial Intelligence (BAAI) Recurrent AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FASTMOE: A FAST MIXTURE-OF-EXPERT TRAINING SYSTEM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-24">24 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.13262v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities. In this paper, we present FastMoE , a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent emergence of large-scale language models, examplified by BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, GPT-2/-3 <ref type="bibr" target="#b10">(Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>, XLNet <ref type="bibr" target="#b19">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>, T5 <ref type="bibr" target="#b11">(Raffel et al., 2020)</ref>, GShard <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> and Switch Transformer <ref type="bibr" target="#b4">(Fedus et al., 2021)</ref>, has drastically reshaped the landscape of the natural language processing research, reestablishing the new state-of-the-art baselines in various benchmarks such as GLUE <ref type="bibr" target="#b17">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr" target="#b18">(Wang et al., 2019)</ref>. Among many possible solutions, scaling model size has been proved to be one of the simplest and most effective way <ref type="bibr" target="#b7">(Kaplan et al., 2020)</ref> toward more powerful models. From BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> with 340 million parameters, to T5 <ref type="bibr" target="#b11">(Raffel et al., 2020)</ref> with 11 billion parameters, to GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> with 175 billion parameters, the model size is enlarged by 500× in merely two years. More recently, GShard <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> scales to a record number of 600 billion parameters, which is quickly broken by Switch Transformer <ref type="bibr" target="#b4">(Fedus et al., 2021)</ref> with 1.6 trillion parameters. The main contributor toward the huge model size of GShard and Switch Transformer is a novel neural network architecture named mixture of experts (MoE) <ref type="bibr" target="#b13">(Shazeer et al., 2017</ref>).</p><p>An MoE layer (an illustrative example can be found in Figure <ref type="figure" target="#fig_0">1</ref>) consists of a gate and a pool of experts. For each input, only a tiny minority of experts are chosen by the gate for computation. The special architecture of MoE is a double-edge sword for large-scale distributed training. On the one hand, due to its sparse activation of experts, MoE is able to enlarge the model size by orders of magnitude without significantly increasing the amount of computation (FLOPs). On the other hand, when scaling up to thounds of experts, the imbalanced all-to-all communication pattern of MoE brings new challenges to the co-design of algorithm and system. Therefore, MoE can not be directly supported by traditional deep learning libraries such as PyTorch <ref type="bibr" target="#b9">(Paszke et al., 2019)</ref> and TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.  Due to the challenge raised by the new model architecture, both the research community and the industry need an MoE implementation that support large-scale distributed training. However, despite the existence of some naive single-GPU implementation in PyTorch <ref type="bibr" target="#b12">(Rau, 2019)</ref>, the only present system that supports scalable MoE training is based on Google's private hardware and software stack -TPU <ref type="bibr" target="#b6">(Jouppi et al., 2017)</ref> and Mesh TensorFlow <ref type="bibr">(Shazeer et al., 2018)</ref>. Thus, there is urgent need to develope an MoE system on publicly available hardware (e.g., GPU) and platforms (e.g., PyTorch <ref type="bibr" target="#b9">(Paszke et al., 2019)</ref>).</p><p>Motivated by the desire to obtain easy-to-use, flexible, efficient, scalable, and open-source solution to large-scale MoE training, we release FastMoE with the following design goals:</p><p>• Easy-to-use: provide a user-friendly interface to define an MoE layer, and seamless support for popular language model training system, Megatron-LM <ref type="bibr" target="#b15">(Shoeybi et al., 2019)</ref>. • Flexible: make it easy for users to customize gate networks and expert networks.</p><p>• Efficient: integrate a highly optimized feadforward (FFN) layer for Transformer.</p><p>• Scalable: support scaling up the size of MoE models by training across multiple GPUs on multiple nodes.</p><p>Different from previous single-GPU PyTorch implementation <ref type="bibr" target="#b12">(Rau, 2019)</ref>, FastMoE concentrates on efficiency and scalability. Dedicated CUDA kernels are included in FastMoE for high performance with specialized optimizations. FastMoE is able to run across multiple GPUs on multiple nodes using NCCL <ref type="bibr" target="#b5">(Jeaugey, 2017)</ref>. The details of communication is hidden from model developers by FastMoE . The model-parallel method of FastMoE allows distributing experts across different GPUs, while other parts of the model remain parallelized by batch dimension (data parallel) or tensor dimension (model parallel). Chances are that the model size, proportional to the number of experts, can scale up with the number of GPUs used for training, being the key to train trillion-scale models.</p><p>In our experiment, we observe that FastMoE is faster than a baseline <ref type="bibr" target="#b12">(Rau, 2019)</ref> implemented by pure PyTorch API on a single GPU. FastMoE also shows reasonable scalability when running across nodes on a cluster connected by Infiniband network. We train a real GPT model with 96 experts per layer using distributed FastMoE with promising end-to-end training speed. Compared to a non-MoE model of the same amount of computation, its performance benefits from the enlarged model size that the MoE architecture.</p><p>This paper is organized as follows. Section 2 introduces the background of MoE and compares existing systems. Section 3 presents the FastMoE system in detail. Section 4 introduces the challenges of achieving high-performance and the FastMoE 's solutions. Section 5 shows results of experiments that demonstrate the efficiency of FastMoE and the performance gain of an MoE model using FastMoE in training. Section 6 summarizes the paper and indicates directions our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MIXTURE-OF-EXPERTS (MOE)</head><p>In this section, we review the architecture of MoE and current systems for training MoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MOE: MODEL STRUCTURE</head><p>Mixture-of-Expert is short for Sparsely-Gated Mixture-of-Experts layers proposed by <ref type="bibr" target="#b13">Shazeer et al. (2017)</ref>. An MoE layer consists of multiple experts, each can be an arbitrary neural network. The Preprint only constraint of the experts is that they should take the same input, and give output in the same vector space. Figure <ref type="figure" target="#fig_0">1</ref> shows a detailed example of an MoE layer. A special neural network, namely the gate network, is introduced to score each expert over a given input. According to the scores, selection of experts is made by a policy, which may vary from model to model. Then, the selected experts, e.g. experts 1 and 3 in the example, are activated to process the input sample. The outputs of the experts, together with the score, are combined into the final output using a certain algorithm.</p><p>A popular way of the expert selection is to select the experts with top k highest score. In synthesis process, the score is used as the weight for the output of the experts to be added into the overall output. This enables training the gate network, as the gradient can be propagated by the score. Algorithm 1 formalizes the method above.</p><p>Algorithm 1 Forward computation of an MoE layer with top-k gating.</p><p>Require: A pool of n experts:</p><formula xml:id="formula_0">{E 1 , E 2 , • • • , E n } Require: Gate G Require:</formula><p>The number of experts k to be selected</p><formula xml:id="formula_1">1: function MOE(x) 2: score ⇐ G(x) 3: indices ⇐ ArgM ax k (score) 4:</formula><p>y ⇐ zero tensor like x 5:</p><p>for each index i ∈ indices do 6:</p><formula xml:id="formula_2">x i ⇐ E i (x) 7: y ⇐ score i * x i + y 8:</formula><p>end for 9:</p><p>return y 10: end function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CURRENT SYSTEMS FOR MOE TRAINING</head><p>The GShard system <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> implements a distributed version of the MoE model. It trains a language model on up to 2048 TPUs, with 1 expert per layer placed on each TPU. As a result, the MoE layers contain 2048× more parameters than a non-MoE layer. In Switch Transformer <ref type="bibr" target="#b4">(Fedus et al., 2021)</ref>, the model is further enlarged to 1.6 trillion, showing strong ability for the system to support training models in large scale. Unfortunately, this system is not publicly available yet. It is strongly binded with the TPU cluster, which makes it hard to reproduce the experiments on commodity devices. Additionally, the design of GShard lacks flexibility to use different number and size of experts with different replication strategy.</p><p>In Tensor2tensor <ref type="bibr" target="#b16">(Vaswani et al., 2018)</ref>, a MoE Transformer model is provided. However, this implementation uses Mesh TensorFlow <ref type="bibr">(Shazeer et al., 2018)</ref>, which does not support GPUs very well. To implement a FFN in Transformer, it takes more than 100 lines of code in TensorFlow with complicated einsum operators, making it burdening for developers to understand the structure and explore other model structures based on the code.</p><p>PyTorch <ref type="bibr" target="#b9">(Paszke et al., 2019)</ref>, as a popular deep learning framework among researchers, provides more straightforward coding style and flexibility against TensorFlow. Efforts are made to train MoE models with PyTorch <ref type="bibr" target="#b12">(Rau, 2019)</ref>. However, as the PyTorch community lacks multi-dimension parallel training tools, none of the PyTorch-based implementations support training on multiple GPUs. As the ultimate target of adopting MoE is training even larger models, the PyTorch-based implementations fails to be a candidate.</p><p>3 FastMoE : SYSTEM DESIGN In this section, we introduce our design of FastMoE with distributed training support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint 3.1 A FLEXIBLE SYSTEM FOR DIVERSE MODEL EXPLORERS</head><p>The Backbone to Run Arbitrary Expert Networks. FastMoE supports using arbitrary network as the expert. The FMoE interface of FastMoE takes any neural network module constructor as input, and replicates the module for multiple times as the expert instances. The expert is defined to take a batch of aligned contiguous input features, and the output should be in the same batch order. Therefore, the expert module implementation is decoupled from the MoE architecture so that developers can focus on the design of their own expert network.</p><p>For even stronger flexibility, the FMoE class contains a member function expert fn, where the expert modules are used to conduct the forward computation. This function can be overloaded for further customized MoE behavior. For example, in the FMoETransformerMLP network, which will be mentioned later in this section. the list of experts is replaced by a specially optimized module that applies the experts in parallel to extremely lower the latency.</p><p>Moreover, FastMoE supports placing multiple experts together on the same worker, enabling more flexible configuration space of the number of experts (i.e., the number of experts does not have to be equal to the number of data parallels), which is different from the the design of GShard.</p><p>A Highly-optimized FFN for Transformer.</p><p>To better support training Transformer with MoE, FastMoE provides a standard and high-performance FFN implementation (FMoETransformerMLP). The detailed optimization strategy is hidden from the developers.</p><p>In particular, when placing multiple experts on the same worker, a naive implementation is to loop over these experts and conduct forward in sequence. However, for certain types of expert networks, it is possible to explore the potential speedup brought by parallel execution. In FastMoE , we mainly optimize the parallel execution of fully-connected layers by a dedicated FMoELinear module. Instead of computing the expert modules sequentially, the specially optimized expert module maintains a pool of available hardware resources, and applies the expert computation in parallel.</p><p>Plugin-style Support for PyTorch and Megatron-LM. The flexibility of FastMoE allows convenient adaption to existing training applications. Take Megatron-LM <ref type="bibr" target="#b15">(Shoeybi et al., 2019)</ref> as an example, a plugin-style module is integrated in FastMoE to quickly replace the FFNs in the original Megatron-LM model with MoE networks. As shown in listing 1, the transformation can be achieved by only 2 lines of code. The fmoefy function can find the FFNs in the Transformer layers. Then, an MoE network using FastMoE is created, which is a module that wraps up the FMoETransformerMLP module for interface-level compatibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ENLARGING THE MODEL CAPACITY DISTRIBUTEDLY</head><p>The Model Parallel Method of FastMoE. As one of the most effective way to enlarge the model capacity, the ability to accommodate a large expert population and train them in parallel is demanded in many MoE models. It is hard for the model developers to handle the complicated data transfer among GPUs and even across nodes. Achieving high training performance and good hardware resource utilization requires expertise in computer architecture and parallel programming, which is beyond the technique stack of common model developers.</p><p>FastMoE supports distributing experts across multiple workers on multiple nodes, which is called the model parallel method in FastMoE . The detail of input data exchange is hidden within the FMoE interface. For model developers, they only need to write code for a single expert, and each expert is given all the input data gathered from all workers by FastMoE . As a result, the model developers do not have to consider the implementation details about cross-worker communication.</p><p>In the design of FastMoE , when the feature to distribute expert across workers is enabled, extra communication operations are included in the forward and backward computation. To better identify the operations, we call them global data exchange operations, in contrast to the local data shuffle process, which will be mentioned in section 4.</p><p>A major challenge in the distributed context is that the total number of input samples assigned to all experts on a worker may vary a lot. It is impossible to have the number of incoming samples before the gate output is available. However, allocating the buffer to place the input samples is dependent on the number. Therefore, before actual exchange of input samples between workers happens after exchanging the amount information between workers, and allocating memory according to the inspection of the expert count information. An example of the global operations in FastMoE is shown in figure <ref type="figure" target="#fig_2">2</ref>. The workers first count the number of samples assigned to each expert on each worker. Then, they exchange the size of expert inputs, so that all workers get the number of incoming input samples, and where they are from. After the offsets of each receiving buffer is calculated, the workers start exchanging data directly. It is worth nothing that the statistics of the incoming and outgoing samples can be reused through the whole process of a training iteration.</p><p>Heterogeneity-aware Synchronization Module. Heterogeneity is introduced as different parts of the network may be replicated across different groups of workers. It is a challenge that the distributed module has to identify whether the gradient of a parameter should be synchronized, and with whom it is synchronized. FastMoE introduces the data parallel communication group tag on each parameter to address the issue.</p><p>The tag can be one of world, data parallel or none, which respectively indicates that the gradient should be synchronized with (1) all other workers, (2) the workers in a data-parallel group that is orthogonal to the model-parallel group, or (3) no worker. For example, the gate network is replicated across all workers, regardless of model parallel settings. The attention layer may be divided into model-parallel sub-layers, so its tag is data parallel. Each worker serves several unique expert networks, whose tag is none. A customized data parallel module instead of PyTorch's original distributed data parallel module is provided by FastMoE , which can identify the tags and perform correct synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATIONS TO ACHIEVE HIGH-PERFORMANCE</head><p>The performance of MoE computation on a single node is significant, as it determines the theoretical upper bound of the system scaling up to any scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>The most intuitive way to compute an MoE layer is slicing the input batch into samples, and compute sample by sample. After that, output features are stacked in the original order. However, it is observed that implementing an MoE model using simple PyTorch operators can hardly achieve high performance. Less than 5% the peak performance of GPUs can be achieved. Without loss of generality, we assume the expert network is an FFN. Note that the major operator within an FFN is from fully-connected layers, which consist of several GeMM operators.When the batch is split up into single samples, the GeMM operation is degraded into GeMV. Figure <ref type="figure" target="#fig_3">3</ref> shows the float-point computation throughput of an example fully-connected layer using different batch size. Given that in modern heterogeneous compute devices, matrix multiplication operators are fine tuned with sophisticated tiling techniques applied on all dimensions, it is not surprising that the throughput can only approach the theoretical peak when the batch size is large enough. This leads to the principle that to achieve high performance in MoE computation, the samples should be batched to fully utilize the hardware resources.</p><p>FastMoE batches all input samples to the same expert together. Due to the limit of data representation, FastMoE performs memory movement with a specially developed CUDA kernel to reduce overhead. Given the index of gate that each sample is going to, the process to put all input samples to the same gate in a contiguous memory space is called scatter. However, in other parts of the neural network, the batch may have to be organized as its original order, e.g., the attention layer in Transformer. A reverse operation is performed after the experts output to another contiguous memory space, i.e. place the scattered feature vectors back to their original order according to the gate indices. This process is denoted as gather in FastMoE . The reordering computation process is shown as Figure <ref type="figure" target="#fig_4">4</ref>. When the assignment from input samples to experts is balance enough, each expert is expected to have a relatively large input batch size that can reach satisfying hardware utilization according to Figure <ref type="figure" target="#fig_3">3</ref>. However, load imbalance always occurs because of the nature of random sampling of the input training data. It is highly possible that one expert receives very few input samples during millions training iterations. Additionally, as multiple experts are placed on one worker, local batch sizes of the experts are, on average, statistically lower than that in data parallel. FastMoE uses a customized stream manager to simultaneously execute the computation of multiple experts to extract the potential throughput gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, the training speed of FastMoE is compared with another PyTorch MoE implementation <ref type="bibr" target="#b12">(Rau, 2019)</ref> on a single GPU. We also report the scalability of FastMoE when distributed training. To the best of our knowledge, FastMoE is the only PyTorch-based MoE system can run across different nodes and GPUs. We also show the end-to-end performance of an MoE Transformer model trained using FastMoE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENT SETUP</head><p>We use the following notations to characterize the computation task: n e experts are placed on each GPU. Each expert applies two linear layers of sizes d m × d h and d h × d m respectively. The input contains n b samples. The gate module scores the fitness of each sample to be processed by each expert. For each input sample, the experts of top k highest score are selected to process the sample.</p><p>Additionally, several warm-up rounds are performed, which perform the same computation but are not counted in the results. For each experiment, the task is executed 16 times, and the average time is used to calculate the performance. The standard deviation values of the execution time are also inspected. All of them are negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING SPEED ON A SINGLE GPU</head><p>The performance of the FMoETransformerMLP is tested, which completes similar task with MoE module in the baseline <ref type="bibr" target="#b12">(Rau, 2019)</ref>, on a NVIDIA TESLA V100 PCIe GPU. The baseline is implemented by pure PyTorch API with hard-coded model structure. For fairness of the comparison, both modules uses a randomly initialized matrix as the weight of the gate network, which consists of one fully-connected layer. The experts also perform the same computation.  As Figure <ref type="figure" target="#fig_6">5</ref> shows, the baseline implementation is constantly slower than FastMoE . As the number of experts grows, the baseline spends much more time in the forward computation, while the latency of FastMoE remains stable, thanks to its customized stream manager mentioned in Section 4. Considering that FastMoE is targeted on training, the backward time is stacked over the forward time. We observed that FastMoE outperforms the baseline in the overall time spent in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CROSS-GPU AND CROSS-NODE SCALABILITY</head><p>To examine the performance of FastMoE expanding on multiple GPUs across nodes, we conduct an experiment on a cluster of 8 nodes, with 1 NVIDIA Tesla V100 GPU on each node. The cluster is interconnected via an Infiniband EDR switch and 8 HCA cards. The FLOPs of the matrix multiplication operations is calculated to represent the training throughput.  According to the result shown in Figure <ref type="figure" target="#fig_8">6</ref>, FastMoE shows scalability across nodes. The overall throughput increases from 10 TFLOPs to 25 TFLOPs, as the number of GPUs increases from 2 to 8, sub-linearly scaling up. We observe that when expanding to 2 GPUs, the performance is half of that on a single GPU, which suggests that FastMoE is bounded by communication. When more GPUs are used for computation, more experts are introduced, and the granularity of exchanging input samples becomes smaller, lowering the efficiency in data transfer over the network.</p><p>As a conclusion, the scalability of FastMoE can support training large MoE model using multiple GPUs across multiple nodes with performance gain. However, space is left for further optimization on throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">END-TO-END PERFORMANCE GAIN USING FastMoE</head><p>We test the end-to-end performance gain using FastMoE by training a 12-layer GPT model on 8 GPUs using Megatron-LM <ref type="bibr" target="#b15">(Shoeybi et al., 2019)</ref>. As mentioned in Section 3, the Megatron adapter of FastMoE is used for MoE structure. For each layer, 96 experts are distributed across the GPUs, i.e. 12 experts are placed on each GPU. For each input token, the top 2 experts with highest score are used to process it. The  We are still working on FastMoE for more features and faster training. Compared to the GShard model <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>, FastMoE lacks functionalities to support load-balancing among experts. The work of load-balance monitor and support for load-balance loss is in progress. We are also trying to make the system more user-friendly on utilities, such as loading and saving of MoE models. The performance across multiple GPUs requires joint efforts from the view of both high-performance computing and machine learning. Any contributions to the open-source project will be appreciated. We are looking forward to your participation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of an MoE layer. In this example, expert 1 and expert 3 are selected by the gate for computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Listing 1 :</head><label>1</label><figDesc>Sample code to use FastMoE in Megatron-LM from fmoe.megatron import fmoefy model = fmoefy(model, num_experts=&lt;number of experts per worker&gt;)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the global operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GeMM performance of different problem sizes using cuBLAS on NVIDIA V100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of the reordered computation of an MoE layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>tested with n b = 4096, dm = 1024, d h = 4096, k = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Computation time comparison between FastMoE and the baseline implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The throughput is tested with ne = 4, n b = 4096, dm = 1024, d h = 4096, k = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scalability of FastMoE across multiple GPUs on multiple nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>d h in expert MLP layer is halved so that the valid FLOPs of the model are almost identical, except for the extra FLOPs introduced by the gate, which is negligible. Both the baseline model and the MoE model are trained for 70 hours. The lm loss metric in training indicates the convergence tendency of the models. From Figure 7, we observed that the training speed of the baseline model is about 3× of FastMoE . As FastMoE performs more computation and communication, it is a reasonable slow down. Fortunately, the MoE model achieves much lower loss with the same training iterations. Also, as a benefit from the efficiency of FastMoE , the MoE model achieves lower loss in the same training time.6 SUMMARY AND FUTURE WORKIn this paper, we present FastMoE , an open-source system to train Mixture-of-Experts models. The system is based on the popular PyTorch framework, and currently supports efficient training on GPUs. Friendly interfaces of multiple levels are provided for different users to explore different aspects of the MoE architecture. The performance of FastMoE on a single GPU is well-optimized toThe narrow dark lines are smoothed exponentially by 0.97 from the original loss curve, represented by the brighter wide curves respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Loss curve of training a GPT model by FastMoE</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Dima Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nccl 2.0</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Jeaugey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference (GTC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
				<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sparsely-gated mixture-of-experts pytorch implementation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rau</surname></persName>
		</author>
		<ptr target="https://github.com/davidmrau/mixture-of-experts" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>CoRR, abs/1803.07416</idno>
		<ptr target="http://arxiv.org/abs/1803.07416" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
