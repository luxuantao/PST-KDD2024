<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Image Sentiment Analysis with Visual Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-05">June 5, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaikai</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qiang</forename><surname>Ling</surname></persName>
							<email>qling@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Jingdong Inc</orgName>
								<address>
									<postCode>101111</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Image Sentiment Analysis with Visual Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-05">June 5, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">30D6B83B439A3359B9648D7EC2FBEB26</idno>
					<idno type="DOI">10.1016/j.neucom.2018.05.104</idno>
					<note type="submission">Received date: 31 March 2018 Revised date: 22 May 2018 Accepted date: 30 May 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Sentiment Analysis</term>
					<term>Convolutional Neural Networks</term>
					<term>Visual Attention</term>
					<term>Saliency Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment analysis plays an important role in behavior sciences, which aims to determine the attitude of a speaker or a writer regarding some topic or the overall contextual polarity of a document. The problem nevertheless is not trivial, especially when inferring sentiment or emotion from visual contents, such as images and videos, which are becoming pervasive on the Web. Observing that the sentiment of an image may be reflected only by some spatial regions, a valid question is how to locate the attended spatial areas for enhancing image sentiment analysis. In this paper, we present Sentiment Networks with visual Attention (SentiNet-A) -a novel architecture that integrates visual attention into the successful Convolutional Neural Networks (CNN) sentiment classification framework, by training them in an end-to-end manner. To model visual attention, we develop multiple layers to generate the attention distribution over the regions of the image. Furthermore, the saliency map of the image is employed as a priori knowledge and regularizer to holistically refine the attention distribution for sentiment prediction. Extensive experiments are conducted on both Twitter and ARTphoto benchmarks, and our framework achieves superior results when compared to the state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the popularity of social networks and mobile devices, there is a huge volume of images and videos captured by users to record all kinds of activities in their lives everyday and everywhere. For example, people may share their travel experiences, their opinions towards some events and so on. Automatically analyzing the sentiment from these multimedia contents is demanded by many practical applications, such as smart advertising, targeted marketing and political voting forecasts. Compared with textbased sentiment analysis which infers emotional signals from short textual description, visual contents, such as color contrast and tone, could provide more vivid clues to reveal the sentiment behind. Figure <ref type="figure">1</ref> shows image examples from Twitter. Apparently, the images in the upper row manifest positive sentiment, while those in the lower row deliver negative emotion.</p><p>Image sentiment analysis is a high-level abstraction concerning the affects to be conveyed by an image, and could bridge the big affective gap between low-level visual features and high-level sentiment. In the literature, there have been several sentiment analysis techniques, including low-level visual feature-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, semantic-level feature-based models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6]</ref> and deep learning architectures <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. Though impressive results have been reported by existing image sentiment analysis approaches, these techniques often encode an entire image into a fixed dimensional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T representation, while leaving the regions of the image that are most indicative to infer the sentiment not fully exploited. This is especially important when there is a lot of clutter in an image. Take the first image in the lower row of Figure <ref type="figure">1</ref> as an example, there are five main objects in the image, including person, car, building, road and tree.</p><p>To predict the sentiment of the image, we need to first locate those objects, then rule out irrelevant objects (e.g., tree and building) and finally pinpoint to the region of person in this case to infer the sentiment. Therefore, we investigate particularly in this paper the architectures by exploiting visual attention for boosting image sentiment analysis.</p><p>By consolidating the idea of visual attention into the analysis of image sentiment, we present a novel Sentiment Networks with visual Attention (SentiNet-A) architecture, as illustrated in Figure <ref type="figure" target="#fig_3">2</ref>. Given an input image, a Convolutional Neural Networks (CNN) is exploited to produce a feature vector for each region of the image, followed by a multi-layer neural network for modeling the attention distribution over all the regions and locating the regions that are most informative to infer the image sentiment.</p><p>Moreover, considering that saliency is in general the focus of attention cortically in an image and being inspired by the work in <ref type="bibr" target="#b10">[10]</ref>, which improves the visual saliency computing with emotion intensity based on the strong relationship between the saliency detection and sentiment analysis, we integrate saliency detection into visual attention learning as a regularizer to holistically ensure the correct attention distribution. Technically, to capture subtle visual contrast among multi-scale feature maps, a multi-scale Fully Convolutional Network (FCN) is employed to generate the saliency map <ref type="bibr" target="#b11">[11]</ref>. As such, it is natural to optimize the whole architecture by simultaneously minimizing the classification loss of image sentiment and the distance between the learnt attention distribution and saliency map. During prediction, the image representations weighted by the attention are input into a fully-connected layer for image sentiment classification.</p><p>It is worth noting that our SentiNet-A framework is trainable in an end-to-end fashion.</p><p>The main contribution of this work is the proposal of visual attention augmented architecture for image sentiment analysis. By identifying the most distinctive regions in an image to infer the sentiment of the image, our work takes a further step forward to enhance image sentiment analysis. Our solution also leads to the elegant views of how visual attention should be modeled and leveraged in sentiment analysis, which The remaining sections are organized as follows. Section 2 describes related works on image sentiment analysis and the exploration of visual attention. Section 3 presents our proposed SentiNet-A architecture. Section 4 provides empirical evaluations, followed by the conclusions in Section 5.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This paper mainly focuses on the research of visual attention learning for image sentiment analysis. We briefly group the related work into two categories: visual sentiment analysis and the explorations of visual attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Sentiment Analysis</head><p>The research in the direction of visual sentiment analysis has proceeded along three different dimensions: low-level visual feature-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b20">19]</ref>, semantic-level feature-based models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b19">18]</ref> and deep learningbased architectures <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32]</ref>.</p><p>Low-level visual feature-based approaches mainly try to solve the problems with the basic hand-crafted features. One of the representative methods is using features from early visual recognition tasks. <ref type="bibr">Wang et al.</ref> propose three fuzzy visual histograms to predict the emotional factor in <ref type="bibr" target="#b14">[14]</ref>. Yanulevskaya et al. design a sentiment categorization system based on the assessment of local image statistics in <ref type="bibr" target="#b16">[15]</ref>. Later in <ref type="bibr" target="#b17">[16]</ref>, more advanced features inspired by psychology and art theories are extracted for sentiment prediction, including color, texture, composition, faces and skins. Lu et al.</p><p>investigate how shape features in natural images influence emotions of human beings <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b18">[17]</ref> approach for sentiment analysis of social network multimedia in <ref type="bibr" target="#b36">[35]</ref>.</p><p>Semantic-level feature-based models concentrate on performing visual sentiment analysis with semantic-level features that are related to object or scene and developed from low-level visual features. In <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr">Borth et al.</ref> propose a framework to build visual sentiment ontology and SentiBank according to 1, 200 adjective noun pairs (ANP) corresponding to different emotions. At the same time, a similar approach is proposed by Yuan et al. in <ref type="bibr" target="#b5">[5]</ref> where semantic-level features are also employed in sentiment analysis. Instead of ANP, 102 scene attributes and facial expressions are employed as semantic-level features. Jiang et al. evaluate the performance of semantic-level features in video sentiment analysis <ref type="bibr" target="#b19">[18]</ref>. Zhao et al. also utilize semantic-level features to predict continuous probability distribution of image emotions <ref type="bibr" target="#b6">[6]</ref>.</p><p>With the recent development of CNNs, particularly the success of AlexNet <ref type="bibr" target="#b37">[36]</ref>,</p><p>GoogLeNet <ref type="bibr" target="#b38">[37]</ref>, VGGNet <ref type="bibr" target="#b39">[38]</ref> and ResNet <ref type="bibr" target="#b40">[39]</ref> in object recognition, researchers have strived to devise deep architectures for analyzing visual sentiment. In <ref type="bibr" target="#b41">[40]</ref> and <ref type="bibr" target="#b7">[7]</ref> age the ambiguity and relationship between emotional categories for visual sentiment prediction <ref type="bibr" target="#b27">[26]</ref>. Another scheme on sentiment analysis is to classify ANP of an image by exploiting a CNN <ref type="bibr" target="#b22">[21]</ref>. Furthermore, a deep coupled adjective and noun neural networks is presented to discover the shared features of the same adjective/noun <ref type="bibr" target="#b9">[9]</ref>. It is also worth mentioning that recently in the work of <ref type="bibr" target="#b32">[31]</ref> and <ref type="bibr" target="#b33">[32]</ref>, information from different image regions is employed for image emotion and sentiment analysis.</p><p>In short, our work belongs to deep learning-based architectures. Different from most of the aforementioned deep models, which often take an image as a whole and convert the entire image into one representation, our approach contributes by not only locating the most indicative regions to infer image sentiment, but also studying how the architecture for sentiment prediction could be better devised by integrating this kind of attention mechanism. Moreover, unlike the work of <ref type="bibr" target="#b32">[31]</ref>, which does sentiment analysis on the basis of exploiting the relationship between region features and visual attributes, and the work of <ref type="bibr" target="#b33">[32]</ref>, which combines global information and local object and sentiment information, our work focuses on solving this problem by directly extracting discriminative features based on visual attention and saliency detection for sentiment analysis, and does not rely on the visual attribute selection and the candidate region proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual Attention</head><p>Visual attention aims to record human eye movements. That is, we move our eyes to bring a particular portion of the visible field to the center of our gaze so that we may see in detail. Most often we divert our attention to the regions of interest where we will focus our concentration on. This will give us some insights or clues what people perceive when they are viewing an image. Recently, visual attention has been studied in the area of multimedia and computer vision for improving the efficacy of vision-related applications. In particular, visual attention which is directly produced via saliency detection can be simply regarded as a pre-processing procedure of several tasks, including image compression, stylized rendering and object recognition <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b44">43]</ref>. Instead of di-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>rectly copying saliency map as attention distribution, the visual attention mechanism is further seamlessly incorporated into deep learning architecture for achieving outstanding performances in vision-related tasks, e.g., image classification <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b47">46]</ref>, image generation <ref type="bibr" target="#b48">[47]</ref>, image captioning <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49]</ref> and visual question answering <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b52">51]</ref>.</p><p>Most specifically, <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b48">47]</ref> are early works that incorporate visual attention into deep learning architecture. Following the elegant receipt, Xu et al. devise two types of attention mechanism into an end-to-end neural networks for image captioning, aiming to automatically focus on salient objects when generating corresponding words <ref type="bibr" target="#b49">[48]</ref>.</p><p>Later on, an increasing number of deep learning architectures are equipped with the similar visual attention mechanism to further improve their performance in various tasks. For example, Yang et al. develop a multi-layer attention network for image question answering in <ref type="bibr" target="#b51">[50]</ref>, which utilizes the representation of a question as a query to search for the regions in an image that are related to the answer. Zhao et al. build a diversified visual attention network to address the problem of fine-grained object classification <ref type="bibr" target="#b47">[46]</ref>. Gao et al. integrate attention mechanism with LSTM to capture salient structures of videos, and explores the correlation between multi-modal representations for generating sentences with rich semantic content in <ref type="bibr" target="#b53">[52]</ref>. Most recently, in <ref type="bibr" target="#b54">[53]</ref>, Zhao et al. introduce an approach with visual attention for unsupervised video representation learning.</p><p>In summary, our work presents the first effort to leverage visual attention in image sentiment analysis. Unlike the learning of attention map in existing techniques which optimize the entire architecture solely towards the classification loss without any constraint, ours is different in the way that saliency map is additionally explored as a regularizer to holistically guide the estimation of visual attention. We capitalize on a multi-scale fully convolutional network for the purpose of saliency detection, which will be elaborated in Section 3.3. We believe that exploiting visual attention is a promising direction for image sentiment analysis and the devised saliency detection based regularizer has a great potential to be an elegant solution of holistically correcting attention distribution. The image representations weighted by the learnt attention are input into a fully-connected layer for sentiment prediction. Furthermore, a multi-scale Fully Convolutional Network (FCN) is employed to generate the saliency map and the distance between saliency map and attention map is utilized as a regularizer in addition to the classification loss of image sentiment. The whole framework is jointly learnt in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sentiment Networks with Visual Attention</head><p>An overview of our Sentiment Networks with visual Attention (SentiNet-A) architecture is shown in Figure <ref type="figure" target="#fig_3">2</ref>. Specifically, the proposed SentiNet-A consists of three main components: a CNN which is pre-trained on object recognition task and exploited for learning image representations, a multi-layer neural network to estimate the attention distribution (map) over all the regions towards image sentiment prediction, and a multi-scale FCN to produce a global saliency map. In the stage of sentiment prediction, the image representations weighted by the learnt attention are input into a fully-connected layer for classification. The training of SentiNet-A is jointly performed by simultaneously minimizing the classification loss of image sentiment and the Euclidean distance between the estimated attention map and output saliency map.</p><p>The whole SentiNet-A framework is end-to-end trainable.</p><p>In the following, we will first introduce the CNN architecture used in this work for image representation extraction, followed by describing the visual attention mech-</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>anism. Then, we detail the component of saliency detection and formulate the joint objective function of SentiNet-A. The whole training process is finally presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Representation Extraction</head><p>The rise of CNN convincingly demonstrates high capability of learning visual representation especially in image domain. In general, the features extracted from different layers in CNN have different properties <ref type="bibr" target="#b7">[7]</ref>. For instance, the features at the top layer (e.g., fully-connected layer) tend to express high-level semantic information while the activations at the bottom layers reflect the features in the low-level forms, e.g., corners or edges. Inspired by recent works <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b51">[50]</ref> which exploit image representation with spatial information from Convolutional and pooling layers for image captioning and image question answering, respectively, we follow this elegant recipe and utilize the output of Convolutional layer to represent local regions of the input image in the stage of image representation extraction in our framework. Without loss of generality, we choose a widely adopted CNN architecture, i.e., VGGNet <ref type="bibr" target="#b39">[38]</ref>, as our basic CNN model for producing image representation. Specifically, the extracted image feature map f I from a raw image I through VGGNet is denoted as:</p><formula xml:id="formula_2">f I = CN N V GGN et (I) . (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>The VGGNet is pre-trained on the ILSVRC image recognition dataset which is a subset of ImageNet <ref type="bibr" target="#b55">[54]</ref>, consisting of about 1.2 million labeled images from 1, 000 different categories. We extract the feature map of Conv5 3 layer with a size of h × x × x, where</p><p>x × x is the number of spatial regions in the image and determined by both the network architecture and the resolution of the input image, h is the representation dimension for each region and only determined by the architecture of the network. More specifically, the feature representation of the i-th region is denoted as</p><formula xml:id="formula_4">f i , i = 1, • • • , x 2 .</formula><p>Hence, the whole feature map can be represented as</p><formula xml:id="formula_5">F I = [f 1 , f 2 , . . . , f x 2 ] ∈ R h×x 2 . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>In our case, we feed the input image with resolution of 225 × 225 into VGGNet. The outputs of Conv5 3 layer is 15 × 15 and the dimension h is 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Estimation</head><p>One of the most curious aspects of the human visual system is the presence of attention, which allows for bringing a particularly salient field to the forefront of our gaze. The attention mechanism is also vital to sentiment analysis, in which we distill the most informative regions in an image to infer the sentiment. While the utilization of attention has been proved as one effective solution toward image understanding in previous works, there are no rigorous studies yet on how to integrate the attention into sentiment classification. Therefore, it necessitates a principle way to learn a model that can attend to salient regions of an image when predicting its sentiment.</p><p>Deriving from the idea of leveraging spatial attention for boosting image question answering in <ref type="bibr" target="#b51">[50]</ref>, we develop a multi-layer neural network next to the CNN for image representation extraction to model where the visual attention focuses on. Technically, given the input image I and its feature matrix F I , we feed it into a neural network consisting of two 1 × 1 convolutional layers and then a sof tmax function to generate the attention distribution over all the local regions of the image:</p><formula xml:id="formula_7">h A = W 1 [tanh (W 2 F I + b)] ,<label>(3)</label></formula><formula xml:id="formula_8">A I = sof tmax (h A ) ,<label>(4)</label></formula><p>where W 1 ∈ R 1×k and W 2 ∈ R k×h are two parameter matrices. tanh (•) is the standard non-linear hyperbolic tangent function and b ∈ R k is a k-dimension bias vector, respectively. Accordingly, A I ∈ R x 2 is a x 2 -dimension vector, which corresponds to the attention distribution over all image regions and its i-th element A Ii is the specific attention probability of the i-th image region.</p><p>Based on the attention distribution, we calculate the sum of the feature vector of each image region weighted by its attention probability. Hence, the aggregated image representation f can be written as</p><formula xml:id="formula_9">f = x 2 i=1 A Ii f i .<label>(5)</label></formula><p>Then, the aggregated feature vector f can be directly regarded as the final image representation and fed into a softmax layer to predict the image sentiment. The softmax loss of sentiment classification is measured as</p><formula xml:id="formula_10">A C C E P T E D M A N U S C R I P T (a) (b) (c)</formula><formula xml:id="formula_11">L s = - 1 n n i=1 m j=1 Γ (yi=j) log e θ j fi m l=1 e θ l fi ,<label>(6)</label></formula><p>where f i is the aggregated image representation of the i-th image in the training batch through visual attention estimation, θ j denotes the parameter matrix in the softmax layer and y i ∈ {1, ..., m} represents the sentiment class label of the i-th image. n is the batch size and the number of classes m is set to 2 in our case, i.e., positive and negative.</p><p>The indicator function Γ condition = 1 if condition is true; otherwise Γ condition = 0. By minimizing the softmax loss, the architecture is learnt to classify the sentiment of each input image correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Saliency Detection</head><p>Ideally, our attention estimation could locate the regions in the image, then filter out the irrelevant ones and finally highlight the most informative regions for predicting the sentiment. Nevertheless, by examining the images from the datasets of sentiment analysis, we found that the regions where the attention focuses on are not always accurate.</p><p>Take the image in Figure <ref type="figure" target="#fig_4">3</ref>(a) as an example, the attention map learnt in Section 3.2 is only scattered on paper boats as shown in Figure <ref type="figure" target="#fig_4">3</ref>(b), while ignoring the regions of two lovely girls which give a main clue to infer the sentiment. This observation motivates us to explore an important issue not fully explored and studied that how different regions should be treated in the attention learning in view that they could vary largely from the saliency perspective. We address this issue by integrating saliency detection [11] into attention estimation as a regularizer to govern the contribution of different regions/objects, following the theory that some form of regularization is needed to improve the expressive power of deep architecture. As such, the saliency map is exploited as a holistic prior to guide the learning of visual attention, resulting in a more reasonable attention map in Figure <ref type="figure" target="#fig_4">3(c)</ref>.</p><formula xml:id="formula_12">A C C E P T E D M A N U S C R I P T  CONV1_1+RELU  CONV1_2+RELU POOL1  CONV2_1+RELU  CONV2_2+RELU POOL2  CONV3_1+RELU  CONV3_2+RELU  CONV3_3+RELU POOL3  CONV4_1+RELU  CONV4_2+RELU  CONV4_3+RELU POOL4  CONV5_1+RELU  CONV5_2+RELU <label>CONV5_3+RELU</label></formula><p>Deriving from the idea of utilizing a fully convolutional network (FCN) for saliency detection <ref type="bibr" target="#b11">[11]</ref>, we develop a multi-scale FCN architecture which takes advantages of capturing visual contrast among multi-scale feature maps to generate a saliency map.</p><p>The basic architecture of our multi-scale FCN is re-purposed from 16-layer VGGNet and illustrated in Figure <ref type="figure" target="#fig_5">4</ref>. Specifically, the two original fully-connected layers in VGGNet are converted into convolutional layers with 1 × 1 kernels. To make the prediction map denser, we modify the pooling stride of POOL4 and POOL5 layers from Fetch input batch with sampled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Compute the feature map F i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute the attention distribution A i in Eq.( <ref type="formula" target="#formula_8">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Aggregate the image representation with attention probability via Eq.( <ref type="formula" target="#formula_9">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Predict the sentiment via a softmax layer and compute the classification loss in Eq.( <ref type="formula" target="#formula_11">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Generate the saliency map S i and calculate the Euclidean distance between A i and S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update the network w.r.t. the joint loss in Eq.( <ref type="formula" target="#formula_14">8</ref>).</p><p>10: end for group of three convolutional layers which consist of 128 kernels with the size of 3 × 3, 128 kernels with the size of 1 × 1 and 1 kernel with the size of 1 × 1, respectively.</p><p>The last output layer of VGGNet is followed by one convolutional layer which has 1 kernel with the size of 1 × 1. This architecture enables the network to discover subtle visual details. Since the six output feature maps generated by the above six groups of convolutional layers have different resolution, interpolation layers are introduced to transform all feature maps to the same size. Finally, we stack these six feature maps after interpolation along the channel dimension and feed them into a convolutional layer with a 1 × 1 kernel and a single output channel, which outputs the saliency map S. It is also worth noting that the sigmoid activation function and softmax function are employed to normalize the final output saliency map.</p><p>To govern the learning of attention map by the obtained saliency map, one natural way is to minimize the Euclidean distance between the generated saliency map and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>target attention map, which is formally calculated by</p><formula xml:id="formula_13">L e = 1 n n i=1 A i -S i 2 2 ,<label>(7)</label></formula><p>where A i and S i denotes the attention map and the saliency map of i-th image in the training batch, respectively. By minimizing this Euclidean distance, the training of attention map leverages the saliency of different regions for improving attention inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Objective</head><p>The overall objective function of our SentiNet-A integrates the softmax loss of sentiment classification in Eq.( <ref type="formula" target="#formula_11">6</ref>) and Euclidean distance in Eq.( <ref type="formula" target="#formula_13">7</ref>). Hence, we obtain the following optimization problem:</p><formula xml:id="formula_14">L = L s + λL e = - 1 n n i=1 m j=1 Γ (yi=j) log e θ j fi m l=1 e θ l fi -λ A i -S i 2 2 , (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>where λ is the tradeoff parameter. With this overall loss objective, the crucial goal of its optimization is to learn the attention map. In the training stage, we evaluate the model's violation of both sentiment classification loss plus the regularizer of saliency map, and back-propagates the gradients with respect to the layers for attention estimation and the lower layers for image representation extraction. Please note that the part of multi-scale FCN for saliency detection is pre-trained on the MSRA-B saliency detection dataset <ref type="bibr" target="#b56">[55]</ref>, and fixed in the learning of attention distribution. The whole training process of our SentiNet-A is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate and compare our proposed SentiNet-A with some state-of-the-art approaches by conducting image sentiment analysis task on two image sentiment benchmarks, i.e., Twitter dataset <ref type="bibr" target="#b8">[8]</ref> and ARTphoto dataset <ref type="bibr" target="#b17">[16]</ref>. The former is the most popular image sentiment benchmark collected from tweets and the latter is a public dataset of artistic photos from eight emotional categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">ARTphoto Dataset</head><p>ARTphoto is a public dataset with 806 artistic photos crawled from an art sharing site. It consists of eight emotional categories, i.e., "amusement," "excitement," "contentment," "awe," "disgust," "anger," "fear," and "sad." These photos are taken by people who attempt to evoke a certain emotion in the viewer of the photograph through the conscious manipulation of the image position, lighting, colors, etc. To conduct image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Twitter Dataset</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the performance of eleven runs on Twitter dataset. Overall, the quantitative results across all the four evaluation metrics, including Precision, Recall,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on ARTphoto Dataset</head><p>Figure <ref type="figure">6</ref> shows the experimental results of classifying ARTphoto dataset into two categories. Overall, our SentiNet-A consistently outperforms others. In particular, the F1 score for positive images and negative images can achieve 0.699 and 0.746 while the Accuracy achieves 0.725, which make the improvement over the best competitor SentiNet-S by 1.0%, 1.4% and 1.3%. Though both runs involve the utilization of visual attention, they are fundamentally different in the way that the performance of SentiNet-S is as a result of directly treating saliency map as attention distribution, while SentiNet-A is by employing saliency map as a prior to holistically govern attention estimation for image sentiment analysis. This somewhat reveals the weakness of SentiNet-S, where the attention distribution is directly copied from saliency map, leaving the relationship between visual attention and image sentiment not fully explored.</p><p>Similar to the observations on Twitter dataset, the methods that analyze sentiment with visual attention (e.g., SentiNet-A, SentiNet-A -and SentiNet-S) in general perform better than the ones without the power of visual attention, which again demonstrates the advantage of incorporating visual attention into image sentiment analysis.</p><p>Moreover, there is also a performance gap between CNN-based methods and lowlevel/semantic-level feature-based methods, which double verifies the effectiveness of CNN on image sentiment analysis.    for investigating the relationship between the performance and the tradeoff parameter.</p><p>Specifically, we report the performance over "Five Agree" partition of Twitter dataset, which has no disagreement in judging the sentiment. From Figure <ref type="figure">10</ref>, we can see that all performance curves are like the "∧" shape when λ varies in a range from 1 × 10 3 to 1 × 10 6 . The peak performance is achieved when λ is about 1 × 10 5 . This proves that it is reasonable to jointly learn the visual attention from both the guidance of saliency map interpreted as the general attention and the sentiment-specific attention. As shown before, our method heavily relies on the performance of the pre-trained saliency detection networks. For further studies, we plan to resolve this issue by exploiting the relationship between the visual attention and some other information, such as the ANP descriptions and the surrounding texts of the images. In the future, we will also try to apply the visual attention to video sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is a problem not yet fully understood in the literature. Moreover, we comprehensively explore how to capitalize on saliency map to holistically refine visual attention learning. Extensive experiments on two datasets validate our proposal in the context of both two-class and eight-class sentiment classification. In addition, we provide thorough discussions on the good practices of training our architecture and integrations with saliency detection in different ways.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, Wang et al. propose interpretable aesthetic features, such as composition and color pattern, to predict emotions. Zhao et al. extract principles-of-art-based A C C E P T E D M A N U S C R I P T emotion features to classify and score image emotions in [33] and Jiang et al. evaluate the performance of different low-level descriptors in video sentiment analysis [18]. Wang et al. exploit high-order dependencies among emotions on the basis of low-level features to do multiple emotion tagging [3]. Rao et al. propose an image emotion classification method based on multiple instance learning [19].Image sentiment analysis with multimodal low-level features also attracts attention from the researchers. For instance, Poria et al. perform multimodal sentiment analysis by fusing audio, visual and textual clues in [34]. Similarly, Baecchi et al. propose a multimodal feature learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, Xu et al. and Campos et al. fulfill the fine-tuning of a CNN for sentiment prediction, respectively. Different from their methods to fine-tune existing deep architectures, You et al. design a progressively learnt CNN for classifying image sentiment in [8]. In another work by You et al. [41], texts and images are jointly leveraged to model multimodality sentiment. Moreover, Pang et al. do affective analysis and retrieval by deep multi-modal learning [20]. Zhao et al. formulate the image sentiment analysis task as a probability distribution learning problem, and solve it with weighted multimodal conditional probability neural networks [25]. The work in [27] propose a unified CNN-RNN model to A C C E P T E D M A N U S C R I P T predict the emotion based on the fused features from different levels by exploiting the dependency among them. Yang et al. develop a multi-task deep framework to lever-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of our Sentiment Networks with visual Attention (SentiNet-A) architecture for image sentiment analysis (better viewed in color). The image representation is extracted by a CNN, followed by a multi-layer neural network for attention estimation over all the regions.</figDesc><graphic coords="9,118.48,151.22,330.98,131.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of visual attention. (a) original image; (b) attention map learnt in Section 3.2; (c) attention map learnt by additionally taking saliency map into account in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of our multi-scale FCN for saliency detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 to 1 Algorithm 1 1 : 2 :</head><label>1112</label><figDesc>to preserve the resolution of the output in these two pooling layers. Moreover, both the input image and the first four pooling layers of VGGNet are followed by a A C C E P T E D M A N U S C R I P T The training of Sentiment Networks with visual Attention (SentiNet-A) framework Input: Given the training images {x i }, initialize the parameters θ and learning rate, and set the number of maximum training iteration T . for t = 1 to T do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4 × 4</head><label>4</label><figDesc>blocks and then calculates a 64-bin RGB histogram for every block. Some other methods make use of the bag of visual word (BoW) features whose dictionary are learnt by SIFT features. Moreover, GCH+BoW and LCH+BoW denotes the feature combination of GCH and BoW, LCH and BoW, respectively. A SVM classifier for sentiment prediction is finally built on each of these low-level features. • Semantic-level Feature-based: SentiBank [4] consists of 1, 200 responses generated by 1, 200 ANPs detectors trained on Flickr images. Similar to SentiBank, Sentribute [5] also employs semantic-level features for image sentiment classification. However, the semantic-level features of Sentribute are defined with scene-based attributes, instead of ANPs in SentiBank. A SVM classifier is constructed on each of the two semantic-level features for sentiment classification. • CNN-based: PCNN [8] fine-tunes CNN by utilizing a progressive strategy for visual sentiment analysis. For fair comparison, we also exploit VGGNet as the basic network in PCNN. Sentiment Networks with visual Attention (SentiNet-A) is the proposal in this paper. Derived from our SentiNet-A, a run namely SentiNet is to averagely fuse the representations of local regions as the final A C C E P T E D M A N U S C R I P T image representation. The attention mechanism is not involved in the learning of SentiNet. Another two different settings of SentiNet-A are named as SentiNet-A -and SentiNet-S, which learns the visual attention without the holistic prior from saliency map or directly takes the saliency map as the learnt attention map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F1</head><label></label><figDesc>score, and Accuracy, consistently indicate that our proposed SentiNet-A outperforms other state-of-the-art image sentiment analysis methods on three partitions of this dataset. In particular, the Accuracy of SentiNet-A on the "Five Agree" partition can achieve 0.851, making the relative improvement over LCH+BoW by 18.7% and Sentribute by 15.3%, which is the best-performing one among low-level feature-based and semantic-level feature-based approaches, respectively. There is a significant performance gap between the CNN-based methods and low-level feature-based/semanticlevel feature-based methods, which demonstrates the effectiveness of CNN on image sentiment classification, which is one specific visual recognition task.Compared to PCNN, SentiNet-A -incorporating visual attention mechanism into deep architecture for sentiment classification reaches 0.842 of Accuracy on the "Five Agree" partition, making improvement over PCNN by 1.7%. SentiNet-A by additionally exploiting saliency map as a holistic prior for visual attention estimation, leads to a performance boost against SentiNet-A -. The result basically indicates the advantage of holistically guiding the learning of visual attention for sentiment prediction with the prior knowledge from saliency map. Furthermore, SentiNet-S by additionally leveraging the visual attention from the saliency perspective, improves SentiNet, but the performances are still lower than SentiNet-A. This confirms the effectiveness of the joint visual attention learning with the guidance from saliency map in SentiNet-A, instead of directly copying saliency map as visual attention for image sentiment analysis in SentiNet-S. Please also note that our SentiNet-A almost outperforms all other runs in terms of the precision, the recall and the F1 score of each category.A C C E P T E D MA N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 Figure 5</head><label>55</label><figDesc>Figure 5 visualizes the saliency/attention maps of eight image examples learnt by SentiNet-S, SentiNet-A -and SentiNet-A. In particular, we randomly select four images with positive sentiment and four images with negative sentiment for visualization. For each example, the four images from left to right are the original image, the saliency map learnt by SentiNet-S, the attention map learnt by SentiNet-A -and the attention map learnt by SentiNet-A, where brightness indicates the strength of focus. Please also note that the size of original image is 225 × 225 and the size of attention map is 15 × 15. Hence, we up-sample the attention distribution and apply a Gaussian filter to make the size of attention map the same as the original image as shown in the Figure. Across all the visualization examples, we can easily observe that both of the two attention maps are scattered on the regions that are informative to infer the image sentiment. Specifically, by additionally guiding the visual attention learning with saliency map, the attention learnt by SentiNet-A is more focused on visually salient and distinctive regions for image sentiment prediction. Take the last image in the right column of Figure 5 as an example, SentiNet-S and SentiNet-A -only steers the attention to the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7</head><label>7</label><figDesc>Figure 7 further shows the accuracy performances of different approaches on eightclass emotion classification in ARTphoto dataset. Again, our SentiNet-A outperforms all the other runs. The results indicate that improvement can be generally expected and larger degree of improvement is attained when attention distribution could be correctly estimated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 : 21 AFigure 6 :</head><label>5216</label><figDesc>Figure 5: Visualization of attention learnt by SentiNet-S, SentiNet-A -and SentiNet-A on Twitter dataset (from left to right: the original image, the saliency map from SentiNet-S, the attention map learnt by SentiNet-Aand the attention map learnt by SentiNet-A). 21</figDesc><graphic coords="22,176.73,589.58,54.22,54.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: The eight-class emotion classification performance of different approaches on ARTphoto dataset.</figDesc><graphic coords="23,159.50,496.60,258.35,108.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 Figure 10 :</head><label>810</label><figDesc>Figure 8 details the performances on "Five Agree" partition of Twitter dataset when exploiting different strategies. Both Contrastive and Triplet strategies lead to a performance boost against the basic training strategy reported in our main experiments. Such comparisons shed more light on the impact of training strategies towards the performance of sentiment prediction and provide a discussion on the good practices of training our proposed SentiNet-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>We have presented Sentiment Networks with visual Attention (SentiNet-A) architecture which explores visual attention to enhance image sentiment analysis. Specifically, we study the problem of identifying the most informative regions to infer the sentiment of the image. To verify our claim, a multi-layer neural network is devised and integrated into standard CNN-based image classification framework to estimate the attention distribution. We optimize the whole architecture by minimizing the classification loss of image sentiment plus a regularizer to holistically govern attention learning by saliency map, which is generated by a multi-scale FCN. Extensive experiments conducted on two benchmarks validate our proposal and analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of the Twitter dataset.</figDesc><table><row><cell>Sentiment</cell><cell>Five Agree</cell><cell>At Least Four Agree</cell><cell>At Least Three Agree</cell></row><row><cell>Positive</cell><cell>581</cell><cell>689</cell><cell>769</cell></row><row><cell>Negative</cell><cell>301</cell><cell>427</cell><cell>500</cell></row><row><cell>Sum</cell><cell>882</cell><cell>1116</cell><cell>1269</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of the ARTphoto dataset.</figDesc><table><row><cell></cell><cell>Positive</cell><cell></cell><cell></cell><cell></cell><cell>Negative</cell><cell></cell><cell></cell></row><row><cell cols="5">Amusement Excitement Contentment Awe Disgust</cell><cell>Anger</cell><cell>Fear</cell><cell>Sad</cell></row><row><cell>101</cell><cell>105</cell><cell>70</cell><cell>102</cell><cell>70</cell><cell>77</cell><cell>115</cell><cell>166</cell></row><row><cell>4.1. Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1.1. Twitter Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>Twitter dataset consists of 1, 269 images collected from tweets. Each image in this dataset is manually annotated with a sentiment label by Amazon Mechanical Turk (AMT) workers. Specifically, five AMT workers were recruited to generate sentiment label (positive or negative) for each image. Table</p>1</p>presents the labeling results from AMT. In this table, "Five Agree" means that all the five AMT workers provide the consistent sentiment label for an identical image. Moreover, only 153 images have significant disagreements across all the five AMT workers (3 VS. 2).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Precision (Pre), Recall (Rec), F1 and Accuracy (Acc) scores of our SentiNet-A and other state-of-the-art methods on Twitter dataset.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Positive</cell><cell></cell><cell>Five Agree</cell><cell>Negative</cell><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell cols="3">At Least Four Agree Negative</cell><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell cols="3">At Least Three Agree Negative</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Acc</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Acc</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Acc</cell></row><row><cell>GCH</cell><cell>0.708</cell><cell>0.888</cell><cell>0.787</cell><cell>0.570</cell><cell>0.290</cell><cell>0.385</cell><cell>0.684</cell><cell>0.687</cell><cell>0.84</cell><cell>0.756</cell><cell>0.597</cell><cell>0.383</cell><cell>0.466</cell><cell>0.665</cell><cell>0.678</cell><cell>0.836</cell><cell>0.749</cell><cell>0.607</cell><cell>0.389</cell><cell>0.474</cell><cell>0.66</cell></row><row><cell>LCH</cell><cell>0.764</cell><cell>0.809</cell><cell>0.786</cell><cell>0.585</cell><cell>0.519</cell><cell>0.550</cell><cell>0.71</cell><cell>0.725</cell><cell>0.753</cell><cell>0.739</cell><cell>0.574</cell><cell>0.539</cell><cell>0.556</cell><cell>0.671</cell><cell>0.716</cell><cell>0.737</cell><cell>0.726</cell><cell>0.578</cell><cell>0.552</cell><cell>0.564</cell><cell>0.664</cell></row><row><cell>GCH+BoW</cell><cell>0.724</cell><cell>0.904</cell><cell>0.804</cell><cell>0.645</cell><cell>0.336</cell><cell>0.441</cell><cell>0.71</cell><cell>0.703</cell><cell>0.849</cell><cell>0.769</cell><cell>0.632</cell><cell>0.420</cell><cell>0.505</cell><cell>0.685</cell><cell>0.683</cell><cell>0.835</cell><cell>0.751</cell><cell>0.614</cell><cell>0.404</cell><cell>0.487</cell><cell>0.665</cell></row><row><cell>LCH+BoW</cell><cell>0.771</cell><cell>0.811</cell><cell>0.79</cell><cell>0.595</cell><cell>0.536</cell><cell>0.564</cell><cell>0.717</cell><cell>0.751</cell><cell>0.762</cell><cell>0.756</cell><cell>0.606</cell><cell>0.592</cell><cell>0.599</cell><cell>0.697</cell><cell>0.722</cell><cell>0.726</cell><cell>0.723</cell><cell>0.574</cell><cell>0.569</cell><cell>0.571</cell><cell>0.664</cell></row><row><cell>SentiBank</cell><cell>0.785</cell><cell>0.768</cell><cell>0.776</cell><cell>0.571</cell><cell>0.595</cell><cell>0.583</cell><cell>0.709</cell><cell>0.742</cell><cell>0.727</cell><cell>0.734</cell><cell>0.572</cell><cell>0.591</cell><cell>0.582</cell><cell>0.675</cell><cell>0.720</cell><cell>0.723</cell><cell>0.721</cell><cell>0.572</cell><cell>0.568</cell><cell>0.570</cell><cell>0.662</cell></row><row><cell>Sentribute</cell><cell>0.789</cell><cell>0.823</cell><cell>0.805</cell><cell>0.626</cell><cell>0.574</cell><cell>0.599</cell><cell>0.738</cell><cell>0.75</cell><cell>0.792</cell><cell>0.771</cell><cell>0.632</cell><cell>0.575</cell><cell>0.602</cell><cell>0.709</cell><cell>0.733</cell><cell>0.783</cell><cell>0.757</cell><cell>0.628</cell><cell>0.562</cell><cell>0.593</cell><cell>0.696</cell></row><row><cell>PCNN</cell><cell>0.868</cell><cell>0.868</cell><cell>0.867</cell><cell>0.743</cell><cell>0.744</cell><cell>0.744</cell><cell>0.825</cell><cell>0.824</cell><cell>0.813</cell><cell>0.817</cell><cell>0.701</cell><cell>0.716</cell><cell>0.708</cell><cell>0.776</cell><cell>0.799</cell><cell>0.778</cell><cell>0.786</cell><cell>0.665</cell><cell>0.692</cell><cell>0.678</cell><cell>0.744</cell></row><row><cell>SentiNet</cell><cell>0.880</cell><cell>0.866</cell><cell>0.872</cell><cell>0.746</cell><cell>0.769</cell><cell>0.758</cell><cell>0.833</cell><cell>0.831</cell><cell>0.812</cell><cell>0.821</cell><cell>0.705</cell><cell>0.731</cell><cell>0.718</cell><cell>0.781</cell><cell>0.803</cell><cell>0.776</cell><cell>0.788</cell><cell>0.668</cell><cell>0.702</cell><cell>0.685</cell><cell>0.747</cell></row><row><cell>SentiNet-S</cell><cell>0.884</cell><cell>0.866</cell><cell>0.874</cell><cell>0.751</cell><cell>0.781</cell><cell>0.766</cell><cell>0.837</cell><cell>0.843</cell><cell>0.840</cell><cell>0.841</cell><cell>0.742</cell><cell>0.746</cell><cell>0.744</cell><cell>0.804</cell><cell>0.808</cell><cell>0.800</cell><cell>0.80</cell><cell>0.691</cell><cell>0.701</cell><cell>0.696</cell><cell>0.761</cell></row><row><cell>SentiNet-A -</cell><cell>0.886</cell><cell>0.875</cell><cell>0.880</cell><cell>0.760</cell><cell>0.778</cell><cell>0.769</cell><cell>0.842</cell><cell>0.841</cell><cell>0.830</cell><cell>0.835</cell><cell>0.728</cell><cell>0.744</cell><cell>0.736</cell><cell>0.797</cell><cell>0.812</cell><cell>0.802</cell><cell>0.806</cell><cell>0.697</cell><cell>0.711</cell><cell>0.704</cell><cell>0.766</cell></row><row><cell>SentiNet-A</cell><cell>0.895</cell><cell>0.878</cell><cell>0.886</cell><cell>0.771</cell><cell>0.799</cell><cell>0.785</cell><cell>0.851</cell><cell>0.851</cell><cell>0.835</cell><cell>0.842</cell><cell>0.739</cell><cell>0.762</cell><cell>0.750</cell><cell>0.807</cell><cell>0.823</cell><cell>0.809</cell><cell>0.814</cell><cell>0.709</cell><cell>0.728</cell><cell>0.718</cell><cell>0.777</cell></row></table><note><p><p><p>• Low-level Feature-based: Siersdorfer et al. define some low-level visual features for image sentiment classification in</p><ref type="bibr" target="#b58">[57]</ref></p>. Specifically, the global color histogram (GCH) method utilizes a 64-bin RGB histogram as feature representation, while the local color histogram (LCH) method first divides an image into</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The standard deviations of Precision (Pre), Recall (Rec), F1 and Accuracy (Acc) scores of SentiNet-A on Twitter</figDesc><table><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>Pre</cell><cell>Positive Rec</cell><cell>F1</cell><cell>Pre</cell><cell>Negative Rec</cell><cell>F1</cell><cell>Acc</cell></row><row><cell>Five Agree</cell><cell>0.028</cell><cell>0.028</cell><cell>0.020</cell><cell>0.041</cell><cell>0.053</cell><cell>0.037</cell><cell>0.025</cell></row><row><cell>At Least Four Agree</cell><cell>0.043</cell><cell>0.027</cell><cell>0.022</cell><cell>0.030</cell><cell>0.055</cell><cell>0.045</cell><cell>0.028</cell></row><row><cell>At Least Three Agree</cell><cell>0.043</cell><cell>0.025</cell><cell>0.013</cell><cell>0.013</cell><cell>0.063</cell><cell>0.029</cell><cell>0.013</cell></row><row><cell cols="8">As the 5-fold cross-validation is employed in the experiments, we get 5 numbers</cell></row><row><cell cols="8">for each metric. We also show the standard deviations of different metrics of SentiNet-</cell></row><row><cell>A in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>From the table, we can see that the standard deviations of SentiNet-A are not large. According to these standard deviations, the performance advantages of SentiNet-A against PCNN, Sentribute, SentiBank and other conventional methods in Table 3 are statistically significant</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T sentiment analysis, following <ref type="bibr" target="#b17">[16]</ref>, we simply group all the eight emotional categories into two generic classes, i.e., positive and negative. Experiments on both two categories and eight categories are implemented. Table <ref type="table">2</ref> shows the detailed composition and statistics of the ARTphoto dataset. Such implementations are also helpful in avoiding the over-fitting problem and thus improve the model's generalization ability. The size of hidden layer k in multi-layer neural network for attention estimation is set to 128. The tradeoff parameter λ leveraging the softmax loss and Euclidean distance in Eq.( <ref type="formula">8</ref>) is empirically set to 1 × 10 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>The sensitivity of λ will be discussed in Section 4.6.3.</p><p>In the testing phase, the images are resized into the same size of 256 × 256 as training images. Then we crop and flip each testing image to generate 10 sub-images with the size of 225 × 225. The final sentiment prediction score is calculated by averaging over all the scores of 10 sub-images. Moreover, 5-fold cross-validation is employed to evaluate the performance.</p><p>We implement the proposed method based on the open-source Caffe <ref type="bibr" target="#b57">[56]</ref>, which is one of widely adopted deep learning frameworks. Specifically, our architecture is trained by stochastic gradient descent with the momentum of 0.9 and the weight decay of 0.0005. The learning rate is initially set as 0.01 and reduced by 10 times every 4, 000 iterations. The mini-batch size is 16. All of our experiments are evaluated on a Linux X86 64 machine with 32G RAM and one NVIDIA Tesla K40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Protocols and Baseline Methods</head><p>We follow four evaluation protocols, i.e., Precision, Recall, F1 score, and Accuracy, which are widely used in <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8]</ref> for image sentiment classification tasks. We compare the following approaches for performance evaluation: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">Effect of Saliency Detection in Image Sentiment Analysis</head><p>We further compare several variants of our SentiNet-A by exploiting different ways </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B A</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How do your friends on social media disclose your emotions?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple emotion tagging for multimedia data by exploiting high-order dependencies among emotions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2185" to="2197" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="459" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Continuous probability distribution prediction of image emotions via multitask shared sparse regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="632" to="645" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diving deep into sentiment: Understanding fine-tuned cnns for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM International Workshop on Affect &amp; Sentiment in Multimedia</title>
		<imprint>
			<biblScope unit="page" from="57" to="62" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond object recognition: Visual sentiment analysis with deep coupled adjective and noun neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="3484" to="3490" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving visual saliency computing with emotion intensity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1201" to="1213" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Affective labeling in a content-based recommender system for images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tkalcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kosir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational affective video-invideo advertising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cavva</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image retrieval by emotional semantics: A study of emotional space and feature extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3534" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geusebroek, Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable aesthetic features for affective image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3230" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting emotions in user-generated videos</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale blocks based image emotion classification using multiple instance learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="634" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for affective analysis and retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2015. 2008-2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8586</idno>
		<title level="m">Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multi-level deep representations for image emotion classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07145</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Generating affective maps for images, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discrete probability distribution prediction of image emotions with shared sparse learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning visual emotion distributions via multimodal features fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint image emotion classification and distribution learning via deep convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3266" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dependency exploitation: a unified cnn-rnn approach for visual emotion recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3595" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distribution of image emotions by multi-modal features fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transfer</title>
		<imprint>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting personalized image emotion perceptions in social networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting personalized emotion perceptions of social images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1385" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual sentiment prediction based on automatic discovery of affective regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring principles-ofart features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fusing audio, visual and textual clues for sentiment analysis from multimodal content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A multimodal feature learning approach for sentiment analysis of social network multimedia</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baecchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2507" to="2525" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visual sentiment prediction with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cetintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5731</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint visual-textual sentiment analysis with deep neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1071" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transition of visual attention assessment in stereoscopic images with evaluation of subjective visual quality and discomfort</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2198" to="2209" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Lcnn: Low-level feature embedded cnn for salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03928</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a thirdorder boltzmann machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning generative models with visual attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1808" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bengio, Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based lstm and semantic consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Videowhisper: Toward discriminative unsupervised video feature learning with attention-based recurrent neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2080" to="2092" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
