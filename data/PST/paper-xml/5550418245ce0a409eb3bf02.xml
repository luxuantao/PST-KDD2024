<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
							<email>karpathy@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<email>ajoulin@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74AC2F36BB5ADF3FC41A2864BAE865A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is significant value in the ability to associate natural language descriptions with images. Describing the contents of images is useful for automated image captioning and conversely, the ability to retrieve images based on natural language queries has immediate image search applications. In particular, in this work we are interested in training a model on a set of images and their associated natural language descriptions such that we can later rank a fixed set of withheld sentences given an image query, and vice versa. This task is challenging because it requires detailed understanding of the content of images, sentences and their inter-modal correspondence. Consider an example sentence query, such as "A dog with a tennis ball is swimming in murky water" (Figure <ref type="figure">1</ref>). In order to successfully retrieve a corresponding image, we must accurately identify all entities, attributes and relationships present in the sentence and ground them appropriately to a complex visual scene. Our primary contribution is in formulating a structured, max-margin objective for a deep neural network that learns to embed both visual and language data into a common, multimodal space. Unlike previous work that embeds images and sentences, our model breaks down and embeds fragments of images (objects) and fragments of sentences (dependency tree relations <ref type="bibr" target="#b0">[1]</ref>) in a common embedding space and explicitly reasons about their latent, inter-modal correspondences. Extensive empirical evaluation validates our approach. In particular, we report dramatic improvements over state of the art methods on image-sentence retrieval tasks on Pascal1K <ref type="bibr" target="#b1">[2]</ref>, Flickr8K <ref type="bibr" target="#b2">[3]</ref> and Flickr30K <ref type="bibr" target="#b3">[4]</ref> datasets. We make our code publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Annotation and Image Search. There is a growing body of work that associates images and sentences. Some approaches focus on the direction of describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, or as a task of automatically generating novel captions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. More closely related to our motivation are methods that allow natural bi-drectional mapping between the two modalities. Socher and Fei-Fei <ref type="bibr" target="#b12">[13]</ref> and Hodosh et al. <ref type="bibr" target="#b2">[3]</ref> use Kernel Canonical Correlation Analysis to align images and sentences, but their method is not easily scalable since it relies on computing kernels quadratic in Figure <ref type="figure">1</ref>: Our model takes a dataset of images and their sentence descriptions and learns to associate their fragments. In images, fragments correspond to object detections and scene context. In sentences, fragments consist of typed dependency tree <ref type="bibr" target="#b0">[1]</ref> relations. number of images and sentences. Farhadi et al. <ref type="bibr" target="#b4">[5]</ref> learn a common meaning space, but their method is limited to representing both images and sentences with a single triplet of (object, action, scene). Zitnick et al. <ref type="bibr" target="#b13">[14]</ref> use a Conditional Random Field to reason about spatial relationships in cartoon scenes and their relation to natural language descriptions. Finally, joint models of language and perception have also been explored in robotics settings <ref type="bibr" target="#b14">[15]</ref>.</p><p>Multimodal Representation Learning. Our approach falls into a general category of learning from multi-modal data. Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines <ref type="bibr" target="#b15">[16]</ref>, log-bilinear models <ref type="bibr" target="#b16">[17]</ref>, and topic models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Ngiam et al. <ref type="bibr" target="#b19">[20]</ref> described an autoencoder that learns audio-video representations through a shared bottleneck layer. More closely related to our task and approach is the work of Frome et al. <ref type="bibr" target="#b20">[21]</ref>, who introduced a model that learns to map images and words to a common semantic embedding with a ranking cost. Adopting a similar approach, Socher et al. <ref type="bibr" target="#b21">[22]</ref> described a Dependency Tree Recursive Neural Network that puts entire sentences into correspondence with visual data. However, these methods reason about the image only on the global level using a single, fixed-sized representation from the top layer of a Convolutional Neural Network as a description for the entire image. In contrast, our model explicitly reasons about objects that make up a complex scene.</p><p>Neural Representations for Images and Natural Language. Our model is a neural network that is connected to image pixels on one side and raw 1-of-k word representations on the other. There have been multiple approaches for learning neural representations in these data domains. In Computer Vision, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b22">[23]</ref> have recently been shown to learn powerful image representations that support state of the art image classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> and object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. In language domain, several neural network models have been proposed to learn word/n-gram representations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, sentence representations <ref type="bibr" target="#b34">[35]</ref> and paragraph/document representations <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>Learning and Inference. Our task is to retrieve relevant images given a sentence query, and conversely, relevant sentences given an image query. We train our model on a set of N images and N corresponding sentences that describe their content (Figure <ref type="figure" target="#fig_0">2</ref>). Given this set of correspondences, we learn the weights of a neural network with a structured loss to output a high score when a compatible image-sentence pair is fed through the network, and low score otherwise. Once the training is complete, all training data is discarded and the network is evaluated on a withheld set of images and sentences. The evaluation scores all image-sentence pairs in the test set, sorts the images/sentences in order of decreasing score and records the location of a ground truth result in the list.</p><p>Fragment Embeddings. Our core insight is that images are complex structures that are made up of multiple entities that the sentences make explicit references to. We capture this intuition directly in our model by breaking down both images and sentences into fragments and reason about their alignment. In particular, we propose to detect objects as image fragments and use sentence dependency tree relations <ref type="bibr" target="#b0">[1]</ref> as sentence fragments (Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>Objective. We will compute the representation of both image and sentence fragments with a neural network, and interpret the top layer as high-dimensional vectors embedded in a common multimodal space. We will think of the inner product between these vectors as a fragment compatibility score, and compute the global image-sentence score as a fixed function of the scores of their respective fragments. Intuitively, an image-sentence pair will obtain a high global score if the sentence fragments can each be confidently matched to some fragment in the image. Finally, we will learn the weights of the neural networks such that the true image-sentence pairs achieve a score higher (by a margin) than false image-sentence pairs. We first describe the neural networks that compute the Image and Sentence Fragment embeddings. Then we discuss the objective function, which is composed of the two aforementioned objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dependency Tree Relations as Sentence Fragments</head><p>We would like to extract and represent the set of visually identifiable entities described in a sentence. For instance, using the example in Figure <ref type="figure" target="#fig_0">2</ref>, we would like to identify the entities (dog, child) and characterise their attributes (black, young) and their pairwise interactions (chasing). Inspired by previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> we observe that a dependency tree of a sentence provides a rich set of typed relationships that can serve this purpose more effectively than individual words or bigrams. We discard the tree structure in favor of a simpler model and interpret each relation (edge) as an individual sentence fragment (Figure <ref type="figure" target="#fig_0">2</ref>, right shows 5 example dependency relations). Thus, we represent every word using 1-of-k encoding vector w using a dictionary of 400,000 words and map every dependency triplet (R, w 1 , w 2 ) into the embedding space as follows:</p><formula xml:id="formula_0">s = f W R W e w 1 W e w 2 + b R .<label>(1)</label></formula><p>Here, W e is a d × 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector representation (we use d = 200). We fix W e to weights obtained through an unsupervised objective described in Huang et al. <ref type="bibr" target="#b33">[34]</ref>. Note that every relation R can have its own set of weights W R and biases b R . We fix the element-wise nonlinearity f (.) to be the Rectified Linear Unit (ReLU), which computes x → max(0, x). The size of the embedded space is cross-validated, and we found that values of approximately 1000 generally work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detections as Image Fragments</head><p>Similar to sentences, we wish to extract and describe the set of entities that images are composed of. Inspired by prior work <ref type="bibr" target="#b6">[7]</ref>, as a modeling assumption we observe that the subject of most sentence descriptions are attributes of objects and their context in a scene. This naturally motivates the use of objects and the global context as the fragments of an image. In particular, we follow Girshick et al. <ref type="bibr" target="#b26">[27]</ref> and detect objects in every image with a Region Convolutional Neural Network (RCNN). The CNN is pre-trained on ImageNet <ref type="bibr" target="#b36">[37]</ref> and finetuned on the 200 classes of the ImageNet Detection Challenge <ref type="bibr" target="#b37">[38]</ref>. We use the top 19 detected locations and the entire image as the image fragments and compute the embedding vectors based on the pixels I b inside each bounding box as follows:</p><formula xml:id="formula_1">v = W m [CNN θc (I b )] + b m ,<label>(2)</label></formula><p>where CNN(I b ) takes the image inside a given bounding box and returns the 4096-dimensional activations of the fully connected layer immediately before the classifier. The CNN architecture is identical to the one described in Girhsick et al. <ref type="bibr" target="#b26">[27]</ref>. It contains approximately 60 million parameters θ c and closely resembles the architecture of Krizhevsky et al <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective Function</head><p>We are now ready to formulate the objective function. Recall that we are given a training set of N images and corresponding sentences. In the previous sections we described parameterized functions that map every sentence and image to a set of fragment vectors {s} and {v}, respectively. All parameters of our model are contained in these two functions. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, our model then interprets the inner product v T i s j between an image fragment v i and a sentence fragment s j as a similarity score, and computes the image-sentence similarity as a fixed function of the scores of their respective fragments.</p><p>We are motivated by two criteria in designing the objective function. First, the image-sentence similarities should be consistent with the ground truth correspondences. That is, corresponding image-sentence pairs should have a higher score than all other image-sentence pairs. This will be enforced by the Global Ranking Objective. Second, we introduce a Fragment Alignment Objective that explicitly learns the appearance of sentence fragments (such as "black dog") in the visual domain. Our full objective is the sum of these two objectives and a regularization term:</p><formula xml:id="formula_2">C(θ) = C F (θ) + βC G (θ) + α||θ|| 2 2 ,<label>(3)</label></formula><p>where θ is a shorthand for parameters of our neural network (θ</p><formula xml:id="formula_3">= {W e , W R , b R , W m , b m , θ c })</formula><p>and α and β are hyperparameters that we cross-validate. We now describe both objectives in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Fragment Alignment Objective</head><p>The Fragment Alignment Objective encodes the intuition that if a sentence contains a fragment (e.g."blue ball", Figure <ref type="figure" target="#fig_1">3</ref>), at least one of the boxes in the corresponding image should have a high score with this fragment, while all the other boxes in all the other images that have no mention of "blue ball" should have a low score. This assumption can be violated in multiple ways: a triplet may not refer to anything visually identifiable in the image. The box that the triplet refers to may not be detected by the RCNN. Lastly, other images may contain the described visual concept but its mention may omitted in the associated sentence descriptions. Nonetheless, the assumption is still satisfied in many cases and can be used to formulate a cost function. Consider an (incomplete) Fragment Alignment Objective that assumes a dense alignment between every corresponding image and sentence fragments:</p><formula xml:id="formula_4">C 0 (θ) = i j max(0, 1 -y ij v T i s j ).<label>(4)</label></formula><p>Here, the sum is over all pairs of image and sentence fragments in the training set. The quantity v T i s j can be interpreted as the alignment score of visual fragment v i and sentence fragment s j . In this incomplete objective, we define y ij as +1 if fragments v i and s j occur together in a corresponding image-sentence pair, and -1 otherwise. Intuitively, C 0 (θ) encourages scores in red regions of Figure <ref type="figure" target="#fig_1">3</ref> to be less than -1 and scores along the block diagonal (green and yellow) to be more than +1.</p><p>Multiple Instance Learning extension. The problem with the objective C 0 (θ) is that it assumes dense alignment between all pairs of fragments in every corresponding image-sentence pair. However, this is hardly ever the case. For example, in Figure <ref type="figure" target="#fig_1">3</ref>, the "boy playing" triplet refers to only one of the three detections. We now describe a Multiple Instance Learning (MIL) <ref type="bibr" target="#b38">[39]</ref> extension of the objective C 0 that attempts to infer the latent alignment between fragments in corresponding image-sentence pairs. Concretely, for every triplet we put image fragments in the associated image into a positive bag, while image fragments in every other image become negative examples. Our precise formulation is inspired by the mi-SVM <ref type="bibr" target="#b39">[40]</ref>, which is a simple and natural extension of a Support Vector Machine to a Multiple Instance Learning setting. Instead of treating the y ij as constants, we minimize over them by wrapping Equation 4 as follows:</p><formula xml:id="formula_5">C F (θ) = min yij C 0 (θ) s.t. i∈pj y ij + 1 2 ≥ 1 ∀j y ij = -1 ∀i, j s.t. m v (i) = m s (j) and y ij ∈ {-1, 1}<label>(5)</label></formula><p>Here, we define p j to be the set of image fragments in the positive bag for sentence fragment j. m v (i) and m s (j) return the index of the image and sentence (an element of {1, . . . , N }) that the fragments v i and s j belong to. Note that the inequality simply states that at least one of the y ij should be positive for every sentence fragment j (i.e. at least one green box in every column in Figure <ref type="figure" target="#fig_1">3</ref>). This objective cannot be solved efficiently <ref type="bibr" target="#b39">[40]</ref> but a commonly used heuristic is to set</p><formula xml:id="formula_6">y ij = sign(v T i s j ).</formula><p>If the constraint is not satisfied for any positive bag (i.e. all scores were below zero), the highest-scoring item in the positive bag is set to have a positive label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Global Ranking Objective</head><p>Recall that the Global Ranking Objective ensures that the computed image-sentence similarities are consistent with the ground truth annotation. First, we define the image-sentence alignment score to be the average thresholded score of their pairwise fragment scores:</p><formula xml:id="formula_7">S kl = 1 |g k |(|g l | + n) i∈g k j∈g l max(0, v T i s j ).<label>(6)</label></formula><p>Here, g k is the set of image fragments in image k and g l is the set of sentence fragments in sentence l. Both k, l range from 1, . . . , N . We truncate scores at zero because in the mi-SVM objective, scores greater than 0 are considered correct alignments and scores less than 0 are considered to be incorrect alignments (i.e. false members of a positive bag). In practice, we found that it was helpful to add a smoothing term n, since short sentences can otherwise have an advantage (we found that n = 5 works well and that this setting is not very sensitive). The Global Ranking Objective then becomes:</p><formula xml:id="formula_8">C G (θ) = k l max(0, S kl -S kk + ∆) rank images + l max(0, S lk -S kk + ∆) rank sentences .<label>(7)</label></formula><p>Here, ∆ is a hyperparameter that we cross-validate. The objective stipulates that the score for true image-sentence pairs S kk should be higher than S kl or S lk for any l = k by at least a margin of ∆.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>We use Stochastic Gradient Descent (SGD) with mini-batches of 100, momentum of 0.9 and make 20 epochs through the training data. The learning rate is cross-validated and annealed by a fraction of ×0.1 for the last two epochs. Since both Multiple Instance Learning and CNN finetuning benefit from a good initialization, we run the first 10 epochs with the fragment alignment objective C 0 and CNN weights θ c fixed. After 10 epochs, we switch to the full MIL objective C F and begin finetuning the CNN. The word embedding matrix W e is kept fixed due to overfitting concerns. Our implementation runs at approximately 1 second per batch on a standard CPU workstation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate our image-sentence retrieval performance on Pascal1K <ref type="bibr" target="#b1">[2]</ref>, Flickr8K <ref type="bibr" target="#b2">[3]</ref> and Flickr30K <ref type="bibr" target="#b3">[4]</ref> datasets. The datasets contain 1,000, 8,000 and 30,000 images respectively and each image is annotated using Amazon Mechanical Turk with 5 independent sentences. Sentence Data Preprocessing. We did not explicitly filter, spellcheck or normalize any of the sentences for simplicity. We use the Stanford CoreNLP parser to compute the dependency trees for every sentence. Since there are many possible relation types (as many as hundreds), due to overfitting concerns and practical considerations we remove all relation types that occur less than 1% of the time in each dataset. In practice, this reduces the number of relations from 136 to 16 in Pascal1K, 170 to 17 in Flickr8K, and from 212 to 21 in Flickr30K. Additionally, all words that are not found in our dictionary of 400,000 words <ref type="bibr" target="#b33">[34]</ref> are discarded. Image Data Preprocessing. We use the Caffe <ref type="bibr" target="#b40">[41]</ref> implementation of the ImageNet Detection RCNN model <ref type="bibr" target="#b26">[27]</ref> to detect objects in all images. On our machine with a Tesla K40 GPU, the RCNN processes one image in approximately 25 seconds. We discard the predictions for 200 ImageNet detection classes and only keep the 4096-D activations of the fully connect layer immediately before the classifier at all of the top 19 detected locations and from the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pascal1K Image Annotation</head><p>Image Search Model R@1 R@5 R@10 Mean r R@1 R@5 R@ Table <ref type="table">1</ref>: Pascal1K ranking experiments. R@K is Recall@K (high is good). Mean r is the mean rank (low is good). Note that the test set only consists of 100 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flickr8K Image Annotation</head><p>Image Search Model R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r Random Ranking 0.1 0. Table <ref type="table">2</ref>: Flickr8K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).</p><p>The starred evaluation criterion (*) in <ref type="bibr" target="#b2">[3]</ref> is slightly different since it discards 4,000 out of 5,000 test sentences.</p><p>Evaluation Protocol for Bidirectional Retrieval. For Pascal1K we follow Socher et al. <ref type="bibr" target="#b21">[22]</ref> and use 800 images for training, 100 for validation and 100 for testing. For Flickr datasets we use 1,000 images for validation, 1,000 for testing and the rest for training (consistent with <ref type="bibr" target="#b2">[3]</ref>). We compute the dense image-sentence similarity S kl between every image-sentence pair in the test set and rank images and sentences in order of decreasing score. For both Image Annotation and Image Search, we report the median rank of the closest ground truth result in the list, as well as Recall@K which computes the fraction of times the correct result was found among the top K items. When comparing to Hodosh et al. <ref type="bibr" target="#b2">[3]</ref> we closely follow their evaluation protocol and only keep a subset of N sentences out of total 5N (we use the first sentence out of every group of 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison</head><p>Methods SDT-RNN. Socher et al. <ref type="bibr" target="#b21">[22]</ref> embed a fullframe CNN representation with the sentence representation from a Semantic Dependency Tree Recursive Neural Network (SDT-RNN). Their loss matches our global ranking objective. We requested the source code of Socher et al. <ref type="bibr" target="#b21">[22]</ref> and substituted the Flickr8K and Flick30K datasets. To better understand the benefits of using our detection CNN and for a more fair comparison we also train their method with our CNN features. Since we have multiple objects per image, we average representations from all objects with detection confidence above a (cross-validated) threshold. We refer to the exact method of Socher et al. <ref type="bibr" target="#b21">[22]</ref> with a single fullframe CNN as "Socher et al", and to their method with our combined CNN features as "SDT-RNN".</p><p>DeViSE. The DeViSE <ref type="bibr" target="#b20">[21]</ref> source code is not publicly available but their approach is a special case of our method with the following modifications: we use the average (L2-normalized) word vectors as a sentence fragment, the average CNN activation of all objects above a detection threshold (as discussed in case of SDT-RNN) as an image fragment and only use the global ranking objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>Our model outperforms previous methods. Our full method consistently outperforms previous methods on Flickr8K (Table <ref type="table">2</ref>) and Flickr30K (Table <ref type="table">3</ref>) datasets. On Pascal1K (Table <ref type="table">1</ref>) the SDT-RNN appears to be competitive on Image Search.</p><p>Fragment and Global Objectives are complementary. As seen in Tables <ref type="table">2</ref> and<ref type="table">3</ref>, both objectives perform well independently, but benefit from the combination. Note that the Global Objective performs consistently better, possibly because it directly minimizes the evaluation criterion (ranking Flickr30K Image Annotation Image Search Model R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r Random Ranking 0.1 0. Table <ref type="table">3</ref>: Flickr30K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).  <ref type="table">2</ref>) Dependency tree relations outperform BoW/bigram representations. We compare to a simpler Bag of Words (BoW) baseline to understand the contribution of dependency relations. In BoW baseline we iterate over words instead of dependency triplets when creating bags of sentence fragments (set w 1 = w 2 in Equation1). As can be seen in the Table <ref type="table">2</ref>, this leads to a consistent drop in performance. This drop could be attributed to a difference between using one word or two words at a time, so we also compare to a bigram baseline where the words w 1 , w 2 in Equation 1 refer to consecutive words in a sentence, not nodes that share an edge in the dependency tree. Again, we observe a consistent performance drop, which suggests that the dependency relations provide useful structure that the neural network takes advantage of.</p><p>Finetuning the CNN helps on Flickr30K. Our end-to-end Neural Network approach allows us to backpropagate gradients all the way down to raw data (pixels or 1-of-k word encodings). In particular, we observed additional improvements on the Flickr30K dataset (Table <ref type="table">3</ref>) when we finetune the CNN. Training the CNN improves the validation error for a while but the model eventually starts to overfit. Thus, we found it critical to keep track of the validation error and freeze the model before it overfits. We were not able to improve the validation performance on Pascal1K and Flickr8K datasets and suspect that there is an insufficient amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Experiments</head><p>Interpretable Predictions. We show some example sentence retrieval results in Figure <ref type="figure" target="#fig_2">4</ref>. The alignment in our model is explicitly inferred on the fragment level, which allows us to interpret the scores between images and sentences. For instance, in the last image it is apparent that the model retrieved the top sentence because it erroneously associated a mention of a blue person to the blue flag on the bottom right of the image.</p><p>Fragment Alignment Objective trains attribute detectors. The detection CNN is trained to predict one of 200 ImageNet Detection classes, so it is not clear if the representation is powerful enough to support learning of more complex attributes of the objects or generalize to novel classes.</p><p>To see whether our model successfully learns to predict sentence triplets, we fix a triplet vector and search for the highest scoring boxes in the test set. Qualitative results shown in Figure <ref type="figure" target="#fig_3">5</ref> suggest that the model is indeed capable of generalizing to more fine-grained subcategories (such as "black dog", "soccer ball") and to out of sample classes such as "rocky terrain" and "jacket". Limitations. Our model is subject to multiple limitations. From a modeling perspective, the use of edges from a dependency tree is simple, but not always appropriate. First, a single complex phrase that describes a single visual entity can be split across multiple sentence fragments. For example, "black and white dog" is parsed as two relations (CONJ, black, white) and (AMOD, white, dog).</p><p>Conversely, there are many dependency relations that don't have a clear grounding in the image (for example "each other"). Furthermore, phrases such as "three children playing" that describe some particular number of visual entiries are not modeled. While we have shown that the relations give rise to more powerful representations than words or bigrams, a more careful treatment of sentence fragments will likely lead to further improvements. On the image side, the non-maximum suppression in the RCNN can sometimes detect, for example, multiple people inside one person. Since the model does not take into account any spatial information associated with the detections, it is hard for it to disambiguate between two distinct people or spurious detections of one person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We addressed the problem of bidirectional retrieval of images and sentences. Our neural network learns a multi-modal embedding space for fragments of images and sentences and reasons about their latent, inter-modal alignment. We have shown that our model significantly improves the retrieval performance on image sentence retrieval tasks compared to previous work. Our model also produces interpretable predictions. In future work we hope to develop better sentence fragment representations, incorporate spatial reasoning, and move beyond bags of fragments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Computing the Fragment and image-sentence similarities. Left: CNN representations (green) of detected objects are mapped to the fragment embedding space (blue, Section 3.2). Right: Dependency tree relations in the sentence are embedded (Section 3.1). Our model interprets inner products (shown as boxes) between fragments as a similarity score. The alignment (shaded boxes) is latent and inferred by our model (Section 3.3.1). The image-sentence similarity is computed as a fixed function of the pairwise fragment scores.</figDesc><graphic coords="3,108.00,81.86,396.00,106.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The two objectives for a batch of 2 examples. Left: Rows represent fragments vi, columns sj. Every square shows an ideal scenario of yij = sign(v T i sj) in the MIL objective. Red boxes are yij = -1. Yellow indicates members of positive bags that happen to currently be yij = -1. Right: The scores are accumulated with Equation 6 into image-sentence score matrix S kl .</figDesc><graphic coords="4,252.03,81.86,249.48,162.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative Image Annotation results. Below each image we show the top 5 sentences (among a set of 5,000 test sentences) in descending confidence. We also show the triplets for the top sentence and connect each to the detections with the highest compatibility score (indicated by lines). The numbers next to each triplet indicate the matching fragment score. We color a sentence green if it correct and red otherwise.cost), while the Fragment Alignment Objective only does so indirectly. Extracting object representations is important. Using only the global scene-level CNN representation as a single fragment for every image leads to a consistent drop in performance, suggesting that a single fullframe CNN alone is inadequate in effectively representing the images. (Table2) Dependency tree relations outperform BoW/bigram representations. We compare to a simpler Bag of Words (BoW) baseline to understand the contribution of dependency relations. In BoW baseline we iterate over words instead of dependency triplets when creating bags of sentence fragments (set w 1 = w 2 in Equation1). As can be seen in the Table2, this leads to a consistent drop in performance. This drop could be attributed to a difference between using one word or two words at a time, so we also compare to a bigram baseline where the words w 1 , w 2 in Equation 1 refer to consecutive words in a sentence, not nodes that share an edge in the dependency tree. Again, we observe a consistent performance drop, which suggests that the dependency relations provide useful structure that the neural network takes advantage of. Finetuning the CNN helps on Flickr30K. Our end-to-end Neural Network approach allows us to backpropagate gradients all the way down to raw data (pixels or 1-of-k word encodings). In particular, we observed additional improvements on the Flickr30K dataset (Table3) when we finetune the CNN. Training the CNN improves the validation error for a while but the model eventually starts to overfit. Thus, we found it critical to keep track of the validation error and freeze the model before it overfits. We were not able to improve the validation performance on Pascal1K and Flickr8K datasets and suspect that there is an insufficient amount of training data.</figDesc><graphic coords="7,108.00,209.07,395.99,133.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We fix a triplet and retrieve the highest scoring image fragments in the test set. Note that ball, person and dog are ImageNet Detection classes but their visual properties (e.g. soccer, standing, snowboarding, black) are not. Jackets and rocky scenes are not ImageNet Detection classes. Find more in supplementary material.</figDesc><graphic coords="8,146.35,81.86,316.78,154.82" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Justin Johnson and Jon Krause for helpful comments and discussions. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. This research is supported by an ONR MURI grant, and NSF ISS-1115313.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">I2t: Image parsing to text description</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Composing simple image descriptions using web-scale n-grams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoNLL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iii</surname></persName>
		</author>
		<editor>EACL.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Joint Model of Language and Perception for Grounded Attribute Learning</title>
		<author>
			<persName><forename type="first">*</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2012 International Conference on Machine Learning</title>
		<meeting>of the 2012 International Conference on Machine Learning<address><addrLine>Edinburgh; Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Multimodal neural language models. ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning cross-modality similarity for multinomial data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Matching words and pictures</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semisupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/LSVRC/2013/" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Miles: Multiple-instance learning via embedded instance selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple instance learning with generalized support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="943" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
