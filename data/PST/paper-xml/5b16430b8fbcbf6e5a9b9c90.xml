<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Methods and Datasets on Semantic Segmentation: A review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-30">April 30, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hongshan</forename><surname>Yu</surname></persName>
							<email>yuhongshancn@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Robot Visual Perception and Control Technology</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen Research Institute of Hunan University</orgName>
								<address>
									<postCode>518057</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengeng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Robot Visual Perception and Control Technology</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Robot Visual Perception and Control Technology</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaonan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Robot Visual Perception and Control Technology</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Robot Visual Perception and Control Technology</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingui</forename><surname>Sun</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Laboratory for Computational Neuroscience</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yandong</forename><surname>Tang</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Shenyang Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Wei Sun, Mingui Sun, Yandong Tang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Methods and Datasets on Semantic Segmentation: A review</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-30">April 30, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">FFF17821FA210ABC39C02865124C35E6</idno>
					<idno type="DOI">10.1016/j.neucom.2018.03.037</idno>
					<note type="submission">Received date: 22 June 2017 Revised date: 31 January 2018 Accepted date: 19 March 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>Convolutional Neural Network</term>
					<term>Markov Random Fileds</term>
					<term>Weakly supervised method</term>
					<term>3D point clouds labeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation, also called scene labeling, refers to the process of assigning a semantic label (e.g. car, people, and road) to each pixel of an image. It is an essential data processing step for robots and other unmanned systems to understand the surrounding scene. Despite decades of efforts, semantic segmentation is still a very challenging task due to large variations in natural scenes. In this paper, we provide a systematic review of recent advances in this field. In particular, three categories of methods are reviewed and compared, including those based on hand-engineered features, learned features and weakly supervised learning. In addition, we describe a number of popular datasets aiming for facilitating the development of new segmentation algorithms. In order to demonstrate the advantages and disadvantages of different semantic segmentation models, we conduct a series of comparisons between them. Deep discussions about the comparisons are also provided. Finally, this review is concluded by discussing future directions and challenges in this important field of research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the ever-increasing range of intelligent applications(e.g. mobile robots), there is an urgent need for accurate scene understanding. As an essential step towards this goal, semantic segmentation thus has received significant attention in recent years. It refers to a process of assigning a semantic label (e.g. car, people) to each pixel of an image. One</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T main challenge of this task is that there are a large amount of classes in natural scenes and some of them show high degree of similarity in visual appearance.</p><p>The emergence of terminology "semantic segmentation" can be dated back to 1970s <ref type="bibr" target="#b0">[1]</ref>.</p><p>At that time, this terminology was equivalent to image segmentation but emphasized that the segmented regions must be "semantically meaningful". In 1990s, "object segmentation and recognition" <ref type="bibr" target="#b2">[2]</ref> further distinguished semantic objects of all classes from background and can be viewed as a two-class image segmentation problem. As the complete partition of foreground objects from the background is very challenging, a relaxed two-class image segmentation problem: the sliding window object detection <ref type="bibr" target="#b3">[3]</ref>, was proposed to partition objects with bounding boxes. It is useful to find where the objects in the scenes with excellent two-class image segmentation algorithms such as constrained parametric mincuts(CPMC) <ref type="bibr" target="#b4">[4]</ref>. However, two-class image segmentation cannot tell what these objects segmented are. As a result, the generic sense of object recognition(or detection) was gradually extended to multi-class image labeling <ref type="bibr" target="#b5">[5]</ref>, i.e., semantic segmentation in present sense, to tell both where and what the objects in the scene.</p><p>In order to achieve high-quality semantic segmentation, there are two commonly concerned questions: how to design efficient feature representations to differentiate objects of various classes, and how to exploit contextual information to ensure the consistency between the labels of pixels. For the first question, most early methods <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref> benefit from using the hand-engineered features, such as Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b9">[9]</ref> and Histograms of Oriented Gradient(HOG) <ref type="bibr" target="#b10">[10]</ref>. With the development of deep learning <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, the using of learned features in computer vision tasks, such as image classification <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>, has achieved great success in past few years. As a result, the semantic segmentation community recently paid lots of attention to the learned features <ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref>, which are usually refer to Convolutional Neural Networks(CNN or ConvNets) <ref type="bibr" target="#b27">[27]</ref>. For the second issue, the most common strategy, no matter the feature used, is to use contextual models such as Markov Random Field(MRF) <ref type="bibr" target="#b28">[28]</ref> and Conditional Random Field(CRF) <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref>. These graphical models make it very easy to leverage a variety of relationships between classes via setting links between adjacent pixels. More recently, the use of Recurrent Neural Networks(RNN) <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref> are more commonly seen in retrieving contextual information.</p><p>Under the weakly supervised framework <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref>, another challenging issue is how to learn class models from weakly annotated images, whose labels are provided at imagelevel rather than pixel-level. To address this challenge, many methods resort to multiple instance learning(MIL) techniques <ref type="bibr" target="#b42">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Although there are many strategies available for addressing the problems mentioned above, these strategies are not yet mature. For example, there are still no universally accepted hand-engineered features while research on learned features has become a focus again only in recent few years. The inference of MRF or CRF is a very challenging issue in itself and often resort to approximation algorithms. Thus, new and creative semantic segmentation methods are being developed and reported continuously.</p><p>The main motivation of this paper is to provide a comprehensive survey of semantic segmentation methods, focus on analyzing the commonly concerned problems as well as the corresponding strategies adopted. Semantic segmentation is now a vast field and is closely related to other computer vision tasks. This review cannot fully cover the entire field. Since excellent reviews on research achievements on traditional image segmentation, object segmentation and object detection already exist <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>, we will not cover these subjects. We will instead focuses on generic semantic segmentation, i.e., multi-class segmentation. Based on the observation that most works published after 2012 are CNN based, we will divide existing semantic segmentation methods into those based on handengineered features and learned features (see Fig. <ref type="figure">1</ref>). We will discuss weakly supervised methods separately because this challenging line of methods is being investigated actively.</p><p>It should be emphasized that there are no clear boundaries between these three categories.</p><p>For each category, we further divide it into several sub-categories and then analyze their motivations and principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The rest of this paper is organized as follows. Before introducing recent progresses on semantic segmentation, preliminaries of commonly used theories are given in the next section. Methods using hand-engineered features and learned features are systematically reviewed in sections 3 and 4, respectively. The efforts devoted to weakly supervised semantic segmentation are described in section 5. In section 6, we describe several popular datasets for semantic segmentation tasks. Section 7 compares some representative methods using several common evaluation criteria. Finally, we conclude the paper in section 8 with our views on future perspectives. Note that semantic segmentation also called as scene labeling in literature, we will not differentiate between them in the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We start by describing the commonly used theories and technologies in the semantic segmentation community, including superpixels and contextual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Superpixels</head><p>As argued by Ren and Malik <ref type="bibr" target="#b45">[45]</ref>, the superpixel that consists of a set of similar and connected pixels is more appropriate for representing entities compared to the pixel. The benefits of using superpixel can be summarized into two aspects. First, the computational complexity is greatly reduced by treating a set of pixels as a single pixel, i.e., the superpixel.</p><p>Second, a region is able to provide much more cues compared with a single pixel. Thus, the use of superpixel becomes very popular in computer vision.</p><p>Generally, the results of conventional segmentation, such as mean-shift <ref type="bibr" target="#b46">[46]</ref>, Normalized cut <ref type="bibr" target="#b47">[47]</ref>, can be directly taken as superpixels. In addition, the numbers of superpixels can be controlled by tuning the parameters of these algorithms. For implementation details of popular conventional methods, we refer readers to several recent excellent reviews <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>However, superpixels produced from conventional methods usually lack of shape control and lead to inefficiency in feature extraction. In recent years, some faster and better superpixel algorithms were presented in succession, such as Turbopixel <ref type="bibr" target="#b49">[49]</ref> and Simple Linear Iterative Clustering(SLIC) <ref type="bibr" target="#b50">[50]</ref>. Turbopixel produces superpixels through dilating a set of seed locations using geometric flow, which is computationally rooted in the curve evolution techniques. The superpixels in SLIC are generated by clustering the pixels using a localized K-means algorithm, which means that the cluster area is restricted to a local window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contextual models</head><p>It is well known that many successful image processing algorithms benefit from using graph. Generally, graph based methods map an image onto an undirected graph G = {V, E}, where V is the set of vertices composed of all the pixels, and E is the set of edges that connect adjacent pixels. Moreover, each edge in the graph is associated with a weight which depends on some characteristics of the two pixels it connects. In the sematic segmentation community, the graph is usually used in conjunction with Markov theory to build contextual models, such as MRF and CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MRF</head><p>Given the number of semantic classes L and the number of pixels N , denoted the semantic label of each pixel i as y i , the ultimate result of scene labeling can be represented by y = {y 1 , ..., y i , ..., y N }, where y i can take any label l from the discrete class set {1, 2, ..., L}. Thus, all the latent label make up a set Y with L N elements. In the MRF framework, the prior over label y(P (y)) is often modeled as a MRF defined on pixel lattice with a neighborhood system ε. Thus, given the observation x of an image, according to the Bayes theory, the posterior distribution of y can be written as P (y|x) ∝ P (x|y)P (y).</p><p>(1)</p><p>In addition, pixels are often assumed to be independent and identically distributed to compute P (x|y) .</p><formula xml:id="formula_0">P (x|y) = N i=1 P (x i |y i )<label>(2)</label></formula><p>Then, the labeling problem is equivalent to maximum a posteriori(MAP)</p><formula xml:id="formula_1">y * = arg max y∈Y P (y|x).<label>(3)</label></formula><p>MRF-MAP based methods are essentially related to generative models that need to estimate the feature distribution for each class. As argued in <ref type="bibr" target="#b51">[51]</ref>, such generative model based MRFs suffer from two main drawbacks when applying to image processing. First, the assumption of conditional independence between pixels is very restrictive. On top of that, the MAP inference may be quite complex even though the class posterior is simple.</p><p>Hence, besides early studies on image restoration <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b53">53]</ref>, only a few recent studies <ref type="bibr" target="#b28">[28]</ref> are trying to solve scene labeling problem using the MRF-MAP framework. In contrast, the CRF-MAP is more popular in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">CRF</head><p>A CRF <ref type="bibr" target="#b54">[54]</ref> can be viewed as a variant of MRF. As a result, some studies do not differentiate between the two concepts. The CRF directly models the posterior distribution P (y|x) as a Gibbs distribution <ref type="bibr" target="#b51">[51]</ref> </p><formula xml:id="formula_2">P (y|x) == 1 Z exp(-U (y)) = 1 Z exp(- c∈C Ψ(y c |x c )), (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where y is one of the label states and U is the corresponding energy. Z is a normalized constant equal to the sum of energy of all states, which ensures the distribution summed to one. The Ψ(y c |x c ) are potential functions defined over a clique c ∈ C that contains a set of connected pixels. Thus, the solution of MAP can be formulated as a problem of energy minimization</p><formula xml:id="formula_4">y * = arg max y∈Y P (y|x) = arg min y∈Y c∈C Ψ(y c |x c ) = arg min y∈Y E(y|x).<label>(5)</label></formula><p>The commonly used energy function defined on two types of cliques is given by</p><formula xml:id="formula_5">E(y) = N i=1 ϕ i (y i |x i ) + (i,j)∈ε ϕ ij (y i , y j |x ij ),<label>(6)</label></formula><p>where ϕ i and ϕ ij are called unary potential and pairwise potential, respectively. The unary potential reflects how appropriate the assigned label y i for a pixel i. It is usually defined as the negative log of the likelihood of a label being assigned to pixel i <ref type="bibr" target="#b30">[30]</ref>.</p><formula xml:id="formula_6">ϕ i (y i ) = -logP (y i |x i )<label>(7)</label></formula><p>The pairwise potential is used to model the relationships between pixels. If there is only the unary potential, minimizing the energy is equivalent to assigning the pixel with the most appropriate label, i.e., performing pixel-wise classification. From this perspective, the pairwise potential can be understood as an additional constraint for ensuring the consistency between predictions of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Inference and Energy minimization</head><p>RF-MAP based methods shift the focus to minimizing energy function. Unfortunately, the global minimization of an energy function is NP-hard due to the fact that there are many local minima <ref type="bibr" target="#b55">[55]</ref>. This problem forces scholars to find an approximate solution and a number of strategies have been reported, such as Iterated Conditional Modes(ICM) <ref type="bibr" target="#b53">[53]</ref> A</p><formula xml:id="formula_7">C C E P T E D M A N U S C R I P T</formula><p>and Simulated Annealing(SA) <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref>. The idea behind these approximation approaches is to iteratively decrease the energy through a standard move process(change the label of one pixel) until a convergence to a local minimum is reached. However, it is much easier to give low quality approximations with standard moves because the energy can be hardly decreased by changing a single pixel's label when falling into a bad local minimum. To solve this problem, Boykov et al. <ref type="bibr" target="#b55">[55]</ref> reported two graph based algorithms using two types of large moves:α-expansion-moves and α-β-swap-moves. Started at an arbitrary labeling y, the α-expansion algorithm iteratively expands each class label on y in a cycle, where α-expansion means α is allowed to be "expanded" to any pixel not belonging to α. If the result of any expansion, which is formulated as a bi-labeling(α,α) problem and solved by s/t graph cuts algorithm <ref type="bibr" target="#b58">[58]</ref>, in a cycle is strictly better than previous, the algorithm continues the expansion until no further improvement happens. The α-β-swap is quite similar to α-expansion in structure. The only difference lies in the process of move, where α-β-swap means that pixels labeled α(β) are allowed to change to β(α).</p><p>The s/t graph cuts attempts to map two types of energy terms in ( <ref type="formula" target="#formula_5">6</ref>) onto the weightededges so that the min-cut algorithm <ref type="bibr" target="#b59">[59]</ref> is available for energy minimization. However, typical graph includes only one type of links(n-links) that connect adjacent pixel pairs, onto which the pairwise term can be mapped. To map the unary term of bi-labeling energy, the s/t graph cuts defines another type of links(t-links) that connect pixels and virtual terminal nodes s and t. Indeed, the terminal nodes represent the label of segmented regions:</p><p>"object" and "background". With this graph model and special designed weighted-edges, the energy based bi-labeling can be realized via min-cut. A simple example is shown in  <ref type="bibr" target="#b58">[58]</ref>. For convenience, we omit most edges between pixels and only reserve connections(t-links) between labels and pixels. The width of lines represents the probability of pixels belonging to its connected label.</p><p>Besides move-making based methods outlined above, another popular strategy for minimizing energy is to use message-passing. The most well-known method in this framework is the Loopy Belief Propagation(LBP) algorithm <ref type="bibr" target="#b60">[60]</ref>, which is developed from the Belief Propagation(BP) <ref type="bibr" target="#b61">[61]</ref>. The principle of LBP is to iteratively update the belief(approximation</p><formula xml:id="formula_8">A C C E P T E D M A N U S C R I P T</formula><p>of marginal probability) of each node by updating messages between nodes. The message msg i-j can be simply understood as the likelihoods of various states of the node j that supposed by the node i. It is usually obtained by computing the sum or the maximum of beliefs produced by all latent labels of the node i, which results in two main variants of LBP: the sum-product LBP and max-product LBP. The max-product version is widely used for energy minimization while the other one is usually used for computing the marginal distribution of each node in a graph. An important advantage of LBP is that its implementation is simple and can achieve a comparable performance to the α-expansion algorithm. The idea of message-passing has also been adopted by other approximation algorithms such as mean-field inference <ref type="bibr" target="#b62">[62]</ref>.</p><p>Here we introduce several most important inference algorithms used in the scene labeling community. While the study of energy minimization is still an open issue, we refer readers to a complete review of recent inference methods <ref type="bibr" target="#b63">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hand-engineered features based scene labeling methods</head><p>In this section, we focus on reviewing scene labeling methods that rely on handengineered features, such as SIFT <ref type="bibr" target="#b9">[9]</ref>, HOG <ref type="bibr" target="#b10">[10]</ref> and Spin images <ref type="bibr" target="#b64">[64]</ref>. Note that we will not discuss the local features in detail owing to page limitations. We refer readers to a comprehensive review <ref type="bibr" target="#b65">[65]</ref>. In addition, it should be emphasized that "visual words" described in this section may partially belong to learned features because, in most cases, they are learned from well-designed hand-engineered features. We treat "visual words" as hand-engineered features despite its dual statuses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methods using pixel(superpixel)-wise classification</head><p>Most early methods address the issue of scene labeling by performing pixel-wise classification according to the features associated with each pixel. Konishi and Yuille <ref type="bibr" target="#b66">[66]</ref>, for example, performed Bayesian classification for each pixel according to the prior probability of classes and the posterior distribution of low-level features. Schroff et al. <ref type="bibr" target="#b67">[67]</ref> used a single histogram of visual words to model each class. The label of each pixel is then decided by the histogram distances between the pixel and all class models. Although this kind of method is easy to implement, classifying pixels separately would easily yield inconsistency in the results since it ignores the fact that pixels are usually associated with each other.</p><p>The auto-context <ref type="bibr" target="#b68">[68]</ref> proposed by Tu and Bai addressed the problem by iteratively using the pixel-wise predictions as context information, in addition to the appearance features,</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T</formula><p>to train a new pixel-wise classifier. This method is similar to a multi-class object detection framework called Mutual Boosting <ref type="bibr" target="#b69">[69]</ref>, which showed that the object detection of a specific class can be improved by employing detection beliefs of other classes. Clearly, the resulting boosting framework involves a tedious learning process. By replacing single label supervision with structure label supervision, Kontschieder et al. <ref type="bibr" target="#b70">[70]</ref> developed a more efficient way to exploit context for pixel-wise classification.</p><p>An alternative way to mitigate the inconsistency problem mentioned above is to enforce pixels belonging to a region have the same label. Thus, several methods <ref type="bibr" target="#b71">[71]</ref><ref type="bibr" target="#b72">[72]</ref><ref type="bibr" target="#b73">[73]</ref> formulated the problem of scene labeling as a superpixel-wise classification. In this framework, features of each superpixel are usually obtained by computing certain statistics(e.g. mean) of local descriptors within the superpixel. For example, a second-order pooling technique was developed in <ref type="bibr" target="#b72">[72]</ref>. Instead of directly pooling local descriptors, Gupta et al. <ref type="bibr" target="#b73">[73]</ref> quantized each local feature into a visual word and then computed a HOG descriptor for each superpixel. The use of superpixel brings at least two benefits to the task of scene labeling. First, the computational complexity is significantly reduced. Second, it is more likely extract robust features from superpixels rather than pixels. Nevertheless, the region-based model has its own limitations. Such model assigns the same label to all pixels of a region with the premise that some superpixels share boundaries with objects in the image. However, even the best over-segmentation approach would produce misleading results, that is, some superpixels may contain several object classes. Using multiple over-segmentations <ref type="bibr" target="#b74">[74]</ref> provides a strategy to alleviate this problem, however, performing multiple "segmentation and classification" processes is computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methods using CRF</head><p>A more powerful way to ensure the spatial consistency is to employ a contextual model, such as MRF and CRF, to explicitly model the relations between pixels or regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Plain CRF</head><p>The most commonly used CRF model in semantic segmentation is the pairwise CRF of ( <ref type="formula" target="#formula_5">6</ref>), here we called it plain CRF for convenience. In this case, the pairwise potential is usually defined in terms of a Potts model or its variants <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b75">[75]</ref><ref type="bibr" target="#b76">[76]</ref><ref type="bibr" target="#b77">[77]</ref>, which is contrast sensitive and can be written as</p><formula xml:id="formula_10">ϕ ij (y i , y j ) =      K(y i , y j ) if y i = y j 0 otherwise ,<label>(8)</label></formula><formula xml:id="formula_11">A C C E P T E D M A N U S C R I P T</formula><p>where K(y i , y j ) is a penalizing function whose output depends on the difference between adjacent pixels. If there is a large difference, the penalization should be small so as to preserve a reasonable label transition. However, the contextual relationships exploited by this pairwise potential are very restrictive and can be interpreted as "guilt by association" <ref type="bibr" target="#b78">[78]</ref>: connected(adjacent) pixels tend to have the same label. In this situation, many works encoded contextual information into the observed data x i when constructing the unary potential. For example, based on the fact that some objects likely to appear simultaneously within a certain range, Shotton et al. <ref type="bibr" target="#b29">[29]</ref> counted the numbers of several textons(visual words) in a rectangular mask that offsets from each pixel and then took them as contextual features. Other works have employed prior information for exploiting contexts.</p><p>He et al. <ref type="bibr" target="#b79">[79]</ref> learned environment-specific label distribution priors and then developed an prior-dependent CRF for scene labeling. Silberman and Fergus <ref type="bibr" target="#b6">[6]</ref> multiplied the unary predictions with 2D and 3D absolute location priors. These methods provide opportunities for exploiting context in associative CRF, however, such context is often very limited.</p><p>For examples, the inter-class correlations are not fully exploited both in prior-dependent CRF <ref type="bibr" target="#b79">[79]</ref> and absolute location priors <ref type="bibr" target="#b6">[6]</ref>, while the offset-texton <ref type="bibr" target="#b29">[29]</ref> do not consider global contexts.</p><p>Associative potential lack of the capability of modeling inter-class relationships(e.g., "on-top-of"). Thus, a number of works suggest to use the non-associative potential, which allows various combinations of labels between adjacent pixels(superpixels), to take much more contextual information into account. An early method <ref type="bibr" target="#b34">[34]</ref> in this direction achieved this goal by extracting prior label patterns, i.e., the arrangements of labels in a region, from the training data. The potentials are then decided by the matching degree between test label patterns and prior patterns. However, there are usually many priors that cannot be reflected by the training data. One way to overcome this limitation is to replace the priors with likelihoods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b81">81]</ref>. Gould et al. <ref type="bibr" target="#b32">[32]</ref>, for example, extracted features for every pair of adjacent regions and learned a multi-class logistic classifier to output the co-occurrence likelihoods of two labels. Similarly, Batra et al. <ref type="bibr" target="#b81">[81]</ref> learned the adjacent relationships between classes based on the bag-of-words representation. Muller and Behnke <ref type="bibr" target="#b82">[82]</ref> were among the first to apply such learned pairwise potential to indoor scene labeling.</p><p>Compared to associative potential, the non-associative potential is able to exploit much more contextual information through encoding various kinds of inter-class relationships. Nevertheless, the model also becomes more complex and hence brings much more difficulties in parameter learning and label inference. Considering the commonly used co-</p><formula xml:id="formula_12">A C C E P T E D M A N U S C R I P T</formula><p>occurrence likelihoods, given the number of classes L and the number of edge features K, it will need to take L 2 class combinations into account and hence require for L 2 K parameters. As a result, the unary model and pair-wise model are usually trained separately in non-associative CRF. Although Szummer et al. <ref type="bibr" target="#b83">[83]</ref> have tried a max-margin based method to jointly learn the CRF parameters, the training process involves multiple approximate MAP inferences, which inevitably lead to a reduction in the accuracy of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Higher order CRF</head><p>The main disadvantage of plain CRF, which defined on small pairwise cliques, is that it is weak to capture long-range dependencies. To alleviate this problem, a number of works have studied various higher order potentials defined on large cliques.</p><p>One well known higher order potentials is proposed by Kohli et al. <ref type="bibr" target="#b30">[30]</ref> and defined on the image segments in the form of robust P n Potts model, i.e.,</p><formula xml:id="formula_13">ϕ c (y c ) =      N i (y c ) 1 Q γ max if N i (y c ) ≤ Q γ max otherwise ,<label>(9)</label></formula><p>where N i (y c ) denotes the number of pixels in a region c not taking the dominate label; Q is a constant which controls the rigidity of the higher order potential, and γ max is the sum of cost of assigning a non-dominate label to a pixel in region c. Such potential encourages pixels lie in the same superpixel to have the same label and hence is useful to produce fine boundaries in the segmentation result. As shown in <ref type="bibr" target="#b31">[31]</ref>, higher order potentials defined on superpixels can be understood as a group of pairwise potentials between a superpixel node and pixel nodes. Thus, by recursively performing superpixel segmentation and then constructing higher order CRFs over them, it is easy to build hierarchical CRFs <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b84">84]</ref> to exploit multi-level contexts. Such process is illustrated in Fig. <ref type="figure">3</ref>.</p><p>Beyond superpixels, several other large cliques have also been utilized to build higherorder potentials. For example, Wojek and Schiele <ref type="bibr" target="#b85">[85]</ref> employed connections between object hypothesis nodes and pixel nodes to ensure consistency between pixel labels and object detection results. The same authors also added a unary detection potential and achieved both object detection and segmentation in a unified CRF. Similarly, Gonfaus et al. <ref type="bibr" target="#b86">[86]</ref> presented a harmony potential, consisting of a group of connections between the image node and pixel nodes, for joint image classification and segmentation. The allowed 2 L possible labels for the image node make the joint optimization intractable. Lucchi et al. <ref type="bibr" target="#b87">[87]</ref> solved this problem by using class-specific image nodes, which sacrifices the capability of model-</p><formula xml:id="formula_14">A C C E P T E D M A N U S C R I P T</formula><p>ing category co-occurrence. More recently, Khan et al. <ref type="bibr" target="#b8">[8]</ref> defined higher-order potentials on 3D planar regions constructed from depth images. These multi-task based higher-order potentials have the advantage that they can be easily extended by incorporating various constraints from other tasks. Nevertheless, the implementation of multi-task is usually very tedious. More importantly, it's possible to produce misleading constraints since the implementation of other vision tasks such as object detection is very challenging in itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unary potential</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise potential</head><p>Higher order potential images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF over pixels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF over regions</head><p>Unary potential</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise potential</head><p>Higher order potential</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical CRF</head><p>Label of pixels Label of superpixels Label of higher-superpixels Figure <ref type="figure">3</ref>: Three architectures of CRF model used in scene labeling. For CRF over pixels , graph nodes (second layer) are composed of labels of each pixel. On the other hand, the graph nodes(third layer) in CRF over regions are the labels of superpixels. The unary potential and pairwise potential are similarly defined in both models. Viewing the pixel as an elementary superpixel, several CRF over regions models can be organized into a hierarchical CRF <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Dense CRF</head><p>Higher-order CRF defined on superpixels suffers from a drawback that superpixels produced from unsupervised segmentation may contain misleading boundaries. The dense CRF is able to avoid this problem while capturing long-range dependencies. Not limited to adjacent pixels, the dense CRF also considers interactions between long-range pixels.</p><p>For example, Torralba et al. <ref type="bibr" target="#b88">[88]</ref> established potentials over pixel pairs that have shown correlations in the training set. This method needs to learn the correlation patterns for each class and hence lack of generality when more classes are considered. The generic dense CRF <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b89">89]</ref> instead connects all pixel pairs no matter whether they are correlated or not. Since the inference of dense CRF is computationally expensive, <ref type="bibr" target="#b89">[89]</ref> still resorted to superpixels to ensure computational efficiency. By expressing the massage passing step as a convolution with a Gaussian kernel, Krahenbuhl and Koltun <ref type="bibr" target="#b62">[62]</ref> developed an efficient mean-field inference algorithm for a dense CRF in which the pairwise potentials</p><formula xml:id="formula_15">A C C E P T E D M A N U S C R I P T</formula><p>are defined by a linear combination of Gaussian kernels. This approach provided a generic and efficient strategy for exploiting long-range dependencies and thus became popular in subsequent works, including methods using learned features introduced later. At this point, we have critically reviewed scene labeling methods based on contextual models. For the sake of clarity, we summarize them in the Table <ref type="table" target="#tab_0">1</ref> according to several key characteristics, including scene type, model structure, etc. The abbreviation of superpixel algorithms used in region based methods are described as follow: Mean-shift(MS) <ref type="bibr" target="#b46">[46]</ref>, Normalized Cuts(Ncut) <ref type="bibr" target="#b47">[47]</ref>, Felzenszwalb-Huttenlocher(FH) <ref type="bibr" target="#b91">[91]</ref>, Turbopixels(TP) <ref type="bibr" target="#b49">[49]</ref>, Simple Linear Iterative Clustering(SLIC) <ref type="bibr" target="#b50">[50]</ref> and globalized probability of boundary-oriented watershed transform(gPb-OWT) <ref type="bibr" target="#b92">[92]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Non-parametric methods</head><p>All methods outlined so far work with a fixed number of classes and need to use sophisticated parametric models, especially the non-associative CRF method. When it is necessary to take new classes into account, all model parameters need to be adjusted,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>which would be very tedious. To avoid this problem, Liu et al. <ref type="bibr" target="#b93">[93]</ref> developed a nonparametric approach for scene labeling. The core idea of this method can be described as "label transfer"(see Fig. <ref type="figure" target="#fig_2">4</ref>), which means the labels of test pixels are transferred from their matching pixels found in labeled images. To accomplish this, scene retrieval is first executed to find a set of most similar labeled images of a test image. Then, dense scene correspondences(pixel to pixel) between input and each of the similar images is established by matching local SIFT descriptors. The images with bottom matching energy are chosen as the ultimate candidates for label transfer. Instead of using pixel-level transferring, Tighe and Lazebnik <ref type="bibr" target="#b94">[94]</ref> transferred labels at the level of superpixels and achieved better performance compared with <ref type="bibr" target="#b93">[93]</ref>.  Non-parametric methods provide a more generic framework for scene labeling. There is no need for tedious training no matter how many classes must be taken into account.</p><p>One can simply add sufficient images including these classes into retrieval sets. The main limitation of non-parametric methods is that the computational complexity increases remarkably with the increase of image size and retrieval sets. To alleviate this problem, without retrieving similar labeled images for a test image, Gould et al. <ref type="bibr" target="#b95">[95]</ref> presented an algorithm for directly finding good matches for test superpixels. This is achieved by first constructing an optimum graph, in which nodes represent superpixels and edges represent match cost defined by a learned distance, from training images. Given a set of superpixel of a test image, the graph can be rapidly reconstructed to find their matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  Another significant difference between 2D and 3D CRF models lies in the selection of learning strategy. Because of the Potts model based potential is simply decided by the difference between pixels(superpixels) and several manual tuned parameters. Most 2D scene labeling methods only need to train a unary classifier, such as neural networks <ref type="bibr" target="#b6">[6]</ref>, Random Forests <ref type="bibr" target="#b76">[76]</ref> and SVM <ref type="bibr" target="#b33">[33]</ref>. A bit more complicated learning strategy used in 2D scene labeling is piece-wise learning, which involves training different types of potential models independently and adding weights to each of them through cross-validation <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>In contrast, most 3D scene labeling methods use structural learning algorithms to obtain parameters of the CRF model, that is, all the parameters are jointly learned from training samples. As we know, a common approach to learn parameters is to perform maximum likelihood estimation. Unfortunately, the computation of the true likelihood is intractable. Instead, the pseudolikelihood approach is widely used in the literature <ref type="bibr" target="#b100">[100]</ref><ref type="bibr" target="#b101">[101]</ref><ref type="bibr" target="#b102">[102]</ref>. Another popular structural learning algorithm is to use a max-margin based objective, which aims to maximize the margin of confidence between the true labels and any other predictions <ref type="bibr" target="#b98">[98,</ref><ref type="bibr" target="#b103">[103]</ref><ref type="bibr" target="#b104">[104]</ref><ref type="bibr" target="#b105">[105]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learned features based scene labeling methods</head><p>In this section, we review the representative scene labeling methods based on learned features, which usually refer to convolutional neural networks(CNN). Typical CNNs take inputs of fixed-size and produce a single prediction for whole-image classification. However, scene labeling aims to assign a class label to each pixel of arbitrary-sized images. In other words, when applying typical CNNs to scene labeling, the main problem is how to produce pixel-dense predictions. To this end, we first describe the principle of the CNN and then analyze different strategies of producing dense-predictions. In other words, the inputs of a unit are from a set of units located in a small window(called receptive field) in the previous layer. In addition, all the units in the same layer share the identical weight mask and the outputs of these units are organized into a so-called feature map. However, a specific weight mask can only extract one type of feature. In order to form a richer representation of the input, a complete convolutional layer usually consists of several feature maps constructed from different weight masks. Let l k be the k-th feature map in an convolution layer l. Then, without padding zeros at image boundaries, the feature value of each unit l k ij is given by</p><formula xml:id="formula_16">l k ij = tanh( H h=1 (w kh * x h i+ s/2 ,j+ s/2 + b kh )),<label>(10)</label></formula><p>where w kh and b kh are the weight vector and bias between l k and (l -1) h , respectively. H is the number of feature maps of layer l -1. x h i+ s/2 ,j+ s/2 is a local vector of feature map (l -1) h centered at (i + s/2 , j + s/2 ), where s is the size of weight mask w kh . tanh is a well-known nonlinear activation function. It should be emphasized that a feature map is not required to connect to all maps in the previous layer although the equation indicates so(just for convenience).</p><p>In the real world, the positions of features vary for different instances of the same object and likely to change with shifts and distortions of input. As a consequence, the convolutional layer is usually followed by a subsampling(also called pooling) layer, which performs local averaging or maximizing over each feature map, to achieve robustness to shifts and distortions.</p><p>The CNN received great attention after its presentation. However, with the development of other efficient algorithms such as SVM, it fell out of fashion due to computational concerns. In recent years, with the rise of deep learning <ref type="bibr" target="#b11">[11]</ref>, the interest in CNN is rekindled by Krizhevsky et al. <ref type="bibr" target="#b13">[13]</ref>. They achieved a considerably higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 2012 through using a deep CNN(called AlexNet). Indeed, there is no difference between the AlexNet and the LeNet5 in structure. However, the scale of the AlexNet is significantly larger and thus many strategies were adopted for efficient training and over-fitting prevention, such as non-saturating activation function(e.g., max(x, 0)), parallel computing with Graphics Processing Unit(GPU) and "dropout" regularization. In the framework of pixel-wise classification, CNNs are usually applied to image patches of fixed-size centered at each pixel <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b109">109]</ref>(see the top of Fig. <ref type="figure" target="#fig_6">6</ref>.). However, labeling each pixel by just observing a small region around it is insufficient. It is necessary to take a wide context into account to make a local decision. This can be achieved simply by increasing the size of image patches. Unfortunately, this solution would increase the number of parameters of CNN drastically and thus lead to much higher computational complexity. In order to take wide contexts into account while ensure computational efficiency, multi-scale strategies were successively proposed. Farabet et al. <ref type="bibr" target="#b15">[15]</ref>, for instance, applied CNN to patches of images in a multi-scale pyramid of the input. The feature maps generated from different scales were then concatenated after a size matching procedure.</p><p>As a result, each pixel is associated with a feature vector encoded from multiple patches with the same sizes but increasing visual fields. This multi-scale architecture was also adopted by Couprie et al. <ref type="bibr" target="#b20">[20]</ref> to extract features from the depth information provided by RGB-D sensors. Similarly, in <ref type="bibr" target="#b23">[23]</ref>, more scales were considered.</p><p>Although it is structurally straightforward to feed image patches to a typical CNN to produce dense predictions, this scheme is computationally inefficient. There are a large amount of redundant convolutions because patches centered at adjacent pixels are significantly overlapped. One way to alleviate this problem is to perform region-wise classification by utilizing the concept of region proposals <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21]</ref>. This type of methods has the advantage that the region proposals, which are usually in the form of rectangular shape, can be directly fed into a CNN for classification after warping. In addition, object detection and segmentation can be achieved simultaneously in this framework. However, rectangular region proposals contain not only the object class but also several other classes(see the bottom of Fig. <ref type="figure" target="#fig_6">6</ref>). As a result, many bottom-up region proposals must be taken into account to determine the label of a single pixel.</p><p>Mostajabi et al. <ref type="bibr" target="#b110">[110]</ref> took a CNN as a feature extractor and produced pixel-level features by up-sampling the last feature maps. Superpixel features were then extracted by pooling pixel-level features. In addition, the authors proposed to concatenate the features of intermediate layers to capture multi-scale information. However, up-sampling these  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Methods using fully convolutional networks</head><p>Recent advances in semantic segmentation are mostly achieved by using fully convolutional networks(FCN). As described in Fig. <ref type="figure" target="#fig_6">6</ref>, the basic idea of FCN is to replace the fully connected layer of a typical CNN with a 1 × 1 convolution layer to produce lowresolution predictions. To obtain pixel dense predictions, a trivial approach <ref type="bibr" target="#b109">[109]</ref> called shift-and-stitch stitches predictions mapped from multiple shifted versions of the input. This method does work because the output of the CNN is shifted with the shift of the input. Considering output score maps with input stride s, the shift-and-stitch method needs to process s 2 shifted versions of the input image. A more efficient strategy performs up-sampling(e.g., bilinear interpolation) over the coarse predictions. Here we called it UP FCN for convenience. For examples, <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b22">22]</ref> adopted a deconvolution layer to upsample the low-resolution predictions. The UP FCN based method enables end-to-end training and thus becomes a main stream method in scene labeling. The end-to-end training means that the feature representation and pixel-wise classifiers are jointly learned.</p><p>Based on the structure of FCN, the strategies designed for improving semantic segmentation can de divided into three categorises. -1 -1 2 We can see that the atrous pipeline produces finer-resolution output by removing the sub-sampling layer in the conventional pipeline. In order not to reduce the receptive fields, the convolutional mask is dilated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><formula xml:id="formula_17">1 0 1 -1 2 -1 Original filter 2 1 -1 0 1 2 -1 -1 -1 Input feature map -1 -1 2 1 0 1 -1 2 -1 atrous conv dilated conv</formula><p>Resolution Refining: Directly up-sampling predictions of low-resolution(typically 1/32 of the original input) to image resolution could cause a great loss. The SegNet <ref type="bibr" target="#b114">[114]</ref> instead used a hierarchy of decoders to recover the feature maps. To control the scale of the model, the authors performed dimensionality reduction over the feature maps, which scarifies the model accuracy. One can also remove several down-sampling operations to obtain finer-resolution predictions before up-sampling. Unfortunately, doing so could decrease the receptive field sizes of the last layer and thus lead to a loss of global and contextual information crucial to semantic segmentation. To solve this problem, several methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b115">115,</ref><ref type="bibr" target="#b116">116]</ref> introduced atrous convolution to output finerresolution(typically 1/8) score(or feature) maps. A similar method called dilated convolution was reported in <ref type="bibr" target="#b117">[117]</ref>. The basic idea is to dilate(insert zeros) the convolution filters behind the removed down-sampling layers. The atrous convolution introduces zeros only inside of the original convolution mask, while the dilated convolution also inserts zeros outside(see Fig. <ref type="figure" target="#fig_8">7</ref>).</p><p>The most notable advantage of atrous FCN model is that it can produce fine-resolution predictions without introducing other parameters. However, the large receptive filed used in this model can hardly capture low-level visual information, which introduces another way of producing finer-resolution outputs that adds skip connections to middle layer features. For examples, <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b118">118]</ref> added score(or score-like) operation to intermediate feature maps and then fused the multi-resolution scores to produce the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T <ref type="bibr" target="#b119">[119]</ref> argued that up-sampling feature maps is better than score maps. All these approaches fused the multi-resolution score or feature maps by a summing operation after up-sampling. By contrast, <ref type="bibr" target="#b18">[18]</ref> employed a CNN based fusion strategy to continuously refine low-resolution features to finer resolution. The same idea has been adopted by several recent methods such as the RefineNet <ref type="bibr" target="#b120">[120]</ref>, RDFNet <ref type="bibr" target="#b121">[121]</ref> and Label Refinement Network(LRN) <ref type="bibr" target="#b122">[122]</ref>. Fig. <ref type="figure" target="#fig_9">8</ref> shows the fusion of multi-resolution feature maps. Context exploiting:Although theoretically the receptive fields of many deep CNNs are close to or even larger than the entire input, it has been shown by Zhou et al. <ref type="bibr" target="#b123">[123]</ref> that the valid receptive field of deep CNN is much smaller than the theoretical one, which indicates that higher-level contexts are not sufficiently exploited in many segmentation networks. To this end, Zhao et al. <ref type="bibr" target="#b25">[25]</ref> further enlarged the receptive field of FCN to different sizes by pyramid pooling(multiple parallel pooling). Features extracted from multiple receptive fields were then fused to capture context in both low and high levels.</p><p>A similar but more efficient method called atrous spatial pyramid pooling(ASPP) was reported in <ref type="bibr" target="#b116">[116]</ref>. Recently, Lin et al. <ref type="bibr" target="#b124">[124]</ref> presented a superpixel based receptive field and generated multiple receptive fields by adjusting the size of superpixels.</p><p>A number of methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b125">125]</ref> have incorporated CRF into UP FCN architecture for further improvements. Chen et al. <ref type="bibr" target="#b16">[16]</ref> defined a dense CRF over up-sampled predictions and obtained sharp boundaries using mean-field inference. Instead of using low-level pairwise potentials defined on color contrast, <ref type="bibr" target="#b24">[24]</ref> computed pairwise pixel affinities using semantic boundaries produced from a trained CNN. Lin et al. <ref type="bibr" target="#b125">[125]</ref> adopted a CNN based non-associative potential. In order to combine the strengths of the CNN and CRF in one unified framework, Zheng et al. <ref type="bibr" target="#b126">[126]</ref> converted several iterations of CRF inference to a recurrent neural network(RNN) and achieved the goal of training CNN and CRF jointly. Following this work, Arnab et al. <ref type="bibr" target="#b127">[127]</ref> demonstrated that the CRF with higher order po-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>tentials can also be embedded into the FCN and produced significant improvements over the CRF-RNN. These CRF embedded FCNs, though effective, have much higher complexity than the basic FCN <ref type="bibr" target="#b128">[128]</ref>. To solve this problem, the Deep Parsing Network(DPN) <ref type="bibr" target="#b129">[129]</ref> approximated the inference of CRF as a convolutional network by casting the pairwise potential to a contextual classifier over unary scores. Given the discrete CRF based methods often resort to approximate algorithms for inference, <ref type="bibr" target="#b130">[130,</ref><ref type="bibr" target="#b131">131]</ref> used Gaussian Conditional Random Fileds(GCRF) to pursue exact inference. While <ref type="bibr" target="#b130">[130]</ref> used the similar idea of unrolling fixed inference steps to a deep network as <ref type="bibr" target="#b126">[126]</ref> for training, <ref type="bibr" target="#b131">[131]</ref> casted the inference as a simplified quadratic optimization module.</p><p>In recent years, the RNN have achieved great success in dependency modeling for timesequenced data <ref type="bibr" target="#b132">[132]</ref>. Built on this insight, several scene labeling approaches incorporated RNN into CNN to capture long-range spatial dependencies. The basic philosophy of RNN is to memorize some information generated in current prediction and then pass it to the next prediction. As a result, each prediction can be influenced by historical information based on this recurrent process. When applied to scene labeling, the concept of time sequence is extended to spatial sequence. For instance, Visin et al. <ref type="bibr" target="#b35">[35]</ref> constructed four 1D spatial sequences by scanning the image horizontally and vertically in both directions.</p><p>In <ref type="bibr" target="#b36">[36]</ref>, a 2D spatial sequence is formed by treating an image as a directed acyclic graph.</p><p>We have described three strategies for incorporating contextual information into FCN.</p><p>All these strategies share certain advantages and disadvantages. Enlarging the receptive filed requires a simpler model and is easier to implement compared with using CRF/RNN. However, the structural correlations between pixels are implicitly and completely modeled by a hard-to-understand learning process. On the other hand, using CRF/RNN requires considerable efforts and tricks to train the model although it can exploit more accurate structure correlations.</p><p>Hard Pixels Aware Learning: Most semantic segmentation datasets suffers from a class-imbalance problem, which results in performance differences in recognizing pixels of different classes. In addition, pixels in the same image are not equally recognizable. For example, as shown in <ref type="bibr" target="#b133">[133]</ref>, pixels located on the boundaries between objects are harder to recognize than those lie in the central part of objects. All these observations suggest that "hard" pixels <ref type="bibr" target="#b134">[134]</ref> should be distinguished from "easy" pixels. To this end, Wu et al. <ref type="bibr" target="#b134">[134]</ref> implicitly forced networks to focus on hard pixels during training. This is achieved by ignoring pixel samples if they have been "correctly recognized", gauged by examining whether the predicted probability of the true label surpasses a threshold. At</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>basic FCN architectures: skip connected UP FCN and atrous UP FCN. These improvements include employing more powerful CNN models(e.g. ResNet), incorporating various CRF or higher level contexts and using cascaded methods.</p><p>From the discussions above, we can see that the FCN can be used as a general model for producing semantic segmentations. A current and common problem faced by FCN model is that it is easy to reach memory limits during training if using finer-resolution feature maps. That's why the commonly used finer-resolution is typically less or equal to 1/8 of the input resolution. This implies a future study to further increase the resolution of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Weakly and semi-supervised scene labeling methods</head><p>The methods mentioned so far require a large number of densely annotated(one pixel with one label) images for training or providing candidate labels for transferring in nonparametric framework. It is well known that such annotation task could be very tedious and time expensive. This has introduced a more challenging research field, namely the weakly supervised scene labeling, where the ground truth is given by only image level labels or bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Methods using image-level labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Weakly supervised learning</head><p>Given the class set {1, 2, ..., L}, image-level label means that each training image I i is labeled by an L dimensional indicator vector y i =&lt; y 1 i , y l i , ..., y L i &gt;, where each element y l i takes the value 1 or 0 to indicate whether the image has the label l within it. In this context, learning class models would be very challenging due to the absence of direct correspondences between patterns and labels. In <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b138">138]</ref>, the authors tried to recover these correspondences by using a variant expectation-maximization(EM) algorithm with the following iterative steps: correspondences estimation using fixed model parameters and model parameters optimization using estimated correspondences.</p><p>A recent trend to learn class model from weakly annotated data is to use multiple instance learning(MIL) techniques. The MIL usually refers to a generalized binary classification problem, in which the label is attached to a so called bag(a set of samples) but not to each sample or instance. The bag is positive only if there is a positive instance within it and otherwise negative. The MIL is usually formulated as a maximum margin problem. MI-SVM <ref type="bibr" target="#b139">[139]</ref> and mi-SVM <ref type="bibr" target="#b139">[139]</ref> are two classic methods in this framework. The key idea of the two methods can be simply described by an iteration process with following two</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>steps: (1) fix labels for several or all instances in the positive bags and then learn the parameters of SVM;(2)reassign labels for several or all instances in the positive bags according to the SVM classifier learned in step 1. From this perspective, these two methods have the flavor of EM mentioned above. The difference between the two methods lies in the margin they used. The margin used in mi-SVM is same to the one used in standard SVM, which is defined by the distance between individual instance and the hyperplane.</p><p>Thus, the labels of all instances in each positive bag are updated in the second step in mi-SVM. For a positive bag, if there is no positive instance after updating, the instance with the largest margin is labeled positive. In MI-SVM, the notion of instance-margin is extended to bag-margin, which is defined by the maximum instance-margin of a positive bag. In other words, only one instance per positive bag matters during learning in step 1.</p><p>Vezhnevets et al. <ref type="bibr" target="#b37">[37]</ref> were among the first to extend binary MIL framework to multiclass weakly supervised segmentation. They learned a Random Forest inspired by the mi-SVM methods. Notice that the CRF model is able to improve the consistency of labeling. The same authors took their weakly supervised classifier as the unary model and presented a multi-image graphical model(MIM) <ref type="bibr" target="#b38">[38]</ref>, which is essentially a region based CRF but allows for links between similar superpixels of different images. Later, they <ref type="bibr" target="#b140">[140]</ref> generalized this MIM model to allow for more types of pairwise potential between different images.</p><p>More recently, several works try to incorporate the CNN into the MIL for scene labeling task. For example, Pathak et al. <ref type="bibr" target="#b40">[40]</ref> used a multi-class MIL loss, in which only the max scoring pixel of each class presented in an image matters, to fine-tune the pre-trained image-level classifiers of <ref type="bibr" target="#b14">[14]</ref>. Papandreou et al. <ref type="bibr" target="#b39">[39]</ref> developed an EM based algorithm for training CNN from weakly annotated images. It is well known that such heuristic algorithm is very sensitive to the initialization. To alleviate this problem, instead of directly estimating the labels of pixels in each iteration, Pathak et al. <ref type="bibr" target="#b141">[141]</ref> computed an optimal probability distribution over the image labeling under the weak annotation constraints.</p><p>Inspired by the observation that different graphlets(small graph composed of several similar superpixels) from the same object often share similar appearance and spatial structures, Zhang et al. <ref type="bibr" target="#b41">[41]</ref> learned a graphlet-wise classifier from weakly annotated images. This is achieved by first transforming graphlets of different sizes(number of supepixles) into equal-length feature vectors through a proposed manifold embedding algorithm. A multi-class SVM was then learned by treating each training image as a 1-sized graphlet,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>i.e., a single feature vector. Thereafter, they further incorporated this learned classifier into a hierarchical Bayesian Network(BN) to model the semantic associations between graphlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Label propagation</head><p>Besides learning parametric models, another type of method focuses on propagating the image-level labels to regions based on the assumption that visually similar regions with common image-level labels are likely to have the same label. For example, supposed that one label of an image only characterizes a single semantic region, Liu et al. <ref type="bibr" target="#b142">[142]</ref> formulated the label-to-region problem as a problem of uncovering the region-level correspondence between all the image pairs. A simple illustration of their idea is shown in Fig. <ref type="figure" target="#fig_10">9</ref>.</p><p>However, it is difficult to directly partition an image into several regions where each region is corresponding to a unique image-level label. To this end, they proposed a bilayer sparse coding formulation to reconstruct a semantic region from a group of atomic patches. Thus, the reconstruction coefficients, which in some sense act as patch-level correspondence between image pairs, can be used to propagate the image-level labels to image patches. constraints and the image labels serving as weak supervision, the image-level labels were then propagated to patches by solving a constrained optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methods using bounding box annotations</head><p>An early work on applying bounding box annotations(coarse object locations) to semantic segmentation was presented by Xia et al <ref type="bibr" target="#b144">[144]</ref>. Based on the fact that each bounding box usually contains only one object, this method firstly performed object-background segmentation for each bounding box. These binary semantic segmentations are then merged and post-processed to obtain the final result. One main limitation of this method lies in its assumption that each image is associated with several bounding boxes provided by either manually or well-designed object detectors. Nevertheless, it provides an intuition that the bounding box annotations can be easily transformed to dense annotations. Such estimated annotations can be then used to conduct fully supervised learning. Following this direction, Papandreou et al. <ref type="bibr" target="#b39">[39]</ref> explored three methods(Bbox-Rect,Bbox-Seg and Bbox-EM-fixed) to estimate dense annotations and then trained a FCN based semantic segmentation model. The Bbox-Rect and Bbox-Seg produced the dense annotations as a pre-processing step, while the Bbox-EM-fixed continuously refined the estimated annotations during subsequent fully supervised training. Similar to the Bbox-EM-fixed, the BoxSup <ref type="bibr" target="#b145">[145]</ref> applied the region proposal mechanism to the process of annotations refinement and produced a better performance.</p><p>From the discussions above, we can see that most weakly supervised scene labeling methods follow a standard two-stage pipeline: estimating dense annotations and then conducting fully supervised training. In addition, this two-stage process is usually iterated many times to refine the estimations and parametric models. Since it is very easy to obtain a good estimation using the bounding box, the estimating stage can be also taken as a pre-processing step in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semi-supervised methods</head><p>Learning semantic segmentation model using only image-level labels is very challenging, yet producing pixel-level labels requires tremendous efforts. To compromise, the semi-supervised methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b146">[146]</ref><ref type="bibr" target="#b147">[147]</ref><ref type="bibr" target="#b148">[148]</ref><ref type="bibr" target="#b149">[149]</ref> have been proposed to use both weakly and strong annotations.</p><p>The commonly used EM-like weakly supervised learning can hardly produce a satisfactory segmentation because the estimated truth is often full of wrong labels even after a number of iterations. Thus, Papandreou et al. <ref type="bibr" target="#b39">[39]</ref> suggested to use a few strongly annotated images in each iteration of their EM-based algorithm and obtained significant</p><formula xml:id="formula_18">A C C E P T E D M A N U S C R I P T improvements.</formula><p>To produce more accurate truth for weakly annotated images, the methods in <ref type="bibr" target="#b147">[147]</ref> and <ref type="bibr" target="#b149">[149]</ref> first conducted a fully supervised training using a small set of strong annotations before applying EM-like semi-supervised learning. In addition, multitask learning, such as object recognition <ref type="bibr" target="#b147">[147,</ref><ref type="bibr" target="#b149">149]</ref>, image reconstruction <ref type="bibr" target="#b147">[147]</ref> and image description <ref type="bibr" target="#b149">[149]</ref>, were also used in these two methods to further improve the accuracy of segmentation.</p><p>Instead of adopting the EM based algorithm, Hong et al. <ref type="bibr" target="#b146">[146]</ref> developed a novel approach for exploiting weak annotations for semi-supervised semantic segmentation. In this approach, the weak annotations were only used to train an image classification network. The probabilities of classes obtained from the classification network were then backpropagated to produce class-specific activation maps. Taking the activation maps and the final feature maps produced from the classification network as inputs, a de-convolutional network was then learned to produce semantic segmentation with strong annotations. This method is close to fully supervised learning if regarding the training of multi-label image classification as a pre-training process. Its success demonstrated that the multi-label image classification network trained from a huge number of weakly annotated images can also yield discriminative features appropriate for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Public datasets for scene labeling</head><p>To inspire new methods and facilitate the comparison between different approaches, many public datasets for scene labeling have been proposed(see Table <ref type="table" target="#tab_3">4</ref>). Sowerby <ref type="bibr" target="#b150">[150]</ref> is one of the earliest datasets which mainly includes a variety of urban and rural scenes with small sizes(96 × 64). MSRC <ref type="bibr" target="#b29">[29]</ref>  and other(person). ImageNet <ref type="bibr" target="#b152">[152]</ref> is very similar to PASCAL but contains more than an order of magnitude in number of object classes and images <ref type="bibr" target="#b153">[153]</ref>. The huge amount of annotated images(nearly 15 million) are collected from the internet and labeled by Amazons Mechanical Turk. The recently released COCO dataset <ref type="bibr" target="#b154">[154]</ref> contains more than 328,000 images with 91 object classes. The most notable characteristic of this dataset is that each category is associated with thousands(27,000 on average) of instances. Hence, it will be very powerful for learning discriminative models of classes. With these large-scale datasets, many derived datasets with smaller number of images were also presented. For example, SIFT-flow <ref type="bibr" target="#b93">[93]</ref> contains 2,688 images obtained from LabelMe system. Similarly, Stanford Background Dataset(SBD) <ref type="bibr" target="#b32">[32]</ref> collected 715 images from LabelMe, PASCAL and MSRC.</p><p>More recently, Cordts et al. <ref type="bibr" target="#b155">[155]</ref> developed a dataset called Cityscapes, which consists of a large amount of street scenes and mainly designed for urban scene understanding.</p><p>Cityscapes contains 5,000 densely labeled images and 20,000 coarse annotated images that can be used for fully supervised methods and weakly supervised methods respectively.</p><p>Another very recent dataset called ADE20K <ref type="bibr" target="#b156">[156]</ref> provides a generic and more challenging  There also exist many datasets designed for specific applications of semantic segmentation. For example, Yamaguchi et al. <ref type="bibr" target="#b163">[163]</ref> collected a dataset called Fashionista for clothes parsing, aiming for segmenting garment pieces (shoes, socks, etc.). The human parsing dataset <ref type="bibr" target="#b164">[164]</ref> contains labels of not only clothes but also body parts(arm, leg, etc.). We do not intend to list more of such application-specific datasets because this review focuses on generic semantic segmentation for natural scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation and comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Evaluation of scene labeling methods</head><p>It is well-known that image segmentation is an ill-defined problem, how to evaluate the property of an algorithm has always been a critical issue. For the case of semantic segmentation, evaluation is usually defined on the comparison between the algorithm's output and the ground-truth. Examples including pixel accuracy, class accuracy, Jaccard index, precision and recall.</p><p>Pixel Accuracy(P-ACC) and Class-average Accuracy(C-ACC)</p><p>The P-ACC is the most widely used evaluation in scene labeling. It defines the accuracy of pixel-wise prediction. Let li and l i be the predicted label and ground truth of pixel i,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T the P-ACC can be written as</p><formula xml:id="formula_19">P -ACC = N i δ( li , l i ) N ,<label>(11)</label></formula><p>where N is the total number of pixels of all test images and δ(x, y) is an indicator function written by</p><formula xml:id="formula_20">δ(x, y) =      1 if x = y 0 otherwise . (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>For each class k, the accuracy of prediction is defined as</p><formula xml:id="formula_22">ACC(k) = N i (δ(l i , k)&amp;δ( li , k)) N i δ(l i , k) ,<label>(13)</label></formula><p>where the denominator represents the total number of pixels with ground truth label k, the numerator is the statistics of correctly predicted pixels of class k. Given the total number of classes L, the class average accuracy is given by</p><formula xml:id="formula_23">C -ACC = L k ACC(k) L .<label>(14)</label></formula><p>Jaccard Index(JI)</p><p>Jaccard Index(JI) is a well-known statistic used for measuring the similarity of two sets. As shown in <ref type="bibr" target="#b15">(15)</ref>, JI is defined as the size of the intersection divided by the size of the union of two sets. Hence, it is also called as intersection over union(IU or IOU) in some literatures <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><formula xml:id="formula_24">JI = |A ∩ B| |A ∪ B| = |A ∩ B| |A| + |B| -|A ∩ B|<label>(15)</label></formula><p>Adopt the same notes mentioned above, the JI for scene labeling of a specific class k can be written as</p><formula xml:id="formula_25">JI(k) = N i (δ(l i , k)&amp;δ( li , k)) N i (δ(l i , k)|δ( li , k)) ,<label>(16)</label></formula><p>As in the case of class average accuracy, the mean JI can be obtained by averaging JI over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision and Recall(PR)</head><p>PR is an evaluation frequently used in information retrieval. In the context of classification, the precision(see ( <ref type="formula" target="#formula_26">17</ref>)) of a class k is defined as the proportion of elements that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>actually belong to k among all elements predicted to be k. The recall of k is identical to class accuracy defined in <ref type="bibr" target="#b13">(13)</ref>.  non-CRF v.s. CRF</p><formula xml:id="formula_26">precision(k) = N i (δ(l i , k)&amp;δ( li , k)) N i δ( li , k)<label>(17</label></formula><p>To evaluate the performance of commonly used CRF model. We implemented the CRF model described in <ref type="bibr" target="#b33">[33]</ref> using the region-wise classifier code provided by the author and the graph cut code of Boykov <ref type="bibr" target="#b55">[55]</ref>. Then, we evaluated the original classifier and its CRF counterpart on the Stanford Background dataset <ref type="bibr" target="#b32">[32]</ref>. The qualitative comparison</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>of the two models is shown in Fig. <ref type="figure" target="#fig_14">10</ref>. We can see that non-CRF models easily produce inconsistency(white rectangle) in labeling. On the other hand, associative CRF could lead to over-smooth(black rectangle) sometimes. Nevertheless, the CRF model performs better than non-CRF model in total according to the quantitative comparison shown in Table <ref type="table" target="#tab_4">5</ref>.  validation set and produced pixel-wise predictions for each of them. The pixel-wise predictions are obtained by using the FCN-8s caffe (Convolution Architecture For Feature Extraction) model provided by the author of FCN <ref type="bibr" target="#b19">[19]</ref>. Then, we implemented a regionbased model by assigning pixels within a superpixel with the same label if there is a label accounting for more than 70 percent. With this simply modification, we found a slight improvement in the boundary details of output. However, the region based model may also produce worse results due to the inaccuracy of superpixel algorithms. Fig. <ref type="figure" target="#fig_15">11</ref> shows both of the two situations. But in general, we can see from Table <ref type="table" target="#tab_5">6</ref> that the region based model achieve a minor improvement on the quantitative performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">Comprehensive comparison of fully supervised methods</head><p>With the available of the commonly used datasets and evaluation methods, it is very easy to conduct a direct comparison between scene labeling methods based on the results they reported. Thus, we choose five typical indoor scene labeling methods, in which handengineered feature based method, CRF based method and learned feature based method all included, to give further insight into the advantages and disadvantages of different models. The results of comparisons is shown in Table <ref type="table" target="#tab_6">7</ref>. Since the results reported may be evaluated on different datasets, we compare them through choosing a reference method (denoted by "#" in table) that experimented with several datasets or tasks. For instance, Gupta et al. <ref type="bibr" target="#b73">[73]</ref> validated the performance of their method on two tasks: the 4-class task and the 40-class task.  <ref type="bibr" target="#b19">[19]</ref> FCN+multi-scale features 65.4% Lin et al.2016 <ref type="bibr" target="#b120">[120]</ref> FCN+cascaded refinement 73.6%</p><p>* associative.</p><p>In the Table <ref type="table" target="#tab_6">7</ref>. we find that <ref type="bibr" target="#b73">[73]</ref>, which use hand-engineered features and non-CRF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T model, performs better than both a learned feature based method <ref type="bibr" target="#b20">[20]</ref> and a CRF based method <ref type="bibr" target="#b157">[157]</ref>. This is reasonable since <ref type="bibr" target="#b73">[73]</ref> takes lots of geometric information from RGB-D images into account. In addition, the authors adopted a learned based over-segmentation algorithm <ref type="bibr" target="#b92">[92]</ref> that can produce superpixels with accurate boundaries. In other words, the success of <ref type="bibr" target="#b73">[73]</ref> is mainly resulted from the careful designed feature representations as well as the high accuracy superpixel algorithm. However, its performance is still far behind those achieved by FCN models <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b120">120]</ref>.</p><p>In order to demonstrate the state-of-the-arts of semantic segmentation, Table <ref type="table" target="#tab_8">8</ref>   We also conduct a comparison between weakly supervised methods and fully supervised methods to evaluate the performance of weakly supervised scene labeling. As shown in the Table <ref type="table" target="#tab_9">9</ref>, a simple weakly supervised learning strategy <ref type="bibr" target="#b40">[40]</ref>, in which only the maximum [39] achieved much higher performance. However, it still significantly lags behind the performance of fully supervised methods. On the other hand, weakly supervised methods using bounding box annotations can produce competitive results. Moreover, the bounding boxes are far easier to collect than pixel-level labels. Based on these, we argue that box annotations will receive more attention in the future.  <ref type="bibr" target="#b145">[145]</ref> Bounding box bounding box to pixel-level labeling+FCN 64.6% Chen et al.2015 <ref type="bibr" target="#b16">[16]</ref> Pixel-level labeling FCN+CRF 66.4% Zhao et al.2017 <ref type="bibr" target="#b25">[25]</ref> Pixel-level labeling FCN+Multi-scale 85.4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and future directions</head><p>In this paper, we critically reviewed existing scene labeling methods. The simplest way to solve this challenge issue is to perform pixel-wise classification using hand-engineered features, such as color and texture. However, this line of work easily produces inconsistency in results. Performing superpixel-wise classification can somehow mitigate this problem.</p><p>Through assigning the same label to a superpixel, the spatial consistency is ensured, but only limited to a local range. A more efficient way to ensure consistency is to exploit contextual information of the scene using the concept of MRF or CRF. For the methods based on hand-engineered features, the main challenge in performance improvement is how to design more feature representations. However, such heuristic designs could be very difficult. An alternative way is to use the learned features that have been proven very useful in computer vision.</p><p>Although methods reviewed in this paper have achieved great success on semantic segmentation, there is still a long way to the goal of enabling computers or robots understanding theirs surrounding scene as we human do. According to the recent developments in both semantic segmentation and other related vision communities, here we suggest several future directions.</p><p>Better understanding of CNN: Despite the success achieved by CNNs, there is a limited understanding why they work so well and how to improve them. Thus, the CNNs have long been used as "black boxes" and the development of better models is usually resort to trial-and-error. It is well known that the cost of every trial is huge since the training of modern CNNs usually requires a large amount of time. For example, it takes five days for Long et al. <ref type="bibr" target="#b19">[19]</ref> to fine-tune the VGG model to the fcn-8s model on a single</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T GPU(NVIDIA Tesla K40). Hence, it is necessary to have deeper insights into the inner working of the CNNs to provide scientific criteria for designing better models. Although there have been a number of works <ref type="bibr" target="#b167">[167]</ref><ref type="bibr" target="#b168">[168]</ref><ref type="bibr" target="#b169">[169]</ref> trying to visualize the learned features to gain intuition about the CNN and have achieved significant progress, there are still a lot of rooms for better understanding of CNN. For example, how the number of feature channels in each layer influences the performance of CNN. This may be very useful for FCN based semantic segmentation in view of its huge requirements for memory.</p><p>3D CNNs for point cloud labeling: In recent years, reports on applying CNNs to 3D vision tasks have increased significantly as well. Two common CNN models used in these reports including Volumetric CNN <ref type="bibr" target="#b170">[170]</ref><ref type="bibr" target="#b171">[171]</ref><ref type="bibr" target="#b172">[172]</ref> and Multi-view CNN <ref type="bibr" target="#b172">[172]</ref><ref type="bibr">[173]</ref><ref type="bibr" target="#b173">[174]</ref>. In the Volumetric CNN, the 3D data are converted into a 3D binary voxel, which is then fed into a 3D convolution network for feature extraction. The Multi-view CNN utilizes multiple 2D views of the 3D scenes and feeds them into 2D CNNs. These CNN based 3D vision algorithms mostly focus on object recognition/detection. A very recent work <ref type="bibr" target="#b174">[175]</ref> on point cloud labeling developed an architecture that directly consumes points. Following this direction, it can be expected more efforts of applying the CNN to 3D semantic segmentation, i.e. 3D point cloud labeling.</p><p>Weakly supervised semantic segmentation: It has been shown that the performance of semantic segmentation can be improved by including additional valid training images. However, it requires tremendous efforts to produce pixel-level annotations.</p><p>Although weakly supervised methods can drastically reduce the annotation cost, their performances are still far from satisfactory. Thus, more novel ideas, such as collecting images <ref type="bibr" target="#b175">[176]</ref> or videos <ref type="bibr" target="#b176">[177]</ref> from the Internet as supervision, are needed to boost the development of weakly supervised semantic segmentation.</p><p>Evaluated on more complex datasets: Most recent methods tested their models on two common datasets: PASCAL <ref type="bibr" target="#b113">[113]</ref> and Cityscapes <ref type="bibr" target="#b155">[155]</ref>. Both of the two datasets use very simple samples and make it easy to arrive at the bottleneck. For example, most annotations in PASCAL have a large background and make the semantic segmentation almost equivalent to object segmentation. Compared to PASCAL, Cityscapes provides annotations with more details, however, it contains only urban street scenes and is not appropriate for generic semantic segmentation. With the newly released ADE20K <ref type="bibr" target="#b156">[156]</ref>,</p><p>which considers 150 semantic classes and contains densely labeled 20,000 scene images of various types, testing with ADE20K-like complex datasets will become a mainstream in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Aiming at applications: In the past several years, great improvements have been made on semantic segmentation by using deep learning. Consequently, there will be greater interest in applying semantic segmentation models to practical applications, such as service or industrial robots, unmanned vehicle, medical image segmentation <ref type="bibr" target="#b177">[178]</ref>, human parsing <ref type="bibr" target="#b164">[164,</ref><ref type="bibr" target="#b178">179]</ref> and so on. Among them, the application to autonomous mobile systems could be in a huge demand. In this context, how to simultaneously guarantee the accuracy and computational efficiency of the model could be the most challenge issue.</p><p>At last, it is worth noting that many image segmentation methods are inspired by the theories and methods studied in cognitive science. For instance, most clustering based methods are inspired by the theory of gestalt <ref type="bibr" target="#b179">[180]</ref>. Similarly, the CNN is motivated from the research on the visual cortex of cats. We predict that the next quantum leap in segmentation community will be closely related to theories in cognitive science, neuroscience or their closely related scientific fields. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 Figure 2 :</head><label>22</label><figDesc>Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Test pixelMatching pixel:label is building Matching pixel:label is tree Matching pixel:label is tree Matching pixel:label is tree Tree:3(√) Buiding:1 Other:0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the idea of label transfer. The red rectangle is the test pixel and its matching pixels found in the training set are shown by the yellow rectangles. The labels of matching pixels can be naturally transferred to the test pixels. The color images and theirs' truth are taken from Stanford Background Dataset<ref type="bibr" target="#b32">[32]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>I n p u t im a g e 4 F e a tu r e m a p s 6 F e a tu r e m a pFigure 5 : 4 . 1 .</head><label>46541</label><figDesc>Figure 5: Simplified architecture of CNN. Different feature maps in the first convolution layer are obtained by performing convolution over the input image with different templates. Each feature map in the intermediate convolution layer are obtained by performing convolution over several or all the maps of previous layer with different templates and then summing all the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 .</head><label>2</label><figDesc>Scene labeling based on CNN4.2.1. Naive approachA straightforward way, what we called naive approach, to produce dense predictions is to take CNNs as image patch classifiers or feature extractors and then perform pixel/region-wise classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A</head><label></label><figDesc>C C E P T E D M A N U S C R I P Tmulti-scale feature maps to image resolution would lead to memory sensitive since there are usually hundreds of feature maps in each scale. In contrast,<ref type="bibr" target="#b111">[111,</ref><ref type="bibr" target="#b112">112]</ref> used a recursive context propagation network, which consists of a stage of bottom-up feature aggregation and a process of top-down context propagation, to enrich each superpixel with contextual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Three common CNN architectures for semantic segmentation. The color image and its truth are taken from PASCAL VOC dataset<ref type="bibr" target="#b113">[113]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Principle of atrous convolution. The top describe conventional pipeline of producing feature maps in FCN model. The bottom is the pipeline of producing finer-resolution feature maps by employing atrous convolution. The dash girds of the outputs indicate the resolution of the input.The purple boxes at different layers represent different level features of a specific pixel. We can see that the atrous pipeline produces finer-resolution output by removing the sub-sampling layer in the conventional pipeline. In order not to reduce the receptive fields, the convolutional mask is dilated accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Fusion of multi-resolution feature maps. The blue boxes depict three feature layers of typical FCNs. From left to right, different resolution feature maps are continuously fused by performing sum operation after size matching or feeding into a ConvNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Label propagation based on region-level correspondence. R1, R2, and R3 are three semantic regions of different images. The region-level correspondence indicates that two regions stand a good chance to share the same label. Thus, the label shared by these three regions can be obtained through inferring the label shared by their candidate labels, i.e. the image-level labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>is another early dataset and consists of 591 photographs in 23 classes. It is well known that a large number of training images are more useful for learning models of object categories. Motivated by the needs of training images, several largescale datasets had been established in recent years. LabelMe[151] provides an web-based annotation tool through which a large number of annotated images can be collected from a large population of web-users. It adopts bounding polygon annotations and users are free to label as many objects in the chosen images. PASCAL is derived from a wellknown competition: the PASCAL Visual Object Classes (VOC) challenge[113]. It provides a large-scale dataset with high quality annotation, including 2,913 images with pixellevel labeling, and has been enlarging continuously. Objects in the PASCAL are divided into 20 classes and organized into four major categories: vehicles, household, animals A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>benchmark for semantic segmentation. It contains more than 20,000 images of various scene types and considers 150 classes for dense labeling.With the advent of consumer RGB-D sensors, many RGB-D datasets have been released in succession. NYU Depth dataset<ref type="bibr" target="#b157">[157]</ref> is the first and the most popular one. It consists of densely labeled 1449 RGB-D images captured from 464 diverse indoor scenes.Washington RGB-D Object dataset<ref type="bibr" target="#b158">[158]</ref> contains visual and depth images of 300 common daily objects with which personal robots are supposed to interact. These objects are organized into a hierarchical category structure that contains 51 classes and each object is associated with different instances taken from multiviews. Apart from these isolated objects, the dataset also includes 22 annotated video sequences of natural scenes. Turn to 3D point cloud labeling, Cornell RGB-D dataset[159] provided 52 labeled 3d point clouds stitched from color and depth images. The Sun RGB-D dataset[160] contains 10,335 RGB-D images taken from four types of RGB-D sensors. All these images are annotated with polygons in 2D and bounding boxes in 3D. A very recent dataset called ScanNet[161] contains 2.5M views in 1,513 scenes annotated with dense 3D voxel labeling. VMR-Okaland-V2[162] is one of the most popular urban 3D datasets. It consists of 36 blocks collected from a terrestrial laser scanner. These blocks contain nearly 3 million 3D points labeled by seven common outdoor objects: wire, pole, leaves, tree trunk, building A C C E P T E D M A N U S C R I P T and vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>) 7 . 2 .</head><label>72</label><figDesc>Comparison of some scene labeling methods 7.2.1. Comparison of representative models Besides the model structure, there are many other differences between scene labeling methods, such as features, superpixel algorithms and classify strategies. Thus, it is difficult to determine which model is better by directly comparing those published methods. To this end, we choose several methods and change the model structures they used to check out the advantages and drawbacks of different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Qualitative comparison of non-CRF model and CRF model. The first and second columns are the input images and ground truths, respectively. The third and fourth columns are the labeling produced by CRF model and non-CRF model, respectively.</figDesc><graphic coords="33,105.79,539.59,358.40,58.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The comparison of pixel based model and region based model. The first column are superpixels generated from a variant of SLIC algorithm(http://www.peterkovesi.com/projects/segmentation/ ). The number of superpixels is set to 300. The middle two columns are the predictions of region based model and pixel based model, respectively. The last column are the ground truth. We can see from the first four rows that the region based model can give much more boundary details, such as beak, airplane wheels and sofa edges. The last two rows show that the inaccuracy of superpixels may result in worse performance in the region based model.</figDesc><graphic coords="34,98.50,569.19,373.87,98.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>scoring pixel of each class matters, can only achieving 25.7%(mIoU) on the PASAL-VOC 2012 test. By taking all of pixels into account and using an EM-based training algorithm, A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of contextual model based scene labeling methods.</figDesc><table><row><cell>Papers</cell><cell>Scene type</cell><cell cols="2">Model Structure</cell><cell>Contextual information</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>CRF structure</cell><cell>others</cell><cell></cell><cell></cell></row><row><cell>Shotton et al.2006[29]</cell><cell>outdoor</cell><cell>Plain CRF</cell><cell>pixel-based</cell><cell>Smoothness preference;</cell><cell>graph cuts</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relative texture pattern;</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2D location prior</cell><cell></cell></row><row><cell>Fulkerson et al.2009[75]</cell><cell>outdoor</cell><cell></cell><cell>region-based</cell><cell>Smoothness preference</cell><cell>ditto</cell></row><row><cell>Zhang et al.2010[76]</cell><cell>outdoor</cell><cell></cell><cell>region-based(TP)</cell><cell>ditto</cell><cell>ditto</cell></row><row><cell>Silberman et al.2011[6]</cell><cell>indoor</cell><cell></cell><cell>pixel-based</cell><cell>Smoothness preference</cell><cell>ditto</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2D/3D location prior</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>geometry location prior;</cell><cell></cell></row><row><cell>Ren et al.2012[33]</cell><cell>indoor</cell><cell></cell><cell>region-based(gPb-OWT)</cell><cell>Smoothness preference</cell><cell>ditto</cell></row><row><cell>Cadena et al.2013[77]</cell><cell>indoor</cell><cell></cell><cell>region-based(SLIC)</cell><cell>ditto</cell><cell>LBP</cell></row><row><cell>Gould et al.2009[32]</cell><cell>outdoor</cell><cell></cell><cell>region-based(MS)</cell><cell>co-occurrence likelihoods</cell><cell>ICM</cell></row><row><cell>Batra et al.2008[81]</cell><cell>outdoor</cell><cell></cell><cell>region-based(FH)</cell><cell>ditto</cell><cell>LBP</cell></row><row><cell>Muller et al.2014[82]</cell><cell>indoor</cell><cell></cell><cell>region-based(SLIC)</cell><cell>ditto</cell><cell>-*</cell></row><row><cell>Kumar et al.2015[90]</cell><cell>outdoor</cell><cell></cell><cell>region-based(MS)</cell><cell>ditto</cell><cell>ICM</cell></row><row><cell>Gould et al.2008[80]</cell><cell>outdoor</cell><cell></cell><cell cols="2">region-based(Ncut and FH) relative location prior</cell><cell>LBP</cell></row><row><cell>Kohli et al.2008[30]</cell><cell>outdoor</cell><cell>Higher-order CRF</cell><cell>superpixel clique</cell><cell>Smoothness preference;</cell><cell>graph cuts</cell></row><row><cell>Ladicky et al.2009[31]</cell><cell>outdoor</cell><cell></cell><cell>hierarchical(MS)</cell><cell>ditto;</cell><cell>ditto</cell></row><row><cell>Plath et al.2009[84]</cell><cell>outdoor</cell><cell></cell><cell>hierarchical(FH)</cell><cell>global consistency;</cell><cell>-</cell></row><row><cell>Wojek et al.2008[85]</cell><cell>outdoor</cell><cell></cell><cell>object clique</cell><cell cols="2">local/object consistency ; LBP</cell></row><row><cell>Gonfaus et al.2010[86]</cell><cell>outdoor</cell><cell></cell><cell>image clique</cell><cell>local/global consistency</cell><cell>-</cell></row><row><cell>Lucchi et al.2011[87]</cell><cell>outdoor</cell><cell></cell><cell>ditto</cell><cell>ditto</cell><cell>-</cell></row><row><cell>Khan et al.2014[8]</cell><cell>indoor</cell><cell></cell><cell>3D plane clique</cell><cell>local consistency</cell><cell>-</cell></row><row><cell>Torralba et al et al.2005[88]</cell><cell>outdoor</cell><cell>Dense CRF</cell><cell>boosting framework</cell><cell>object correlations</cell><cell>BP</cell></row><row><cell>Rabinovich et al.2007[89]</cell><cell>outdoor</cell><cell></cell><cell>region-based(NCut)</cell><cell>co-occurrence prior</cell><cell>-</cell></row><row><cell>Krahenbuhl et al.2011[62]</cell><cell>outdoor</cell><cell></cell><cell>pixel-based;</cell><cell>local/global dependency</cell><cell>mean-field</cell></row></table><note><p>* Inference algorithms that not introduced in this paper</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>With the development of 3D sensors, such as Light Detection and Ranging(LIDAR), 3D laser scanners and RGB-D cameras, it is feasible to obtain 3D point clouds of the environment or objects<ref type="bibr" target="#b96">[96]</ref>. As a result, the concept of image labeling has been extended to 3D point clouds labeling. Like methods outlined in 3.2, most 3D scene labeling methods are formulated as the problem of finding the most appropriate labeling of a CRF. The representative methods are summarized in the Table2.</figDesc><table><row><cell>3.4. 3D scene labeling methods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of influential 3D scene labeling methods.</figDesc><table><row><cell cols="2">CRF Model Structure</cell><cell>paper</cell><cell>3D region</cell><cell>Learning Strategy</cell></row><row><cell>associative</cell><cell>point-based</cell><cell>Lu et al.2012[97]</cell><cell>-</cell><cell>Piecewise learning</cell></row><row><cell></cell><cell></cell><cell cols="2">Anguelov et al.2005[98] -</cell><cell>Max-margin Structural</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>learning</cell></row><row><cell></cell><cell cols="2">3D region-based Valentin et al.2013[99]</cell><cell>Meshes</cell><cell>Piecewise learning</cell></row><row><cell cols="2">non-associative point-based</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">3D region-based Xiong et al.2010[100]</cell><cell>Planar patches</cell><cell>Pseudolikelihood</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Structural learning</cell></row><row><cell></cell><cell></cell><cell>Kahler et al. 2013[101]</cell><cell>SLIC-like supervoxels</cell><cell>ditto</cell></row><row><cell></cell><cell></cell><cell>Pham et al. 2015[102]</cell><cell>ditto</cell><cell>ditto</cell></row><row><cell></cell><cell></cell><cell>Anand et al. 2012[103]</cell><cell>region growing based super-</cell><cell>Max-margin Structural</cell></row><row><cell></cell><cell></cell><cell></cell><cell>voxels</cell><cell>learning</cell></row><row><cell></cell><cell></cell><cell>Kim et al. 2013[104]</cell><cell>Small cubic</cell><cell>ditto</cell></row><row><cell></cell><cell></cell><cell>Wolf et al. 2015[105]</cell><cell>SLIC-like supervoxels</cell><cell>ditto</cell></row><row><cell></cell><cell></cell><cell>Najafi et al. 2014[106]</cell><cell>K-means based supervoxels</cell><cell>Piecewise learning</cell></row></table><note><p>Unlike 2D scene labeling, in which the using of associative potential is very common, the non-associative potential is more popular in 3D point cloud labeling. This can be explained by the difference between the information provided by 2D images and 3D point clouds. With the explicit and discriminative visual appearance(e.g., color), the unary classifier is able to provide a good(albeit imperfect) result. Hence, majority of CRF based 2D scene labeling methods set the pairwise potential with a simple Potts model, merely aim for enforcing local smoothness. However, the information that can be exploited from 3D point clouds is more of implicit geometric structure, within which considerable geometry contextual are hidden. Consequently, the research community has shifted its focus to non-associative potential to employ as much geometric relationships as possible.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Popular datasets for 2D/3D semantic segmentation</figDesc><table><row><cell>Dataset</cell><cell>Scene type</cell><cell>Number</cell><cell></cell><cell>Annotation</cell><cell>Website</cell></row><row><cell></cell><cell></cell><cell>images/scenes</cell><cell>classes</cell><cell></cell><cell></cell></row><row><cell>MSRC[29]</cell><cell>outdoor</cell><cell>591</cell><cell>23</cell><cell>pixel-level labeling</cell><cell>https://www.microsoft.com/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>en-us/research/project/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>image-understanding/</cell></row><row><cell>LabelMe[151]</cell><cell>hybird</cell><cell>187,240</cell><cell>unlimited</cell><cell>bounding polygon</cell><cell>http://labelme2.csail.mit.edu/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Release3.0/</cell></row><row><cell>PASCAL VOC[113]</cell><cell>hybird</cell><cell>21,738</cell><cell>20</cell><cell>hybrid</cell><cell>http://host.robots.ox.ac.uk/</cell></row><row><cell>ImageNet[152]</cell><cell>hybird</cell><cell>1,431,167</cell><cell>1000</cell><cell>ditto</cell><cell>http://image-net.org/download</cell></row><row><cell>MSC COCO[154]</cell><cell>hybird</cell><cell>328,000</cell><cell>91</cell><cell>object masks</cell><cell>http://mscoco.org/dataset/</cell></row><row><cell>SIFT-FLOW[93]</cell><cell>outdoor</cell><cell>2,688</cell><cell>33</cell><cell>pixel-level labeling</cell><cell>http://people.csail.mit.edu/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>celiu/LabelTransfer/code.html</cell></row><row><cell>SBD[32]</cell><cell>outdoor</cell><cell>715</cell><cell>8</cell><cell>pixel-level labeling</cell><cell>http://dags.stanford.edu/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>projects/scenedataset.html</cell></row><row><cell>CityScapes[155]</cell><cell>street</cell><cell>25,000</cell><cell>19</cell><cell>hybrid</cell><cell>https://www.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cityscapes-dataset.com/</cell></row><row><cell>ADE20K[156]</cell><cell>hybird</cell><cell>20,000+</cell><cell>150</cell><cell>pixel-level labeling</cell><cell>http://groups.csail.mit.edu/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vision/datasets/ADE20K/</cell></row><row><cell>NYU Depth V2[157]</cell><cell>indoor</cell><cell>1,449</cell><cell>894</cell><cell>pixel-level labeling</cell><cell>http://cs.nyu.edu/ ~silberman/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>datasets/nyu\_depth\_v2.html</cell></row><row><cell>Washington RGB-D[158]</cell><cell>indoor</cell><cell>250,000</cell><cell>300</cell><cell>segmented object</cell><cell>http://rgbd-dataset.cs.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>washington.edu/</cell></row><row><cell>Cornell RGB-D[159]</cell><cell>indoor</cell><cell cols="2">52(point cloud) undetailed</cell><cell cols="2">point level labeling http://pr.cs.cornell.edu/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sceneunderstanding/</cell></row><row><cell>Sun RGB-D[160]</cell><cell>indoor</cell><cell>10,335</cell><cell>undetailed</cell><cell>hybrid</cell><cell>http://rgbd.cs.princeton.edu/</cell></row><row><cell>ScanNet[161]</cell><cell>indoor</cell><cell>1513</cell><cell>20</cell><cell>voxel level labeling</cell><cell>http://www.scan-net.org/</cell></row><row><cell>VMR-Okaland-V2[162]</cell><cell>outdoor</cell><cell>36(3D blocks)</cell><cell>7</cell><cell>point level labeling</cell><cell>http://www.cs.cmu.edu/\%CB\</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>%9Cvmr/datasets/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of CRF model and non-CRF model.</figDesc><table><row><cell>Models</cell><cell cols="2">P-ACC mean JI(IOU)</cell></row><row><cell cols="2">Region-wise classifier[33] 82.85%</cell><cell>63.75%</cell></row><row><cell>Region based CRF[33]</cell><cell>84.18%</cell><cell>65.00%</cell></row><row><cell>pixel v.s. superpixel</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of pixel based model and region based model</figDesc><table><row><cell>Models</cell><cell cols="2">P-ACC mean JI(IOU)</cell></row><row><cell>Pixel-wise predictions of FCN[19]</cell><cell>90.97%</cell><cell>60.14%</cell></row><row><cell cols="2">Region-wise predictions of FCN[19] 91.11%</cell><cell>60.73%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of indoor scene labeling methods using NYU datasets</figDesc><table><row><cell>Methods</cell><cell>Descriptions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>presents top 10 performances achieved on PASCAL VOC 2012 segmentation task. The numbers in the brackets represent the rankings(up to Nov. 2017) in performance. Several rankings are missing because some works listed in the leader board 1 do not provide corresponding technical reports. We can see that these methods have three characters in common: using</figDesc><table /><note><p><p><p>a very deep CNN(ResNet, Inception-ResNet) to produce feature maps, using additional data(e.g. COCO, JFT-300M</p><ref type="bibr" target="#b165">[165]</ref></p>) for pre-training and using FCN based model to produce dense predictions. It is worth noting that most of them achieve the state-of-the-art even without using CRF. This is mainly due to the strength of the deep CNN on learning discriminative features. In addition, although the CRF is waived, the contextual information is also encoded by other strategies(e.g. multi-scale).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>State-of-the-arts on PASCAL VOC 2012 test dataset</figDesc><table><row><cell>Methods</cell><cell>ResNet</cell><cell>COCO</cell><cell cols="2">Improvement over basic FCN</cell><cell>Others</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">atrous conv. skip connection</cell><cell></cell><cell></cell></row><row><cell>DeepLabv3-JFT(1)[166]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>pre-trained on JFT-300M</cell><cell>86.9</cell></row><row><cell>DIS(2)[147]</cell><cell>yes</cell><cell>yes</cell><cell>n/a</cell><cell>n/a</cell><cell>multi-task learning</cell><cell>86.8</cell></row><row><cell>IDW-CNN(4)[149]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>multi-task learning</cell><cell>86.3</cell></row><row><cell>DeepLabv3(5)[166]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>n/a</cell><cell>85.7</cell></row><row><cell>PSPNet(6)[25]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>n/a</cell><cell>85.4</cell></row><row><cell>ResNet-38 COCO(7)[115]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>n/a</cell><cell>84.9</cell></row><row><cell>Multipath-RefineNet(8)[120]</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>yes</cell><cell>n/a</cell><cell>84.2</cell></row><row><cell>Large Kernel Matters(9)[26]</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>yes</cell><cell>n/a</cell><cell>83.6</cell></row><row><cell cols="5">7.2.3. Weakly supervised methods v.s. fully supervised methods</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison between weakly supervised methods and fully supervised methods</figDesc><table><row><cell>Methods</cell><cell>Annotations</cell><cell>Descriptions</cell><cell>mIoU(VOC2012)</cell></row><row><cell>Pathak et al.2015[40]</cell><cell>Image-level labels</cell><cell>MIL+Fully CNN</cell><cell>25.66%</cell></row><row><cell>Papandreou et al. 2015[39]</cell><cell>ditto</cell><cell>EM based training +Deep CNN</cell><cell>39.6%</cell></row><row><cell>Dai et al.2015</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work has been supported by National Natural Science Foundation of China (Grant No. 61573135), National Key Technology Support Program(Grant No. 2015BAF11B01), National Key Scientific Instrument and Equipment Development Project of China (Grant No. 2013YQ140517), Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing (Grant No.2018001), Science and Technology Plan Project of Shenzhen City (JCYJ20170306141557198), Key Project of Science and Technology Plan of Guangdong Province(Gran No. 2013B011301014), Open foundation of State Key Laboratory of Robotics of China (Grant No. 2013O09), and the National Institutes of Health of the United States (Grant No.R01CA165255 and R21CA172864).</p><p>References</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the beginning of training, pixels are equally treated because initial predictions for most pixels are of low confidences with respect to ground truths. As the training proceeds, easy pixels are gradually recognized and ignored. Instead of fully ignoring easy pixels, the Deep Layer Cascade <ref type="bibr" target="#b133">[133]</ref> ignores easy pixels only in deep layers. A more advanced work <ref type="bibr" target="#b135">[135]</ref> was introduced to adaptively evaluate the contributions of each pixel, where pixels with higher loss are weighted more than pixels with a lower loss. Note that these methods can be also regarded as cost-sensitive learning with hard(class)-specific weights. Although it is difficulty to design hard(class)-aware loss function, this kind of method can improve the performance of FCN without changing the model structure. Thus, it can be used as a generic strategy for improving semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Papers</head><p>Year Description Basic CNN Alvarez et al. <ref type="bibr" target="#b23">[23]</ref> Pixel-wise multi-scale CNN +assoc * CRF as post-processing none Farabet et al. <ref type="bibr" target="#b15">[15]</ref> Pixel-wise multi-scale CNN +superpixels+assoc CRF as post-processing none Couprie et al. <ref type="bibr" target="#b20">[20]</ref> ditto none Girshick et al. <ref type="bibr" target="#b17">[17]</ref> Region-wise CNN AlexNet[13] Gupta et al. <ref type="bibr" target="#b21">[21]</ref> ditto ditto Mostajabi et al. <ref type="bibr" target="#b110">[110]</ref> Superpixel classification using multi-scale CNN features VGG[14] Sharma et al. <ref type="bibr" target="#b111">[111]</ref> Superpixel classification using features learned from a two-stage process Multi-scale CNN[15] Pinheiro et al. <ref type="bibr" target="#b109">[109]</ref> Shift-and-Stitch none Long et al. <ref type="bibr" target="#b19">[19]</ref> Skip connected UP FCN VGG Noh et al. <ref type="bibr" target="#b22">[22]</ref> UP FCN on regions VGG Badrinarayanan et al. <ref type="bibr" target="#b114">[114]</ref> UP FCN+decoder network VGG Hariharan et al. <ref type="bibr" target="#b118">[118]</ref> Skip connected UP FCN on regions unknown Zheng et al. <ref type="bibr" target="#b126">[126]</ref> Skip connected UP FCN+embedded CRF(as RNN) VGG[19] Liu et al. <ref type="bibr" target="#b129">[129]</ref> Atrous UP FCN+embedded CRF(as CNN) VGG Arnab et al. <ref type="bibr" target="#b127">[127]</ref> Skip connected UP FCN+embedded higher order CRF VGG Peng et al. <ref type="bibr" target="#b26">[26]</ref> Skip connected UP FCN+large receptive field ResNet[136] Lin et al. <ref type="bibr" target="#b125">[125]</ref> Skip connected UP FCN+non-assoc CRF using CNN based pair-pot @ VGG Chen et al. <ref type="bibr" target="#b16">[16]</ref> Atrous UP FCN+ dense assoc CRF as post-processing VGG Chen et al. <ref type="bibr" target="#b116">[116]</ref> Atrous UP FCN + ASPP + dense assoc CRF as post-processing ResNet Bertasius et al. <ref type="bibr" target="#b24">[24]</ref> Atrous UP FCN+assoc CRF using pair-pot computed from learned boundaries deeplab[16] Zhao et al. <ref type="bibr" target="#b25">[25]</ref> Atrous UP FCN+pyramid pooling for considering high-level contexts ResNet Wu et al. <ref type="bibr" target="#b115">[115]</ref> Atrous UP FCN+better basic CNN ResNet Vemulapalli et al. <ref type="bibr" target="#b130">[130]</ref> Atrous UP FCN+embed Gaussian CRF deeplab Chandra <ref type="bibr" target="#b131">[131]</ref> Atrous UP FCN+Gaussian CRF ditto Visin et al. <ref type="bibr" target="#b35">[35]</ref> CNN+RNN VGG Shuai et al. <ref type="bibr" target="#b36">[36]</ref> ditto VGG Eigen et al. <ref type="bibr" target="#b18">[18]</ref> Skip connected UP FCN+CNN based feature fusion AlexNet/VGG Lin et al. <ref type="bibr" target="#b120">[120]</ref> ditto ResNet Park et al. <ref type="bibr" target="#b121">[121]</ref> ditto ResNet Islam et al. <ref type="bibr" target="#b122">[122]</ref> Skip connected UP FCN+CNN based feature fusion+deeply supervision VGG Li et al. <ref type="bibr" target="#b133">[133]</ref> Atrous UP FCN+hard-aware learning Inception-ResNet[137] Bulo et al. <ref type="bibr" target="#b135">[135]</ref> Atrous UP FCN+ASPP+hard-aware learning ResNet</p><p>* associative. @ pairwise potential.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y.-I</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis system for scenes containing objects with substructures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Joint Conference on Pattern Recognitions</title>
		<meeting>the Fourth International Joint Conference on Pattern Recognitions</meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="752" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating visual cues for object segmentation and recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics News</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cpmc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A statistical model for general contextual object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="350" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geometry driven semantic labeling of indoor scenes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="679" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<title level="m">Indoor semantic segmentation using depth information</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic road segmentation via multi-scale ensembles of learned features</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia ; Arxiv Preprint A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02719</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Region classification with markov field aspect models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Á</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">695</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3249" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with a multiimage model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7144</idno>
		<title level="m">Fully convolutional multi-class multiple instance learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A probabilistic associative model for segmenting weakly supervised images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4150" to="4159" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiple instance classification: Review, taxonomy and comparative study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="27" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Survey of the problem of object detection in real images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="441" to="466" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey of graph theoretical approaches to image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1020" to="1038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Turbopixels: Fast superpixels using geometric flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discriminative random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="201" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning, ICML</title>
		<meeting>the eighteenth international conference on machine learning, ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Optimization by simmulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Černỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of optimization theory and applications</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient belief propagation for early vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reverend bayes on inference engines: A distributed hierarchical approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second National Conference on Artificial Intelligence</title>
		<meeting>the Second National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A comparative study of modern inference techniques for structured discrete energy minimization problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="184" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Spin-images: a representation for 3-d surface matching</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A comprehensive review of current local features for computer vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1771" to="1787" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Statistical cues for domain specific image segmentation with performance analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Single-histogram class models for image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks and 3d brain image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Mutual boosting for contextual inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1515" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Structured class-labels in random forests for semantic image labelling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2190" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multiple class segmentation using a unified framework over mean-shift patches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Semantic segmentation with second-order pooling, Computer Vision-ECCV</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Indoor scene understanding with rgb-d images: Bottomup segmentation, object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Object recognition by integrating multiple image segmentations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="481" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Class segmentation and object localization with superpixel neighborhoods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="708" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Semantic parsing for priming object detection in rgb-d scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Košecka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>rd Workshop on Semantic Perception, Mapping and Exploration</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning associative markov networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning and incorporating top-down cues in image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="338" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multi-class segmentation with relative location prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning class-specific affinities for image labelling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning depth-sensitive conditional random fields for semantic segmentation of rgb-d images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6232" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning crfs using graph cuts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Multi-class image segmentation using conditional random fields and global classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Plath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">A dynamic conditional random field model for joint labeling of object and scene classes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Harmony potentials for joint classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V D</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Are spatial and global constraints really necessary for segmentation?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Contextual models for object detection using boosted random fields</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1401" to="1408" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Parameter estimation and energy minimization for region-based semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Turki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1373" to="1386" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Superpixel graph label transfer with learned distance metric</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="632" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">3d is here: Point cloud library (pcl), in: Robotics and Automation (ICRA)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Simplified markov random fields for efficient semantic labeling of 3d point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Discriminative learning of markov random fields for segmentation of 3d scan data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskarf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Mesh based semantic modelling for indoor and outdoor scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2067" to="2074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Using context to create semantic 3d models of indoor environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Efficient 3d scene labeling using fields of trees</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3064" to="3071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Hierarchical higher-order regression forest fields: An application to 3d indoor scene labelling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2246" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Contextually guided semantic labeling and search for three-dimensional point clouds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">3d scene understanding by voxel-crf</title>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3d point clouds using a dense crf with learned parameters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4867" to="4873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Non-associative higher-order markov networks for point cloud classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Namin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="500" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Receptive fields and functional architecture of monkey striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="243" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.27955</idno>
		<title level="m">Recurrent convolutional neural networks for scene parsing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoomout features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Deep hierarchical parsing for semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06612</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Label refinement network for coarse-to-fine semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00551</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<title level="m">Object detectors emerge in deep scene CNNs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1311" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks, in: IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Higher order conditional random fields in deep neural networks, in: European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01344</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Loss max-pooling for semantic image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2126" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun ; C C E P T E D M A N U S C R I P T On</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Pattern Recognition</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F G D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Weakly supervised structured output learning for semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="845" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Label to region by bi-layer sparsity priors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM international conference on Multimedia</title>
		<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Weakly supervised graph propagation towards collective image parsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="373" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Semantic segmentation without annotating segments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2176" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Boxsup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04924</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Learning object interactions and descriptions for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Fundamental bounds on edge detection: An information theoretic evaluation of different edge cues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft coco: Common objects in context, in: European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ade20k dataset</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3d point clouds for indoor scenes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04405</idno>
		<title level="m">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">3-d scene analysis via sequenced predictions over points and regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="2609" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Deep human parsing with active template regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2414" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Salient deconvolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="120" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07759</idno>
		<title level="m">Multi-view 3d object detection network for autonomous driving</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Webly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Ortiz Segovia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3626" to="3635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using webcrawled videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7322" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Iterative deep convolutional encoder-decoder network for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Learning to segment human by watching youtube</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1462" to="1468" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Laws of organization in perceptual forms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wertheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A source book of Gestalt psychology</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge &amp; Kegan Paul</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">He is currently an associate professor of Hunan University and associate dean of National Engineering Laboratory for Robot Visual Perception and Control</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Hongshan Yureceived The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S D</forename><surname>Ph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include autonomous mobile robot and machine vision. Zhengeng Yang received the B.S. and M.S. degrees from Central South University</title>
		<meeting><address><addrLine>Changsha, China; Changsha, China; Changsha, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2004. 2007. 2011 to 2012. 2009. 2012</date>
		</imprint>
		<respStmt>
			<orgName>Hunan University ; Laboratory for Computational Neuroscience of University of Pittsburgh, USA ; Hunan University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer vision, image analysis and machine learning. especially focus on the problems of semantic segmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
