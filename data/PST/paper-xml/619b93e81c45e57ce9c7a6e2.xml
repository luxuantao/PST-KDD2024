<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fluency Boost Learning and Inference for Neural Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
							<email>tage@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fluency Boost Learning and Inference for Neural Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence's fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally through multi-round seq2seq inference until the sentence's fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence-to-sequence (seq2seq) models <ref type="bibr" target="#b1">(Cho et al., 2014;</ref><ref type="bibr" target="#b51">Sutskever et al., 2014)</ref> for grammatical error correction (GEC) have drawn growing attention <ref type="bibr">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b56">Xie et al., 2016;</ref><ref type="bibr" target="#b24">Ji et al., 2017;</ref><ref type="bibr" target="#b48">Schmaltz et al., 2017;</ref><ref type="bibr" target="#b47">Sakaguchi et al., 2017;</ref><ref type="bibr" target="#b7">Chollampatt and Ng, 2018)</ref> in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure <ref type="figure" target="#fig_1">1(a)</ref>. Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is  common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure <ref type="figure" target="#fig_1">1(b)</ref>. Second, the seq2seq models usually cannot perfectly correct a sentence with many grammatical errors through single-round seq2seq inference, as shown in Figure <ref type="figure" target="#fig_1">1</ref>(b) and 1(c), because some errors in a sentence may make the context strange, which confuses the models to correct other errors.</p><p>To address the above-mentioned limitations in model learning and inference, this paper proposes a novel fluency boost learning and inference mechanism, illustrated in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>For fluency boosting learning, not only is a seq2seq model trained with original errorcorrected sentence pairs, but also it generates less fluent sentences (e.g., from its n-best outputs) to establish new error-corrected sentence pairs by pairing them with their correct sentences during training, as long as the sentences' fluency<ref type="foot" target="#foot_0">1</ref> is be- For model inference, fluency boost inference mechanism allows the model to correct a sentence incrementally with multi-round inference as long as the proposed edits can boost the sentence's fluency, as Figure <ref type="figure" target="#fig_2">2</ref>(b) shows. For a sentence with multiple grammatical errors, some of the errors will be corrected first. The corrected parts will make the context clearer, which may benefit the model to correct the remaining errors.</p><p>Experiments demonstrate fluency boost learning and inference enable neural seq2seq models to perform better for GEC and achieve state-of-theart results on multiple GEC benchmarks.</p><p>Our contributions are summarized as follows:</p><p>• We present a novel learning and inference mechanism to address the limitations in previous seq2seq models for GEC.</p><p>• We propose and compare multiple novel fluency boost learning strategies, exploring the learning methodology for neural GEC.</p><p>• Our approaches are proven to be effective to improve neural seq2seq GEC models to achieve state-of-the-art results on CoNLL-2014 and JFLEG benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Neural grammatical error correction</head><p>As neural machine translation (NMT), a typical neural GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model <ref type="bibr" target="#b51">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Cho et al., 2014)</ref> with attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref> to edit a raw sentence into the grammatically correct sentence it should be, as Figure <ref type="figure" target="#fig_1">1</ref>(a) shows. Given a raw sentence x r = (x r 1 , • • • , x r M ) and its corrected sentence</p><formula xml:id="formula_0">x c = (x c 1 , • • • , x c N ) in which x r</formula><p>M and x c N are the M -th and N -th words of sentence x r and x c respectively, the error correction seq2seq model learns a probabilistic mapping P (x c |x r ) from error-corrected sentence pairs through maximum likelihood estimation (MLE), which learns model parameters Θ crt to maximize the following equation:</p><formula xml:id="formula_1">Θ * crt = arg max Θ crt (x r ,x c )∈S * log P (x c |x r ; Θcrt) (1)</formula><p>where S * denotes the set of error-corrected sentence pairs. For model inference, an output sequence</p><formula xml:id="formula_2">x o = (x o 1 , • • • , x o i , • • • , x o L</formula><p>) is selected through beam search, which maximizes the following equation: </p><formula xml:id="formula_3">P (x o |x r ) = L i=1 P (x o i |x r , x o &lt;i; Θcrt) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fluency boost learning</head><p>Conventional seq2seq models for GEC learns model parameters only from original errorcorrected sentence pairs. However, such errorcorrected sentence pairs are not sufficiently available. As a result, many neural GEC models are not very well generalized.</p><p>Fortunately, neural GEC is different from NMT. For neural GEC, its goal is improving a sentence's fluency 2 without changing its original meaning; thus, any sentence pair that satisfies this condition (we call it fluency boost condition) can be used as a training instance.</p><p>In this paper, we define f (x) as the fluency score of a sentence x:</p><formula xml:id="formula_4">f (x) = 1 1 + H(x) (3) H(x) = − |x| i=1 log P (xi|x&lt;i) |x|<label>(4)</label></formula><p>where P (x i |x &lt;i ) is the probability of x i given context x &lt;i , computed by a language model, and |x| is the length of sentence x. H(x) is actually the cross entropy of the sentence x, whose range is [0, +∞). Accordingly, the range of f (x) is (0, 1].</p><p>The core idea of fluency boost learning is to generate fluency boost sentence pairs that satisfy the fluency boost condition during training, as Figure 2(a) illustrates, so that these pairs can further help model learning.</p><p>In this section, we present three fluency boost learning strategies: back-boost, self-boost, and 2 Fluency of a sentence in this paper refers to how likely the sentence is written by a native speaker. In other words, if a sentence is very likely to be written by a native speaker, it should be regarded highly fluent.</p><p>dual-boost that generate fluency boost sentence pairs in different ways, as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type="bibr" target="#b49">(Sennrich et al., 2016)</ref> in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors. Since the less fluent sentences are generated by the error generation seq2seq model trained with error-corrected data, they usually do not change the original sentence's meaning; thus, they can be paired with their correct sentences, establishing fluency boost sentence pairs that can be used as training instances for error correction models, as Figure <ref type="figure" target="#fig_3">3(a)</ref> shows.</p><p>Specifically, we first train a seq2seq error generation model Θ gen with S * which is identical to S * except that the source sentence and the target sentence are interchanged. Then, we use the model</p><formula xml:id="formula_5">Θ gen to predict n-best outputs x o 1 , • • • , x on</formula><p>given a correct sentence x c . Given the fluency boost condition, we compare the fluency of each output x o k (where 1 ≤ k ≤ n) to that of its correct sentence x c . If an output sentence's fluency score is much lower than its correct sentence, we call it a disfluency candidate of x c .</p><p>To formalize this process, we first define Y n (x; Θ) to denote the n-best outputs predicted by model Θ given the input x. Then, disfluency candidates of a correct sentence x c can be derived: for each (x r , x c ) ∈ St do 9:</p><formula xml:id="formula_6">D back (x c ) = {x o k |x o k ∈ Yn(xc; Θgen) ∧ f (x c ) f (x o k ) ≥ σ} (5)</formula><p>Establish a fluency boost pair (x , x c ) by randomly sampling x ∈ D back (x c ); 10:</p><p>S ← S ∪ {(x , x c )}; 11:</p><p>end for 12:</p><p>Update error correction model Θcrt with S * ∪ S ; 13: end for where D back (x c ) denotes the disfluency candidate set for x c in back-boost learning. σ is a threshold to determine if x o k is less fluent than x c and it should be slightly larger<ref type="foot" target="#foot_1">3</ref> than 1.0, which helps filter out sentence pairs with unnecessary edits (e.g., I like this book. → I like the book.).</p><p>In the subsequent training epochs, the error correction model will not only learn from the original error-corrected sentence pairs (x r ,x c ), but also learn from fluency boost sentence pairs (x o k ,x c ) where</p><formula xml:id="formula_7">x o k is a sample of D back (x c ).</formula><p>We summarize this process in Algorithm 1 where S * is the set of original error-corrected sentence pairs, and S can be tentatively considered identical to S * when there is no additional native data to help model training (see Section 3.4). Note that we constrain the size of S t not to exceed |S * | (the 7th line in Algorithm 1) to avoid that too many fluency boost pairs overwhelm the effects of the original error-corrected pairs on model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-boost learning</head><p>In contrast to back-boost learning whose core idea is originally from NMT, self-boost learning is original, which is specially devised for neural GEC. The idea of self-boost learning is illustrated by Figure <ref type="figure" target="#fig_3">3</ref> for each (x r , x c ) ∈ St do 10:</p><p>Update D self (x c ) according to Eq (6); 11:</p><p>Establish a fluency boost pair (x , x c ) by randomly sampling x ∈ D self (x c ); 12:</p><p>S ← S ∪ {(x , x c )}; 13:</p><p>end for 14: end for sentence's meaning; thus, they can be used to establish fluency boost sentence pairs.</p><p>For self-boost learning, given an error corrected pair (x r , x c ), an error correction model Θ crt first predicts n-best outputs x o 1 , • • • , x on for the raw sentence x r . Among the n-best outputs, any output that is not identical to x c can be considered as an error prediction. Instead of treating the error predictions useless, self-boost learning fully exploits them. Specifically, if an error prediction x o k is much less fluent than that of its correct sentence x c , it will be added to x c 's disfluency candidate set D self (x c ), as Eq (6) shows:</p><formula xml:id="formula_8">D self (x c ) = D self (x c ) ∪ {x o k |x o k ∈ Yn(xr; Θcrt) ∧ f (x c ) f (x o k ) ≥ σ}<label>(6)</label></formula><p>In contrast to back-boost learning, self-boost generates disfluency candidates from a different perspective -by editing the raw sentence x r rather than the correct sentence x c . It is also noteworthy that D self (x c ) is incrementally expanded because the error correction model Θ crt is dynamically updated, as shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-boost learning</head><p>As introduced above, back-and self-boost learning generate disfluency candidates from different perspectives to create more fluency boost sentence pairs to benefit training the error correction model. Intuitively, the more diverse disfluency candidates generated, the more helpful for training an error correction model. Inspired by <ref type="bibr" target="#b22">He et al. (2016)</ref> and <ref type="bibr" target="#b61">Zhang et al. (2018)</ref>, we propose a dual-boost learning strategy, combining both back-and selfboost's perspectives to generate disfluency candidates. As Figure <ref type="figure" target="#fig_3">3</ref>(c) shows, disfluency candidates in dual-boost learning are from both the error generation model and the error correction model :</p><formula xml:id="formula_9">D dual (x c ) = D dual (x c ) ∪ {x o k |x o k ∈ Yn(xr; Θcrt) ∪ Yn(xc; Θgen) ∧ f (x c ) f (x o k ) ≥ σ}<label>(7)</label></formula><p>Moreover, the error correction model and the error generation model are dual and both of them are dynamically updated, which improves each other: the disfluency candidates produced by error generation model can benefit training the error correction model, while the disfluency candidates created by error correction model can be used as training data for the error generation model. We summarize this learning approach in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fluency boost learning with large-scale native data</head><p>Our proposed fluency boost learning strategies can be easily extended to utilize the huge volume of native data which is proven to be useful for GEC. As discussed in Section 3.1, when there is no additional native data, S in Algorithm 1-3 is identical to S * . In the case where additional native data is available to help model learning, S becomes: S = S * ∪ C where C = {(x c , x c )} denotes the set of selfcopied sentence pairs from native data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fluency boost inference</head><p>As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference shows, fluency boost inference allows a sentence to be incrementally edited through multiround seq2seq inference as long as the sentence's fluency can be improved. Specifically, an error correction seq2seq model first takes a raw sentence x r as an input and outputs a hypothesis x o 1 . Instead of regarding x o 1 as the final prediction, fluency boost inference will then take x o 1 as the input to generate the next output x o 2 . The process will not terminate unless x ot does not improve x o t−1 in terms of fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and evaluation</head><p>As previous studies <ref type="bibr" target="#b24">(Ji et al., 2017)</ref>, we use the public Lang-8 Corpus <ref type="bibr" target="#b31">(Mizumoto et al., 2011;</ref><ref type="bibr" target="#b52">Tajiri et al., 2012)</ref>, Cambridge Learner Corpus (CLC) <ref type="bibr" target="#b37">(Nicholls, 2003)</ref> and NUS Corpus of Learner English (NUCLE) <ref type="bibr" target="#b13">(Dahlmeier et al., 2013)</ref> as our original error-corrected training data.</p><p>Table <ref type="table">1</ref> shows the stats of the datasets. In addition, we also collect 2,865,639 non-public errorcorrected sentence pairs from Lang-8.com. The native data we use for fluency boost learning is English Wikipedia that contains 61,677,453 sentences.</p><p>We use CoNLL-2014 shared task dataset with original annotations <ref type="bibr">(Ng et al., 2014)</ref>, which contains 1,312 sentences, as our main test set for evaluation. We use MaxMatch (M 2 ) precision, recall and F 0.5 <ref type="bibr" target="#b11">(Dahlmeier and Ng, 2012b)</ref> as our evaluation metrics. As previous studies, we use CoNLL-2013 test data as our development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setting</head><p>We set up experiments in order to answer the following questions: • Whether does our fluency boost inference improve normal seq2seq inference for GEC?</p><p>• Whether can our approach improve neural GEC to achieve state-of-the-art results?</p><p>The training details for our seq2seq error correction model and error generation model are as follows: the encoder of the seq2seq models is a 2-layer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism <ref type="bibr" target="#b29">(Luong et al., 2015)</ref>. Both the dimensionality of word embeddings and the hidden size of GRU cells are 500. The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 respectively. The models' parameters are uniformly initialized in <ref type="bibr">[-0.1,0.1]</ref>. We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128. Dropout is applied to non-recurrent connections at a ratio of 0.15. For fluency boost learning, we generate disfluency candidates from 10-best outputs. During model inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN language model <ref type="bibr" target="#b30">(Mikolov et al., 2010)</ref> through shallow fusion <ref type="bibr" target="#b20">(Gülc ¸ehre et al., 2015)</ref> with weight β = 0.15. The RNN language model is trained from the native data mentioned in Section 5.1, which is also used for computing fluency score in Eq (3). UNK tokens are replaced with the source token with the highest attention weight.</p><p>We resolve spelling errors with a public spell checker<ref type="foot" target="#foot_2">4</ref> as preprocessing, as <ref type="bibr" target="#b56">Xie et al. (2016)</ref> and <ref type="bibr" target="#b47">Sakaguchi et al. (2017)</ref> do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effectiveness of fluency boost learning</head><p>Table <ref type="table" target="#tab_3">2</ref> compares the performance of seq2seq error correction models with different learning and inference methods. By comparing by row, one can observe that our fluency boost learning approaches improve the performance over normal seq2seq learning, especially on the recall metric, since the fluency boost learning approaches generate a variety of grammatically incorrect sentences, allowing the error correction model to learn to correct much more sentences than the conventional learning strategy. Among the proposed three fluency boost learning strategies, dual-boost achieves the best result in most cases because it produces more diverse incorrect sentences (average |D dual | ≈ 9.43) than either back-boost (avg |D back | ≈ 1.90) or self-boost learning (avg |D self | ≈ 8.10). With introducing large amounts of native text data, the performance of all the fluency boost learning approaches gets improved. One reason is that our learning approaches produce more error-corrected sentence pairs to let the model be better generalized. In addition, the huge volume of native data benefits the decoder to learn better to generate a fluent and error-free sentence.</p><p>We test the effect of hyper-parameter σ in Eq (5-7) on fluency boost learning and show the result in Table <ref type="table" target="#tab_5">3</ref>. When σ is slightly larger than 1.0 (e.g., σ = 1.05), the model achieves the best performance because it effectively avoids generating sentence pairs with unnecessary or undesirable edits that affect the performance, as we discussed in Section 3.1. When σ continues increasing, the disfluency candidate set |D dual | drastically decreases, making the dual-boost learning gradually degrade to normal seq2seq learning.</p><p>Table <ref type="table">4</ref> shows some examples of disfluency  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct sentence</head><p>How autism occurs is not well understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disfluency candidates</head><p>How autism occurs is not good understood. How autism occur is not well understood. What autism occurs is not well understood. How autism occurs is not well understand. How autism occurs does not well understood.</p><p>Table <ref type="table">4</ref>: Examples of disfluency candidates for a correct sentence in dual-boost learning.</p><p>candidates<ref type="foot" target="#foot_3">5</ref> generated in dual-boost learning given a correct sentence in the native data. It is clear that our approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effectiveness of fluency boost inference</head><p>The effectiveness of various inference approaches can be observed by comparing the results in Table <ref type="table" target="#tab_3">2</ref> by column. Compared to the normal seq2seq inference and seq2seq (+LM) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on F 0.5 respectively, which is a significant<ref type="foot" target="#foot_4">6</ref> improvement, demonstrating multi-round edits by fluency boost inference is effective. Take our best system (the last row in Table <ref type="table" target="#tab_3">2</ref>) as an example, among 1,312 sentences in the CoNLL-2014 dataset, seq2seq inference with shallow fusion LM edits 566 sentences. In contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving F 0.5 from 52.59 to 52.72.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Towards the state-of-the-art for GEC</head><p>Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result.</p><p>We first compare our best models -dual-boost learning (+native) with fluency boost inference and shallow fusion LM -to top-performing GEC systems evaluated on CoNLL-2014 dataset:  denotes the system uses the non-public error-corrected data from Lang-8.com.</p><p>• CAMB14, CAMB16 SMT , CAMB16 NMT and CAMB17: GEC systems <ref type="bibr">(Felice et al., 2014;</ref><ref type="bibr">Yuan et al., 2016;</ref><ref type="bibr">Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b57">Yannakoudakis et al., 2017)</ref> developed by Cambridge University.</p><p>• AMU14 and AMU16: SMT-based GEC systems <ref type="bibr">(Junczys-Dowmunt and</ref><ref type="bibr">Grundkiewicz, 2014, 2016)</ref> developed by AMU.</p><p>• CUUI and VT16: the former system <ref type="bibr" target="#b41">(Rozovskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type="bibr" target="#b44">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC systems <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr" target="#b5">Chollampatt et al., 2016a;</ref><ref type="bibr" target="#b6">Chollampatt and Ng, 2017</ref>) that combine SMT with other techniques (e.g., classifiers).</p><p>• Char-seq2seq: a character-level seq2seq model <ref type="bibr" target="#b56">(Xie et al., 2016)</ref>. It uses a rule-based method to synthesize errors for data augmentation.</p><p>• Nested-seq2seq: a nested attention neural hybrid seq2seq model <ref type="bibr" target="#b24">(Ji et al., 2017)</ref>.</p><p>• Adapt-seq2seq: a seq2seq model adapted to incorporate edit operations <ref type="bibr" target="#b48">(Schmaltz et al., 2017)</ref>.</p><p>Table <ref type="table" target="#tab_7">5</ref> shows the evaluation results on the CoNLL-2014 dataset. Without using the nonpublic training data from Lang-8.com, our sin-gle model obtains 50.04 F 0.5 , larlgely outperforming the other seq2seq models and only inferior to CAMB17 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16's framework); thus, they are ensemble models. When we build our approach on top of AMU16 (i.e., we take AMU16's outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F 0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F 0.5 score respectively, which is a stateof-the-art result 7 on CoNLL-2014 dataset.</p><p>Moreover, we evaluate our approach on JFLEG corpus <ref type="bibr" target="#b35">(Napoles et al., 2017)</ref>. JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach's generalization ability, we evaluate our single models used for CoNLL evaluation (in Table <ref type="table" target="#tab_7">5</ref>) on JFLEG without re-tuning.</p><p>Table <ref type="table" target="#tab_8">6</ref> shows the JFLEG leaderboard. Instead of M 2 score, JFLEG uses GLEU <ref type="bibr" target="#b33">(Napoles et al., 2015)</ref> as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU <ref type="bibr" target="#b38">(Papineni et al., 2002)</ref> and has several advantages over M 2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result 8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Most of advanced GEC systems are classifierbased <ref type="bibr" target="#b4">(Chodorow et al., 2007;</ref><ref type="bibr" target="#b15">De Felice and Pulman, 2008;</ref><ref type="bibr" target="#b21">Han et al., 2010;</ref><ref type="bibr" target="#b28">Leacock et al., 2010;</ref><ref type="bibr" target="#b53">Tetreault et al., 2010a;</ref><ref type="bibr" target="#b14">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5=54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5=56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models. 8 The recently proposed SMT-NMT hybrid system <ref type="bibr" target="#b19">(Grundkiewicz and Junczys-Dowmunt, 2018)</ref>, which is tuned towards GLEU on JFLEG Dev set, reports a higher result (GLEU=61.50 on JFLEG test set). or MT-based <ref type="bibr" target="#b2">(Brockett et al., 2006;</ref><ref type="bibr">Dahlmeier and</ref><ref type="bibr">Ng, 2011, 2012a;</ref><ref type="bibr">Yoshimoto et al., 2013;</ref><ref type="bibr" target="#b60">Yuan and Felice, 2013;</ref><ref type="bibr">Behera and Bhattacharyya, 2013)</ref>. For example, top-performing systems <ref type="bibr">(Felice et al., 2014;</ref><ref type="bibr" target="#b41">Rozovskaya et al., 2014;</ref><ref type="bibr" target="#b25">Junczys-Dowmunt and Grundkiewicz, 2014)</ref> in <ref type="bibr">CoNLL-2014</ref><ref type="bibr">shared task (Ng et al., 2014)</ref> use either of the methods. Recently, many novel approaches <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr">Chollampatt et al., 2016b,a;</ref><ref type="bibr" target="#b44">Rozovskaya and Roth, 2016;</ref><ref type="bibr" target="#b26">Junczys-Dowmunt and Grundkiewicz, 2016;</ref><ref type="bibr" target="#b32">Mizumoto and Matsumoto, 2016;</ref><ref type="bibr">Yuan et al., 2016;</ref><ref type="bibr" target="#b23">Hoang et al., 2016;</ref><ref type="bibr" target="#b57">Yannakoudakis et al., 2017)</ref> have been proposed for GEC. Among them, seq2seq models <ref type="bibr">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b56">Xie et al., 2016;</ref><ref type="bibr" target="#b24">Ji et al., 2017;</ref><ref type="bibr" target="#b47">Sakaguchi et al., 2017;</ref><ref type="bibr" target="#b48">Schmaltz et al., 2017;</ref><ref type="bibr" target="#b7">Chollampatt and Ng, 2018)</ref> have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC <ref type="bibr" target="#b2">(Brockett et al., 2006;</ref><ref type="bibr" target="#b18">Foster and Andersen, 2009;</ref><ref type="bibr">Rozovskaya and</ref><ref type="bibr">Roth, 2010, 2011;</ref><ref type="bibr" target="#b45">Rozovskaya et al., 2012;</ref><ref type="bibr">Felice and Yuan, 2014;</ref><ref type="bibr" target="#b56">Xie et al., 2016;</ref><ref type="bibr" target="#b39">Rei et al., 2017)</ref>. Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence's fluency can be improved.</p><p>To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT <ref type="bibr" target="#b55">(Xia et al., 2017)</ref>.</p><p>In addition to the studies on GEC, there is also much research on grammatical error detection <ref type="bibr" target="#b28">(Leacock et al., 2010;</ref><ref type="bibr" target="#b40">Rei and Yannakoudakis, 2016;</ref><ref type="bibr" target="#b27">Kaneko et al., 2017)</ref> and GEC evaluation <ref type="bibr" target="#b54">(Tetreault et al., 2010b;</ref><ref type="bibr">Madnani et al., 2011;</ref><ref type="bibr" target="#b12">Dahlmeier and Ng, 2012c;</ref><ref type="bibr" target="#b33">Napoles et al., 2015;</ref><ref type="bibr" target="#b46">Sakaguchi et al., 2016;</ref><ref type="bibr" target="#b34">Napoles et al., 2016;</ref><ref type="bibr" target="#b3">Bryant et al., 2017;</ref><ref type="bibr" target="#b0">Asano et al., 2017)</ref>. We do not introduce them in detail because they are not much related to this paper's contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a novel fluency boost learning and inference mechanism to overcome the limitations of previous neural GEC models. Our proposed fluency boost learning fully exploits both errorcorrected data and native data, largely improving the performance over normal seq2seq learning, while fluency boost inference utilizes the characteristic of GEC to incrementally improve a sentence's fluency through multi-round inference. The powerful learning and inference mechanism enables the seq2seq models to achieve state-ofthe-art results on both CoNLL-2014 and JFLEG benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>She see Tom is catched by policeman in park at last night. She saw Tom caught by a policeman in the park last night. She sees Tom is catched by policeman in park at last night. She sees Tom caught by a policeman in the park last night. She sees Tom caught by a policeman in the park last night. She saw Tom caught by a policeman in the park last night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fluency boost learning and inference: (a) given a training instance (i.e., an error-corrected sentence pair), fluency boost learning establishes multiple fluency boost sentence pairs from the seq2seq's n-best outputs during training. The fluency boost sentence pairs will be used as training instances in subsequent training epochs, which helps expand the training set and accordingly benefits model learning; (b) fluency boost inference allows an error correction model to correct a sentence incrementally through multi-round seq2seq inference until its fluency score stops increasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Three fluency boost learning strategies: (a) back-boost, (b) self-boost, (c) dual-boost; all of them generate fluency boost sentence pairs (the pairs in the dashed boxes) to help model learning during training. The numbers in this figure are fluency scores of their corresponding sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) and was already briefly introduced in Section 1 and Figure 2(a). Unlike back-boost learning in which an error generation seq2seq model is trained to generate disfluency candidates, self-boost learning allows the error correction model to generate the candidates by itself. Since the disfluency candidates generated by the error correction seq2seq model trained with error-corrected data rarely change the input Algorithm 2 Self-boost learning 1: for each sentence pair (x r , x c ) ∈ S do 2: D self (x c ) ← ∅; 3: end for 4: S ← ∅ 5: for each training epoch t do 6: Update error correction model Θcrt with S * ∪ S ; 7: S ← ∅ 8: Derive a subset St by randomly sampling |S * | elements from S; 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 3 Dual-boost learning 1: for each (x r , x c ) ∈ S do</figDesc><table><row><cell>2:</cell><cell>D dual (x c ) ← ∅;</cell></row><row><cell cols="2">3: end for</cell></row><row><cell cols="2">4: S ← ∅; S ← ∅;</cell></row><row><cell cols="2">5: for each training epoch t do</cell></row><row><cell>6:</cell><cell>Update error correction model Θcrt with S  *  ∪ S ;</cell></row><row><cell>7:</cell><cell>Update error generation model Θgen with S  *  ∪ S ;</cell></row><row><cell>8:</cell><cell>S ← ∅; S ← ∅;</cell></row><row><cell>9:</cell><cell>Derive a subset St by randomly sampling |S  *  | ele-</cell></row><row><cell></cell><cell>ments from S;</cell></row><row><cell>10:</cell><cell>for each (x r , x c ) ∈ St do</cell></row><row><cell>11: 12:</cell><cell>Update D dual (x c ) according to Eq (7); Establish a fluency boost pair (x , x c ) by ran-</cell></row><row><cell>13:</cell><cell>domly sampling x ∈ D dual (x c ); S ← S ∪ {(x , x c )};</cell></row><row><cell>14:</cell><cell>Establish a reversed fluency boost pair (x c , x )</cell></row><row><cell>15:</cell><cell>by randomly sampling x ∈ D dual (x c ); S ← S ∪ {(x c , x )};</cell></row><row><cell>16:</cell><cell>end for</cell></row><row><cell cols="2">17: end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of seq2seq for GEC with different learning (row) and inference (column) methods on CoNLL-2014 dataset. (+LM) denotes decoding with the RNN language model through shallow fusion. The last 3 systems (with ) use the additional non-public Lang-8 data for training.</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>seq2seq R</cell><cell>F0.5</cell><cell>P</cell><cell>fluency boost R F0.5</cell><cell>P</cell><cell>seq2seq (+LM) R F0.5</cell><cell>fluency boost (+LM) P R F0.5</cell></row><row><cell>normal seq2seq</cell><cell cols="8">61.06 18.49 41.81 61.56 18.85 42.37 61.75 23.30 46.42 61.94 23.70 46.83</cell></row><row><cell>back-boost</cell><cell cols="8">61.66 19.54 43.09 61.43 19.61 43.07 61.47 24.74 47.40 61.24 25.01 47.48</cell></row><row><cell>self-boost</cell><cell cols="8">61.64 19.83 43.35 61.50 19.90 43.36 62.13 24.45 47.49 61.67 24.76 47.51</cell></row><row><cell>dual-boost</cell><cell cols="8">62.03 20.82 44.44 61.64 21.19 44.61 62.22 25.49 48.30 61.64 26.45 48.69</cell></row><row><cell>back-boost (+native)</cell><cell cols="8">63.93 22.03 46.31 63.95 22.12 46.40 62.04 27.43 49.54 61.98 27.70 49.68</cell></row><row><cell>self-boost (+native)</cell><cell cols="8">64.33 22.10 46.54 64.14 22.19 46.54 62.18 27.59 49.71 61.64 28.37 49.93</cell></row><row><cell>dual-boost (+native)</cell><cell cols="8">65.77 21.92 46.98 65.82 22.14 47.19 62.64 27.40 49.83 62.70 27.69 50.04</cell></row><row><cell>back-boost (+native)</cell><cell cols="8">67.37 24.31 49.75 67.25 24.35 49.73 64.61 28.44 51.51 64.46 28.78 51.66</cell></row><row><cell>self-boost (+native)</cell><cell cols="8">66.52 25.13 50.03 66.78 25.33 50.31 63.82 30.15 52.17 63.34 31.63 52.21</cell></row><row><cell>dual-boost (+native)</cell><cell cols="8">66.34 25.39 50.16 66.45 25.51 50.30 64.72 30.06 52.59 64.47 30.48 52.72</cell></row><row><cell cols="6">• Whether is fluency boost learning mechanism</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">helpful for training the error correction model,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and which of the strategies (back-boost, self-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">boost, dual-boost) is the most effective?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The effect of σ on dual-boost learning with normal seq2seq inference. |D dual | is the average size of dual-boost disfluency candidate sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance of systems on CoNLL-2014 dataset. The system with bold fonts are based on seq2seq models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>JFLEG Leaderboard. Ours denote the single dual-boost models in Table5. The systems with bold fonts are based on seq2seq models. * denotes the system is tuned on JFLEG.</figDesc><table><row><cell>System</cell><cell cols="2">JFLEG Dev JFLEG Test GLEU GLEU</cell></row><row><cell>Source</cell><cell>38.21</cell><cell>40.54</cell></row><row><cell>CAMB14</cell><cell>42.81</cell><cell>46.04</cell></row><row><cell>CAMB16SMT</cell><cell>46.10</cell><cell>-</cell></row><row><cell>CAMB16NMT</cell><cell>47.20</cell><cell>52.05</cell></row><row><cell>CAMB17 (CAMB16SMT based)</cell><cell>47.72</cell><cell>-</cell></row><row><cell>CAMB17 (AMU16 based)</cell><cell>43.26</cell><cell>-</cell></row><row><cell>NUS16</cell><cell>46.27</cell><cell>50.13</cell></row><row><cell>NUS17</cell><cell>51.01</cell><cell>56.78</cell></row><row><cell>AMU16  *</cell><cell>49.74</cell><cell>51.46</cell></row><row><cell>Nested-seq2seq</cell><cell>48.93</cell><cell>53.41</cell></row><row><cell>Sakaguchi et al. (2017)  *</cell><cell>49.82</cell><cell>53.98</cell></row><row><cell>Ours</cell><cell>51.35</cell><cell>56.33</cell></row><row><cell>Ours (with non-public Lang-8 data)</cell><cell>52.93</cell><cell>57.74</cell></row><row><cell>Human</cell><cell>55.26</cell><cell>62.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A sentence's fluency score is defined to be inversely proportional to the sentence's cross entropy, as is in Eq (3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">In this paper, we set σ = 1.05 since the corrected sentence in our training data improves its corresponding raw sentence about 5% fluency on average.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://azure.microsoft.com/en-us/services/cognitiveservices/spell-check/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We give more details about disfluency candidates, including error type proportion, in the supplementary notes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">6 p &lt; 0.0005 according to Wilcoxon Signed-Rank Test.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their professional and constructive comments. We also thank Shujie Liu for his insightful discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems</title>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated grammar correction using hierarchical phrase-based statistical machine translation</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">IJC-NLP</title>
				<imprint>
			<date type="published" when="2013">2014. 2013</date>
		</imprint>
	</monogr>
	<note>Neural machine translation by jointly learning to align and translate</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correcting esl errors using phrasal smt techniques</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><surname>Acl. Kyunghyun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bart</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Caglar</forename><surname>Van Merrienboer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dzmitry</forename><surname>Gulcehre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bengio</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2017. 2014</date>
		</imprint>
	</monogr>
	<note>Automatic annotation and evaluation of error types for grammatical error correction</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of grammatical errors involving prepositions</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-SIGSEM workshop on prepositions</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adapting grammatical error correction based on the native language of writers with neural network joint models</title>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc</forename><forename type="middle">Tam</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connecting the dots: Towards human-level grammatical error correction</title>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Use of NLP for Building Educational Applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A multilayer convolutional encoder-decoder neural network for grammatical error correction</title>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08831</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00189</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correcting semantic collocation errors with l1-induced paraphrases</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A beamsearch decoder for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/CoNLL</title>
				<imprint>
			<date type="published" when="2012">2012a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2012">2012b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2012">2012c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on innovative use of NLP for building educational applications</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Helping our own: The hoo 2011 pilot shared task</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Natural Language Generation</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A classifier-based approach to preposition and determiner error correction in l2 english</title>
		<author>
			<persName><forename type="first">Rachele</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felice</forename><surname>Stephen G Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating artificial errors for grammatical error correction</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Student Research Workshop at EACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generrate: generating errors for use in grammatical error detection</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on innovative use of nlp for building educational applications</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Near human-level performance in grammatical error correction with hybrid machine translation</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05945</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>¸aglar Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using an error-annotated learner corpus to develop an esl/efl error correction system</title>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hwa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Young</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting n-best hypotheses to improve an smt approach to grammatical error correction</title>
		<author>
			<persName><forename type="first">Tam</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The amu system in the conll-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06353</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grammatical error detection using error-and grammaticality-specific word embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Sakaizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated grammatical error detection for language learners</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis lectures on human language technologies</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<editor>
			<persName><forename type="first">Nitin</forename><surname>Emnlp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><surname>Madnani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Tetreault</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alla</forename><surname>Chodorow</surname></persName>
		</editor>
		<editor>
			<persName><surname>Rozovskaya</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2015. 2011</date>
		</imprint>
	</monogr>
	<note>Effective approaches to attention-based neural machine translation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<idno>INTER- SPEECH</idno>
		<imprint>
			<date type="published" when="2010-01">Jan Cernock, and Sanjeev Khudanpur. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mining revision log of language learning sns for automated japanese error correction of second language learners</title>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno>IJC- NLP</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative reranking for grammatical error correction with statistical machine translation</title>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">There&apos;s no comparison: Referenceless evaluation metrics in grammatical error correction</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Jfleg: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04066</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><surname>Bryant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The cambridge learner corpus: Error coding and analysis for lexicography and elt</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus Linguistics 2003 conference</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Artificial error generation with machine translation and syntactic patterns</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05236</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional sequence labeling models for error detection in learner writing</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The illinoiscolumbia system in the conll-2014 shared task</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL (Shared Task)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training paradigms for correcting errors in grammar and usage</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The ui system in the hoo 2012 shared task on error correction</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roth</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Building Educational Applications Using NLP</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Grammatical error correction with neural reinforcement learning</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adapting sequence models for sentence correction</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">System combination for grammatical error correction</title>
		<author>
			<persName><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for esl learners using global context</title>
		<author>
			<persName><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2010">2010a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking grammatical error annotation and evaluation with the amazon mechanical turk</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Joel R Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Use of NLP for Building Educational Applications</title>
				<imprint>
			<date type="published" when="2010">2010b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09727</idno>
		<title level="m">Neural language correction with character-based attention</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural sequencelabelling models for grammatical error correction</title>
		<author>
			<persName><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL (Shared Task)</title>
				<editor>
			<persName><forename type="first">Emnlp</forename><forename type="middle">Ippei</forename><surname>Yoshimoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomoya</forename><surname>Kose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kensuke</forename><surname>Mitsuzawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2017. 2013</date>
		</imprint>
	</monogr>
	<note>Naist at 2013 conll grammatical error correction shared task</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Candidate re-ranking for smt-based grammatical error correction</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Use of NLP for Building Educational Applications</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Constrained grammatical error correction using statistical machine translation</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Joint training for neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00353</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
